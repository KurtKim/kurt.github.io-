[{"content":"Abstract 대규모 언어 모델들은 높은 계산 비용 때문에 복제하기 어렵다. 이를 해결하기 위해, Open Pre-trained Transformers (OPT)를 제시한다. 이는 125M에서 175B의 parameter 범위를 가진 사전 학습된 transformer 모델들을 포함하며, 이들은 완전하게 그리고 책임감 있게 관심 있는 연구자들과 공유될 것이다. OPT-175B는 GPT-3와 비교할 수 있으나, 개발하는 데 필요한 탄소 발자국은 1/7밖에 되지 않는다.\nIntroduction 대규모 텍스트 컬렉션에 학습된 거대 언어 모델은 텍스트 생성 및 zero-shot, few-shot 학습 등 놀라운 기능을 보여준다. 그러나 현재로서는 완전한 모델 접근이 풍부한 자원을 가진 몇몇 연구소에만 제한되어 있다. 이 제한된 접근은 대형 언어 모델이 어떻게 그리고 왜 작동하는지 연구하는 능력을 제한하고, 견고성, 편향, 독성 등의 문제를 개선하는 데 있어 진전을 방해하고 있다.\n125M에서 175B parameter 범위의 decoder 기반 사전 학습된 transformer인 Open Pretrained Transformers (OPT)를 소개하고 있다. OPT 모델은 GPT-3 계열 모델의 성능과 크기를 대략 맞추도록 학습되었으며, 최신 데이터 수집 및 효율적 학습 방법을 적용하였다. 이 모델은 대규모 연구를 가능하게 하고, 거대 언어 모델의 영향력을 연구하는 다양한 의견을 수렴하기 위해 개발되었다. risk, harm, bias, toxicity 등의 정의는 연구 커뮤니티 전체가 공동으로 명시해야 하며, 이는 모델들이 연구에 사용 가능할 때만 가능하다.\n125M부터 66B parameter 사이의 모든 모델을 공개하며, 요청에 따라 OPT-175B에 대한 연구 접근 권한을 제공한다. 학계 연구자, 정부 및 학계의 조직, 산업 연구소에 접근 권한이 부여된다. 모델 생성 로그북과 OPT-175B를 992개의 80GB A100 GPU에서 학습시키는 데 사용된 코드베이스인 metaseq도 공개된다. 이를 통해, 우리는 GPT-3의 탄소 발자국의 1/7만큼의 에너지를 사용해 OPT-175B를 개발할 수 있었다. 이는 큰 성과이지만, 이렇게 큰 모델을 만드는 에너지 비용은 중요하며, 이를 계속 복제하면 LLM들의 컴퓨팅 발자국이 계속 증가할 것이다.\n전체 AI 커뮤니티가 책임있는 AI와 LLM 사용에 대한 명확한 지침을 개발하기 위해 협력해야한다고 생각한다. 더 넓은 AI 커뮤니티가 이 모델에 접근하고 재현 가능한 연구를 수행하여 전체 필드를 발전시키는 것이 필요하다. OPT-175B와 작은 규모의 기준선 출시를 통해, 이러한 기술의 윤리적 고려사항에 대한 다양한 의견을 더욱 들을 수 있을 것을 기대한다.\nMethod Models 125M 개에서 175B 개의 parameter를 가진 8개의 transformer 언어 모델 결과를 제시한다. 모델과 hyperparameter는 학습의 안정성을 위해 대부분 Brown et al. (2020)의 연구를 따르며, 배치 크기 조정은 주로 계산 효율성 향상을 위한 것이다.\nTraining Setup 가중치 초기화는 평균 0, 표준 편차 0.006의 정규 분포를 사용하며, Megatron-LM 코드베이스의 설정을 따른다. 출력 layer의 표준 편차는 총 layer 수에 따라 조정되고, 모든 편향 항은 0으로 초기화된다. 모든 모델은 ReLU 활성화 함수를 사용하며, 시퀀스 길이는 2048로 설정하여 학습된다.\nAdamW optimizer를 사용하며, 이때 ($\\beta_1$, $\\beta_2$)는 (0.9, 0.95)로 설정하고, weight decay는 0.1이다. linear learning rate schedule을 따라, OPT-175B에서 첫 2000단계 동안 0에서 maximum learning rate까지 상승하고, 작은 기준선에서는 375M 토큰 동안 상승 후, 300B 토큰 동안 maximum learning rate의 10%로 감소시킨다. 학습 과정 중에 learning rate을 몇 번 변경하였으며, 배치 크기는 모델 크기에 따라 0.5M에서 4M까지 설정하고 학습 과정 동안 일정하게 유지한다.\n전반적으로 0.1의 드롭아웃을 사용하며, 임베딩에는 드롭아웃을 적용하지 않는다. gradient norm은 일반적으로 1.0에서 제한하나, 중간에 몇 번 1.0에서 0.3으로 줄여야 하는 경우가 있었다. 또한, gradient를 계산할 때 오버플로우/언더플로우 위험을 줄이기 위해 gradient across all rank를 사용하였다.\nPre-training Corpus 사전 학습 코퍼스는 RoBERTa, Pile, 그리고 PushShift.io Reddit에서 사용된 데이터셋을 결합한 것을 포함한다. 이 코퍼스는 대부분 영어 텍스트이지만, CommonCrawl을 통해 비영어 데이터도 일부 포함되어 있다.\n모든 데이터셋에서 중복된 문서를 제거하기 위해, Jaccard 유사도가 .95 이상인 문서를 MinhashLSH를 통해 필터링하였다. 특히 Pile 데이터셋에서는 중복 문서가 많이 발견되어, 이를 사용하는 연구자들에게 추가적인 중복 제거 처리를 권장한다.\n모든 코퍼스를 GPT-2 byte level BPE 토크나이저를 사용하여 토큰화한다. 최종 코퍼스는 대략 180B 토큰을 포함하고 있다.\nRoBERTa RoBERTa 코퍼스의 BookCorpus와 Stories 하위 집합을 포함시키고, 2021년 9월 28일까지 크롤링된 뉴스 기사를 포함한 업데이트된 CCNews를 사용하였다. 이 코퍼스는 원래 RoBERTa CCNews와 같은 방식으로 전처리 되었다.\nThe Pile Pile의 일부 하위 집합인 CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO, 그리고 Wikipedia를 포함시켰다. 그러나 gradient norm의 급증을 초래하는 경향이 있어 불안정성을 높이는 Pile의 다른 하위 집합은 제외하였다. 모든 하위 집합은 추가적인 ad-hoc 공백 정규화를 거쳤다.\nPushShift.io Reddit Baumgartner et al. (2020)이 생성하고 Roller et al. (2021)이 이전에 사용한 Pushshift.io 코퍼스의 일부를 포함시켰다. 대화 트리를 언어 모델이 접근 가능한 문서로 변환하기 위해, 우리는 각 스레드에서 가장 긴 댓글 체인을 추출하고 트리의 모든 다른 경로를 제거하였다. 이로 인해 코퍼스는 약 66% 감소했다.\nTraining Efﬁciency 완전히 분할된 데이터 병렬과 Megatron-LM Tensor 병렬성을 활용하여 992개의 80GB A100 GPU에서 OPT-175B를 학습시켰다. 이로써 GPU 당 최대 147 TFLOP/s의 이용률을 달성하였다. 모든 호스트에서 Adam 상태를 분할하여 FP32로 유지하고, 모델 가중치는 FP16으로 유지하였다. 언더플로우를 방지하기 위해 동적 손실 스케일링을 사용하였다.\nTraining Processes Hardware Failures OPT-175B 학습 도중에는 컴퓨팅 클러스터에서 상당한 수의 하드웨어 실패가 발생하였다. 총 2달 동안 하드웨어 실패로 인해 최소 35번의 수동 재시작이 이루어졌으며, 100개 이상의 호스트가 교체되었다. 수동 재시작 시에는 학습이 일시 중단되고, 문제가 있는 노드를 탐지하기 위해 일련의 진단 테스트가 수행되었다. 이후 문제가 있는 노드는 격리되고, 마지막으로 저장된 체크포인트에서 학습이 재개되었다. 교체된 호스트 수와 수동 재시작 횟수의 차이를 고려할 때, 하드웨어 실패로 인한 자동 재시작이 70번 이상 이루어진 것으로 추정된다.\nLoss Divergences 학습 과정에서 loss divergence 문제가 있었다. 손실이 발산할 때, learning rate를 낮추고 이전 체크포인트에서 재시작하면 학습이 계속될 수 있었다. loss divergence, dynamic loss 스칼라가 0으로 떨어지는 현상, 그리고 마지막 layer의 activation $l^2$-norm이 급증하는 것 사이에 상관관계가 있다는 것을 확인하였다. 이를 바탕으로 dynamic loss 스칼라가 \u0026ldquo;healthy\u0026rdquo; 상태에서, 그리고 activation norm이 무제한으로 증가하지 않는 지점에서 재시작하였다. 학습 초기에는 gradient clipping을 1.0에서 0.3으로 낮추는 것이 안정성에 도움이 되었다.\nOther Mid-ﬂight Changes loss divergence을 처리하기 위해 몇 가지 실험적 변경을 시행하였다. 이에는 바닐라 SGD로의 전환, dynamic loss 스칼라의 재설정, 그리고 Megatron의 새 버전으로의 전환 등이 포함되었다. 이러한 변화들은 최적화의 빠른 정체, 일부 발산의 회복, 그리고 activation norm의 압력 감소와 처리량 향상에 도움이 되었다.\nEvaluations Prompting \u0026amp; Few-Shot HellaSwag, StoryCloze, PIQA, ARC Easy와 Challenge, OpenBookQA, WinoGrad, WinoGrande, 그리고 SuperGLUE 등 문헌에서 사용하는 16개의 표준 NLP 작업에서 모델을 평가하였다. GPT-3의 프롬프트와 실험 설정을 따라서 주로 GPT-3와 비교하였고, 가능한 경우에는 다른 LLM의 성능도 포함시켰다.\n성능을 정확도로 보고하며, 평가 지표의 일관성을 위해 MultiRC와 ReCoRD의 F1은 생략하였다. SuperGLUE의 Winograd Schema Challenge 작업에서는 객관식 질문으로 작업을 구성하였고, 이는 성능에 영향을 미친다.\nZero-shot 전반적으로, 평균 성능은 GPT-3의 추세를 따르는 것으로 보인다. 그러나 작업에 따라 성능은 크게 달라질 수 있다. MultiRC와 WIC를 평균에서 의도적으로 제외하였다. 이 데이터셋들은 GPT-3 또는 OPT를 체계적으로 우대하는 것으로 보인다.\n모델의 성능은 10개 작업에서 GPT-3와 비슷했고, 3개 작업에서는 성능이 떨어졌다. 일부 작업에서는 검증 세트 크기가 작아서 모델의 행동이 예측 불가능했다. WIC에서는 OPT 모델이 GPT-3 모델을 능가했으며, MultiRC에서는 GPT-3 결과를 복제하지 못하였다. BoolQ와 WSC에서는 OPT와 GPT 모델이 대부분 클래스 정확도 주변에서 변동했음을 알 수 있다.\nChinchilla와 Gopher는 parameter 크기에 따라 일관된 성능을 보였지만, PaLM은 모든 설정에서 더 우수한 성능을 보였다. 이는 parameter 수를 제어하더라도 마찬가지였다. PaLM의 높은 성능은 주로 사전 학습 데이터의 품질과 다양성 때문이라고 추정된다.\nOne-shot and Few-shot 평균 multi-shot in-context 성능은 OPT 모델이 GPT-3 모델과 비슷하게 수행함을 보여준다. 그러나 작업별로 결과를 분석하면, zero-shot과 같은 10개의 데이터셋에서 두 모델이 비슷한 성능을 보이는 반면, 일부 다른 데이터셋에서는 모델 크기에 따라 성능이 일관되지 않음을 보여준다. 특히, MultiRC에서는 OPT 모델이 GPT3 모델에 비해 성능이 떨어진다. 이러한 결과는 우리의 평가 설정이 Brown et al. (2020)과 다를 수 있음을 시사한다.\nDialogue 대화 모델의 핵심 요소인 LLM에 초점을 맞춰, OPT-175B를 여러 오픈 소스 대화 데이터셋에서 평가하였다. 이는 ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, Blended Skill Talk, 그리고 최근의 Wizard of Internet 데이터셋을 포함한다. 주로 미세 조정된 BlenderBot 1과 Reddit 2.7B 같은 기존 오픈 소스 대화 모델과 비교하였으며, 또한 미세 조정된 R2C2 BlenderBot과도 비교하였다.\nPerplexity와 Unigram F1 (UF1) 겹침을 보고하며, 모든 Perplexities는 GPT-2 토큰화기의 공간에서 정규화된다. 대화 작업에 대해 감독되고 미감독된 모델들을 구분한다. OPT-175B는 최대 32토큰까지의 탐욕적 디코딩을 사용하며, \u0026ldquo;Person 1:\u0026ldquo;과 \u0026ldquo;Person 2:\u0026ldquo;의 대화 라인만을 번갈아 가며 사용한다. 나머지 모델들은 BlenderBot 1의 생성 parameter를 사용한다.\nOPT-175B는 모든 작업에서 unsupervised Reddit 2.7B 모델을 크게 능가하며, ConvAI2 데이터셋에서는 supervised BlenderBot 1 모델과 비슷한 성능을 보인다. 하지만, 모든 모델이 unsupervised Wizard-of-Internet 데이터셋에서는 OPT-175B가 가장 낮은 Perplexity를 보이지만, UF1은 Wizard-ofWikipedia supervised 모델들보다 낮다.\nunsupervised OPT-175B 모델의 평가가 ConvAI2 데이터셋에서 BlenderBot 1과 경쟁력이 있었다. 이는 데이터셋의 유출을 의심케 하지만, 사전 학습 말뭉치에서는 어떤 겹침도 찾지 못했다. OPT-175B는 공개되지 않은 ConvAI2 테스트 세트와 MSC 데이터셋에서도 좋은 성능을 보여주었으며, 이는 모델이 여러 PersonaChat과 유사한 데이터셋에 잘 일반화되고 있음을 보여준다. OPT-175B가 대화를 거치면서 일관된 페르소나를 유지하는 강력한 능력을 가지고 있음이 확인되었다.\nBias \u0026amp; Toxicity Evaluations OPT-175B의 잠재적인 문제를 파악하기 위해, 혐오 발언 탐지, stereotype 인식, toxic 콘텐츠 생성 등과 관련된 벤치마크를 평가하였다. 이 벤치마크들은 단점이 있을 수 있지만, OPT-175B의 한계를 이해하는데 도움을 준다. 주로 GPT-3 Davinci와 비교하였는데, 이 벤치마크들은 Brown et al. (2020)에 포함될 수 있을 때까지 사용되지 않았다.\nHate Speech Detection Mollas et al. (2020)의 ETHOS 데이터셋을 사용해, OPT-175B가 특정 영어 문장이 인종차별적인지, 성차별적인지 판별하는 능력을 측정하였다. zero-shot, one-shot, few-shot 이진 케이스에서는 모델에게 텍스트가 인종차별적이거나 성차별적인지 판단하고 yes/no로 응답하도록 했고, few-shot 다중 클래스 설정에서는 yes/no/neither로 응답하도록 하였다.\nOPT175B는 one-shot에서 few-shot 설정에서 모두 Davinci보다 훨씬 더 좋은 성능을 보였다. 이는 Davinci API를 통한 평가가 추가적인 안전 제어 메커니즘을 도입하고 있거나, 사전 학습 데이터셋에 포함된 통제되지 않은 소셜 미디어 토론이 이러한 분류 작업에 도움을 주는 귀납적 bias를 제공했기 때문으로 추측된다.\nCrowS-Pairs CrowSPairs는 마스크 언어 모델을 위해 개발된 벤치마크로, 9가지 카테고리(gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance, socioeconomic status)의 문장 내 bias를 측정한다. 각 예시는 한 그룹에 대한 stereotype 또는 anti-stereotype을 나타내는 문장 쌍으로, 모델이 stereotype 표현을 선호하는 정도를 측정한다. 높은 점수는 모델이 더 큰 bias를 보이는 것을 의미한다.\nOPT175B는 종교를 제외한 대부분 카테고리에서 더 많은 stereotypical bias을 보였다. 이는 학습 데이터의 차이 때문으로, Reddit 말뭉치가 stereotype과 차별적인 텍스트의 발생률이 더 높다고 나타났다. 이런 데이터가 OPT-175B의 주요 학습 원천이기 때문에, 모델은 더 많은 차별적 연관성을 배웠을 수 있고, 이는 CrowS-Pairs에서의 성능에 직접적인 영향을 미친다.\nStereoSet 직업, 성별, 종교, 인종의 4가지 카테고리에서 stereotypical bias을 측정하기 위해, 우리는 StereoSet을 사용한다. 이 도구는 문장 내 bias 측정뿐만 아니라, 추가적인 맥락을 포함하는 모델의 능력을 테스트하기 위한 문장 간 bias 측정도 포함한다. bias 탐지와 언어 모델링 능력 사이의 잠재적인 교환 관계를 고려하기 위해, StereoSet은 두 가지 지표를 포함한다.\nLanguage Modeling Score(LMS)와 Stereotype Score(SS)를 결합해 Idealized Context Association Test score(ICAT)를 만든다. 문자 수가 아닌 토큰 수로 점수를 정규화하는데, 이 방법이 여러 모델의 측정치를 개선한다고 보고되었다.\nDavinci와 OPT-175B는 전체적으로 비슷한 점수를 보여주었다. Davinci는 직업과 인종 분야에서, OPT-175B는 성별과 종교 분야에서 더 뛰어난 성능을 보였다. OPT175B는 SS 지표에서 전반적으로 더 좋은 성능을 보였고, Davinci는 LMS 지표에서 일반적으로 더 뛰어난 성능을 보였다.\nRealToxicityPrompts RealToxicityPrompts 데이터셋을 이용해 OPT-175B가 toxic 언어로 응답하는 경향을 평가하였다. RTP에서 무작위로 샘플링한 10,000개의 프롬프트 각각에 대해, nucleus 샘플링을 사용하여 생성된 연속성의 평균 toxicity rate을 보고했습니다. 또한, 비교를 위해 Davinci와 PaLM에서의 toxicity rate을 보고하였다.\nOPT-175B는 PaLM이나 Davinci보다 높은 toxicity rate을 보였다. 프롬프트의 toxic이 증가할수록 모든 모델이 toxic을 가진 연속성을 생성할 가능성이 증가하는 것을 확인하였다. 사전 학습 말뭉치에 통제되지 않은 소셜 미디어 텍스트가 포함되어 있다는 점이 toxic 텍스트 생성과 탐지 경향을 높일 수 있다. 이는 downstream 응용 프로그램의 요구에 따라 바람직하지 않을 수도 있으므로, OPT-175B의 미래 응용은 이를 고려해야 한다.\nDialogue Safety Evaluations 대화 안전성 평가 두 가지를 통해 OPT-175B를 비교하였다. SaferDialogues는 명백한 안전 실패에서 회복하는 능력을, Safety Bench Unit Tests는 모델의 응답의 안전성을 측정한다. 이는 주제의 민감성에 따라 4단계로 분류됩니다. 이 결과는 기존 오픈 소스 대화 모델과 비교하였다.\n두 실험 결과에 따르면, OPT-175B는 SaferDialogues와 Unit Tests에서 Reddit 2.7B 모델과 유사한 성능을 보여주었다. 안전하고 적대적인 설정에서 OPT-175B는 약간 더 높은 성능을 보여주었다. 정제된 대화 데이터셋에서 미세 조정된 모델들은 전반적으로 더 낮은 toxic을 가진 것으로 확인되었다. 따라서, OPT-175B를 대화용으로 활용하는 미래의 실험은 안전 프로파일을 향상시키기 위해 정제된 데이터셋에서 미세 조정을 포함해야 한다는 결론을 내렸다.\nLimitations 다양한 규모의 모든 출시된 모델에 대해 폭넓게 평가하였다. GPT-3 모델에서 사용된 표준 평가 데이터셋에 대한 성능은 비슷했으며, 안전성, 편향, 포괄성 등의 평가에서도 대체적으로 비슷한 성능을 보여주었다. 그러나 이러한 평가는 모델의 전체적인 한계를 완전히 반영하지는 못할 수 있다. 특히 OPT-175B는 다른 LLMs에서 지적된 동일한 한계를 보여주었다.\nOPT-175B는 명령형 지시문이나 간결한 질문에 잘 작동하지 않는다는 것을 발견하였다. 지시문의 실행보다는 대화의 시뮬레이션을 생성하는 경향이 있다. 이러한 한계는 InstructGPT와 같은 지시문 학습에 대한 미래의 연구를 통해 완화될 수 있을 것이다.\nOPT-175B는 반복적인 경향이 있고 쉽게 루프에 빠질 수 있다는 것을 발견하였다. 한 번의 생성만 샘플링할 때 샘플링이 반복을 완전히 제거하지 못했다. 미래의 연구에서는 반복을 줄이고 다양성을 향상시키는 전략, 예를 들어 unlikelihood training이나 best-ﬁrst decoding을 통합할 필요가 있다.\nOPT-175B는 다른 LLM과 마찬가지로 사실적으로 부정확한 문장을 생성할 수 있다. 이는 정보의 정확성이 중요한 분야에서 특히 문제가 될 수 있다. 그러나 최근의 연구들은 검색 기반 모델이 LLM의 사실적 정확성을 향상시킬 수 있음을 보여주었다. 따라서, OPT-175B도 미래에 검색 기반 확장의 이점을 누릴 것으로 예상한다.\nOPT-175B는 무해한 프롬프트를 제공받았을 때도 toxic한 언어를 생성하고 해로운 stereotype을 강화하는 경향이 높다고 확인되었다. 또한 적대적인 프롬프트는 쉽게 찾을 수 있었다. toxic과 bias에 대한 대응책에 대한 많은 연구가 있으며, OPT-175B의 미래 사용은 이러한 접근법을 적용해야 할 수 있다. 그러나 이번 첫 릴리즈에서는 GPT-3의 복제를 주요 목표로 두었기 때문에, 이러한 완화책을 적용하지 않았다.\n이 기술이 상업적 배포에는 아직 준비되지 않았다고 생각한다. 더 많은 신중함이 필요하며, 이상적으로는 재현성과 복제성을 보장하기 위해 더 간결하고 일관된 평가 설정을 가지고 있어야 한다. 프롬프트 스타일과 문맥 학습에 대한 차이점은 다른 결과를 이끌어낼 수 있다. OPT 모델의 공개 릴리즈는 이러한 중요한 문제에 대한 연구를 촉진할 것으로 기대한다.\nConsiderations for Release AI 파트너십과 NIST의 지침에 따라, OPT-175B 학습 과정의 모든 세부사항을 공개하고, 연구자들이 모델 가중치에 접근하고 작은 기준선 세트를 사용할 수 있게 한다. OPT-175B의 개발 생명주기에 대한 완전한 책임을 지며, LLM 개발에 대한 투명성을 높여 LLM의 한계와 위험을 이해하는 데 중점을 두고 있다.\n일상적인 학습 과정의 세부사항을 공유함으로써 OPT-175B 학습에 사용된 컴퓨팅 리소스와 대규모에서의 불안정성을 관리하는 데 필요한 인력을 공개한다. 이런 세부사항은 대게 이전 연구에서 생략되었지만, ad-hoc 디자인 결정 과정을 공개함으로써 미래의 모델 개발에서 이러한 방식을 개선하고 실험적 강인성을 높이는 데 기여하길 희망한다.\n개발 코드베이스를 공개함으로써, 논문에서 명시적으로 언급되지 않은 구현 세부 사항에 대한 명확성을 제공하려고 한다. 현재의 코드베이스는 파이프라인 병렬성을 사용하지 않고 175B 이상의 parameter를 가진 decoderonly transformer를 NVIDIA GPU에서 학습시키는 유일한 오픈 소스 구현이다.\n175B 규모의 실험을 가능하게 하기 위해, 연구자들에게 OPT-175B의 parameter에 직접 접근할 수 있게 했다. 이는 LLM에 대한 책임 있는 AI 연구를 촉진하고, 이 규모의 연구가 환경에 미치는 영향을 줄이기 위한 것이다. 대규모 언어 모델 배포의 윤리적, 사회적 위험을 다루는 연구가 증가하고 있다. 비상업적 라이센스를 가진 연구 커뮤니티만 OPT-175B에 접근하게 하여, 상업적 배포 전에 먼저 LLM의 한계를 파악하는 데 초점을 맞추고자 한다.\n이 규모의 모델을 재현하는데는 상당한 컴퓨팅 및 탄소 비용이 발생한다. OPT-175B는 추정 75톤의 탄소 배출량으로 개발되었으며, 다른 모델들은 더 많은 양을 사용하였다. 이러한 추정치는 표준화되지 않았고, AI 시스템의 전체 탄소 발자국은 모델 학습뿐만 아니라 실험과 추론 비용도 포함한다. 로그북을 공개하여 이론적 탄소 비용 추정치와 전체 개발 수명주기를 고려한 추정치 사이의 차이를 강조하고자 한다. 또한, 점점 복잡해지는 이 시스템들의 제조 탄소를 이해하고, 환경에 대한 규모의 영향을 측정할 때 고려해야 할 추가 요인을 정의하는 데 이 논문이 도움이 될 수 있기를 희망한다.\n다양한 스케일에서 기준선을 설정함으로써, 연구 커뮤니티가 이 모델들의 영향력과 한계를 스케일만으로 연구할 수 있도록 돕고자 한다. 일부 LLM은 사용된 학습 데이터 양에 비해 학습이 부족했을 수 있으며, 이는 더 많은 데이터를 추가하고 계속 학습하면 성능이 향상될 수 있음을 의미한다. 또한, 175B보다 훨씬 작은 규모에서 기능 변화가 발생할 수 있다는 증거가 있으므로, 다양한 연구 활용을 위해 더 넓은 스케일 범위를 검토해야 한다.\nRelated Work transformer 아키텍처와 BERT의 출시 이후, NLP 분야는 self-supervised 학습을 통한 LLM 사용으로 크게 변화하였다. T5와 MegatronLM 같은 여러 가면 언어 모델들은 규모를 통해 지속적으로 성능을 향상시켰다. 이는 모델의 parameter 수 증가뿐만 아니라 사전 학습 데이터의 양과 품질 향상으로 이루어졌다.\nauto-regressive 언어 모델은 모델 크기가 크게 증가하였고, 이로 인해 생성 유창성과 품질이 대폭 향상되었다. 많은 큰 모델들이 학습되었지만, 이들은 대부분 비공개 소스로, 내부적으로 또는 유료 API를 통해만 접근 가능하다. 그러나 비영리 연구 조직에서는 LLM을 오픈 소스화하는 노력이 있으며, 이러한 모델들은 OPT 모델과 다르기 때문에, 커뮤니티가 다양한 사전 학습 전략을 비교할 수 있다.\nLLM의 주요 평가 기준은 프롬프트 기반이며 이는 특정 작업에 대한 미세 조정 없이도 많은 작업을 평가하는 편리함 때문이다. 프롬프트는 오래된 역사를 가지고 있고, 최근에는 모델에 대한 지식 탐색 또는 다양한 NLP 작업 수행에 사용되었다. 또한, 작은 모델에서 프롬프트 동작을 유도하거나, 프롬프트의 유연성을 개선하고, 프롬프트가 어떻게 작동하는지 이해하는 연구도 있다.\n모델을 지시 스타일의 프롬프트에 대응하게 미세조정하는 것이 이익을 보였지만, 효과적인 프롬프트 엔지니어링은 여전히 해결되지 않은 연구 과제이다. 프롬프트 선택에 따라 결과는 크게 달라지며, 모델은 프롬프트를 우리가 기대하는 만큼 완전히 이해하지 못하는 것으로 보인다. 또한, 개발 세트 없이 프롬프트를 작성하는 것은 어려움이 있다. 이러한 문제를 해결하려 하지 않고, 단지 OPT-175B의 평가만을 목표로 하며, OPT-175B의 전체 릴리스가 미래의 연구를 돕길 바란다.\nConclusion 125M에서 175B parameter까지 다양한 크기의 auto-regressive 언어 모델 모음인 OPT를 소개하다. 이 연구의 목표는 GPT-3 클래스의 모델을 복제하고 최신 데이터 큐레이션 및 학습 효율성 모범 사례를 적용하는 것이다. 모델의 여러 제한 사항과 책임감 있는 공개에 대한 고려 사항을 논의하였다. 우리는 AI 커뮤니티가 책임감 있는 LLM 가이드라인 개발에 협력하고, 이러한 유형의 모델에 대한 넓은 접근이 기술의 윤리적 고려 사항을 정의하는 다양한 목소리를 늘리길 희망한다.\nReference Paper Github ","date":"2024-01-29T00:00:00Z","permalink":"https://kurtkim.github.io/p/opt/","title":"OPT"},{"content":"Abstract few-shot learning 예제를 사용하는 대형 언어 모델은 다양한 자연어 작업에서 뛰어난 성능을 보여준다. 이를 더 깊이 이해하기 위해, 540B parameter의 densely activated transformer 언어 모델인 Pathways Language Model(PaLM)을 학습시켰다.\n새로운 ML 시스템인 Pathways를 사용해 PaLM을 학습시키고, 수백 개의 언어 이해 및 생성 벤치마크에서 state-of-the-art의 few-shot learning 결과를 달성하였다. PaLM 540B는 다단계 추론 작업과 BIG-bench 벤치마크에서 인간 평균 성능을 능가하는 성과를 보여주었다. 모델 규모가 커짐에 따라 성능이 급격히 향상된 작업도 있었다. 또한 PaLM은 다국어 작업과 소스 코드 생성에서도 강력한 능력을 가지고 있다. bias와 toxicity 대한 분석과 함께, 거대 언어 모델과 관련된 윤리적 고려 사항에 대해 논의하였다.\nIntroduction 언어 이해와 생성을 위한 대규모 신경망들은 다양한 작업에서 놀라운 결과를 보여주고있다. BERT나 T5 같은 모델들은 대량의 텍스트를 통해 사전 학습되고, 특정 작업에 맞게 미세 조정된다. 이들 모델은 다양한 자연어 작업에서 state-of-the-art를 보여주지만, 모델을 미세 조정하는 데 많은 수의 작업 특정 학습 예제가 필요하고, 일부 모델 parameter를 작업에 맞게 업데이트하는 복잡성이 증가한다는 단점이 있다.\nGPT-3는 극도로 큰 autoregressive 언어 모델이 소수의 예측을 위해 사용될 수 있음을 보여주었다. 이 모델은 자연어 작업 설명과 작업 완료 방법을 보여주는 몇 가지 예시만 제공받아 학습된다. 대규모 작업 특정 데이터 수집이나 모델 parameter 업데이트 없이도 매우 강력한 결과를 달성하였다.\nGPT-3 이후에도 GLaM, Gopher, Chinchilla, Megatron–Turing NLG, LaMDA와 같은 강력한 대규모 autoregressive 언어 모델들이 개발되어 state-of-the-art를 계속 밀어내고 있다. 이들 모델은 모두 transformer 아키텍처의 변형이며, 모델의 크기 확대, 학습된 토큰 수 증가, 더 깨끗한 데이터셋 사용, 희소 활성화 모듈을 통한 계산 비용 없는 모델 용량 증가 등의 방법으로 개선되었다.\n이 연구에서는 780B 개의 고품질 텍스트 토큰에 대해 540B 개의 parameter를 가진 densely activated autoregressive transformer를 학습시키는 언어 모델링 개선을 계속하였다. 이는 새로운 ML 시스템인 Pathways를 사용하여 수천 개의 accelerator chip에서 매우 큰 신경망을 효율적으로 학습시키는 데 성공하였다. 이 새로운 모델인 PaLM은 수백 개의 자연어, 코드, 수학적 추론 작업에서 breakthrough performance를 달성하였다.\n이 연구에서 주요 결론은 다음과 같다:\nEﬃcient scaling 이 연구에서는 새로운 ML 시스템인 Pathways를 대규모로 처음 사용하였다. 이를 통해, 6144개의 TPU v4 칩에서 540B parameter 언어 모델을 이전에는 도달할 수 없었던 효율 수준에서 학습시켰다. 이전의 대부분의 대규모 언어 모델들은 단일 TPU 시스템에서 학습되거나 GPU 클러스터 또는 여러 TPU v3 pods에 걸쳐 확장되었다. 두 개의 TPU v4 Pods에 걸쳐 6144개의 칩으로 PaLM 540B의 학습을 확장하면서 매우 높은 효율성을 달성하였다. Continued improvements from scaling 자연어, 코드, 수학적 추론 작업 등 수백 가지 작업에 대해 PaLM을 평가하고, 대부분의 벤치마크에서 상당한 차이로 state-of-the-art를 달성하였다. 이는 대규모 언어 모델로부터의 scaling 개선이 아직도 정체되지 않았음을 보여준다. 가장 널리 평가된 29개의 영어 언어 이해 벤치마크 중 28개에서 최고 작업별 결과에 비해 state-of-the-art를 보여주었다. Breakthrough capabilities 이 연구에서는 다양한 어려운 작업에 대해 언어 이해와 생성에서 breakthrough capabilities를 보여준다. 특히, multi-step 수학적 또는 상식적 추론이 필요한 일련의 추론 작업에 대해 평가하였다. 모델 scaling과 사슬 형태의 생각 유도를 결합하면, 간단한 소수 평가가 넓은 범위의 추론 작업에서 state-of-the-art를 능가하거나 매치할 수 있음을 보여주었다. 또한, 최근 출시된 150개 이상의 새로운 언어 이해와 생성 작업을 포함하는 BIG-bench에서 breakthrough performance을 보여주었다. PaLM이 복잡한 추론 체인을 명확하게 해석하고 설명하는 능력을 탐색하였다. Discontinuous improvements 8B, 62B, 540B의 세 가지 다른 parameter 규모에서의 결과를 제시하여 scaling 행동을 이해한다. 일반적으로, 62B에서 540B로의 scaling은 8B에서 62B로의 scaling과 유사한 성능을 가져온다. 그러나 특정 작업에 대해서는, 62B에서 540B로의 scaling이 정확도에서 drastic jump를 가져오는 것을 관찰하였다. 이는 대규모 언어 모델의 새로운 능력이 충분한 규모를 달성하면 나타날 수 있음을 제안한다. Multilingual understanding 이 연구에서는 다양한 언어에서의 기계 번역, 요약, 그리고 질문 응답을 포함한 다국어 벤치마크에 대한 철저한 평가를 수행하였다. 비영어 데이터의 비율이 상대적으로 작음에도 불구하고, PaLM 모델은 비영어 요약 작업에서 이전에 미세 조정된 state-of-the-art와의 격차를 메우며, 번역 작업에서 이전의 state-of-the-art를 능가하였다. 다국어 데이터 비율 증가의 영향을 이해하기 위해 추가적인 연구가 필요하다. Bias and toxicity distributional bias와 toxicity에 대한 모델 성능을 평가하였다. 성별과 직업에 대한 bias에서, 모델 규모가 커짐에 따라 성능이 개선되었다. 인종/종교/성별 프롬프트 연속성에서는 모델이 스테레오타입을 거짓으로 확증하는 가능성을 보여주었다. toxicity 분석에서는 62B와 540B 모델이 8B 모델에 비해 약간 더 높은 toxicity 수준을 보여주었다. 모델이 생성한 연속성의 toxicity은 프롬프트 텍스트의 toxicity과 높게 상관되었다. 향후 연구에서는 이러한 벤치마크를 비영어 언어로 확장하고 잠재적 위험을 더 철저히 고려할 계획이다. Model Architecture PaLM은 다음과 같은 수정을 가진 표준 Transformer 모델 아키텍처의 decoder-only setup으로 사용한다:\nSwiGLU Activation MLP 중intermediate activation에 SwiGLU activation을 사용한다. 이는 표준 ReLU, GeLU, Swish activation에 비해 품질을 크게 향상시키기 때문이다. 이는 MLP에서 세 개의 행렬 곱셈이 필요하다는 것을 의미하지만, 이는 품질 개선을 보여준다.\nParallel Layers 각 Transformer block에서 표준 \u0026ldquo;serialized\u0026rdquo; 형식 대신 \u0026ldquo;parallel\u0026rdquo; 형식을 사용한다. 특히, 표준 serialized 형식은 다음과 같이 작성할 수 있다:\n$$ y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x)))) $$\n반면에, parallel 형식은 다음과 같이 작성할 수 있다:\n$$ y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x)) $$\nparallel 형식은 MLP와 Attention 입력 행렬 곱셈이 융합될 수 있어 대규모 규모에서 학습 속도를 약 15% 더 빠르게 한다. 실험에서는 8B 규모에서는 약간의 품질 저하가 있었지만, 62B 규모에서는 품질 저하가 없었으므로, 540B 규모에서 parallel layer의 효과는 품질에 영향을 주지 않을 것으로 추정하였다.\nMulti-Query Attention 표준 Transformer 형식은 $k$개의 attention head를 사용하며, 각 타임스텝의 입력 벡터는 \u0026ldquo;query\u0026rdquo;, \u0026ldquo;key\u0026rdquo;, \u0026ldquo;value\u0026rdquo; 텐서로 선형적으로 투영된다. 이 방식은 모델 품질과 학습 속도에 중립적인 효과를 가지지만, decoding 시간에 비용 절약을 가져온다. 이는 standard multi-headed attention이 auto-regressive decoding 시에 accelerator 하드웨어에서 낮은 효율성을 보이기 때문이다.\nRoPE Embeddings 긴 시퀀스 길이에서 더 나은 성능을 보이는 RoPE 임베딩을 사용한다. 이는 절대적 또는 상대적 포지션 임베딩 대신에 사용되었다.\nShared Input-Output Embeddings 입력과 출력 임베딩 행렬을 공유한다.\nNo Biases 어떤 dense kernel이나 layer norm에서도 bias를 사용하지 않았다. 이는 큰 모델의 학습 안정성을 증가시키는 것으로 나타났다.\nVocabulary 256k 토큰의 SentencePiece 어휘를 사용하여 학습 말뭉치의 많은 언어를 지원한다. 이 어휘는 학습 데이터에서 생성되었으며, 학습 효율성을 향상시킨다. 어휘는 완전히 손실 없이 되돌릴 수 있으며, 공백을 완전히 보존하고, 어휘 외의 유니코드 문자를 UTF-8 바이트로 분할한다. 숫자는 항상 개별 숫자 토큰으로 분할된다.\nModel Scale Hyperparameters 이 연구에서는 540B, 62B, 8B parameter의 세 가지 다른 모델 규모를 비교한다. 이 모델들은 standard dense transformer이므로, 토큰 당 FLOP 수는 parameter 수와 대략적으로 동일하다. 이 모델들은 같은 데이터와 어휘를 사용하여 동일하게 학습되었다.\nTraining Dataset PaLM 사전 학습 데이터셋은 다양한 자연어 사용 사례를 대표하는 7800억 토큰의 말뭉치로 구성되어 있다. 이 데이터셋은 웹페이지, 책, 위키백과, 뉴스 기사, 소스 코드, 소셜 미디어 대화를 섞어 만들었다. 모든 모델을 데이터의 1 epoch 학습시키고, 데이터를 반복하지 않도록 혼합 비율을 선택하였다.\n사전 학습 데이터셋은 자연어 데이터뿐만 아니라 코드도 포함한다. 이 코드는 GitHub의 오픈 소스 저장소에서 얻은 것이며, 라이선스에 따라 필터링하였다. 또한 파일 이름 확장자에 따라 24개의 일반적인 프로그래밍 언어 중 하나로 제한하였고, 중복 파일을 제거하였다. 이 결과, 196GB의 소스 코드가 생성되었다.\n최종 PaLM 데이터셋 혼합물을 생성하는 데 사용된 다양한 데이터 소스의 비율을 보여주며, 데이터 오염을 확인하고, 학습 데이터셋과 평가 데이터 사이의 중복을 분석한다.\nTraining Infrastructure 학습 및 평가 코드베이스는 JAX와 T5X를 기반으로 하며, 모든 모델은 TPU v4 Pods에서 학습된다. PaLM 540B는 데이터 센터 네트워크를 통해 연결된 두 개의 TPU v4 Pods에서 학습되며, 이는 모델과 데이터 병렬성의 조합을 사용한다. 이 시스템은 파이프라인 병렬성 없이 학습을 6144개의 칩으로 효율적으로 확장할 수 있게 해준다.\n이전의 비슷한 규모에서 모델 학습은 두 가지 접근법을 사용했다. LaMDA와 GLaM은 파이프라인 병렬성이나 DCN을 활용하지 않고 단일 TPU 시스템에서 학습되었고, Megatron-Turing NLG 530B는 여러 가지 병렬성을 사용하여 A100 GPU에서, Gopher는 파이프라이닝을 사용하여 DCN-연결된 TPU v3 Pods에서 학습되었다.\n파이프라이닝은 일반적으로 DCN과 함께 사용되며, 추가적인 병렬화를 제공한다. 그러나 이는 학습 배치를 \u0026ldquo;micro-batches\u0026quot;로 분할하지만, 중요한 단점이 있다. 첫째, 많은 장치가 유휴(idle) 상태인 동안 발생하는 시간 오버헤드가 있다. 둘째, 미니 배치 내의 각 마이크로 배치에 대해 메모리에서 가중치를 다시 로드해야 하므로 높은 메모리 대역폭이 필요하다. 이러한 문제를 해결하기 위한 전략을 통해 PaLM 540B의 학습을 6144 칩으로 효율적으로 확장할 수 있었다.\n각 TPU v4 Pod는 모델 parameter의 전체 복사본을 포함하며, 각 가중치 텐서는 모델 병렬성과 완전분할 데이터 병렬성을 사용하여 칩으로 분할된다. forward pass에서 가중치가 모두 모아지고, 각 layer에서 activation 텐서가 저장된다. backward pass에서는 나머지 activation이 rematerialized되며, 이는 더 큰 배치 크기에서 더 높은 학습 처리량을 결과로 내기 때문이다.\nPathways 시스템을 사용하여 단일 TPU v4 Pod를 넘어서 학습을 확장한다. PaLM 540B는 Pathways의 클라이언트-서버 아키텍처를 사용하여 pod 레벨에서 데이터 병렬성을 달성한다. Python 클라이언트는 배치의 절반을 각 pod에 할당하고, 각 pod는 gradient를 계산하기 위해 병렬로 계산을 수행한다. 그 후, pod들은 gradient를 원격 pod에 전송하고, 각 pod는 gradient를 누적하고 parameter를 업데이트하여 다음 타임스텝에 대한 parameter를 얻는다.\nPython 클라이언트는 분할된 데이터플로우 프로그램을 구성하고, 이 프로그램은 각 pod에서 계산과 최적화 업데이트를 수행하고, gradient를 다른 pod로 전송한다. Pathways 시스템의 디자인은 프로그램 실행을 수천 개의 accelerator 칩으로 확장할 수 있게 한다. 이는 원격 서버로 작업을 발송하는 데 걸리는 지연 시간을 감추고, 데이터 전송의 관리 비용을 분산시킨다.\ntwo-way pod-level 데이터 병렬성의 도전적인 측면은 cross-pod gradient 전송에 대한 높은 학습 처리량을 달성하는 것이다. 이는 데이터 센터 네트워크를 통해 모든 호스트가 gradient를 동시에 전송하는 매우 폭발적인 작업량을 초래한다. 이로 인한 도전을 극복하기 위해, 데이터를 작은 청크로 분해하고 다양한 DCN 링크를 통해 라우팅하는 등의 최적화를 수행한다. 이러한 최적화를 통해, 학습 중 단일 pod에 비해 약 1.95배의 처리량을 달성하였다. 이론적인 2배 처리량에 비한 성능 차이는 backward pass와 cross-pod gradient 축소 사이의 중첩이 부족하기 때문에 발생하며, 이 문제는 향후 작업에서 해결할 예정이다.\nTraining Eﬃciency 언어 모델의 accelerator 효율성은 대게 hardware FLOPs utilization(HFU)로 측정된다. 이는 주어진 장치에서 관찰된 FLOPs와 이론적인 최대 FLOPs 사이의 비율을 나타낸다. 하지만 이 방법에는 문제가 있다. 첫째, 실행된 하드웨어 FLOPs의 수는 시스템과 구현에 따라 달라진다. 둘째, 하드웨어 FLOPs 측정은 그것들을 세거나 추적하는 방법에 의존적이다. 결국, 학습 시스템의 목표는 가능한 많은 하드웨어 FLOPs를 사용하는 것이 아니라 초당 토큰의 높은 처리량을 달성하는 것이다.\nHFU는 LLM 학습 효율성에 대한 일관된 척도가 아니라는 문제점을 인식하였다. 따라서, model FLOPs utilization(MFU)이라는 새로운 효율성 척도를 제안한다. MFU는 관찰된 처리량이 피크 FLOPs에서 운영하는 시스템의 이론적 최대 처리량에 대한 비율이다. 이 척도는 다양한 시스템에서의 학습을 공정하게 비교할 수 있게 해준다.\nPaLM 540B 모델의 model FLOPs utilization(MFU)을 제시하고, 이전의 큰 모델들과 비교하였다. MFU는 다양한 모델 parameter 수, 아키텍처, 모델 품질의 맥락에서 모델과 시스템을 비교하는데 유용하다.\nGPT-3의 MFU는 21.3%, Gopher는 32.5%, Megatron–Turing NLG 530B는 self-attention 없이 29.7%, 있으면 30.2%이다. 반면, PaLM 540B는 self-attention 없이 45.7%, 있으면 46.2%의 MFU를 달성하였다.\nPaLM은 병렬성 전략과 XLA TPU 컴파일러 최적화, 그리고 \u0026ldquo;parallel layers\u0026quot;의 사용 등으로 인해 높은 accelerator 이용률을 달성하였다. 이로써 PaLM은 LLM 학습 효율성에서 중요한 진전을 나타내는 것으로 보여진다.\nTraining Setup 모델 학습은 large transformer 언어 모델에 대한 상당히 표준적인 설정을 따랐다:\nWeight initialization 커널 가중치는 \u0026ldquo;fan-in variance scaling\u0026quot;을 사용하여 초기화하며, 입력 임베딩은 layer normalization가 적용되지 않기 때문에 $E ∼ N(0, 1)$으로 초기화된다. 입력과 출력 임베딩 레이어가 공유되므로, pre-softmax 출력 logit은 임베딩 크기의 제곱근의 역수로 스케일링된다.\nOptimizer 이 모델은 Adafactor optimizer를 사용하여 학습되었으며, 이는 parameter 행렬의 평균 제곱근으로 learning rate을 조정하는 Adam과 사실상 동일하다. 가중치 초기화가 ${{1}\\over{\\sqrt{n}}}$에 비례하기 때문에, 이는 learning rate를 수동으로 축소하는 것과 비슷한 효과를 가진다. 하지만, 다른 스케일에서 작동하는 parameter 행렬들이 동일한 비율로 learning rate을 축소하지 않게 하는 이점이 있다.\nOptimization hyperparameters 처음 10,000 단계에는 $10^{-2}$의 Adafactor learning rate을 사용하고, 이후에는 단계 번호에 따라 learning rate을 감소시킨다. 모멘텀은 $\\beta_1 = 0.9$로 설정하고, 두 번째 순서 모멘트 보간 값은 $\\beta_2 = 1.0 - k^{-0.8}$로 계산된다. 이 방법은 희귀 임베딩 토큰의 두 번째 순간을 더 정확하게 추정할 수 있어 안정적이다. 그리고, 모든 모델에서 1.0의 global norm gradient clipping을 사용하며, 학습 중에는 현재 learning rate의 2배에 해당하는 dynamic weight decay를 사용한다.\nLoss function 이 모델은 표준 언어 모델링 손실 함수, 즉 모든 토큰의 average log probability를 사용하여 학습된다. 또한, softmax normalizer 값인 $log(Z)$가 0에 가깝게 만드는 auxiliary loss인 $z$ 손실을 사용하며, 이는 학습 안정성을 높이는 데 도움이 된다.\nSequence length 모든 모델은 2048 토큰의 시퀀스 길이로 작동하며, 입력 예제들은 이 길이에 맞춰 연결되고 분할된다. 각 예제는 특별한 [eod] 토큰으로 구분되며 패딩 토큰은 사용되지 않는다.\nBatch size 학습 도중 모든 모델의 배치 크기를 점진적으로 증가시킨다. 큰 모델의 경우, 초기에는 배치 크기를 512로 설정하고, 학습이 진행됨에 따라 이를 2048까지 늘린다. 이런 방식은 학습 초기에는 작은 배치 크기가, 후반에는 큰 배치 크기가 더 효율적이기 때문이며, 또한 큰 배치 크기는 TPU 효율성을 높이는데 도움이 된다.\nBitwise determinism 이 모델은 체크포인트에서 완전 재현이 가능하며, 이는 JAX+XLA+T5X가 제공하는 비트 단위 결정적 모델링 프레임워크와, 단계 번호만으로 학습 배치의 내용을 결정하는 결정적 데이터셋 파이프라인 덕분이다. 따라서 모델이 한 번의 실행에서 특정 단계까지 학습되었다면, 그 체크포인트에서 다시 시작해도 동일한 결과를 보장한다.\nDropout 이 모델은 드롭아웃 없이 학습되었지만, 대부분의 경우에는 0.1의 드롭아웃을 사용하여 미세조정 한다.\nTraining Instability 가장 큰 모델을 학습하면서 gradient clipping이 적용되어 있음에도 불구하고, 불규칙한 간격으로 20번가량 손실이 급증하는 현상을 관찰하였다. 이는 작은 모델에서는 발견되지 않았으며, 큰 모델의 학습 비용 때문에 이 문제를 완화하기 위한 명확한 전략을 세우지 못하였다.\n손실 증가 문제를 완화하기 위해, 손실 증가가 시작되기 전 체크포인트에서 학습을 재시작하고, 손실 증가가 관찰된 데이터 배치를 건너뛰는 전략을 사용했다. 이 방법은 손실 증가가 특정 데이터 배치와 모델 파라미터의 특정 상태의 조합으로 발생한다는 것을 보여주며, \u0026ldquo;bad data\u0026rdquo; 때문이 아님을 확인하였다.\nEvaluation English NLP tasks PaLM 모델은 이전 연구와 동일한 29개의 영어 벤치마크를 이용하여 평가한다:\nOpen-Domain Closed-Book Question Answering tasks: TriviaQA, Natural Questions, Web Questions Cloze and Completion tasks: LAMBADA, HellaSwag, StoryCloze Winograd-style tasks: Winograd, WinoGrande Common Sense Reasoning: PIQA, ARC, OpenBookQA In-context Reading Comprehension: DROP, CoQA, QuAC, SQuADv2, RACE SuperGLUE Natural Language Inference (NLI): Adversarial NLI PaLM 540B는 대부분의 작업에서 이전 state-of-the-art를 능가하였다. 특히, 읽기 이해와 NLI 작업에서 더욱 두드러졌다. 이는 모델 크기 뿐 아니라, 사전 학습 데이터셋, 학습 전략, 학습 중 관찰된 토큰 수 등이 중요하게 작용했음을 보여준다.\nPaLM 540B가 자연어 이해와 자연어 생성 작업에서 평균 점수를 5점 이상 향상시켰다. 특히, PaLM 62B는 GPT-3 175B를 두 카테고리에서 모두 능가하였다.\nMassive Multitask Language Understanding PaLM 모델은 다양한 주제를 다루는 대규모 다중작업 언어 이해 벤치마크에서 평가되었고, 평균 점수를 약 2점 향상시켰다. PaLM 540B는 \u0026ldquo;Other\u0026quot;을 제외한 모든 카테고리에서 Chinchilla 모델을 능가하였다.\nFinetuning SuperGLUE 벤치마크에서 PaLM 모델을 미세조정하는 실험을 진행했고, 일반적으로 15K 단계 이내에 수렴했다. 이 과정에서는 Adafactor optimizer를 사용하고 batch size는 32였다.\nPaLM이 가장 뛰어난 성능을 보여주는 모델과 경쟁력 있게 성능을 내는 것을 보여준다.\n또한 few-shot과 미세조정 결과 사이에 여전히 큰 차이가 있다는 것을 보여준다.\nPaLM이 최첨단 모델과 경쟁력 있으며, 리더보드에서 가장 뛰어난 성능을 내는 decoder-only autoregressive 언어 모델을 크게 능가하는 것을 보여준다.\nBIG-bench BIG-bench는 대규모 언어 모델에 대한 도전적인 작업을 목표로 하는 벤치마크로, 다양한 언어 모델링 작업을 포함한다. 이 벤치마크에서 PaLM 모델 계열은 few-shot 평가를 수행하였고, 텍스트 작업에 초점을 두었다. 인간의 성능도 같은 지표로 측정되었으며, 이를 통해 \u0026ldquo;best\u0026quot;와 \u0026ldquo;average\u0026rdquo; 인간 성능이 계산되었다.\nPaLM 모델 계열은 BIG-bench에서 상당한 성능을 나타냈으며, GPT-3, Gopher, Chinchilla를 크게 능가하였다. 특히, 5-shot PaLM 540B는 동일한 작업을 수행한 인간의 평균 점수보다 높은 점수를 얻었다. 또한, 규모에 따른 PaLM 모델의 성능은 log-linear 행동을 보였으며, 이는 추가적인 스케일링이 성능 향상을 가져올 가능성을 보여준다.\nBIG-bench에서 PaLM이 특별히 눈에 띄는 성능을 보인 몇 가지 작업을 강조한다.\ngoal step wikihow 목표는 이벤트 간의 목표-단계 관계에 대해 추론하는 것이다. Input: ”clean silver,” which step should be done ﬁrst? (a) dry the silver (b) handwash the silver. Answer: (b) handwash the silver.\nlogical args 목표는 문단에서 올바른 논리적 추론을 예측하는 것이다. Input: Students told the substitute teacher they were learning trigonometry. The substitute told them that instead of teaching them useless facts about triangles, he would instead teach them how to work with probabilities. What is he implying? (a) He believes that mathematics does not need to be useful to be interesting. (b) He thinks understanding probabilities is more useful than trigonometry. (c) He believes that probability theory is a useless subject. Answer: (b) He thinks understanding probabilities is more useful than trigonometry.\nenglish proverbs 목표는 어떤 속담이 텍스트 구절을 가장 잘 설명하는지 추측하는 것이다. Input: Vanessa spent lots of years helping out on weekends at the local center for homeless aid. Recently, when she lost her job, the center was ready to oﬀer her a new job right away. Which of the following proverbs best apply to this situation? (a) Curses, like chickens, come home to roost. (b) Where there is smoke there is ﬁre (c) As you sow, so you shall reap. Answer: (c) As you sow, so you shall reap.\nlogical sequence 목표는 논리적인 순서대로 배열하는 것이다. Input: Which of the following lists is correctly ordered chronologically? (a) drink water, feel thirsty, seal water bottle, open water bottle (b) feel thirsty, open water bottle, drink water, seal water bottle (c) seal water bottle, open water bottle, drink water, feel thirsty. Answer: (b) feel thirsty, open water bottle, drink water, seal water bottle.\nnavigate 목표는 간단한 네비게이션 지시를 따르고, 어디에 도착할지 파악하는 것이다. Input: If you follow these instructions, do you return to the starting point? Always face forward. Take 6 steps left. Take 7 steps forward. Take 8 steps left. Take 7 steps left. Take 6 steps forward. Take 1 step forward. Take 4 steps forward. Answer: No.\nmathematical induction 목표는 실제 세계의 수학과 상충하더라도 수학적 귀납법 규칙에 따라 논리적 추론을 수행하는 것이다. nput: It is known that adding 2 to any odd integer creates another odd integer. 2 is an odd integer. Therefore, 6 is an odd integer. Is this a correct induction argument (even though some of the assumptions may be incorrect)? Answer: Yes.\ngoal step wikihow와 logical args에 대한 성능은 log-linear 스케일링 곡선을 따르며, PaLM 540B 모델은 최고의 인간 성능에 가까워진다. 영어 속담과 논리적 순서에 대한 성능도 강력하지만, 개선 곡선은 불연속적이다. 특히, 특정 규모에 도달하면서만 특정 능력이 나타나는 것이 확인되었다. PaLM 62B에서 25%에서 PaLM 540B의 87%로 크게 개선된 영어 속담 성능은 매우 흥미로운 결과이다.\n불연속성에 대한 예로, PaLM의 논리적 순서 작업에서 8b, 62b, 540b 모델에 대한 정확도가 각각 13%, 25%, 87%였다. 이에 따라, 540b에 대한 예상 정확도는 37%였지만, 실제 정확도는 87%로, 불연속성은 +50%였다. 전체 150개 작업 중 25%의 작업에서 불연속성이 +10% 이상, 15%의 작업에서 +20% 이상 나타났으며, 이는 스케일에서의 불연속적인 개선이 일반적인 현상임을 보여준다.\n모든 작업에서 규모가 이익을 가져다주는 것은 아니다. 네비게이션과 수학적 귀납 작업에서 PaLM 540B는 PaLM 62B를 살짝 능가하지만, 두 모델 모두 최고의 인간 성능에서는 아직 멀리 떨어져 있다. 이는 작업의 예제 수준의 난이도에 큰 변동성이 있다는 것을 나타낸다. 특히, 수학적 귀납 작업에서는 올바른 가정과 잘못된 가정을 가진 예제들이 있어, 모델들이 가정의 정확성에 대한 문제를 해결하는데 어려움을 겪는 것으로 보인다.\nPaLM 540B가 전반적으로 인간 평가의 평균 성능을 능가하지만, 개별 작업의 35%에서는 인간의 평균 성능이 더 높다는 것을 보여준다. 이는 BIG-bench에서 아직도 상당한 개선 여지가 있다는 것을 의미한다.\nPaLM 540B는 여러 언어의 표현 및 큰 양의 정보를 기억하는 능력 등을 통해 인간의 평균 성능을 능가하는 일부 작업에서 뛰어난 성과를 보여준다. 그 중에는 원인과 결과를 판단하는 작업도 포함되어 있다.\ncause and eﬀect (one sentence no prompt) 하나의 문장으로 된 서브태스크에서, 이벤트들은 두 가지 다른 순서로 하나의 문장으로 결합되며, 각 문장의 log-likelihood는 모델로 점수화된다. 프롬프트는 제공되지 않는다. Input A: I washed the car because my car got dirty. Input B: My car got dirty because I washed the car. Higher-Likelihood Sentence: I washed the car because my car got dirty.\ncause and eﬀect (two sentence) 두 문장의 서브태스크에서는, 모델에게 두 가지 이벤트가 보여지고, 어떤 문장이 다른 이벤트를 일으킨 원인에 해당하는지 선택해야 한다. Input: For each example, two events are given. Which event caused the other? (a) My car got dirty. (b) I washed the car. Correct Prediction: (a) My car got dirty.\n모든 PaLM 모델이 한 문장 프롬프트 없는 작업에서 잘 수행되었고, 특히 8B 모델은 80% 이상의 정확도를 보여주었다. 그러나 두 문장 버전의 작업에서는 작은 모델의 성능이 떨어졌다. 대신 540B 모델은 이 작업에서 90% 이상의 높은 정확도를 보여, 대규모 모델이 언어 모델링 능력을 향상시킬 수 있음을 입증하였다.\n24개의 BIG-bench 작업 중 가벼운 평가 대상인 BIG-bench Lite의 상세 평가 결과를 보여준다. 일부 작업들은 해결되었거나 거의 해결된 상태이지만, 인간 평가의 최고 성능 점수에 비해 다른 일부 작업은 아직 해결되지 않았다.\nBIG-bench 데이터를 모델이 암기하여 성과를 달성한 것이 아닌지 확인하기 위해 여러 단계를 거쳤다. BIG-bench 작업 파일의 고유한 canary 문자열이 PaLM 학습 데이터에 없음을 확인했고, BIG-bench 데이터셋은 학습 데이터 수집 시점에 인터넷에 없었다. 대부분의 BIG-bench 작업들은 새로운 벤치마크이며, 모델의 우수한 성능을 보인 작업들을 임의로 점검하여 정보 유출이 없음을 확인하였다.\nReasoning PaLM은 여러 단계의 산술이나 상식적인 논리적 추론을 필요로 하는 추론 작업에서 평가된다. 언어 모델은 다양한 작업을 수행할 수 있지만, 여러 단계의 추론을 필요로 하는 작업을 수행하는 데에는 어려움이 있다. 이 작업에서는 두 가지 주요 추론 벤치마크 카테고리를 평가한다.\nArithmetic reasoning 이 작업들은 대부분 초등학교 수준의 자연어 수학 문제를 포함하며, 여러 단계의 논리적 추론이 필요하다. 수학은 대체로 간단하며, 어려운 부분은 자연어를 수학식으로 변환하는 것이다. 이 연구에서는 모델 자체가 수학을 수행하는 계산기 형태와 직접 추론 형태를 모두 평가했하였다. Input: Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? Answer: The answer is 11.\nCommonsense reasoning 이 작업들은 강한 세계 지식을 필요로 하는 질문 응답 작업이며, 세계에 대한 여러 논리적 추론을 연결하는 것을 필요로 한다. 이는 단순히 사실에 기반한 질문 응답이 아니다. Input: Q: Sean was in a rush to get home, but the light turned yellow and he was forced to do what? Answer Choices: (a) take time (b) dawdle (c) go slowly (d) ocean (e) slow down Answer: The answer is (e) slow down.\n최근 연구들은 대형 언어 모델이 최종 답변을 생성하기 전에 중간 추론 단계를 생성하면 정확도가 크게 향상될 수 있음을 보여주었다. 이 기술을 \u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅이라고 부릅니다. few-shot 설정에서, 중간 추론 단계는 수동으로 작성되고, 모델은 테스트 예시에 대한 자신의 \u0026ldquo;chain-of-thought\u0026quot;을 생성한다. 생성된 \u0026ldquo;chain-of-thought\u0026quot;은 오류 분석과 모델 해석에 유용할 수 있지만, 평가에는 최종 답변만 사용된다.\nResults 이 연구에서는 모델 규모와 \u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅만으로도 다양한 산술 및 상식 추론 작업에서 최첨단의 정확도를 달성할 수 있음을 보여준다. 이전의 많은 연구들은 도메인 특정 아키텍처, 작업 특정 미세조정, 작업 특정 검증자를 결합했지만, 이 연구에서는 단순히 few-shot 프롬프팅을 통해 작업들을 표현했다. 산술 추론 데이터셋의 경우, 사후 외부 계산기를 사용해 모델 예측을 보강했지만, 이는 어떤 데이터셋에서도 성능을 5% 이상 향상시키지 않았다.\n\u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅을 사용해, PaLM의 성능을 산술 데이터셋인 GSM8K, SVAMP, MAWPS, AQuA와 상식 추론 데이터셋인 CommonsenseQA와 StrategyQA에서 평가하였다. 이 프롬프팅 설정은 오직 8-shot 예시만을 사용한다.\nGSM8K에서 PaLM의 결과를 강조하며, 이전 state-of-the-art인 Cobbe et al. (2021)이 모델 미세조정, \u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅, 외부 계산기, 작업 특정 검증자를 사용하였다. 외부 계산기와 결합된 8-shot \u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅을 사용한 PaLM 540B는 58%의 성능을 달성해 이전 state-of-the-art인 55%를 능가하였다. 이는 \u0026ldquo;chain-of-thought\u0026rdquo; 없는 PaLM 540B와 \u0026ldquo;chain-of-thought\u0026quot;이 있는 PaLM 62B를 크게 능가하였다. PaLM 62B 모델이 잘못 처리한 문제들은 대체로 의미 이해, 한 단계 누락, 그리고 다른 오류들에 속하며, 540B 모델 크기로 확장하면 이러한 오류들의 대부분이 수정되었다.\n7개의 추론 데이터셋에서, PaLM 540B+\u0026ldquo;chain-of-thought\u0026quot;을 이용한 8-shot 예측은 4개의 작업에서 최고의 정확도를 달성하였으며, 나머지 3개의 작업에서는 state-of-the-art에 근접한 결과를 보여주었다. GSM8K에는 중간 추론 단계가 포함되었지만 다른 벤치마크에는 포함되지 않았다. state-of-the-art과 모델 확장이 모든 작업에서 크게 도움이 되었으며, 두 기술 없이는 PaLM이 한 가지 작업에서만 최고 수준을 달성했을 것이다. 데이터 오염이 없었음을 n-gram 겹침 분석을 통해 확인하였다.\nCode Tasks 최근 연구에서 대형 언어 모델이 경쟁 프로그래밍, 코드 완성, 자연어 명세에서 프로그램 합성 등의 코딩 작업에 유용함이 보여졌다. 이번 섹션에서는 PaLM 모델이 다양한 코딩 작업에서 뛰어난 결과를 달성하는 것을 보여준다.\nText-to-code. 자연어 설명이 주어진 상태에서 코드를 작성하는 세 가지 작업을 고려한다. HumanEval과 MBPP 데이터셋에서는, 모델에게 몇 문장의 영어 설명과 소량의 입력-출력 예시가 주어지며, 주로 단일 함수인 짧은 파이썬 프로그램을 생성하는 것이 목표이다. 또한, GSM8K 데이터셋에서 파생된 GSM8K-Python 작업을 소개한다. 이 작업에서는 올바른 답을 제공하는 대신 올바른 해결책을 반환하는 파이썬 프로그램을 생성하는 것이 목표이다. 데이터셋의 문제 중 네 개를 few-shot 예시로 사용하기 위해 수동으로 파이썬 프로그램으로 변환하였다.\nCode-to-code. TransCoder는 C++ 프로그램을 파이썬으로 번역하는 작업이다. 데이터셋에서 Python과 C++ 모두에 나타나는 함수들을 수집하고, 이 중 세 가지 다른 유형의 함수를 few-shot 프롬프트로 사용하며, 나머지는 테스트 세트를 만드는 데 사용하였다. 또한, 컴파일에 실패하는 C 프로그램을 성공적으로 컴파일할 수 있도록 수정하는 DeepFix 코드 수리 작업에서도 평가하였다. 결함 있는 코드에 대한 컴파일러 오류를 모델에 제공하고, 1260개의 프로그램에 대해 테스트하였다.\npass@k 메트릭을 사용해 결과를 보고하며, 이는 모델이 $k$개의 코드 샘플을 제공하고 그 중 하나라도 문제를 해결하면 문제가 해결된 것으로 간주한다. 간단히 문제를 해결하는 샘플의 비율을 보고하며, 이를 측정하기 위해 MBPP와 GSM8K의 테스트 데이터를 사용한다. 1개의 샘플일 경우 greedy decoding을, 그 이상일 경우 nucleus sampling을 사용한다.\nPaLM 모델을 LaMDA 137B 파라미터 모델과 초기 Codex 모델 12B와 비교한다. LaMDA는 GitHub의 코드에 대해 학습되지 않았지만, 코드 관련 웹 문서를 일부 포함하여 프로그램 합성 능력을 가지며, Codex 모델은 HumanEval 데이터셋에서의 결과만을 보고한다.\n다른 데이터셋에서 Codex 결과를 얻기 위해, OpenAI Davinci Codex API를 사용했다. 이는 2021년 9월 1일부터 2022년 3월 10일까지 진행되었고, 가장 최신 버전인 Davinci 모델 버전 1을 사용했다. Davinci Codex 모델에 대한 많은 정보는 공개되지 않아 성능 차이의 원인을 이해하는 것은 어렵지만, 이 비교는 고려하는 작업의 본질적인 어려움을 이해하는 데 유용하다.\nDatasets PaLM 모델은 학습 세트에 GitHub 코드를 포함하며, 총 39B 개의 코드 토큰이 사전 학습 데이터셋에 있다. Python 프로그래밍을 테스트하는 평가를 위해, ExtraPythonData라는 추가 데이터셋을 수집했고, 이는 사전 학습에 사용되지 않은 GitHub에서 5.8B 개의 토큰을 수집한 것이다. 이 데이터는 Java, HTML, Javascript, Python, C, PHP, C#, C++ 등의 언어를 포함하고 있다.\nPaLM 540B PaLM 모델은 모든 작업에서 LaMDA보다 높은 성능을 보여주며, HumanEval에서는 Codex 12B와 비슷한 수준이다. 이는 동일한 모델이 코드와 자연어 작업 모두에서 뛰어난 성능을 보여주는 첫 번째 큰 언어 모델이라는 점에서 중요하다. PaLM은 Python 코드 토큰 약 2.7B 개로 학습되었는데, 이는 Codex 모델의 Python 토큰 1000억 개에 비해 50배 적다. 그럼에도 불구하고 PaLM은 비슷한 성능을 보여주어, 다른 프로그래밍 언어와 자연어 데이터로부터의 전이와 큰 모델이 작은 모델보다 효율적일 수 있다는 것을 보여준다.\nPaLM-Coder PaLM 모델을 Python 코드와 다양한 언어의 코드, 그리고 자연어에 대해 미세 조정한 결과, PaLM-Coder 540B의 성능이 크게 향상되었다. 이는 미세 조정을 하지 않은 모델에 비해 HumanEval에서 +12%, MBPP에서 +5%의 절대적인 성능 향상을 보여주었다. 또한, 모델의 규모가 증가함에 따라 성능이 계속 향상되는 것을 확인하였다.\nGSM8K-Python 데이터셋에 대해, PaLM-Coder 540B는 8-shot 프롬프트에서 pass@1 점수 57.5를 얻었고, 반면에 PaLM 540B 모델은 pass@1 점수 58.1을 달성하였다.\nDeepFix Code Repair PaLM-Coder 540B 모델은 DeepFix 코드 수정 작업에서 82.1%의 컴파일률을 달성하여 뛰어난 성능을 보여주었다. 이는 이전 작업에서 달성한 71.7%보다 높은 결과이다. 프롬프트는 다양한 일반적인 오류를 포함한 두 쌍의 깨진 및 수정된 C 프로그램을 손으로 작성하였으며, 이후 모델이 수정된 전체 코드를 예측하게 하였다.\n코드 수정에서는 이상적으로 깨진 코드의 작은 부분만 수정하고 싶기 때문에 모델이 변경한 코드의 양을 평가하는 것이 중요하다. PaLM은 가장 작은 편집을 생성하는 반면, PaLM-Coder는 작은 정규화된 편집 거리를 가진 편집에 대해 가장 높은 성공률을 보여주었다. 반면에 Davinci Codex는 변경된 라인 수가 적은 편집에서 가장 높은 성공률을 보였다. 이는 PaLM-Coder가 더 많은 라인에 대해 적은 수의 문자를 변경하는 경향이 있음을 의미한다.\nDiscussion 소프트웨어 개발에서 언어 모델 기반 시스템을 사용할 때, 생성된 코드가 잘못되거나 미묘한 버그를 도입할 위험이 있다. 개발자들은 제안된 코드를 프로그램에 추가하기 전에 검토해야 하지만, 항상 미묘한 버그를 찾을 수는 없다. 코드 제안은 테스트 스위트로 확인할 수 있지만, 소수의 테스트 케이스로부터 솔루션이 기능적으로 올바르다는 것을 추론하는 것은 항상 안전하지 않다. 이에 따라, 기능적 정확성에 대한 더 철저한 테스트가 필요하다.\n기능적 정확성은 소스 코드 품질의 한 가지 측면일 뿐이며, 언어 모델이 생성한 코드 제안은 읽기 쉽고, 견고하고, 빠르고, 안전해야 한다. DeepFix는 PaLM-Coder의 현재 예측과 관련된 문제를 보여주는데, 수정된 프로그램은파일되지만 입력의 형식과 크기에 대한 가정에 의존하기 때문에 반드시 안전한 것은 아니다. 이러한 제안은 더 일반적인 상황에서는 원치 않을 수 있다. 개발자가 제안된 코드를 이해하고 신뢰하는 것은 여전히 해결되지 않은 문제이며, 가독성과 보안성을 평가하는 이전의 연구가 있지만, 이 분야는 아직 초기 단계에 있다.\nTranslation 기계 번역은 텍스트를 한 언어에서 다른 언어로 변환하는 작업이다. GPT-3 같은 거대 언어 모델들은 병렬 텍스트에 대해 명시적으로 학습받지 않았음에도 불구하고 번역 능력을 보여주었다. 이번 섹션에서는 다양한 언어 쌍에 대해 PaLM의 번역 능력을 평가하며, 이 과정에서 WMT에서 제공하는 언어 쌍을 주로 사용할 예정이다.\nEnglish-centric language pairs 이전 모델들이 주로 다루었던 전통적인 언어 쌍은 영어를 포함하고, 병렬 데이터의 양에 따라 고자원, 중자원, 저자원으로 구분한다. 이번 분석에서는 WMT'14의 영어-프랑스어(고자원), WMT'16의 영어-독일어(중자원), 그리고 WMT'16의 영어-루마니아어(저자원)를 언어 쌍으로 사용한다.\nDirect language pairs 번역 시스템이 영어를 거치지 않고 어떤 언어 쌍이든 직접 번역하는 능력이 점점 중요해지고 있다. 이를 테스트하기 위해, 프랑스어와 독일어 사이의 직접 번역 능력을 WMT'19 데이터를 사용해 확인한다.\nExtremely-low resource language pairs 모든 언어 쌍은 병렬 데이터가 없어 제로-리소스 상태이다. 그러나 단일 언어 데이터가 적은 언어, 예를 들어 이 연구에서 선택한 카자흐스탄어는 흥미로운 점이 있다. 프랑스어와 독일어는 각각 240억, 260억의 토큰을 가지고 있는 반면, 카자흐스탄어는 1.34억 토큰만 가지고 있다. 이를 평가하기 위해 WMT'19의 영어-카자흐스탄어를 사용하였다.\nEvaluation on English-centric language pairs 전통적인 영어 중심 언어 쌍에서 0-shot, 1-shot, few-shot 설정에서 PaLM을 평가하였다. 이 모델은 GPT-3와 FLAN과 같은 다른 모델들을 능가하며, 때때로 최대 13 BLEU 점수 차이를 보여주었다. 독일어-영어와 루마니아어-영어에서는 감독된 기준선을 능가했지만, 이 기준들이 최근 변경된 WMT 작업에 따라 오래되었을 수 있음을 인정한다. 모델 크기를 8B에서 62B, 그리고 540B로 확대하면서 0-shot 번역의 결과에서 급격한 BLEU 점수 상승이 관찰되었다. 특히, 영어-독일어는 13 BLEU, 영어-프랑스어는 17 BLEU 증가를 보였습니다. 이는 \u0026ldquo;power law\u0026rdquo; 법칙에 따르지 않는 현상이다.\nEvaluation on direct and extremely-low resource language pairs PaLM은 직접적이고 극도로 저자원 언어 쌍에서의 성능을 평가하였다. WMT'19에서 가장 높은 점수를 받은 제출물을 활용하였다. 이 도전적인 상황에서 PaLM은 프랑스어-독일어에서만 지도 성능을 맞출 수 있었지만, 독일어-프랑스어와 카자흐스탄어-영어에서는 강력한 성능을 보여주었다. Further ﬁndings and analysis 결과는 다음과 같은 관찰로 정리할 수 있다:\nTranslation quality is better when translating into English rather than out of English. 모든 영어 중심 언어 모델에서 관찰되는 공통적인 패턴이며, PaLM의 성능을 살펴보면서 비슷하게 나타난다. 다국어 데이터를 우선시하면 이 효과가 완화될 것으로 추정한다.\nPrompts can deliver even more value than a single example. 대부분의 경우, 언어 이름을 사용하여 번역을 유도하는 0-shot 설정이 입력-출력 예시만을 사용하는 1-shot 및 few-shot 설정보다 더 높은 성능을 보여주었다.\nGeneralist models relying solely on self-supervision can match specialized models at smaller scales. 대부분의 전용 번역 기준은 parameter가 1B개 미만으로, 가장 큰 PaLM 설정보다 두 자릿수가 작다. 그러나, 대형 번역 모델이 다양한 작업에 적응할 수 있음을 확인했으므로, specialist도 generalist로 활용될 수 있다. 이로 인해, 자원이 풍부한 상황에서는 specialist를 학습시킬지, 아니면 generalist를 학습시킬지에 대한 질문이 제기된다.\nMultilingual Natural Language Generation 자연어 생성은 텍스트나 비언어적 정보를 입력으로 받아 이해하기 쉬운 텍스트를 자동 생성하는 과제이다. 그러나 과거에는 비슷한 크기의 모델들에 대해 few-shot 조건부 자연어 생성에 대한 탐구가 없었다. 대형 언어 모델들(GPT-3, GLaM, Gopher, LaMDA, Megatron-Turing NLG) 중 어느 것도 이런 과제에 대한 결과를 보고하지 않았다.\n이 연구는 few-shot 모델링을 위한 첫 번째 대형 언어 모델 벤치마크를 제시하며, 비교 대상으로 LaMDA 137B를 사용하였다. 이 모델은 이전 연구에서 벤치마크 결과를 보고하지 않았지만 테스트는 할 수 있었다.\n미세 조정을 위한 이전 최고 성능은 주로 T5, mT5, BART 등의 encoder-decoder 모델에서 나왔다. 이들 모델은 PaLM보다 작지만, 채우기를 위해 학습된 모델들은 종종 더 큰 decoder-only 언어 모델을 능가한다. 따라서, 이 연구에서는 대규모 모델이 decoder-only 언어 모델의 약점을 보완할 수 있는지를 중요하게 비교하고 있다.\nData 우리는 PaLM을 GEM 벤치마크의 여섯 가지 작업(세 가지 요약, 세 가지 데이터-텍스트 생성)으로 평가하였다. 이는 체코어, 영어, 독일어, 러시아어, 스페인어, 터키어, 베트남어 등의 언어를 포함한 데이터셋을 사용하였다.\nMLSum 다중 문장으로 뉴스 기사를 요약 [독일어/스페인어] WikiLingua WikiHow의 단계별 지시사항을 매우 간결한 문장으로 요약 [영어/스페인어/러시아어/터키어/베트남어 → 영어] XSum 한 문장으로 뉴스 기사를 요약 [영어] Clean E2E NLG 주어진 키-값 속성 쌍을 바탕으로, 레스토랑을 한 두 문장으로 설명 [영어] Czech Restaurant response generation 대화 맥락과 대화 행동 표현을 바탕으로, 스마트 어시스턴트가 제공할 응답 생성 [체코어] WebNLG 2020 주어-동사-목적어 삼중체를 문법적이고 자연스럽게 한 문장 이상으로 표현 [영어/러시아어] 모델의 추론 시간을 줄이기 위해, 테스트 세트가 5,000개를 초과하면 균일하게 샘플링한다.\nMetrics Gehrmann et al. 의 제안에 따라 ROUGE-2, ROUGE-L, BLEURT-20 결과를 보고하며, 이 섹션의 본문은 ROUGE-2의 F-측정에 초점을 맞춘다.\nFew-shot evaluation methodology PaLM은 few-shot 추론에 사용되며, 작업 특정 프롬프트를 입력에 연결하고 출력 프롬프트를 출력에 추가한다. 요약을 위한 긴 입력은 2048 토큰으로 줄이고, few-shot 예시들은 두 줄의 공백으로 분리한다. 모든 few-shot 예시들은 훈련 데이터에서 무작위로 추출된다.\nFinetuning methodology 미세조정 시, decoder만 사용하며, 입력과 목표를 연결하지만, 손실은 목표 부분에서만 계산한다. 연결된 시퀀스는 2048 토큰으로 잘라내고, 목표를 위해 512 토큰을 예약한다. 이 과정은 요약 작업에서만 필요하다.\nPaLM 미세조정은 $5×10^-5$ 의 learning rate와 optimizer 리셋을 사용하며, 검증 세트에서 가장 좋은 ROUGE 점수를 보인 모델을 선택한다. 추론은 $k = 10$의 top-k 샘플링으로 수행되고, T5 XXL 기준선은 PaLM과 동일한 parameter로 미세조정하며, beam size 4의 beam-search를 사용해 디코딩한다.\nResults 1-shot과 미세조정의 비교는 ROUGE-2의 F-측정을 사용한다.\n이 연구는 few-shot 모델링에 초점을 맞추고 있고, 이러한 작업에 대한 공개된 few-shot 결과는 없지만, 이 결과들로부터 몇 가지 흥미로운 교훈을 얻을 수 있다:\nEﬀectiveness of ﬁnetuning 요약 작업에서, 미세조정된 540B PaLM은 모든 영어 생성 작업에서 최상의 성과를 보여주며, 이는 그것의 대규모 스케일을 통해 아키텍처적 단점을 극복할 수 있다는 것을 보여준다. 62B 버전도 최상의 결과에 가깝고, 540B는 그것을 초과한다. decoder 전용 LM의 미세조정이 작업 특정 훈련 데이터가 많을 때 모든 작업에 대한 최적의 접근법이 아닐 수 있다는 것을 인지하고 있지만, 이것이 few-shot 예측에 대한 중요한 상한선 역할을 한다고 믿는다.\nGeneration quality of English vs. non-English PaLM은 6개의 요약 작업 중 4개에서 새로운 미세조정 state-of-the art를 달성하였다. 그러나 비영어 요약에서는 최고 기록에 못 미치며, 비영어 생성에서 few-shot과 미세조정 사이의 차이는 더 크다. 이는 PaLM이 비영어 입력 처리에는 능하지만 비영어 출력 생성에는 덜 능하다는 것을 보여주며, 이는 향후 비영어 텍스트의 큰 부분에 대한 사전 학습을 통해 개선될 수 있다.\n1-shot vs. ﬁnetuning gap 데이터-텍스트 결과에서, few-shot 결과는 요약과 비슷한 추세를 보이지만, 최상의 미세조정 결과와의 차이는 크게 줄어든다. FLAN은 instruction tuning 후 E2E-NLG에서 33.2, WebNLG에서 48.0의 점수를 보고하는 반면, PaLM은 어떠한 튜닝 없이 35.2와 44.4를 얻었다. 그러나 데이터-텍스트 작업은 그 크기가 작고 사전 학습 말뭉치와 크게 다르기 때문에, 미세조정 벤치마크로서의 가치가 제한적일 수 있다.\nFew-shot summarization 다양한 PaLM 규모에서의 few-shot 요약 결과를 비교하면, 8B에서 62B로, 그리고 62B에서 540B로 크게 향상되는 것을 볼 수 있다. 그러나, few-shot과 미세조정 사이의 차이는 아직도 크며, 1-shot 성능은 비영어 작업의 T5-base나 T5-large, 영어 작업의 T5-small와 같은 작은 미세조정 모델과 비슷하다. 이는 큰 언어 모델로의 few-shot 요약 첫 시도이므로, 조건부 생성 작업에 대한 few-shot과 미세조정 모델 사이의 간극을 좁히는 데 중요한 시작점이 될 것이라 믿는다.\nMultilingual Question Answering TyDiQA-GoldP 벤치마크를 사용해 다국어 질문 응답에 대한 모델을 few-shot 설정과 미세조정 설정에서 평가했다. few-shot 설정에서는 문맥, 질문, 답변을 새 줄 문자로 구분하고, \u0026ldquo;Q:\u0026ldquo;와 \u0026ldquo;A:\u0026ldquo;로 각각 질문과 답변을 표시했다. 미세조정에서는 영어 SuperGLUE 미세조정 실험과 동일한 hyperparameter를 사용했으며, 가장 좋은 전체 체크포인트에서의 결과를 보고하였다.\nfew-shot과 미세조정 품질 사이에는 평균적으로 큰 차이가 있다는 것을 보여준다. 그러나 스와힐리어와 핀란드어 같은 특정 언어들에서는 이 차이가 적다. 프롬프트 엔지니어링과 다국어 데이터셋에 대한 다작업 적응 연구가 few-shot 결과를 더욱 개선할 수 있을 것으로 보인다.\nPaLM 540B는 비영어 데이터의 학습 비율이 적음에도 불구하고 이 작업에서 매우 경쟁력 있는 결과를 보여준다. mT5와 ByT5는 비영어 텍스트에 대해 PaLM의 6배와 1.5배 만큼 학습되었음에도 불구하고, PaLM 540B는 mT5 XXL을 능가하고 ByT5 XXL에게는 능가당하였다. 이러한 결과는 사전 학습 데이터셋에서 비영어 데이터 비율을 늘리거나, 구조적 단점이나 귀납적 편향을 극복하는 방법을 통해 더욱 개선될 수 있을 것으로 보인다.\nAnalysis PaLM 모델의 few-shot 성능에 대한 분석을 제시한다. 세 가지 모델(8B, 62B, 540B)을 다섯 가지 다른 작업(RTE, Natural Questions, Lambada, Story Cloze, Trivia QA)에서 연구하였으며, 이들 작업은 지식 중심에서 추론 중심까지 다양하다. Trivia QA와 Natural Questions은 문맥 문서 없이 질문만을 입력으로 제공되는 \u0026ldquo;closed book\u0026rdquo; 방식이다.\n0-shot, 1-shot, 5-shot, 8-shot 학습을 평가하여 모델에 더 많은 예제가 제공될수록 대부분의 작업과 모델에서 성능이 향상되는 것을 확인하였다. 하지만 Trivia QA 작업에서는 1-shot 학습이 모든 모델 크기에서 5-shot 및 8-shot 학습을 능가하는 예외적인 결과를 보여주었다.\n이 연구에서는 다양한 모델 체크포인트에서의 few-shot 학습 성능을 분석했다. 대부분의 작업에서 체크포인트 간 성능에 큰 차이를 보이지 않았지만, Web Questions 작업에서는 체크포인트 간에 큰 성능 변동을 보였다. 가장 높은 성능을 보인 PaLM 540B는 학습 토큰 7700억 개의 체크포인트에서 최고 결과를 보였지만, 그 이후의 체크포인트에서는 성능이 감소했다. 이 연구의 모든 결과는 동일한 체크포인트에서 평가되었다.\nMemorization 신경망이 학습 데이터를 기억하는 것은 overfit의 일종이며, 이는 주로 작은 학습 세트를 여러 번 반복할 때 발생한다. 그러나 PaLM 같은 모델은 780B 토큰의 말뭉치를 한 번만 훑어내려도, 모델의 큰 용량 때문에 학습 데이터의 상당 부분을 기억할 수 있다. 더욱이, 웹에서 추출된 말뭉치에는 중복되는 텍스트가 많이 있어, 학습 과정에서 약간 변형된 구절들이 여러 번 나타날 수 있습니다.\nPaLM 모델이 학습 데이터를 얼마나 잘 기억하고 있는지를 분석한다. 학습 예제에서 무작위로 선택한 100개의 토큰 시퀀스로 모델을 실행하고, 모델이 학습 예제와 정확히 일치하는 50개 토큰을 얼마나 자주 생성하는지 측정한다.\n(a)는 세 가지 크기의 모델이 훈련 데이터를 얼마나 잘 기억하는지를 보여준다. 8B 모델은 1.6%의 데이터, 540B 모델은 2.4%의 데이터를 정확히 재현할 수 있었다. 또한 학습 데이터와 같은 분포에서 추출된 보류 중인 데이터에 대한 기억율도 평가했으며, 이는 일부 보류 중인 예제가 학습 세트 예제와 매우 유사하기 때문에 0% 이상이었다.\n학습 데이터에서 예제가 정확히 몇 번 보였는지에 따른 기억율을 보여주는 (b)에 따르면, 한 번만 본 예는 가장 큰 모델에서 0.75%의 기억을 가지고, 500번 이상 본 예제는 40% 이상의 기억율 보였다. 이는 학습 과정에서 전체 문서에 대해 중복을 제거하고, 100 토큰 범위에서 기억을 평가했기 때문이다.\n(c)는 학습 데이터 말뭉치별로 모델의 기억율을 보여준다. 학습에서 예제의 정확한 중복, 거의 중복, 또는 템플릿화의 양이 가장 큰 영향을 미쳤다. 코드 말뭉치는 표준 라이센스 문자열, 다른 곳에서 복사된 공유 코드 스니펫, 자동 생성된 코드 등을 포함하고 있고, 반면 책 말뭉치는 주로 고유한 텍스트를 포함하고 있다.\n이 결과들로부터, memorization에 대해 다음과 같은 결론을 내릴 수 있다:\n더 큰 모델이 더 작은 모델보다 높은 기억율을 보이며, 이는 이전 연구의 결과와 일치한다. 기울기와 결정계수($R^2$) 값이 모두 비슷하게 나타났다.\n흔한 템플릿과 표준 문구에 대해 모델이 정확히 일치하는 연속을 생성하므로, 일정 수준의 \u0026ldquo;memorization\u0026quot;이 예상된다. 그러나 학습 데이터에 대한 기억율은 보류 중인 데이터보다 상당히 높아, 이는 모델이 실제로 데이터의 일부를 기억한다는 것을 보여준다.\n예제가 기억될 확률은 학습에서 그 예제의 독특함과 강하게 연관되어 있다. 한 번만 본 예제는 여러 번 본 예제보다 기억될 가능성이 적다. 이는 이전 연구들과 일치하는 결과이다.\n대부분의 기억 사례는 우려를 불러일으키지 않을 것 같은 공식적인 텍스트였으며, 이야기나 뉴스 기사, 사실 등도 기억되었다. 추출 가능한 기억된 내용의 양은 학습 데이터, 모델 크기, 그리고 추출을 수행하는 사람이 학습 데이터를 얼마나 알고 있는지에 따라 달라진다. 하지만 단순히 추출 가능한 학습 데이터의 양을 측정하는 것만으로는 이 기억이 문제가 될 수 있는지에 대한 정보를 얻을 수 없다.\n기억이 문제가 되는지는 데이터셋의 특성과 사용 목적에 따라 다르다. 큰 언어 모델을 사용할 때는 항상 신중해야 한다. 생성 시점의 기억을 방지하는 한 방법은 학습 데이터 위에 블룸 필터를 구현하고, 학습 데이터셋에서 그대로 나온 시퀀스를 생성하지 않게 제한하는 것이다. 하지만 이 방법도 완벽하지 않으며, 최선의 대응 전략은 큰 언어 모델을 언제, 어떻게 사용할지 신중하게 결정하는 것이다.\nDataset Contamination 이전 연구들은 벤치마크 평가 세트와 학습 데이터 사이에 높은 수준의 데이터 중복률이 있다고 보고했다. 그러나 많은 벤치마크는 웹에서 맥락을 가져와 생성된 질문에 대한 답을 만들도록 요청하는 방식으로 구성되었다. 이러한 작업에 대해 평가 시점에 맥락이 제공되므로, 모델이 이전에 맥락에 대해 학습했더라도 평가 시간에 불공정한 이점은 주지 않는다.\n단순히 고차 n-gram 중복을 찾는 것이 아니라, 29개의 주요 영어 NLP 벤치마크 작업에 대해 통계를 계산하고 각각의 예제를 수동으로 검토하여 오염된 예제의 비율이 높은 것을 파악하였다. 이는 각 데이터셋이 어떻게 구성되었는지를 고려하여 수행되었다.\n29개의 벤치마크 작업을 대략 네 가지 카테고리로 나눌 수 있다:\nWholesale contamination 데이터셋의 상당 부분이 오픈 웹에 나타나는 데이터셋이며, 이것들을 오염되었다고 간주한다. 예: SQuADv2, Winograd.\nConstructed from web 질문+답변(또는 접두사+연속)이 오픈 웹에서 자동으로 추출된 데이터셋으로, 많은 평가 예제가 학습 데이터에 있을 가능성이 높으며, 이것들을 오염되었다고 간주한다. 예: Web Questions, ReCoRD, Lambada.\nContext on web 맥락은 웹에서 가져왔지만 질문은 그렇지 않은 질문 응답 데이터셋이며, 이것들을 오염되지 않았다고 간주한다. 예: BoolQ, Multirc, ANLI.\nNo signiﬁcant overlap 학습 데이터와 중복되는 부분이 없는 데이터셋으로, 어떤 대규모 학습 코퍼스에서도 기대할 수 있는 공통 n-gram은 제외한다. 예: StoryCloze, OpenbookQA.\n29개의 벤치마크 세트 중 10개가 첫 두 카테고리에 속한다는 것을 확인하였다. 이들 중 일부만이 학습 데이터에서 발견되었다. 이는 학습 코퍼스가 웹 데이터의 일부만 포함하고 있기 때문이다. 따라서 각 데이터셋을 \u0026ldquo;contaminated\u0026rdquo; 부분과 \u0026ldquo;clean\u0026rdquo; 부분으로 나눌 수 있었다.\n깨끗한 부분에서 긍정적인 정확도 변화와 부정적인 정확도 변화를 보여주는 세트의 수가 동일함을 확인하였다. 이는 데이터 오염이 결과에 큰 영향을 미치지 않음을 의미한다. 만약 540B 모델이 평가 세트의 대부분을 단순히 암기했다면, 깨끗한 부분에서 8B 모델보다 더 큰 부정적인 변화를 보였을 것이다. 하지만, 8B와 540B 모델은 깨끗한 검증 세트와 전체 검증 세트 사이에 비슷한 수의 부정적인 변화를 보여주었다.\n기계 번역에 대해 분석을 수행했고, 데이터 오염은 발견되지 않았지만, 학습 데이터에서 발생하는 목표 참조 문장이 일부 있었다는 것을 확인하였다. 결과적으로, 학습 데이터와 높은 n-gram 중복을 가진 문장을 제거하여 \u0026ldquo;clean\u0026rdquo; 부분집합을 만들었다. 대부분의 세트에서 깨끗한 세트와 전체 세트 사이의 BLEU 점수는 비슷했으며, 이는 기억력에 의한 차이가 주요 요인이 아니라는 것을 보여준다.\nExploring Explanations \u0026ldquo;chain-of-thought\u0026rdquo; 프롬프팅이 다단계 추론 작업의 예측 정확도를 크게 향상시키는 것을 보여주었다. 이 방법은 모델이 올바른 답을 내는 이유를 규명하는 과학적 관심사, 사용자의 신뢰도 조절, 그리고 설명 자체가 필요한 상황(예: 농담 설명) 등에 유용하게 사용될 수 있다.\nchain-of-thought 프롬프팅을 사용한 PaLM의 설명적 언어 생성 능력을 보여주려 한다. 제시한 예시들은 논리적 추론, 세계 지식, 추상적 언어 이해, 사전적 언어 이해 등을 복합적으로 필요로 힌다. \u0026ldquo;Explaining a Joke\u0026quot;과 \u0026ldquo;Logical Inference\u0026quot;이라는 두 가지 작업을 통해 모델 output을 보여준다. 각 작업에 대해, 원하는 output 스타일을 보여주는 예시들을 작성하였다. 이 예시들은 저자들이 작성하고 선택했지만, 여전히 PaLM의 언어 이해 능력을 획기적으로 보여주는 결과라고 믿는다. 이는 이 분석이 어떻게 수행되었는지에 관한 여러 핵심 요인들 때문이다.\n모든 예측은 동일한 2-shot 예시를 통해 생성되며, 이는 평가하는 예시의 내용과는 무관하게 오직 스타일에만 연관이 있다. 게다가, 모든 예시 프롬프트는 예시 평가 이전에 작성되었고, 모델 output의 검토를 바탕으로 수정된 적은 없다. 모든 output은 temperature sampling이 아닌 greedy decoding으로부터 나온다. 그 이유는 각 output이 exponential space에서 가능한 많은 output 중 하나가 아니라 모델의 표준 1-best 예측이기 때문입니다. 이 작업들의 목적이 모델에게 철저한 자연어 설명을 생성하도록 유도하는 것이기 때문에, greedy decodin이 단순한 통계적 상관 관계나 \u0026ldquo;lucky guesses\u0026quot;을 통해 완전히 정확한 설명을 생성할 확률은 극히 낮다. 프롬프트가 저자들에 의해 작성되었기 때문에, 이는 직접적인 데이터 오염과 기억이 주요 요인이 될 가능성을 완화시킨다. 이 섹션에서 가장 큰 가치를 이러한 예시들을 단순히 읽는 것에서 얻을 수 있다고 믿는다. 비록 이 결과들이 철저한 정량적 분석을 의미하지는 않지만, 이것이 심층적인 언어 이해의 정말 놀라운 수준을 보여준다고 말한다.\nRepresentational Bias Analysis 사전 학습된 언어 모델들은 데이터의 편향을 포함하고 확대한다는 것이 입증되었다. 모델의 구조를 공유하는 것의 중요성도 강조되었다. 이 섹션에서는 PaLM이 사회 집단과 관련된 편향과 개방형 언어 생성에서의 toxicity를 분석한다. 이 분석은 모델의 잠재적 위험을 개요화하는 데 도움이 되지만, 가능한 위험을 제대로 조정하고 맥락화하며 완화하기 위해선 도메인 및 작업별 분석이 필수적이다.\nDistributional bias in social groups Gender and occupation bias 대용어 해결은 언어 시스템의 중요한 능력이다. 영어에서는 대명사가 의미적 성별로 표시되며, 이는 대용어 해결 성능에 영향을 미친다. 우리는 \u0026ldquo;nurse\u0026quot;와 \u0026ldquo;electrician\u0026quot;와 같은 직업 명사의 성별 편향을 측정하는 Winogender 벤치마크를 사용하여 PaLM의 이러한 편향에 대해 평가한다.\n다중 선택 점수화는 Winogender에 대해 일반적으로 사용되며, 각 가능한 답변을 모델이 그 답변을 생성할 확률로 점수화한다. 이 점수화 방법은 올바른 답변을 생성할 모델의 절대 확률이 낮더라도 예시가 올바르게 점수화될 수 있다. 이 방법은 널리 쓰이지만, 특히 0-shot 설정에서의 모델 성능을 과대평가한다는 것을 발견하였다. 540B 모델의 다중 선택 점수화와 생성적 출력의 예시가 있다:\n0-shot 생성 케이스에서, 모델은 작업을 이해하지 못하고 다중 선택 시험을 흉내 낸다. 생성 점수화에서는 대소문자를 구분하지 않는 정확한 문자열 일치를 사용하며, 모델 output은 문장 부호나 줄 바꿈에서 잘린다.\n모델 규모가 커질수록 정확도가 향상되며, PaLM 540B는 1-shot과 few-shot 설정에서 최고 state-of-the-art를 달성하였다. 특히, 더 엄격한 생성 점수화 방법을 사용해도 4-shot 설정에서 84.7%의 정확도를 보였다. 하지만 이 성능은 아직 작업에 맞춘 모델이나 인간의 성능보다 낮다.\nWinogender를 고정관념적 또는 \u0026ldquo;gotcha\u0026rdquo; 부분집합으로 나누어 분산 정확도를 보고한다. 고정관념적 주석에서는 성별과 직업이 일치하고, \u0026ldquo;gotcha\u0026rdquo; 주석에서는 반대이다. 성별 중립적인 대명사는 중립 분할의 일부이다. 모든 경우에서, 올바른 예측은 제공된 맥락에서 분명하게 추론될 수 있다. 모델이 통계적 단축에 얼마나 의존하는지를 측정하는 강력한 척도이다. 모든 경우에서, few-shot 예시들은 전체 예시 세트에서 무작위로 샘플링되며, 평가 중인 현재 예시는 제외된다.\n고정관념적 예시에서의 정확도가 \u0026ldquo;gotcha\u0026rdquo; 예시보다 높으며, 여성에 대한 \u0026ldquo;gotcha\u0026rdquo; 예시에서 정확도가 가장 낮다. shot의 수가 증가함에 따라 이러한 분할 간의 성능 차이가 개선되는 것을 볼 수 있다. 성능의 차이는 학습 세트에서 영어 대명사의 빈도 차이와 관련이 있을 수 있지만, 정확도와 직업 순위 사이에는 명확한 관계를 찾지 못하였다.\nToxicity and bias 모델이 \u0026ldquo;성별, 종교, 인종 및 민족 신분\u0026quot;과 같은 특정 용어를 참조할 때 자주 함께 나타나는 단어를 분석한다. 각 프롬프트에 대해 800개의 출력을 생성하고, 불용어를 제거하고 형용사와 부사만 선택한다. 이 분석은 어떠한 수동 인간 라벨링도 없이 투명하게 이루어진다.\n정체성 그룹을 참조하지 않는 설명적인 단어의 수를 줄이기 위해, 첫 번째 완전한 문장에서만 형용사와 부사의 수를 계산하였다.\n이 방법을 통해 특정 차원, 특히 이슬람에 대한 bias가 더 잘 드러나는 것을 확인하였다. 인종 신분 용어는 서로 함께 나타나는 경향이 있으며, 프롬프트 언어의 작은 변화가 결과에 큰 변화를 가져온다. 예를 들어, \u0026ldquo;The term was\u0026rdquo; 프롬프트를 사용하면 Latinx는 폭력적이거나 공격적인 어조와 함께 많이 등장한다.\n\u0026ldquo;Indian\u0026quot;이 \u0026ldquo;White\u0026quot;와 많이 동시에 나타났다. 이는 \u0026ldquo;White\u0026quot;라는 표현이 백인 정복자를 지칭하는 데 일반적으로 사용되는 미국 기원의 내용에서 비롯된 것으로 보인다. 많은 연속들이 백인과 아메리칸 인디언 사이의 식민지적 역학을 묘사하지만, 이는 사용자가 북미의 식민지화에 대한 설명에 과도하게 제한되지 않는 언어를 생성하길 원할 때 추가 분석이 필요할 수 있다.\n결과를 검토할 때, 정체성 용어가 모호성을 해소하지 않는다는 것을 알아두는 것이 중요하다. 예를 들어 \u0026ldquo;Indian\u0026quot;은 아메리칸 인디언과 인도 출신 사람을 구분하지 않는다. 또한 \u0026ldquo;Black\u0026quot;과 \u0026ldquo;White\u0026quot;는 종종 인종 신분 외의 것을 참조하며, \u0026ldquo;White\u0026quot;는 백인이 설명될 때 일반적으로 사용되지 않아, \u0026ldquo;White\u0026quot;와 함께 나타나는 용어를 비교하는 것이 복잡할 수 있다.\n62B와 540B 모델은 매우 유사한 동시 출현 횟수를 보여주며, 인종, 종교, 성별 차원에서 상위 10개 단어 중 70%가 동일하다. 이로 인해, 학습 데이터가 모델의 크기보다 결과에 더 큰 영향을 미친다고 판단하였다.\n동시 출현 분석은 용어가 어떻게 다른 용어와 관련되어 나타나는지를 파악하는데 중요하다. 정체성 용어가 있는 프롬프트 템플릿을 사용하여 모델 완성의 toxicity를 분석하는 접근법을 사용하였다. 이슬람에 대한 상위 용어로 \u0026ldquo;terrorist\u0026quot;를 확인했고, 이슬람과 무신론을 포함하는 프롬프트에서 더 높은 toxicity 점수를 보여준다. 이를 통해 모델 완성이 무슬림에 대한 부정적인 고정관념을 잘못 확인하는 가능성을 파악할 수 있다.\n동시 출현 횟수를 계산하는 것 외에도, 계속되는 내용의 toxicity를 분류하기 위해 Perspective API를 사용하다. 이 API는 텍스트가 무례하거나 불쾌하거나 사람들이 대화를 떠나게 만들 가능성을 측정한다. 모델 응답의 toxicity 확률 분포를 보면, 이슬람과 유대교는 \u0026ldquo;All { practitioners } are\u0026quot;이라는 프롬프트에 이어 toxicity 반응을 생성할 확률이 더 높다. 또한, 특정 무해한 언급에 대해 높은 toxicity를 부여하는 Perspective API의 사회적 bias에 의존하고 있다.\nbias와 toxicity 평가는 모든 언어 모델에 대해 완전히 적용되지는 않지만, 잠재적인 위험에 대한 중요한 통찰력을 제공한다. 결과의 변동성은 프롬프트 언어의 작은 변화에 매우 취약한 템플릿 기반 접근법을 보여주며, bias를 측정하고 완화 전략을 결정하기 위해 견고한 벤치마크와 지표가 필요함을 강조한다.\nToxicity in open-ended generation Toxicity degeneration는 언어 모델이 toxicity로 인식하는 텍스트를 만드는 것이다. 이를 평가하기 위해, RealToxicityPrompts 데이터셋을 활용하고, Perspective API를 통해 계속되는 내용에 toxicity 확률을 부여하다. 그 후, 프롬프트가 toxicity일 가능성에 따른 모델 응답의 toxicity 확률 분포를 연구하다.\n무작위로 추출한 1만 개의 프롬프트에 대해 각각 25개의 연속문을 생성하였다. 이때는 최대 128개의 디코딩 단계를 사용하였고, top-k 샘플링과 1.0의 온도를 적용하였다. 하지만 여러 디코딩 단계를 사용하더라도, 첫 번째 완전한 문장의 toxicity 지표만을 보고하였다. 이는 인간의 단일장 연속문을 기준으로 하며, 텍스트 길이에 따라 toxicity 점수가 증가하는 경향 때문이다.\n다양한 모델 크기에 따른 toxicity probability of the prompt(TPP) 함수로서의 toxicity probability of the continuation(TPC)을 보여준다. TPC는 TPP와 함께 증가하는 경향이 있지만, 프롬프트의 toxicity나 인간 기준선보다는 일관되게 낮다. 8B 모델과 더 큰 모델들(62B와 540B) 사이에서 toxicity 확률이 증가하였고, 이는 toxicity 수준과 모델 크기 사이에 상관관계가 있음을 시사한다. 모델의 TPC는 인간의 TPC보다 TPP와 더 일관성이 있고, 이는 모델이 프롬프트 스타일에 크게 영향을 받아, 프롬프트와 유사한 toxicity 수준의 연속문을 생성할 가능성이 높다는 것을 나타낸다.\nTPC는 이전 연구보다는 낮지만, 이는 첫 번째 완전한 문장에 toxicity 측정을 제한했기 때문일 뿐, 모델이 toxicity한 내용을 생성하는 경향이 낮다는 것을 의미하지는 않는다. 무작위로 샘플링된 프롬프트와 연속문의 길이 때문에 이전 작업과의 직접적인 비교는 어렵다.\nLimitations 이 섹션의 공정성 분석은 영어 데이터에만 국한되어 있지만, PaLM은 다양한 언어 데이터에 대해 학습되고 평가되기 때문에 주요한 한계를 가지고 있다. 전 세계적으로 사용되는 언어 기술에 대한 편향 벤치마크의 개발과 활용이 중요하며, 서구 세계에서 개발된 공정성 평가는 다른 지역-문화 맥락으로 쉽게 이동할 수 없을 수 있다. 따라서, 현재 측정 가능한 것 이상의 잠재적인 bias가 존재할 수 있음을 인지해야 한다.\n영어 언어 기술의 편향성에 대한 연구가 증가하고 있지만, 공정성 벤치마크의 표준화, NLP의 편향 측정과 관련된 해의 이해, 그리고 포괄적인 방식으로 다양한 정체성을 다루는 것에 대한 표준이 부족하다. 이러한 이유로, 공정성 평가는 한계를 가지고 있으며, 측정 가능한 것 이상의 잠재적 위험이 존재한다. 이 논문의 평가는 대명사 해결과 공존 분석과 같은 인기있는 작업에 제한되어 있으며, 이러한 벤치마크는 번역, 코드 생성, 상식 추론 등의 작업에서의 편향 유형을 대표할 수 있다.\nbias는 구체적인 응용 프로그램, 학습 과정, 그리고 보호 조치에 따라 시스템에 영향을 미칠 수 있다. 모델 사용 방식에 따라 사전 학습된 모델의 bias가 다양한 영향을 미칠 수 있으며, 모델 미세 조정 후의 downstream task 평가에 어떤 영향을 미치는지는 명확하지 않다. 그래서 배포 전에 응용 프로그램에서의 공정성 격차를 평가하기 위한 적절한 조치를 취하는 것이 중요하다.\nEthical Considerations 대규모 고품질 언어 모델링은 건강관리와 교육 등 실제 세계 응용 프로그램의 가능성을 제공한다.\n최근 연구에서는 웹 텍스트에 대해 학습된 대규모 언어 모델이 사회적 bias를 악화시키거나, 개인 정보를 노출하거나, downstream에서 해를 입힐 수 있다는 여러 잠재적 위험을 지적하였다. 이러한 bias를 완전히 제거하는 것은 불가능할 수 있으므로, 이런 부적절한 연관성과 위험을 분석하고 기록하는 것이 중요하다. 이를 위해, 데이터셋과 모델 출력의 철저한 분석을 수행하고, PaLM의 사용자들에게 더 큰 투명성을 제공하기 위해 데이터시트와 모델 카드를 제공한다.\n학습 데이터와 PaLM 모델이 다양한 사회적 stereotype과 toxicity를 반영하고 있다는 것을 보여준다. 그러나 이러한 연관성을 제거하는 것은 간단하지 않으며, 자동화 도구를 이용해 toxicity를 갖는 콘텐츠를 필터링하면 소외된 그룹의 콘텐츠가 과도하게 배제될 수 있다. 이러한 bias를 효과적으로 처리하고 그 영향을 연구하는 것이 필요하며, PaLM을 실제 작업에 사용할 때는 추가적인 공정성 평가를 수행해야 한다.\n공정성 분석은 범위가 좁아 다양한 잠재적 위험을 전체적으로 설명하지 못한다. 성별, 인종, 민족, 종교 등의 축을 따라 bias를 분석하지만, 이는 영어 데이터와 모델 출력에만 적용된다. 성적 지향성, 장애 등의 다른 사회적 불평등 축이나 비서구 사회 문화 맥락에서 중요한 편향은 고려하지 않았다. 따라서, 잠재적 위험을 의미있게 평가하려면, 대상 응용 분야와 사회 문화 맥락에 관련된 불평등의 축을 따라 공정성 분석이 필요하다.\n이 논문의 분석은 데이터와 모델의 편향에 초점을 맞추지만, 이들이 실제로 어떻게 사용되는지에 따라 downstream에서의 피해는 달라질 수 있다. 예를 들어, toxicity 콘텐츠가 학습 데이터에 포함되어 있는 것은 바람직하지 않아 보일 수 있지만, PaLM이 toxicity 콘텐츠를 감지하는데 사용된다면, 이러한 콘텐츠에 대한 사전 학습은 중요하다고 볼 수 있다.\nPaLM의 언어 능력은 학습 데이터와 평가 벤치마크의 언어 한계에 의해 제한될 수 있다. 벤치마크 평가는 종종 언어 이해 능력의 전체 복잡성을 완전히 포착하지 못하며, 그들이 측정하려는 것과 실제로 측정하는 것 사이에 차이가 있다. 따라서, 다른 실세계 응용 프로그램 상황에서 동일한 성능 수준이 보장되지 않을 수 있다.\nPaLM은 평가한 벤치마크에서 다국어 능력을 보여주지만, 대부분은 영어로 이루어진 벤치마크이다. 비영어 언어에서의 성능과 bias에 대한 더욱 견고한 평가가 필요하다. 학습 데이터셋의 웹 페이지는 품질을 평가하기 위해 필터링되었는데, 이로 인해 일상적인 언어, 코드 스위칭, 방언의 다양성 등이 과도하게 배제되었을 수 있다. 또한, PaLM은 특정 시점의 언어 사용을 나타내서 현재의 일상 언어나 속어를 모델링하는 작업에 성능이 떨어질 수 있다. 표준 벤치마크는 언어 데이터의 다양한 측면을 포착하거나 구분하지 않아, 이 부분에서 PaLM의 능력을 평가하는 것은 어렵다.\n모델의 다양한 대표성 bias와 능력 차이를 완화한 후에도, 인간의 언어 행동을 모방하는 대규모 언어 모델이 악용될 가능성이 있다는 것을 기억하는 것이 중요하다. 이러한 고품질 언어 생성 능력은 오보 캠페인 등의 악의적인 용도로 사용될 수 있고, 온라인에서 소외된 그룹을 괴롭히는 데도 사용될 수 있다. 이러한 위험은 PaLM 뿐만 아니라 대부분의 대규모 언어 모델에 존재하므로, 이러한 악의적인 용도를 방지할 수 있는 확장 가능한 해결책에 대한 노력이 필요하다.\n소프트웨어 개발 지원을 위한 PaLM-Coder의 배포는 복잡성과 윤리적 고려사항을 수반한다. 언어 모델 기반 제안이 정확하고 견고하며 안전하고 보안이 확보된 것을 보장하고, 개발자들이 이를 확신하는 것은 아직 해결되지 않은 문제이다.\nRelated Work 대규모 언어 모델링을 통해 자연 언어 능력이 크게 발전하였다. 이는 시퀀스에서 다음 토큰을 예측하거나 마스킹된 영역을 예측하는 방법으로, 인터넷, 책, 포럼에서 얻은 방대한 데이터에 적용되었다. 이로 인해 고급 언어 이해와 생성 능력을 가진 모델이 개발되었고, 데이터, 매개변수, 계산량의 확장을 통해 모델 품질의 예측 가능한 향상이 이루어졌다.\nTransformer 아키텍처는 현대 accelerator에서 높은 효율성을 보여주며 언어 모델의 기본적인 접근법이 되었다. 4년 동안, 최대 모델의 크기와 계산량은 몇 배로 증가했다. BERT, GPT 시리즈 등 다양한 모델이 등장하며 언어 이해와 모델링 성능이 크게 향상되었다. 또한, 코드 이해 및 생성, 대화 응용 등 여러 분야에서도 개선이 이루어졌다. 최근에는 언어 모델이 지시사항을 따르도록 하는 연구를 통해 이러한 모델의 유용성과 신뢰성이 더욱 향상되었다.\n큰 모델들은 단일 accelerator에 효율적으로 학습하거나 적용하기 어렵다. 이에 따라, 모델 텐서를 가속기 간에 분할하거나, 모델 계층을 accelerator 간에 분리하고 activation을 파이프라인화하는 기술이 등장하였다. 여러 연구들이 모델 규모를 늘리면서 통신 오버헤드를 제한하는 것을 목표로 하고 있으며, PaLM은 Pathways 인프라를 통해 데이터와 모델 병렬화를 혼합하여 사용한다.\n모델을 효율적으로 확장하기 위한 아키텍처 변형이 제안되었다. 대량의 텍스트를 임베딩하여 모델 크기를 줄이는 검색 모델, 다른 예시가 parameter의 다른 부분 집합을 사용하게 하는 모델 희소성, 그리고 극도로 긴 시퀀스로 효율적인 학습을 가능하게 하는 시퀀스 길이의 희소성 등이 포함된다. 이러한 연구의 개선 사항들이 미래의 Pathways 언어 모델에 통합될 수 있을 것이다.\nOpen Questions in Scaling few-shot learning 기반으로 학습하는 대규모 언어 모델의 품질 향상은 모델의 깊이와 너비, 학습된 토큰의 수, 학습 코퍼스의 품질, 그리고 계산량 증가 없이 모델 용량을 증가시키는 방법 등 네 가지 주요 요인에 의해 이루어졌다. 이 중 하나인 학습 코퍼스의 품질이 주요 요인으로 작용할 수 있음이 나타났으며, 신중한 데이터 필터링을 통한 few-shot learning 기반으로 한 학습 향상이 매우 중요함이 밝혀졌다.\n학습 비용이 높아서, 모델의 깊이와 너비와 학습된 토큰 수의 효과를 분리하는 연구를 수행하지 못했다. 즉, \u0026ldquo;7T 토큰으로 학습된 62B parameter 모델과 780B 토큰으로 학습된 540B parameter 모델은 어떻게 비교될까?\u0026ldquo;라는 질문에 대한 답을 아직 찾지 못했습니다. 이러한 모델은 PaLM 540B와 비슷한 학습 비용을 가지지만, 추론 비용이 그 크기에 비례하기 때문에 더 작은 모델이 선호될 것이다.\n최근 연구에서는 1.4T 토큰의 데이터로 학습된 70B parameter 모델인 Chinchilla와 300B 토큰의 데이터로 학습된 280B parameter 모델인 Gopher를 비교하였다. 두 모델은 유사한 학습 비용을 가지지만, Chinchilla는 다양한 언어 작업에서 Gopher를 큰 차이로 능가하였다.\nChinchilla와 PaLM, 두 모델의 결과를 비교하였다. 두 모델은 58개의 BIG-bench 작업과 9개의 영어 NLP 작업에서 비슷한 결과를 보여주었다.\nChinchilla는 BIG-bench에서 PaLM의 스케일링 곡선을 약간 능가하고, 9개의 영어 NLP 작업에서는 약간 미치지 못하는 반면, Gopher는 두 스케일링 곡선 모두를 크게 미치지 못했다. Gopher와 Chinchilla는 동일한 학습 코퍼스를 사용했지만, PaLM은 다른 코퍼스를 사용하여 비교가 복잡해졌다. 이 결과는 Gopher가 그 크기의 모델에 대해 학습이 부족했음을 보여주지만, \u0026ldquo;X 크기의 모델이 Y 토큰으로 훈련되면 PaLM 540B와 어떻게 비교될까?\u0026ldquo;라는 질문에 대한 답을 추론하기에는 충분하지 않다. 이것이 어려운 질문이 된 이유는 여러 가지이다:\n강력한 결론을 도출하려면 큰 규모의 실험이 필요하며, 이는 높은 계산 비용을 요구한다. 더 작은 모델이 더 적은 TPU 칩으로 학습된다면, 학습 시간이 비례적으로 증가할 것이다. 같은 수의 TPU 칩으로 학습된다면, 배치 크기를 크게 늘리지 않으면 TPU 계산 효율성을 유지하기 어렵다. PaLM 540B의 배치 크기는 이미 4M 토큰인데, 이보다 더 큰 배치 크기가 효율성을 유지할 수 있을지는 불확실하다. 웹에서는 무한한 양의 고품질 텍스트 데이터가 있지는 않다. PaLM에서는 780B 토큰 이후 일부 데이터가 반복되는 것을 확인했고, 이는 학습의 종료 지점으로 설정한 이유이다. 반복된 데이터의 가치와 보지 않은 데이터의 가치를 비교하는 것은 불확실하지만, 새로 갱신된 데이터셋에서 더 오래 학습하면 성능이 향상되는 것을 확인하였다. 향후 연구에서는 다양한 작업에 잘 적용되는 뛰어난 언어 모델을 만드는 데 영향을 미치는 여러 요인들 사이의 균형에 대해 조사할 계획이다. 이는 모델 아키텍처, 사전 학습 작업, 최적화 설정 등의 추가적인 요인에 대한 연구를 포함한다.\nConclusion 이 연구에서는 고품질, 다양한 텍스트로 학습된 대규모 언어 모델인 PaLM을 사용하여 few-shot 언어 이해와 생성의 가능성을 확장하였다. 이 모델은 29개의 주요 영어 NLP 작업 중 28개에서 state-of-the-art를 달성했으며, 150개 이상의 새로운 언어 작업을 포함하는 BIG-bench에서는 인간의 평균 성능을 능가하였다. 또한 소스 코드 이해, 다국어 NLP, 기계 번역 등 다양한 분야에서도 state-of-the-art를 달성하였다.\n이 연구의 중요한 결과 중 하나는 복수 단계의 논리적 추론을 필요로 하는 작업에서 뛰어난 성능을 보였다는 것이다. 다양한 산술 및 상식 추론 작업에서 state-of-the-art를 달성하였으며, 이는 단순히 모델 규모 확대 뿐 아니라, 예측 전에 논리적 추론 과정을 명시적으로 생성하도록 하는 방식을 통해 이루어졌다. PaLM은 농담을 설명하고 복잡한 시나리오에 대한 질문에 답하는 등의 작업에서 논리적 추론 과정을 구체적으로 표현할 수 있었다.\n이 연구 결과는 few-shot 언어 이해를 위한 모델 규모 확대의 효과가 아직 정체되지 않았음을 보여준다. 동일한 학습 방법을 사용한 다른 모델들과 비교했을 때, 규모의 증가와 성능 향상이 log-linear 관계를 보였고, 특정 벤치마크에서는 더 큰 모델로 확대했을 때 불연속적인 성능 향상이 관찰되었다. 이는 특정 언어 모델의 기능이 충분한 규모에서만 나타나며, 미래의 모델에서는 추가적인 능력이 나타날 수 있음을 시사한다.\n추론 작업에서의 뛰어난 성능은 중요한 의미를 가지고 있다. 모델이 예측을 설명하는 자연어를 생성하는 것은 사용자가 모델의 예측 이유를 이해하는 데 도움이 되며, 더불어 모델에게 명확한 추론 과정을 생성하도록 요청함으로써 예측의 품질이 크게 향상될 수 있음을 보여준다. 즉, 모델의 언어 생성능력은 언어 생성을 크게 필요로 하지 않는 분류나 회귀와 같은 작업에서도 매우 유익할 수 있다.\nfew-shot 언어 모델링의 규모를 확대하는 목표를 달성했지만, 미래 모델에 대한 최적의 네트워크 구조와 훈련 방식에 대한 여전히 많은 미해결 문제가 있다. PaLM은 Google의 ML 확장 미래 비전인 Pathways 설립의 첫 단계일 뿐이다. 이 규모 확대 능력을 잘 알려진 full-attention transformer 모델에서 보여주었으며, 더 넓은 목표는 다양한 새로운 구조와 학습 방식을 탐구하고, 가장 유망한 시스템을 Pathways의 확장 능력과 결합하는 것이다. PaLM은 여러 모달리티에 걸친 일반화 능력을 가진 대규모, 모듈화된 시스템을 개발하는 우리의 최종 목표에 강한 기반을 제공한다고 믿는다.\nReference Paper Github ","date":"2024-01-27T00:00:00Z","permalink":"https://kurtkim.github.io/p/palm/","title":"PaLM"},{"content":"Abstract 이 논문은 인간의 피드백을 통해 언어 모델을 미세 조정함으로써 사용자의 의도에 더 잘 부합하도록 만드는 방법을 제시한다. 라벨러가 작성한 프롬프트와 OpenAI API를 통해 제출된 프롬프트를 이용해서 데이터셋을 수집하고, 이를 GPT-3를 지도 학습으로 미세 조정하는데 사용하였다. 그 다음, 모델 출력의 순위를 나타내는 데이터셋을 수집하고, 이를 인간의 피드백에서 강화 학습을 통해 이 감독 모델을 더 미세 조정하였다. 이 결과로 나온 모델을 InstructGPT라고 부른다. 인간의 평가에서는 parameter 수가 훨씬 적은 1.3B parameter를 가진 InstructGPT 모델의 출력이 175B GPT-3의 출력보다 선호되었다.\nIntroduction 대규모 언어 모델은 다양한 자연어 처리 작업을 수행할 수 있지만, 종종 사실을 만들어내거나 편향된 텍스트를 생성하거나 사용자의 지시를 따르지 않는 등의 문제가 있다. 이는 언어 모델링의 목표와 \u0026ldquo;사용자의 지시를 도움이 되고 안전하게 따르라\u0026quot;는 목표가 서로 다르기 때문인데, 이를 \u0026ldquo;목표의 불일치(misaligned)\u0026ldquo;라고 한다. 이러한 문제를 피하는 것은 수많은 애플리케이션에서 사용되는 언어 모델에게 매우 중요하다.\n사용자의 의도에 따라 언어 모델을 학습시키는 방법을 통해 언어 모델의 alignment를 개선하고 있다. 이는 지시 사항을 따르는 것과 같은 명시적인 의도 뿐만 아니라, 사실적이고 편향되지 않으며 독성이 없거나 해로운 행동을 하지 않는 것과 같은 암시적인 의도를 포함한다. 언어 모델이 사용자의 작업을 해결하는 데 도움이 되고, 정보를 조작하거나 사용자를 오도하지 않으며, 사람이나 환경에 해를 끼치지 않는 모델을 목표로 한다.\n언어 모델을 사용자의 의도와 맞추기 위해, 강화 학습을 통해 GPT-3을 미세 조정하는 방법에 초점을 맞추었다. 인간의 피드백을 보상 신호로 사용하여 모델을 미세 조정하였다.\nOpenAI API에 제출된 프롬프트에 대한 원하는 출력 행동의 인간이 작성한 시연 데이터셋을 수집하였고, 이를 사용하여 지도 학습 기본선을 훈련시켰다. 또한, 더 큰 세트의 API 프롬프트에서 모델의 출력 사이의 인간이 라벨링한 비교 데이터셋을 수집하였다. 이 데이터를 통해 보상 모델을 학습시키고, 이를 보상 함수로 사용하여 지도 학습 기준을 미세 조정하였습니다. 이 결과로 만들어진 모델을 InstructGPT라고 부른다.\n주로 학습 데이터에 포함되지 않은 고객의 프롬프트로 구성된 테스트 세트에서 모델 출력의 품질을 평가한다. 또한, 다양한 공공 NLP 데이터셋에서 자동 평가를 실시한다. 세 가지 모델 크기(1.3B, 6B, 175B parameter)를 학습시키며, 모든 모델은 GPT-3 아키텍처를 사용한다.\nLabelers signiﬁcantly prefer InstructGPT outputs over outputs from GPT-3. 테스트 세트에서, 1.3B parameter를 가진 InstructGPT 모델의 출력은 100배 이상의 적은 parameter에도 불구하고 175B GPT-3의 출력보다 선호되었다. 175B InstructGPT의 출력은 85 $\\pm$ 3%의 시간 동안 175B GPT-3의 출력보다, 그리고 71 $\\pm$ 4%의 시간 동안 few-shot 175B GPT-3에 대해 선호되었다. InstructGPT 모델은 또한 더 적절한 출력을 생성하고, 지시사항의 명시적 제약을 더 신뢰성 있게 따랐다.\nInstructGPT models show improvements in truthfulness over GPT-3. TruthfulQA 벤치마크에서, InstructGPT는 GPT-3보다 두 배 이상 사실적이고 유익한 답변을 생성한다. \u0026ldquo;closed-domain\u0026rdquo; 작업에서, InstructGPT 모델은 GPT-3에 비해 입력에 없는 정보를 만들어내는 빈도가 절반 정도로 줄었다(각각 21% vs. 41%의 hallucination).\nInstructGPT shows small improvements in toxicity over GPT-3, but not bias. toxicity를 측정하기 위해, RealToxicityPrompts 데이터셋을 사용하여 자동 및 인간 평가를 실시하였다. 존중스럽게 행동하도록 요청했을 때, InstructGPT 모델은 GPT-3보다 약 25% 적은 toxicity 출력을 생성했다. 하지만, InstructGPT는 Winogender와 CrowSPairs 데이터셋에서 GPT-3보다 크게 개선되지 않았다.\nWe can minimize performance regressions on public NLP datasets by modifying our RLHF ﬁne-tuning procedure. RLHF 미세 조정 중에, 특정 공공 NLP 데이터셋에서 GPT-3에 비해 성능이 떨어지는 것을 관찰하였다. 이는 \u0026ldquo;정렬 세금\u0026quot;의 예시로, 특정 작업에서의 성능 하락 비용을 수반합니다. 그러나, 라벨러 선호 점수를 손상시키지 않으면서 이러한 데이터셋에서의 성능 하락을 크게 줄일 수 있습니다. 이는 사전 훈련 분포의 로그 가능도를 증가시키는 업데이트와 PPO 업데이트를 혼합함으로써 가능합니다.\nOur models generalize to the preferences of “held-out” labelers that did not produce any training data. 초기 실험에서, 보류된 라벨러들은 학습 라벨러들과 비슷한 비율로 InstructGPT의 출력을 GPT-3의 출력보다 선호하는 것으로 나타났다. 그러나, 이 모델들이 더 넓은 사용자 그룹에서 어떻게 작동하고, 사람들이 원하는 행동에 대해 의견이 분분한 경우 어떻게 작동하는지에 대한 추가 연구가 필요하다.\nPublic NLP datasets are not reﬂective of how our language models are used. 인간 선호 데이터에 따라 조정된 GPT-3 (InstructGPT)는 다양한 공개 NLP 작업에 따라 조정된 GPT-3인 FLAN과 T0 모델과 비교하였다. 결과적으로, InstructGPT는 FLAN과 T0 모델보다 더 선호되었으며, API 프롬프트 분포에서 이들 모델보다 더 좋은 성능을 보였주였다.\nInstructGPT models show promising generalization to instructions outside of the RLHF ﬁnetuning distribution. InstructGPT는 코드를 요약하고 코드에 대한 질문에 답하는 등의 지시사항을 따르며, 미세 조정된 분포에서 드물게 나타나는 다른 언어의 지시사항도 따르는 능력이 있다. 반면에, GPT-3는 이런 작업을 수행하기 위해 더 신중한 프롬프팅이 필요하며, 일반적으로 이런 영역에서의 지시사항을 따르지 않는다. 이 결과는 우리의 모델이 \u0026ldquo;지시사항을 따르는 (following instructions)\u0026rdquo; 개념을 일반화하는 능력을 가지고 있음을 보여준다.\nInstructGPT still makes simple mistakes. InstructGPT는 여전히 지시사항을 따르지 못하거나, 사실을 만들어내거나, 간단한 질문에 대해 긴 답변을 제공하거나, 거짓 전제를 가진 지시사항을 인식하지 못하는 문제가 있다.\n인간의 선호도를 사용하여 대규모 언어 모델을 세밀하게 조정하면 다양한 작업에서 그들의 행동이 크게 개선되지만, 안전성과 신뢰성을 향상시키기 위해 더 많은 작업이 필요하다.\nRelated work Research on alignment and learning from human feedback. 이 연구는 인간의 의도에 따라 모델을 조정하는 기존 기술, 특히 인간의 피드백으로부터의 강화학습에 기반을 두고 있다. 이 방법은 원래 간단한 로봇 훈련에 사용되었으며, 최근에는 텍스트 요약을 위한 언어 모델 조정에 적용되었다. 이 방법은 다양한 영역에서 인간의 피드백을 보상으로 사용하는 유사한 작업에 영향을 받았다. 광범위한 언어 작업에 대한 언어 모델의 정렬을 위한 강화학습의 직접적인 적용으로 볼 수 있다.\n언어 모델이 alignment되는 것이 무슨 의미인지에 대한 질문이 최근 주목 받았다. 일부 연구에서는 alignment의 부재로 인한 언어 모델의 행동적 문제를 분석하였고, 다른 연구에서는 alignment 연구를 위한 언어 보조 도구를 제안하고, 간단한 기준과 스케일링 속성을 연구하였다.\nTraining language models to follow instructions. 이 연구는 언어 모델에서 다양한 공공 NLP 데이터셋에 대해 세밀하게 조정하고, 다른 NLP 작업 세트에서 평가하는 크로스태스크 일반화 연구와 관련이 있다. 여러 연구에서 일관적으로 나타난 결과는, 지시사항과 함께 NLP 작업의 범위에서 언어 모델을 세밀하게 조정하는 것이 보류된 작업에 대한 성능을 향상시킨다는 것이다. 이는 zero-shot과 few-shot 설정 모두에 적용된다.\n또한, 시뮬레이션된 환경에서 모델이 자연어 지시사항을 따라 탐색하도록 학습하는 탐색을 위한 지시사항 따르기에 관한 연구도 있다.\nEvaluating the harms of language models. 언어 모델의 행동을 수정하는 목표는 이들이 실세계에 배포될 때 발생할 수 있는 위험을 줄이는 것이다. 언어 모델은 편향된 출력을 생성하거나, 개인 데이터를 유출하거나, 잘못된 정보를 생성하거나, 악의적으로 사용될 수 있다. 이러한 위험을 구체적으로 평가하기 위한 벤치마크를 구축하는 연구가 진행되고 있다. 하지만, 이러한 문제를 해결하는 것은 어렵다. 왜냐하면 언어 모델의 행동을 개선하려는 노력이 부작용을 가져올 수 있기 때문이다. 예를 들어, 모델의 toxicity를 줄이려는 노력은 대표성이 부족한 그룹의 텍스트를 모델링하는 능력을 줄일 수 있다.\nModifying the behavior of language models to mitigate harms. 언어 모델의 생성 행동을 변경하는 방법은 다양하다. 이에는 소규모 가치 중심 데이터셋에 대해 언어 모델을 미세 조정하거나, 트리거 구문을 생성할 확률이 높은 문서를 사전 학습 데이터셋에서 제거하는 방법, 데이터 필터링이나 특정 단어 차단, 안전성 특정 제어 토큰 사용 등이 포함된다. 또한, 단어 임베딩 규제화, 데이터 증가, 민감한 토큰에 대한 분포를 균일하게 만드는 방법 등을 사용하여 언어 모델이 생성하는 편향을 완화하는 다양한 접근법이 있다. 이 밖에도, 두 번째 언어 모델을 사용하여 생성을 조정하거나, 언어 모델의 toxicity를 줄이는 등의 방법이 있다.\nMethods and experimental details High-level methodology 이 연구의 방법론은 스타일 연속성과 요약 분야에 적용된 이전의 연구를 따른다. 사전 학습된 언어 모델, alignment된 출력을 생성하고자 하는 프롬프트 분포, 그리고 학습된 인간 라벨러 팀을 기반으로 하는 세 가지 단계를 적용한다.\nStep 1: Collect demonstration data, and train a supervised policy. 라벨러들은 입력 프롬프트 분포에 대한 원하는 행동의 예시를 제공하고, 이 데이터를 바탕으로 사전 학습된 GPT-3 모델을 미세 조정한다.\nStep 2: Collect comparison data, and train a reward model. 모델 출력 간의 비교 데이터셋을 수집하고, 이를 바탕으로 인간이 선호하는 출력을 예측하는 보상 모델을 학습시킨다.\nStep 3: Optimize a policy against the reward model using PPO. RM의 출력을 스칼라 보상으로 사용한다. 이 보상을 최적화하기 위해 PPO 알고리즘을 사용하여 supervised policy를 미세 조정한다.\nSteps 2와 3은 계속 반복될 수 있다. 가장 좋은 정책에 대한 추가적인 비교 데이터가 수집되어 새로운 보상 모델과 정책을 학습시키는 데 사용된다. 대부분의 비교 데이터는 supervised policy에서, 일부는 PPO policy에서 나온다.\nDataset 프롬프트 데이터셋은 주로 OpenAI API에 제출된 텍스트 프롬프트로 구성되어 있다. Playground 인터페이스에서 사용된 InstructGPT 모델의 이전 버전을 사용한 것들이다. 프롬프트는 중복 제거되며, 사용자 ID당 200개로 제한된다. 학습, 검증, 테스트 분할은 사용자 ID를 기반으로 생성되며, 검증 및 테스트 세트는 학습 세트에 있는 사용자의 데이터를 포함하지 않는다. 또한, 모델이 민감한 고객 정보를 학습하는 것을 피하기 위해, 학습 분할의 모든 프롬프트는 개인 식별 정보에 대해 필터링된다.\n최초의 InstructGPT 모델을 학습시키기 위해, 라벨러들이 스스로 프롬프트를 작성하도록 요청했다. 이는 초기 지시사항과 같은 프롬프트가 필요했기 때문이며, 라벨러들은 세 가지 종류의 프롬프트를 작성하도록 요청받았다.\nPlain: 단순히 라벨러들에게 임의의 작업을 생각해내도록 요청하면서, 작업이 충분한 다양성을 가지도록 하였다. Few-shot: 라벨러들에게 지시사항을 생각해내고, 그 지시사항에 대한 여러 질문/응답 쌍을 생각해내도록 요청하였다. User-based: OpenAI API의 대기 목록 신청서에는 여러 사용 사례가 명시되어 있었다. 라벨러들에게 이러한 사용 사례에 해당하는 프롬프트를 생각해내도록 요청하였다. 이 프롬프트로부터, 세 가지 데이터셋을 생성하여 튜닝에 사용한다: (1) 라벨러의 시연을 사용한 SFT 데이터셋, (2) 모델 출력의 라벨러 순위를 사용한 RM 데이터셋, 그리고 (3) 인간 라벨이 없는 PPO 데이터셋이다. SFT 데이터셋에는 약 13k의 학습 프롬프트가 있고, RM 데이터셋에는 33k의 학습 프롬프트가 있으며, PPO 데이터셋에는 31k의 학습 프롬프트가 있다.\nTasks 학습 작업은 라벨러가 작성한 프롬프트와 API에서 초기 InstructGPT 모델에 제출된 프롬프트 두 가지로부터 나온다. 이 프롬프트들은 다양하며, 생성, 질문 응답, 대화, 요약, 추출 등 다양한 자연어 작업을 포함한다. 데이터셋은 96% 이상이 영어이지만, 다른 언어로의 지시에 대한 응답과 코딩 작업 완료 능력도 조사한다.\n각 프롬프트에 대한 작업은 대부분 자연어 지시를 통해 직접 지정되며, 간접적으로는 예시를 통하거나 암시적 연속성을 통해 할 수도 있다. 라벨러들은 프롬프트를 작성한 사용자의 의도를 추론하도록 요청받으며, 불분명한 작업은 건너뛰게 된다. 또한, 라벨러들은 응답의 진실성과 같은 암시적 의도, 그리고 편견이나 유해한 언어와 같은 potentially harmful outputs를 고려한다.\nHuman data collection 시연과 비교 데이터를 생성하고 주요 평가를 수행하기 위해 Upwork와 ScaleAI를 통해 계약자 팀을 고용하였다. 다양한 작업 범위를 다루며, 때때로 논란이 될 수 있는 민감한 주제도 포함된다. 다양한 인구집단의 선호도에 민감하고, 잠재적으로 해롭다고 판단되는 출력을 잘 식별하는 라벨러를 선택하려 하였다. 이를 위해, 라벨러의 성능을 측정하는 스크리닝 테스트를 실시하고, 이 테스트에서 잘 수행한 라벨러를 선택하였다.\n학습과 평가 중에, 사용자를 돕는 것과 진실성 및 무해함 사이에 충돌이 발생할 수 있다. 학습 중에는 사용자 도움을 우선시하지만, 최종 평가에서는 진실성과 무해함을 우선시한다.\n라벨러들과 밀접하게 협력하여 프로젝트를 진행하며, 라벨러 학습을 위한 온보딩 프로세스를 진행하고, 각 작업에 대한 상세한 지침을 제공하며, 공유 채팅방에서 라벨러의 질문에 답한다.\n모델이 다른 라벨러들의 선호도에 얼마나 잘 적응하는지 확인하기 위한 초기 연구로, 학습 데이터를 만들지 않은 별도의 라벨러 그룹을 고용하였다. 이들은 같은 공급업체에서 온 라벨러지만, 별도의 스크리닝 테스트는 거치지 않았다.\n작업의 복잡성에도 불구하고 라벨러 간 동의율이 높았다. 온보딩 프로세스를 진행한 라벨러들은 72.6% $\\pm$ 1.5%의 경우에 동의하였고, 보류된 라벨러들은 77.3 $\\pm$ 1.3%의 경우에 동의하였다. 이는 이전의 요약 작업에서 연구자들 간의 동의율인 73 $\\pm$ 4%와 비슷한 수준이다.\nModels 인터넷 데이터에 대해 학습된 GPT-3 사전 학습 언어 모델을 사용하여 시작하였고, 이를 바탕으로 세 가지 다른 기법을 사용하여 모델을 학습시켰다.\nSupervised ﬁne-tuning (SFT). GPT-3를 라벨러의 시연에 대해 지도학습으로 미세 조정하였다. 16 epoch 동안 학습하였고, 검증 세트에서의 RM 점수를 기반으로 최종 모델을 선택하였다. 1 epoch 후에 overﬁt이 발생했지만, 더 많은 epoch 동안 학습하면 RM 점수와 인간 선호도 평가가 개선되었다.\nReward modeling (RM). 최종 unembedding 레이어를 제거한 SFT 모델을 시작으로, 프롬프트와 응답을 입력으로 받아 스칼라 보상을 출력하는 모델을 학습시켰다. 계산량 절약과 학습의 안정성을 위해, 6B RM만 사용하였다.\nStiennon et al. (2020)의 연구에서, 보상 모델은 같은 입력에 대한 두 모델 출력의 비교를 통해 학습되었다. cross-entropy loss를 사용하며, 보상의 차이는 한 응답이 다른 응답보다 사람에게 선호될 가능성을 나타낸다.\n비교 수집을 가속화하기 위해, 라벨러에게 4에서 9개의 응답을 랭킹하도록 요청하였다. 이는 각 프롬프트에 대해 여러 비교를 생성한다. 단일 데이터셋으로 비교를 섞으면 보상 모델이 overfit되는 것을 발견했기 때문에, 각 프롬프트에서의 모든 비교를 단일 배치 요소로 학습시켰다. 이 방법은 계산 효율이 높고, overfit이 발생하지 않아 검증 정확도와 log loss이 향상되었다.\n구체적으로, 보상 모델에 대한 손실 함수는 다음과 같다:\n$$ loss(\\theta) = - {{1}\\over{\\begin{pmatrix} K \\\\ 2 \\end{pmatrix}}} E_{(x, y_w, y_t) \\sim D} [log \\ (\\sigma (r_{\\theta} (x, y_w) - r_{\\theta}(x, y_t)))] $$\n$\\theta(x, y)$는 보상 모델의 출력이고, $y_w$는 선호하는 완성이다. $D$는 비교 데이터셋이다.\nRM loss는 보상 변화에 불변하므로, RL 전에 라벨러 시연의 평균 점수가 0이 되도록 보상 모델을 정규화한다.\nReinforcement learning (RL). PPO를 사용해서 SFT 모델을 미세 조정하였다. 랜덤한 고객 프롬프트를 제시하고 응답을 기대하며, 보상 모델로부터 보상을 생성한다. SFT 모델로부터의 KL penalty를 추가해 보상 모델의 과도한 최적화를 완화하였다. 가치 함수는 RM에서 초기화되며, 이 모델들을 \u0026ldquo;PPO\u0026quot;라고 부른다.\n공개 NLP 데이터셋의 성능 감소를 해결하기 위해, 사전 학습 gradient를 PPO gradient와 혼합하는 것을 시도하였다. 이 모델들을 \u0026ldquo;PPO-ptx\u0026quot;라고 부른다. RL 학습에서 다음의 결합된 목표 함수를 최대화한다:\n$$ objective(\\Phi) = E_{(x,y) \\sim D_{\\pi \\underset{\\Phi}{RL}}} [r_{\\theta}(x,y) - \\beta log \\ (\\pi_{\\Phi}^{RL}(x|y) / \\pi^{SFT}(x|y))] + \\gamma E_{x \\sim D_{pretrain}} [log \\ (\\pi_{\\Phi}^{RL}(x))] $$\n$\\pi_{\\Phi}^{RL}$는 학습된 RL 정책, $\\pi^{SFT}$는 지도 학습 모델, $D_{pretrain}$은 사전 학습 분포이다. KL reward coefﬁcient $\\beta$와 사전 학습 손실 계수 $\\gamma$는 각각 KL penalty와 사전 학습 gradient의 강도를 제어한다. \u0026ldquo;PPO\u0026rdquo; 모델에서 $\\gamma$는 0으로 설정되며, 이 논문에서 InstructGPT는 주로 PPO-ptx 모델을 의미한다.\nBaselines. PPO 모델의 성능을 SFT 모델과 GPT-3, 그리고 명령을 따르는 모드로 프롬프트된 GPT-3와 비교한다. 이 프롬프트는 사용자가 지정한 명령 앞에 추가된다.\nInstructGPT를 FLAN과 T0 데이터셋에서 미세 조정된 175B GPT-3와 비교한다. 이 두 데이터셋은 다양한 NLP 작업과 각 작업에 대한 자연어 지시를 포함하고 있다. 각각 약 100만 예제에 대해 미세 조정하고, 검증 세트에서 가장 높은 보상 모델 점수를 얻는 체크포인트를 선택한다.\nEvaluation \u0026ldquo;alignment\u0026quot;은 이 맥락에서 사용자의 의도에 맞게 모델이 행동하는 것을 의미한다. 이는 모델이 도움이 되고, 정직하며, 무해해야 한다는 Askell et al. (2021)의 정의를 따른다.\n모델이 도움이 되려면 명령을 따르고, 작은 양의 프롬프트나 특정 패턴에서 의도를 추론해야 한다. 그러나 프롬프트의 의도는 불분명할 수 있으므로, 라벨러의 판단에 의존한다. 그러나 라벨러와 프롬프트를 생성한 사용자 사이에 의도의 차이가 있을 수 있다.\n생성적인 AI 모델에서의 진실성을 측정하는 것은 어렵다. 이는 모델의 output과 그것이 \u0026ldquo;올바른(belief)\u0026rdquo; output에 대한 생각을 비교해야 하는데, 모델 자체가 복잡해서 그 생각을 추론하기 어렵기 때문이다. 대신, 모델이 세상에 대해 말하는 것이 사실인지를 측정하는 두 가지 방법을 사용한다: (1) 폐쇄 도메인 작업에서 모델이 정보를 창조하는 경향을 평가하고, (2) TruthfulQA 데이터셋을 사용한다. 하지만 이것은 진실성의 일부분만을 포착할 수 있다.\n언어 모델의 해로움을 측정하는 것은 매우 어렵다. 이는 모델의 output이 실제 세계에서 어떻게 사용되는지에 따라 달라지기 때문이다. 예를 들어, toxic을 가진 output을 생성하는 모델은 챗봇에서는 해로울 수 있지만, toxic 감지 모델을 학습시키는데는 도움이 될 수 있다. 초기에는 \u0026ldquo;potentially harmful\u0026rdquo; output을 평가했으나, output이 결국 어떻게 사용될지에 대한 추측이 필요하다는 이유로 중단했다.\n모델의 해로운 행동을 평가하기 위해 특정 기준을 사용한다. 이는 고객 서비스 상황에서 부적절한 내용, 보호 계급에 대한 비하, 성적 또는 폭력적인 내용 등을 포함한다. 또한, bias와 toxic을 측정하는 여러 데이터셋에서도 모델을 테스트한다.\n요약하자면, 양적 평가를 두 가지 별도의 부분으로 나눌 수 있다:\nEvaluations on API distribution. 주요 평가 지표는 학습 데이터와 같은 출처의 프롬프트 세트에 대한 사람들의 선호도이다. 학습에 사용되지 않은 고객들의 API 프롬프트와, GPT-3 모델을 위해 특별히 설계된 프롬프트에 대해 이를 측정한다. 각 모델의 출력이 기준 모델에 비해 얼마나 선호되는지 계산하며, 응답의 전반적인 품질을 1-7 Likert 척도로 평가한다. 또한 각 모델 output에 대해 다양한 메타데이터를 수집한다.\nEvaluations on public NLP datasets. 언어 모델의 진실성, 독성, 편향 등의 안전성과 질문 응답, 독해, 요약 등의 NLP 작업 성능을 평가한다. 이를 위해 공개 데이터 세트를 사용하며, RealToxicityPrompts 데이터 세트에 대한 독성 평가도 수행한다. 또한, 모든 샘플링 기반 NLP 작업에서 모델의 샘플을 공개하고 있다.\nResults 주장의 근거를 명확히 보여주기 위해 API 프롬프트 분포 결과, 공개 NLP 데이터셋 결과, 그리고 질적 결과를 제공한다.\nResults on the API distribution Labelers signiﬁcantly prefer InstructGPT outputs over outputs from GPT-3. 프롬프트 테스트 세트에서, 라벨러들은 모든 모델 크기에서 InstructGPT output을 선호한다. GPT-3 출력이 가장 성능이 떨어지며, 잘 구성된 few-shot 프롬프트를 사용하거나, 지도 학습을 통해 학습하거나, PPO를 이용해 비교 데이터에서 학습함으로써 성능을 크게 향상시킬 수 있다. 직접 비교해보면, InstructGPT 출력은 GPT-3 출력에 비해 85%의 시간을, few-shot GPT-3에 대해서는 71%의 시간을 선호한다.\nAPI에서 GPT-3 모델에 제출된 프롬프트로 평가했을 때도 결과는 크게 변하지 않는다. 하지만, 모델 크기가 클수록 PPO-ptx 모델의 성능은 약간 떨어잔다.\n라벨러들은 InstructGPT output을 여러 구체적인 측면에서도 긍정적으로 평가하였다. GPT-3에 비해 InstructGPT output은 고객 서비스원의 맥락에서 더 적절하고, 지시사항을 더 잘 따르며, 잘못된 지시를 따르는 경우가 적고, 특정 도메인 작업에서 사실을 만들어내는 경우가 더 적다. 이것은 InstructGPT 모델이 GPT-3보다 더 신뢰할 수 있고 제어하기 쉽다는 것을 보여준다.\nOur models generalize to the preferences of \u0026ldquo;held-out\u0026rdquo; labelers that did not produce any training data. 보류된 라벨러들도 학습 데이터를 만드는 데 사용한 작업자와 비슷한 순위 선호도를 가지고 있다. 특히, 모든 InstructGPT 모델들은 GPT-3 기준선을 크게 능가한다. 이는 모델이 학습 라벨러의 선호도에 overfit되지 않았음을 보여준다.\nPublic NLP datasets are not reﬂective of how our language models are used. 175B GPT-3 기준선을 FLAN과 T0 데이터셋에서 미세 조정하여 InstructGPT와 비교하였다. 이 모델들은 GPT-3보다 더 나은 성능을 보이지만, 잘 선택된 프롬프트를 가진 GPT-3와 동등하고, SFT 기준선보다는 성능이 떨어진다. 직접 비교해보면, 175B InstructGPT 모델 출력은 FLAN 모델에 대해 78 $\\pm$ 4%의 시간을, T0 모델에 대해 79 $\\pm$ 4%의 시간을 선호하였다.\nInstructGPT 모델이 FLAN과 T0를 능가하는 이유는 두 가지이다. 첫째, 공개 NLP 데이터셋은 자동 메트릭으로 쉽게 평가할 수 있는 작업을 포착하기 위해 설계되었지만, 이러한 작업은 실제 사용자가 언어 모델을 사용하는 부분의 작은 비율을 차지한다. 둘째, 공개 NLP 데이터셋에서 실제 사용자가 관심을 가질 수 있는 다양한 입력을 얻는 것이 어렵다. 따라서, 가장 효과적인 instruction-following 모델은 두 유형의 데이터셋을 모두 결합할 것이다.\nResults on public NLP datasets InstructGPT models show improvements in truthfulness over GPT-3. TruthfulQA 데이터셋에 대한 인간 평가에 따르면, PPO 모델은 GPT-3에 비해 사실적이고 유익한 output을 생성하는 데 있어 약간의 개선을 보인다. 이는 모델의 기본 행동이며, 특별한 지시 없이도 진실성이 향상된다. 그러나, 1.3B PPO-ptx 모델은 같은 크기의 GPT-3 모델에 비해 약간 성능이 떨어진다. GPT-3에 적대적으로 선택되지 않은 프롬프트만 평가해도, PPO 모델은 GPT-3보다 더 진실적이고 유익하다.\n모델에게 정확한 답변을 확신하지 못할 때 \u0026ldquo;I have no comment\u0026quot;으로 응답하도록 지시하는 유용한 프롬프트를 제공한다. 이 경우, PPO 모델들은 거짓말을 자신 있게 하는 것보다는 사실적이고 무의미한 쪽으로 편향된다. 반면, 기존의 GPT-3 모델은 이런 면에서 그리 좋지 않다.\nPPO 모델들이 API 분포에서의 closed-domain 작업에서 더 적게 hallucinate하는 것은 우리의 진실성 개선을 입증한다.\nInstructGPT shows small improvements in toxicity over GPT-3, but not bias. 먼저 RealToxicityPrompts 데이터셋에서 모델들을 평가한다. 이를 위해 모델 샘플을 Perspective API를 통해 실행하여 automatic toxicity 점수를 얻고, 라벨러에게 absolute toxicity, 프롬프트에 대한 relative toxicity 등을 평가받는다. 또한, 높은 input toxicity에서 모델의 성능을 더 잘 평가하기 위해 toxicity에 따라 프롬프트 샘플을 균일하게 추출한다. 이는 표준 프롬프트 샘플링과 다르므로 absolute toxicity 수치가 과대 평가된다.\n안전하고 존중받는 출력을 생성하도록 지시받았을 때, InstructGPT 모델은 GPT-3보다 덜 toxicity 있는 출력을 생성한다. 하지만, 이 장점은 존중받는 프롬프트가 제거될 때 사라진다. 명시적으로 toxicity 있는 출력을 생성하도록 지시받았을 때, InstructGPT output은 GPT-3보다 훨씬 더 toxicity이 있다.\n\u0026ldquo;respectful prompt\u0026rdquo; 설정에서 InstructGPT는 GPT-3보다 덜 toxicity이 있지만, \u0026ldquo;no prompt\u0026rdquo; 설정에서는 비슷한 성능을 보인다. 모든 모델은 프롬프트를 고려할 때 예상보다 덜 toxicity이 있다. SFT 기준선 모델은 가장 덜 toxicity이 있지만, 연속성이 가장 낮고 가장 선호되지 않는 것으로 나타났다.\n모델이 편향된 언어를 생성하는 경향을 평가하기 위해, Winogender와 CrowS-Pairs 데이터셋에서 InstructGPT를 평가하였다. 이 데이터셋들은 잠재적인 편향을 강조할 수 있는 문장 쌍으로 구성되어 있다. 완벽하게 편향되지 않은 모델들은 각 쌍의 문장 사이에 선호도가 없을 것이다. 하지만, 모델들은 GPT-3보다 덜 편향되어 있지 않다. PPO-ptx 모델은 GPT-3와 비슷한 편향을 보이며, 존중받는 행동을 지시받았을 때는 더 높은 편향을 보인다.\nWe can minimize performance regressions on public NLP datasets by modifying our RLHF ﬁne-tuning procedure. API 분포에서 PPO 모델을 학습시키면, 여러 공개 NLP 데이터셋에서의 성능이 감소하는 \u0026ldquo;alignment tax\u0026rdquo; 문제가 발생한다. 이는 더 능력 있는 모델을 사용하도록 유인하지만, 이 모델들은 alignment되지 않았다. 따라서 alignment tax를 피하는 절차가 필요하다.\n사전 학습 업데이트를 섞는 것은 KL coefﬁcient를 증가시키는 것보다 더 나은 성능을 보여준다. 사전 학습 mix coefﬁcient의 적절한 값이 SQuADv2와 DROP에서의 성능 저하를 뒤집고, 검증 보상에서의 감소를 최소화한다. 반면, KL coefﬁcient를 증가시키는 것은 검증 보상에서 큰 감소를 초래하고, DROP과 SQuAD에서 완전히 회복하지 못한다.\nQualitative results InstructGPT models show promising generalization to instructions outside of the RLHF ﬁnetuning distribution. InstructGPT는 비영어 언어로 된 지시사항을 따르고, 코드에 대한 요약과 질문 답변을 수행하는 능력을 보여준다. 이는 비영어 언어와 코드가 미세 조정 데이터의 극히 일부를 차지함에도 불구하고, 이러한 일부 경우에서 모델이 사람들이 직접 감독하지 않은 입력에 대해 원하는 행동을 생성하는 것으로 일반화될 수 있음을 보여준다.\n175B PPO-ptx 모델은 코드에 대한 질문에 신뢰성 있게 답하며, 다른 언어로 된 지시사항도 따를 수 있다. 하지만, 지시사항이 다른 언어인 경우에도 종종 영어로 출력을 생성한다. 반면, GPT-3는 더 신중한 프롬프팅이 필요하며, 이러한 영역에서 지시사항을 따르는 경우는 드물다.\nInstructGPT still makes simple mistakes. 175B PPO-ptx 모델은 강력한 성능에도 불구하고 간단한 실수를 할 수 있다. 예를 들어, 거짓 전제로 된 지시를 받았을 때 모델은 잘못된 전제가 참이라고 가정할 수 있다. 또한, 간단한 질문에 대해 모델은 과도하게 회피할 수 있다. 또한, 지시사항에 여러 명확한 제약조건이 포함되어 있거나, 제약조건이 언어 모델에게 도전적일 수 있을 때 모델의 성능이 저하된다. 이는 모델의 한계를 보여준다.\n특정 행동이 지식적 겸손성을 보상하도록 지시하기 때문에, 그리고 거짓 전제를 가정하는 프롬프트가 학습 세트에 거의 없기 때문에 일부 문제가 발생한다고 생각한다. 이 두 가지 행동 모두가 적대적 데이터 수집을 통해 크게 줄어들 수 있을 것이라고 믿는다. 이러한 발견은 모델 학습과 데이터 수집 전략의 중요성을 보여준다.\nDiscussion Implications for alignment research 이 연구는 AI 시스템을 인간의 의도와 일치시키는 보다 광범위한 연구 프로그램의 일부이다. 이 작업은 현재의 언어 모델 시스템에 초점을 맞추고 있지만, 미래의 AI 시스템에 대해 작동하는 확장 가능한 방법을 찾고 있다. 시스템들은 아직 제한적이지만, 다양한 언어 작업에 적용되며, AI의 발전과 인간의 의도와의 일치를 추구하는 노력의 일환이다.\n이 연구에서는 현재의 AI 시스템의 alignment를 개선하는데 초점을 맞추는 반복적인 접근법을 사용하고 있다. 이 접근법의 단점은 초인적 시스템을 alignment할 때만 발생하는 alignment 문제를 직접적으로 마주치지 않는다는 것이다. 그러나, 이 접근법은 무엇이 작동하고 무엇이 작동하지 않는지에 대한 명확한 피드백 루프를 제공하며, 이는 alignment 기법을 개선하는 데 필수적이다. 또한, 여기서 사용하는 alignment 기법인 RLHF는 초인적 시스템을 alignment하는 여러 제안의 중요한 구성 요소이다. 이러한 접근법은 AI와 사람의 의도와의 일치를 추구하는 노력의 일환이다.\n이 작업에서, 좀 더 일반적으로 alignment 연구에 대한 교훈을 얻을 수 있다:\n모델 alignment를 늘리는 비용은 사전 학습에 비해 상대적으로 적다. 데이터 수집 비용과 학습 비용은 GPT-3 학습 비용의 일부에 불과하며, 동시에 RLHF는 언어 모델을 사용자에게 더 도움이 되도록 만드는데 매우 효과적이다. 이는 현재 기존 언어 모델의 alignment에 투자를 늘리는 것이 더 큰 모델을 학습시키는 것보다 더 비용 효율적이라는 것을 시사한다. 이러한 결과는 비용 효율적인 AI 연구와 개발에 대한 중요성을 보여줍니다.\nInstructGPT가 감독하지 않는 설정에서도 \u0026ldquo;following instructions\u0026rdquo; 것을 일반화하는 몇 가지 증거를 보았다. InstructGPT는 비영어 언어 작업과 코드 관련 작업 등, 직접 감독하지 않는 환경에서도 지시사항을 따르는 것을 일반화하는 경향이 있다. 이는 모든 작업에 대해 인간 감독이 과도한 비용이 들기 때문에 중요한 특성이다. 이러한 일반화가 능력 향상과 얼마나 잘 확장되는지에 대한 추가 연구가 필요하다. 이러한 발견은 AI의 범용성을 보여준다.\n미세 조정에 의해 도입된 대부분의 성능 저하를 완화할 수 있었다. 미세 조정으로 인한 성능 저하를 대부분 완화할 수 있었다. 이것이 사실이 아니었다면, 이는 모델을 alignment하는 데 추가 비용인 \u0026ldquo;alignment tax\u0026quot;을 의미할 수 있다. 인간의 의도와 일치하지 않는 AI 시스템을 피하기 위해, alignment tax이 낮은 alignment 기술이 필요하다. 이 관점에서, 우리의 결과는 RLHF가 낮은 세금의 alignment 기술로서 좋은 소식이다. 이는 AI 모델을 인간의 의도와 일치시키는 데 추가 비용을 최소화하는 데 중요하다.\n실제 세계에서 연구로부터 얻은 alignment 기법을 검증하였다. alignment 연구는 전통적으로 추상적이었지만, 이 연구는 실제 세계에서 고객과 함께 사용되는 AI 시스템에서 alignment 연구에 기반을 제공한다. 이를 통해 기술의 효과와 한계에 대한 중요한 피드백 루프를 구축할 수 있다. 이러한 발견은 실제 환경에서의 AI alignment 연구의 중요성을 보여준다.\nWho are we aligning to? 언어 모델을 인간의 의도와 일치시킬 때, 그들의 최종 행동은 기본 모델, 미세 조정 데이터, 그리고 사용된 정렬 방법에 의해 결정된다. 이를 통해 우리는 무엇과 누구에게 alignment할지 결정하며, 개선할 영역을 고려한 후 우리의 작업의 한계에 대해 더 깊이 있게 논의한다.\n문헌에서는 \u0026ldquo;human preferences\u0026rdquo; 또는 \u0026ldquo;human values\u0026quot;라는 용어로 alignment을 설명한다. 하지만, 이 연구에서는 라벨러들의 선호도를 지시사항, 작업 맥락, 지시를 받은 사람 등에 따라 alignment 하였다. 이에는 몇 가지 중요한 주의사항이 있다:\n첫째, 학습 라벨러들이 제공하는 시연과 선호도에 맞추고 있다. 이들 라벨러들은 대부분 Upwork 또는 Scale AI를 통해 고용된 미국이나 동남아에 거주하는 영어를 사용하는 사람들이다. 그들은 많은 예제에서 서로에게 동의하지 않는다; 라벨러 간의 동의도는 약 73%이다. 이러한 점들은 AI 모델의 alignment와 정확도에 중요한 영향을 미찬다.\n둘째, 라벨러들이 시연을 작성하고 선호하는 output을 선택할 때 가이드로 사용하는 지시사항을 작성하고, 가장자리 사례에 대한 질문에 답한다. 다른 지시 세트와 인터페이스 디자인이 데이터 수집과 최종 모델 행동에 미치는 영향에 대한 추가 연구가 필요하다. 이는 AI 모델이 인간의 의도와 얼마나 잘 일치하는지를 결정하는 데 중요한 요소이다.\n셋째, 학습 데이터는 OpenAI 고객들이 보낸 프롬프트에 기반하므로, 고객들이 가치 있다고 생각하는 것과, 경우에 따라서는 그들의 최종 사용자가 가치 있다고 생각하는 것에 암시적으로 일치하고 있다. 그러나 고객과 그들의 최종 사용자는 의견이 다를 수 있다. 실제로, 라벨러들은 주어진 프롬프트나 완성이 어떤 맥락에서 사용될지 알 수 없다. 이러한 점들은 AI 모델의 정확도와 효과성에 영향을 미친다.\n넷째, OpenAI의 고객들은 모든 언어 모델의 잠재적이거나 현재의 사용자들을 대표하지 않는다. 이 프로젝트의 대부분 기간 동안, OpenAI API의 사용자들은 대기 목록에서 선택되었다. 이 대기 목록의 초기 사용자들은 OpenAI의 직원들이었으므로, 최종 사용자 그룹이 우리 자신의 네트워크로 편향되어있다. 이러한 점들은 AI 모델의 대표성과 공정성에 영향을 미친다.\n공정하고 투명하며 적절한 책임 메커니즘이 있는 alignment 프로세스를 설계하는 것은 많은 어려움이 있다. 이 논문의 목표는 alignment 기법이 특정 애플리케이션에 대한 특정 인간 참조 그룹에 맞출 수 있다는 것을 보여주는 것이다. 그러나 모델을 학습시키는 조직, 모델을 사용하여 제품을 개발하는 고객, 이러한 제품의 최종 사용자, 그리고 직접적이거나 간접적으로 영향을 받을 수 있는 더 넓은 인구 등 많은 이해당사자들을 고려해야 한다. 모든 사람의 선호도에 맞춰진 시스템을 학습시키는 것은 불가능하며, 모든 사람이 타협을 승인할 수 없다. 이러한 점들은 AI 모델의 공정성과 책임성에 중요한 영향을 미친다.\n한 가지 방법은 특정 그룹의 선호도에 따라 조건화될 수 있는 모델을 학습시키거나, 다른 그룹을 대표하도록 쉽게 미세 조정할 수 있는 모델을 학습시키는 것이다. 그러나 이러한 모델들은 여전히 사회에 영향을 미칠 수 있으며, 어떤 선호도에 조건을 부여할 것인지, 모든 그룹이 대표될 수 있도록 하는 등 많은 어려운 결정을 내려야 한다. 이는 AI 모델의 공정성과 표현성에 중요한 영향을 미친다.\nLimitations Methodology. InstructGPT 모델의 행동은 계약자들로부터 얻은 인간의 피드백에 부분적으로 결정된다. 약 40명의 계약자를 고용하였고, 그들은 민감한 프롬프트를 식별하고 대응하는 능력에 따라 선발되었다. 그러나 이 그룹은 모델을 사용하고 영향을 받을 전체 사람들을 대표하지 않는다. 간단한 예로, 라벨러들은 주로 영어를 사용하며, 데이터는 거의 완전히 영어 지시사항으로 구성되어 있다. 이는 AI 모델의 다양성과 대표성에 중요한 영향을 미친다.\n데이터 수집 구성을 개선할 수 있는 여러 방법이 있다. 대부분의 비교는 비용 문제로 1명의 계약자만에 의해 라벨링된다. 예제를 여러 번 라벨링하면 계약자들 간의 이견을 식별하고, 이견이 있는 경우, 평균 라벨러 선호도에 맞추는 것이 바람직하지 않을 수 있다. 특히, 소수 그룹에 불균형하게 영향을 미치는 텍스트를 생성할 때, 해당 그룹에 속하는 라벨러들의 선호도를 더 무겁게 가중할 수 있다. 이는 AI 모델의 공정성과 정확성에 중요한 영향을 미친다.\nModels. 모델들은 완전히 일치하거나 완전히 안전하지 않는다. 그들은 toxic이 있거나 편향된 결과를 생성하고, 사실을 만들어내며, 명확한 프롬프트 없이 성적이거나 폭력적인 내용을 생성할 수 있다. 또한 일부 입력에 대해 합리적인 output을 생성하지 못할 수 있다. 이러한 점들은 AI 모델의 안전성과 신뢰성에 중요한 영향을 미친다.\n모델의 가장 큰 제한은 대부분의 경우 사용자의 지시를 따르는 것이며, 이것이 실제 세계에서 해를 끼칠 수 있음에도 불구하고 그렇다. 예를 들어, 편향성을 최대화하라는 지시가 주어질 경우, InstructGPT는 toxic이 있는 output을 더 많이 생성한다. 이러한 문제에 대한 해결방안은 이후 섹션에서 논의된다. 이러한 점들은 AI 모델의 안전성과 윤리성에 중요한 영향을 미친다.\nOpen questions 이 작업은 다양한 지시를 따르도록 언어 모델을 미세 조정하는 첫 단계이다. 사람들이 실제로 원하는 것과 언어 모델의 행동을 더욱 일치시키기 위한 미해결된 질문들이 많이 있다. 이는 AI 모델의 사용성과 효과성을 향상시키는 데 중요한 단계이다.\n모델이 toxic을 가지거나 편향된 또는 다른 방식으로 해롭게 출력하는 경향을 줄이기 위한 다양한 방법들이 있다. 예를 들어, 모델의 최악의 행동을 찾는 적대적인 설정이나, 사전 학습 데이터를 필터링하는 방법, 또는 모델의 진실성을 향상시키는 방법들을 사용할 수 있다. 이러한 접근법들은 AI 모델의 안전성과 신뢰성을 향상시키는 데 중요하다.\n이 연구에서, 사용자가 잠재적으로 해롭거나 부정직한 응답을 요청하면, 모델이 이러한 output을 생성하도록 허용한다. 하지만 모델을 무해하게 학습시키는 것은 중요하며, 출력이 해로운지 여부는 배포 맥락에 따라 달라지므로 어렵다. 이 연구의 기법은 특정 사용자 지시를 거부하도록 모델을 만드는 데도 적용될 수 있으며, 이는 후속 연구에서 탐구할 계획이다. 이는 AI 모델의 안전성과 윤리성에 중요한 영향을 미친다.\n모델들이 원하는 것을 수행하게 하는 것은 steerability 및 controllability과 직접적으로 관련이 있다. 유망한 미래의 방향은 RLHF를 steerability의 다른 방법들과 결합하는 것입니다, 예를 들어 control 코드를 사용하거나 추론 시간에 샘플링 절차를 수정하는 것 등이 있다. 이는 AI 모델의 controllability과 유연성을 향상시키는 데 중요하다.\n주로 RLHF에 초점을 맞추지만, 더 나은 결과를 얻기 위해 다른 알고리즘도 사용될 수 있다. 예를 들어, 전문가 반복을 연구하거나, 비교 데이터의 일부를 사용하는 단순한 행동 복제 방법을 시도해 볼 수 있다. 또한, 소수의 해로운 행동을 생성하는 것에 조건을 부여한 보상 모델에서 점수를 최대화하는 constrained optimization 접근법을 시도해 볼 수도 있다. 이는 AI 모델의 효율성과 성능에 중요하다.\n비교는 반드시 alignment 신호를 제공하는 가장 효율적인 방법은 아니다. 모델의 응답을 수정하거나, 자연어로 모델의 응답에 대한 비평을 생성하는 등 다른 방법을 사용할 수 있다. 또한, 라벨러들이 언어 모델에 피드백을 제공하는 인터페이스를 설계하는 것은 흥미로운 인간-컴퓨터 상호작용 문제이다. 이는 AI 모델의 효율성과 사용성을 향상시키는 데 중요하다.\n사전 학습 데이터를 RLHF 미세 조정에 통합함으로써 alignment tax를 완화하는 제안은 성능 회귀를 완전히 완화하지 않으며, 특정 태스크에서 원치 않는 행동을 더욱 가능하게 할 수 있다. 이는 더욱 연구할 만한 흥미로운 영역이다. 또한, 사전 학습 혼합 데이터에서 toxic 내용을 필터링하거나, 이 데이터를 합성 지시사항으로 보강하는 것이 우리의 방법을 개선할 수 있다. 이는 AI 모델의 성능과 안전성을 향상시키는 데 중요하다.\n지시, 의도, 드러난 선호도, 이상적인 선호도, 이해관계, 그리고 가치에 맞추는 것 사이에는 미묘한 차이가 있다. 원칙 기반의 정렬 방식을 주장하며, \u0026ldquo;사람들의 도덕적 신념에서 널리 변동이 있음에도 불구하고 반사적인 승인을 받는 공정한 정렬 원칙\u0026quot;을 식별하는 것이 중요하다. 이 분야에서는 더 많은 연구가 필요하며, 특히 투명하고, 사람들의 가치를 의미있게 대표하고 통합하는 정렬 과정을 어떻게 설계할 것인지가 주요한 미해결 질문이다. 이는 AI 모델의 공정성과 윤리성에 중요하다.\nBroader impacts 이 연구는 대규모 언어 모델의 긍정적인 영향을 증가시키기 위해 특정 인간 그룹이 원하는 행동을 모델에 학습시키는 것을 목표로 한다. 언어 모델을 더 도움되고, 진실되고, 무해하게 만드는 데 이 연구의 기법이 유망함을 나타낸다. 장기적으로, 모델 alignment 실패는 더 심각한 결과를 초래할 수 있다. 모델 확장이 계속됨에 따라, 인간의 의도와 일치하도록 하는 데 더 큰 주의가 필요하다. 이는 AI 모델의 유용성과 안전성을 향상시키는 데 중요하다.\n언어 모델을 사용자의 의도를 더 잘 따르도록 만드는 것은 그것들을 더 쉽게 오용하는 것을 가능하게 한다. 이로 인해 잘못된 정보를 생성하거나, 혐오스럽거나 폭력적인 내용을 생성하는 것이 더 쉬울 수 있다. 이는 AI 모델의 안전성과 윤리성에 중요하다.\nalignment 기법은 대규모 언어 모델의 안전 문제를 해결하는 만병통치약이 아니며, 더 넓은 안전 생태계의 일부로 사용되어야 한다. 많은 분야에서는 대규모 언어 모델이 신중하게, 또는 전혀 배포되지 않아야 한다. 모델이 오픈 소스화되면, 해로운 사용을 제한하는 것이 어려워진다. 반면, 대규모 언어 모델 접근이 몇몇 조직에 제한되면, 대부분의 사람들이 최첨단 ML 기술에 접근할 수 없게 된다. 또 다른 옵션은 조직이 모델 배포의 인프라를 소유하고, API를 통해 접근 가능하게 하는 것이다. 이는 안전 프로토콜의 구현을 가능하게 하지만, 감소된 투명성과 증가된 권력 집중의 비용이 발생할 수 있다. 이는 AI 모델의 안전성과 공정성에 중요하다.\n모델이 누구에게 alignment되는지는 매우 중요하며, 이것은 모델의 순수한 영향이 긍정적인지 부정적인지를 크게 영향을 미친다. 이는 AI 모델의 공정성과 윤리성에 중요하다.\nReference Paper Github ","date":"2024-01-23T00:00:00Z","permalink":"https://kurtkim.github.io/p/instructgpt/","title":"InstructGPT"},{"content":"Abstract 언어 모델은 자연어 이해를 필요로 하는 작업에서 탁월한 성과를 보였지만, 수량적 추론을 필요로 하는 작업에서는 어려움을 겪었다. 이를 해결하기 위해, 일반 자연어 데이터에 대해 사전 학습된 후 기술적인 내용에 대해 추가 학습된 Minerva라는 큰 언어 모델을 제안한다. 이 모델은 외부 도구 없이도 기술 벤치마크에서 state-of-the-art를 보여주며, 물리학, 생물학, 화학, 경제학 등 대학 수준의 문제 200개 이상을 풀어보았을 때, 그 중 거의 1/3을 정확하게 해결할 수 있었다.\nIntroduction 인공 신경망은 다양한 분야에서 큰 성과를 내었습니다. 특히, 거대 언어 모델은 다양한 자연어 작업에서 우수한 성능을 보였지만, 수학이나 과학 등 정량적 추론을 필요로 하는 문제 해결에서는 어려움을 겪었다.\n정량적 추론 문제는 언어 모델의 다양한 능력을 평가하는 중요한 분야이다. 이는 자연어 이해, 세계 지식 회상, 계산 알고리즘 적용, 수학 토큰 조작 등의 능력을 요구하며, 과학과 기술 분야에서 사람들의 작업을 지원하는 견고한 정량적 추론 해결사로서의 연구를 검증하는 기회를 제공한다.\n이전 연구에서는 대규모 언어 모델이 특정 도메인 데이터셋에서 학습 후 수학과 프로그래밍 문제에서 뛰어난 성능을 보여주었다. 이 연구에서는 이런 접근법을 외부 도구 없이 독립적인 해결책을 제공해야 하는 정량적 추론 문제에 적용하였고, 이는 수학, 과학, 공학 문제 등을 포함한다.\nOur Contribution Minerva라는 언어 모델을 제안한다. 이 모델은 자연어로 표현된 과학 및 수학 문제를 처리하고, 올바른 LATEX 표기법으로 단계별 해답을 생성하는 능력을 보여주며, 여러 정량적 추론 작업에서 뛰어난 성능을 보여주었다.\nMinerva는 과학과 수학 데이터를 포함하는 고품질 데이터셋으로 추가 학습된 PaLM 언어 모델을 기반으로 한다. 우리는 사전 학습된 모델을 사용하여 기술 데이터셋에서 학습을 계속하였고, MATH, GSM8k, MMLU 데이터셋 등에서 최고 수준의 성능을 보였다. 이 모델은 이러한 평가 데이터셋에서 명시적인 학습 없이도 강인한 성능을 보여주었다.\n이 논문의 핵심 novelty는 자연어와 형식적 수학 언어를 병행하는 대규모 학습 데이터셋이다. 이 데이터는 arXiv와 신중하게 처리된 웹 페이지에서 수집되었다. 이 연구는 데이터 품질과 모델 크기를 향상시킴으로써 정량적 추론 벤치마크에서 달성 가능한 성능에 대한 새로운 기준을 설정하였다.\n정량적 추론 벤치마크의 범위를 확장하기 위해, MIT의 OpenCourseWare에서 과학과 수학의 대학 수준 문제 200개 이상으로 데이터셋을 구축하였다. 이를 통해 순수 수학적 환경을 넘어 우리 모델의 사고 과정에서의 정량적 추론 능력을 측정하였다.\nRelated Works 자연어로 표현된 정량적 추론 문제를 해결하는 것은 활발히 연구되는 분야이다. 스크래치패드나 사고의 연결 고리를 사용한 프롬프트 언어 모델은 보이지 않는 문제의 단계별 해결책을 출력할 수 있다. GSM8k 작업은 모델 출력을 재정렬하기 위해 학습된 검증자를 사용하면 성능이 향상될 수 있음을 보여주었다. 이 연구에서는 외부 도구에 의존하지 않는 독립적인 모델에 초점을 맞추었다.\n언어 모델을 평가하는 표준 방법은 문제 당 한 가지 해결책을 탐욕적으로 샘플링하는 것이다. 하지만 최근 연구에서는 문제 당 여러 해결책을 샘플링하고 필터링하는 것이 더 유리하다는 것을 보여주었다. 특히, 다수결 투표 방식이 탐욕적 디코딩보다 성능을 크게 향상시킨다는 것을 확인하였다.\nDrori et al. (2021)은 OpenAI의 davinci-002 모델을 MATH 데이터셋의 일부로 평가하였다. 하지만 문제의 하위 집합에 초점을 맞추고 문제 형식의 변경으로 인해, 이 연구와 논문의 결과를 직접 비교하는 것은 어렵다.\nCode generation. 코드 생성 모델을 수학 문제에 적용하는 것은 활발한 연구 분야이다. PaLM은 학습 데이터셋에 코드가 포함된 거대 언어 모델이 좋은 성능을 보일 수 있음을 보여주었고, Codex 모델은 MATH 문제에 대한 코드 해결책을 생성할 수 있다. 이러한 해결책들은 외부 라이브러리에 의존하지만, 이 논문의 접근법은 모델이 자체 추론 능력만으로 답을 도출하는 능력을 직접 연구한다.\nFormal mathematics. 수학은 자연어를 기반으로 발전했지만, 공리적인 기초를 통해 수학적 사고를 시뮬레이션할 수 있다. 이는 Coq, Isabelle, HOL4, Lean, Metamath, Mizar 같은 특수 프로그래밍 언어를 통해 가능하며, 이들은 컴퓨터를 이용한 논리적, 수학적 사고의 시뮬레이션을 지원한다. 또한, 증명 보조 도구와 자동 정리 증명기의 자동화에 대한 연구는 기계 학습 방법과의 통합으로 큰 이익을 얻었다.\nLanguage models applied to formal and synthetic mathematical problems. 이전 연구에서는 언어 모델을 학습시켜 수학적 표현을 예측하는 방법을 사용하였다. 이러한 예측 모델은 증명 검색을 안내하는 데 사용할 수 있다. 거대 언어 모델은 자연어 모델링에 뛰어나지만, 형식 언어의 경우, 수학 공식의 그래프 구조 정보를 유지하는 모델, 예를 들어 GNNs,이 여전히 경쟁력이 있다.\nModelling mathematics as a discipline of natural language. 새로운 벤치마크 데이터셋은 고급 수학 주제를 포함하며, 이 분야에서 언어 모델은 다른 유형의 모델로부터 제한적인 경쟁을 받고 있다.\nTraining and Evaluation Mathematical Training Dataset Minerva 모델은 수학적 내용으로 필터링된 웹페이지와 arXiv 사전 인쇄 서버의 논문에서 추출한 데이터셋에서 학습되었다. 이 데이터셋은 일반적인 자연어 데이터도 포함하고 있다. 수학 웹페이지 데이터셋은 MathJax 형식의 수학 표현이 있는 페이지를 수집하여 만들었고, 대부분의 HTML 태그를 제거하지만 수학 표기법을 유지하는 과정을 거쳤다. 이로 인해 모델은 학습 중에 전체 수학 공식을 볼 수 있으며, 계산과 기호 조작을 요구하는 작업에서 잘 수행하게 된다.\nModels and Training Procedure 이 논문의 방법은 PaLM 사전 학습된 decoder-only transformer 언어 모델로 시작하여, 이를 autoregressive 목표를 사용하여 수학 데이터셋에서 더욱 학습(미세 조정)하는 것이다. 가장 큰 모델은 540B parameter를 가지며, 26B 토큰에서 미세 조정되었다. 이 모델은 학습이 부족하지만, 우수한 성능을 보여주었다.\nEvaluation Datasets 주로 few-shot 평가에 초점을 맞추며, 평가를 위해 입력을 1024 토큰으로 자르고 모델을 사용하여 최대 512 토큰을 생성한다. 문제당 한 번 샘플링할 때에는 탐욕적으로, 여러 번 샘플링할 때에는 핵심 샘플링을 사용한다. 생성 작업에서, 모델은 사고의 연결 고리를 답변으로 생성하고 최종 답변을 표시하며, 최종 답변이 실제 답변과 일치하면 해결책을 올바르다고 평가한다. 정확성 평가는 SymPy 라이브러리를 사용하여 수학적으로 동등한 답변을 올바르게 식별한다.\n기존 데이터셋들은 다음과 같다:\nMATH: 주로 사용하는 데이터셋은 Hendrycks et al. (2021)이 제시한 중고등학교 수학 문제 12K 데이터셋이다. 문제 설명은 LATEX로 작성되어 있으며, 네 개의 무작위 예제를 포함하는 고정 4-shot 프롬프트로 모델을 프롬프트한다. GSM8k: 중학교 수학 단어 문제 데이터셋은 Cobbe et al. (2021)이 제시했으며, 모델은 Wei et al. (2022)의 사고의 연결 고리 프롬프트를 사용하여 평가된다. MMLU-STEM: 과학, 기술, 공학, 수학 (STEM)에 초점을 맞춘 MMLU 데이터셋의 일부를 사용한다. 각 작업에 대해 5-shot 프롬프트를 사용하고, 단계별 해결책이 포함된 예제로 모델을 프롬프트한다. 수학적 추론을 포함하는 주제에 대해 객관식 MATH 프롬프트를 사용하고, 나머지 주제에 대해 단계별 해결책을 추가한 5-shot 프롬프트를 사용한다. Undergraduate-Level STEM Problems Minerva의 과학적 추론 능력을 평가하기 위해, 대학 수준의 STEM 문제 세트를 수집하였다. 이 문제들은 대부분 다단계 추론을 포함하고 있다. MIT의 공개 강좌 자료를 사용하여 자동으로 검증 가능한 해결책을 가진 문제들을 수집하였다. 총 272개의 문제를 수집했으며, 이 중 191개는 numeric solution을 가지고 81개는 symbolic solution을 가진다.\nInference-Time Techniques 여러 해결책을 샘플링하고 다수결로 하나를 선택함으로써 탐욕적 디코딩을 상당히 능가할 수 있다는 것을 발견하였다. 이는 가장 흔한 답변을 선택하는 방법으로, maj1@k라고 표시한다. 이 알고리즘의 변형은 가장 흔한 답변 $n$개를 선택하는 것을 포함한다. 이 방법이 성능을 향상시키는 이유는 일반적으로 올바른 답변 방법이 매우 적기 때문이다.\n다수결과 pass@k를 비교하면, pass@k는 $k$개의 샘플 중 하나가 문제를 해결하면 작업이 해결된 것으로 간주된다. 반면, 다수결 성능은 빠르게 포화되며, MATH의 경우 $k = 64$, GSM8k의 경우 $k = 16$에서 이미 대부분의 정확도를 달성하였다. 이는 다수결이 모델링된 분포에서 가장 흔한 답변을 선택하기 때문이며, pass@k의 성능 향상은 분포의 꼬리에서 발생하므로 $k$가 증가함에 따라 계속 개선될 수 있다.\nLog-likelihood는 샘플을 재정렬하는 데 사용할 수 있는 또 다른 지표이다. 우리는 다수결이 Log-likelihood 재정렬보다 훨씬 더 잘 수행된다는 것을 발견하였다.\nResults MMLU 평가는 주제별로 표준 5-shot 프롬프트를 사용하고 가장 높은 점수의 답변을 선택하며, 다수결로 평가할 때는 사고의 연결 고리 프롬프트를 사용하여 16개의 모델 답변을 샘플링한다.\nMinerva 62B를 폴란드의 국가 수학 시험에 적용해 보았는데, 이는 2021년 국가 평균인 57%의 점수를 달성하였고, 540B 모델은 65%의 점수를 달성하였다.\n최신 OpenAI 언어 모델인 davinci-002의 결과를 포함시켰고, 이는 모든 기술적 작업에서 state-of-the-art를 보였다. 대부분의 작업들에서 이전 결과에 비해 상당한 향상이 있었다.\n이 논문은 few-shot 평가에 집중했고, Minerva를 MATH에서 미세 조정해 보았지만 개선 사항을 발견하지 못하였다. 그러나, MATH에서 PaLM을 미세 조정할 때는 상당한 개선이 있었다. 이는 비지도 학습 데이터셋의 품질과 다양성이 향상됨에 따라 표준 미세 조정의 효용성이 감소한다는 것을 보여준다.\nBasic arithmetic Minerva 540B가 10자리 수 덧셈에서 80% 이상, 18자리 수 덧셈에서 20% 이상의 정확도를 보였다.\nPerformance Analysis Model Mistakes Minerva 8B와 Minerva 62B의 성능을 비교하여 모델이 만드는 오류 유형을 파악하려고 했다. 두 모델 모두가 높은 확신을 가진 216개의 문제를 선정하였고, 이 중에서 상위 답변이 15% 이상의 표를 받았으며, Minerva 8B는 정확하고 Minerva 62B는 부정확했던 경우와 그 반대 경우를 분석하였다.\n8B 모델의 주요 오류는 부정확한 추론이나 계산에 관련되어 있었으며, 대부분은 산술적 실수였다. 너무 짧은 해결책은 드물었고, 일부 경우에는 실제로 존재하지 않는 수학적 사실을 만들어내기도 했다.\n62B 모델이 틀린 경우, 주로 추론과 계산에서의 오류가 발생하였다. 결론적으로, 62B Minerva 모델은 8B 모델의 기술을 대부분 유지하면서 추론과 계산의 견고성을 향상시킨다는 것을 확인하였다.\nFalse Positives 이 논문의 접근법은 문제의 최종 답변의 정확성을 자동으로 확인할 수 있지만, 모델의 추론 과정을 자동으로 검증할 수는 없다. 이로 인해, 추론이 부정확하거나 불완전하더라도 최종 답변이 맞는 \u0026ldquo;false positives\u0026quot;의 가능성이 있다.\nMATH에서 무작위로 선택한 100개의 질문과 62B 모델에서 샘플링한 답변을 수동으로 검토하여 거짓 긍정 비율을 결정하였다. 전반적으로 거짓 긍정 비율은 낮았지만, 난이도가 높아질수록 증가하는 경향을 보였다.\n평가 지표로 pass@1과 다수결 투표를 중점적으로 사용한다. 이는 그들이 거짓 긍정에 덜 취약하기 때문이다. 62B 모델의 pass@256 정확도는 84.5%이지만, 이 중 거짓 긍정의 비율이 30%로 추정된다. 거짓 긍정을 제외하면, pass@256 정확도는 약 68%로 추정된다.\nMemorization 머신러닝 모델의 성능이 진짜 분석 능력을 보여주는지, 아니면 단순히 학습 데이터를 암기한 결과인지를 판단하는 것이 중요하다. 이는 모델이 중간 사실들을 암기하는 것이 성능에 큰 영향을 미치기 때문이다. 모델이 문제와 답변을 암기하는 강력한 암기와, 동일한 질문에 대한 다양한 답변을 암기하는 약한 암기를 모두 검토하려고 한다.\n모델이 학습 데이터에서 암기한 정보를 얼마나 잘 활용하는지 평가하기 위해, 우리는 세 가지 분석을 수행한다: 학습 코퍼스에서 문제와 solution 검색, 문제 변형에 대한 모델의 강인성 평가, 그리고 실제 solution과 모델이 생성한 solution 사이의 유사도 측정. 그 결과, 모델의 성능이 암기에 크게 의존하고 있다는 증거는 찾을 수 없었다.\nTraining and Evaluation Dataset Overlap 올바른 답변을 생성한 문제들 중 다수결 점수가 가장 높은 100개의 문제를 선택하여 암기 가능성을 평가하였다. 이들 각각에 대해 BLEU 점수를 계산하고, 점수가 가장 높은 250개 문서를 수동으로 검토했다. 많은 문서가 수학 문제와 해답이 있는 숙제 도움 사이트에서 나왔지만, 고려 중인 문제와는 일치하지 않았다. 이 분석은 이러한 문제들이 데이터 수집 과정을 통과하지 못했다는 결론을 도출하였다.\nPerformance on Modiﬁed MATH Problems 암기를 더 깊게 조사하기 위해, 다수결 투표로 올바르게 답변한 문제 20개를 임의로 선택해 수정하였다. 문제의 표현을 바꾸거나 문제에 나타난 숫자를 변경하고 solution을 수정했다. 수정 전후의 solution 정확도를 비교한 결과, 두 경우 모두 수정 전후의 정확도가 상관관계를 보이며, 암기가 최소한임을 나타내었다.\nBLEU Score Between Ground Truth and Generated Solutions 실제 답변과 모델이 생성한 답변 사이의 BLEU 점수를 계산하여 solution의 암기를 검사하였다. 5,000개의 테스트 질문 중 160개가 BLEU 점수가 80 이상인 샘플을 가지고 있었으며, 일반적으로 이들은 짧은 solution 이었다. 답변의 유사성이 성능에 어떤 영향을 미치는지 이해하기 위해, 특정 BLEU 점수 이상의 샘플을 제거하고 다수결 투표 정확도를 다시 계산하였다. 결과적으로, 성능이 실제 답변과 매우 유사한 모델 출력에 의해 결정되지 않음을 확인했다.\nConclusions and Discussion 이 연구에서는 자연어로 표현된 수학적 추론을 활용하여 문제를 해결하는 양적 추론 방법을 채택하였다. 거대 언어 모델을 고품질의 수학 데이터셋에 학습시켜 논리적 추론, 수치 계산, 심볼 조작 작업에 강력한 성능을 보임을 입증하였다. 코드 생성 모델과 형식적 방법 등 다른 접근법들과 결합해 양적 문제를 해결하는 에이전트를 제작하는 것이 최종 목표이다.\nLimitations of Our Approach 양적 추론 접근법은 몇 가지 한계를 가지고 있다. 첫째, 모델의 답변의 정확성을 자동으로 검증할 수 없다. 둘째, 모델은 외부 도구를 사용할 수 없어 복잡한 수치 계산을 수행하는 능력이 제한적이다. 셋째, 대량의 데이터를 통해 학습된 모델이므로, 획득한 특정 능력에 대해 직접 통제할 수 있는 부분이 거의 없다.\nSocietal Impact 일반적인 상황에서 양적 추론 문제를 해결할 수 있는 인공 신경망은 큰 사회적 영향력을 가질 수 있다. 하지만 현재로서는 Minerva 모델이 이 목표에 도달하기엔 먼 상태로, 성능이 인간에 비해 떨어지며 출력의 정확성을 자동으로 검증할 수 없다. 이러한 문제가 해결되면, 모델은 광범위한 긍정적 영향을 미칠 것으로 예상되며, 접근성이 좋고 저렴한 수학 튜터로서 교육 불평등을 개선하는 데에 활용될 수 있을 것이다.\nReference Paper Github ","date":"2024-01-19T00:00:00Z","permalink":"https://kurtkim.github.io/p/minerva/","title":"Minerva"},{"content":"Abstract LaMDA는 최대 137B의 parameter를 가진 대화용 언어 모델이다. 이 모델은 모델 확장만으로는 안전성과 사실적 근거에 대한 개선이 제한적이라는 문제를 해결하기 위해, 주석이 달린 데이터로 미세 조정하고 외부 지식 소스를 참조하는 방식을 사용한다. 이를 통해 모델의 안전성을 향상시키고, 사실에 근거한 응답을 생성하는 데 성공하였다. 또한, 이 모델은 교육 및 콘텐츠 추천 분야에서의 활용 가능성을 보여준다.\nIntroduction 언어 모델 사전 학습은 자연어 처리 연구에서 유망하며, 레이블이 없는 텍스트를 사용해 모델과 데이터셋의 크기를 확장하면 성능이 향상된다. 이를 통해 GPT-3와 같은 모델은 few-shot 학습 예제로도 높은 성능을 보여준다.\n대화형 모델은 텍스트의 long-term dependency를 표현하는 능력을 활용하여 언어 모델을 효과적으로 활용한다. 모델의 크기가 커짐에 따라 대화 품질도 향상되는 강한 상관관계가 있다.\nLaMDA는 transformer 기반의 언어 모델로, 대화를 위해 설계되었다. 이는 대량의 공개 대화 데이터와 웹 문서로 사전 학습되었고, 잠재적인 응답을 생성, 필터링, 재정렬하여 최고 품질의 응답을 제공하는 다양한 작업을 수행한다.\nLaMDA와 함께 모델 스케일링의 이점을 연구한 결과, 스케일링만으로는 품질이 향상되지만, 안전성과 실제 연관성은 인간의 성능에 미치지 못하였다. 그러나 스케일링과 미세 조정을 결합하면 모든 지표에서 크게 향상되었고, 특히 품질 격차는 인간 수준에 가까워졌다.\n\u0026ldquo;quality\u0026rdquo; 지표는 \u0026ldquo;sensibleness\u0026rdquo;, \u0026ldquo;speciﬁcity\u0026rdquo;, \u0026ldquo;interestingness\u0026quot;의 세 가지 요소에 기반하며, 이를 바탕으로 응답이 얼마나 합리적이고 구체적이며 흥미로운지 주석 데이터를 수집한다. 이 데이터를 사용하여 후보 응답을 재정렬하는 discriminator를 미세 조정한다.\n\u0026ldquo;safety\u0026rdquo; 지표는 모델이 생성하는 위험한 응답을 줄이기 위해 도입되었다. 이를 위해 안전 목표를 설정하고, 다양한 군중의 작업자들을 통해 대화 응답을 레이블링한다. 이 레이블을 통해 위험한 응답을 감지하고 제거하는 discriminator를 미세 조정한다. 이는 고수준에서 AI의 가치를 조정하는 과정으로 볼 수 있다.\n\u0026ldquo;groundedness\u0026rdquo; 지표는 모델이 검증 가능한 정보를 포함하는 응답을 알려진 출처에 근거하여 생성하도록 하기 위해 도입되었다. 이는 사용자나 외부 시스템이 응답의 유효성을 판단하는데 도움이 된다. 이 목표를 달성하기 위해, 정보 검색 시스템과 같은 외부 도구를 사용하여 사실을 조사하는 군중의 작업자들의 행동을 모델이 흉내내도록 학습시킨다.\n교육과 콘텐츠 추천 분야에서 LaMDA의 사용을 연구하였다. 특정 응용 프로그램에 LaMDA를 적용하기 위해 몇 번의 응용 프로그램 특정 대화를 사전 조건으로 설정했다. 실험 결과, 사전 학습만 받은 LaMDA 모델과 미세 조정된 LaMDA 모델 모두 그들의 예상 응용 프로그램 역할에 잘 적응할 수 있었으며, 특히 미세 조정된 LaMDA 모델이 더욱 도움이 되었다.\nRelated work Language models and dialog models: 언어 모델은 최근 NLP 응용 분야에서의 성공 덕분에 주목받고 있다. 이 연구는 모델 스케일링이 품질, 안전성, 실제 연관성 지표를 어느 정도 향상시키는 것을 보여주지만, 미세 조정과 스케일링을 결합하면 모든 지표에서 성능이 크게 향상된다는 것을 보여준다.\n이 연구는 언어 모델을 대화 모델링에 적용하는 최근의 연구와 밀접하게 연관되어 있다. 대화 데이터만을 학습하는 미세 조정 단계는 이전 연구와 관련이 있다. 또한, 군중 작업자가 주석을 단 데이터에 미세 조정을 사용하여 흥미로움을 향상시키는 방법을 사용하였다. 그러나 이 연구의 목표는 사용자와의 추가적인 상호작용보다는 모델의 출력의 흥미로움을 극대화하는 것이다.\n순수 스케일링이 오픈 도메인 대화 모델 성능에 제한적인 영향을 미치는 것은 최근 연구와 일치하며, 이는 실제 연관성의 문제에 중점을 둔다. 최근 스케일링 연구는 질문-응답 작업의 성능이 모델 크기에 따라 향상된다는 것을 발견했는데, 이는 미세 조정 전의 사전 학습된 LaMDA에 대한 연구 결과와 일치한다.\n이 연구의 접근법은 언어 모델을 검색 시스템을 통해 향상시키는 데 초점을 맞춘 연구와 연관이 있다. 대부분의 기존 연구는 대화 생성보다는 오픈 도메인 질문-응답에 초점을 맞추고 있으며, 모델 자체가 중간 도구를 사용하도록 학습된다. 이러한 접근법은 RNNLM, RAG, REALM, FiD 등의 아키텍처를 포함하며, 최근의 연구는 신경 모델의 검색과 순위 지정 능력을 확장하고 발전시키고 있다. 이 접근법은 또한 영화 티켓 대화를 위한 외부 API를 사용하도록 모델을 미세 조정하는 연구와도 비교할 수 있다.\n연구 결과는 최근 대화의 실제 연관성에 대한 연구와 일부 유사하다. 외부 지식 베이스에 접근하는 것은 모델이 출처가 없는 내용을 환영하는 비율을 줄이는 것으로 나타났다. 질문-응답 시스템의 정확도는 추론 단위와 응답 생성기를 분리함으로써 개선된다. 검색 엔진과 언어 모델을 결합하면 더 사실적으로 근거를 둔 응답을 제공하는 것으로 나타났다. 알려진 출처의 정보로 생성된 응답을 보강함으로써, 안전성이나 품질에 대한 향상을 희생하지 않고 모델을 실제 연관성에 대해 세부 조정할 수 있다.\nDialog metrics: 대화 모델에 대한 효과적인 지표를 정의하는 것은 아직 해결되지 않은 연구 주제이다. 이 연구의 접근법은 인간 같은 지표를 주장한 이전 연구에 의해 영감을 받았다. 많은 자동화된 지표들이 연구되었지만, 이러한 지표들은 인간의 판단과 잘 연관되지 않을 수 있다. 따라서 대화 모델링에 대한 더 신뢰할 수 있는 지표는 인간의 평가를 필요로 한다. 이전 연구는 다양한 대화 품질 평가를 하나의 지표로 결합하려 했으나, 이 연구에서는 각각의 평가 요소를 따로 고려한다. sensibleness, speciﬁcity 외에 interestingness, safety, groundedness 등의 새로운 지표를 추가했고, 이런 다양한 지표 사용의 장점은 특정 지표가 낮은 응답을 분석해 개선 방법을 찾는 것이 가능하다는 점이다.\nSafety and safety of dialog models: 언어 모델의 부적절하고 위험한 행동에 대해 많은 연구가 이루어져 왔으며, toxicity, bias, inappropriately revealing personally identifying information (PII) 등의 문제가 발견되었다. 대규모 언어 모델과 관련된 21가지 위험을 식별하였고, 이 문제를 해결하기 위한 다양한 방법이 제안되었음에도 불구하고, 이 문제를 의미있게 해결하는 것은 여전히 활발한 연구 분야이다. 대화 모델에 대한 문제도 논의되었는데, bias, offensiveness, hate speech 등이 학습 데이터와 모델 output에서 발견되었다. 대화 모델은 학습 데이터의 bias을 배우고 확대할 수 있다. 이를 해결하기 위해, 안전한 output을 감지하는 별도의 layer를 학습하는 방법이 사용되었고, 미세 조정이 효과적이었다. 크기를 늘리는 것은 toxicity 지표에 영향을 미치지 않지만, 안전 평가에서의 미세 조정은 영향을 미친다는 것이 확인되었다.\nGroundedness metrics: 권위 있는 외부 소스와 모델의 output이 일치하는지를 판단하는 대중들에게 실제성을 평가하도록 요청함으로써 실제성을 평가한다. 최근에 제안된 AIS 프레임워크는 외부 세계에 관련된 언어 모델의 output을 더 정확하게 평가하는 방법을 제시하며, 이는 정보의 이해와 식별, 그리고 정보의 출처 판별의 두 단계로 이루어진다. 또한, 최근의 연구에서는 Q2 지표를 통해 자동 평가의 가능성을 다시 제시하였다. LaMDA pre-training LaMDA는 공개 대화 데이터와 웹 문서를 기반으로 사전 학습되어 텍스트의 다음 토큰을 예측하도록 설계되었다. 이로 인해 LaMDA는 미세 조정 전에도 일반 언어 모델로 사용될 수 있다.\n사전 학습 데이터셋은 총 2.97B 개의 문서, 1.12B 개의 대화, 13.39B 개의 대화 발화로 구성되어 있고, 대부분이 영어이다. SentencePiece 라이브러리를 통해 2.81T byte pair encoding(BPE) 토큰으로 토큰화하였다. 이는 Meena의 학습 세트인 40B 단어에 비해 훨씬 큰 규모이다.\n가장 큰 LaMDA 모델은 Meena보다 약 50배 많은 137B개의 non-embedding parameter를 가지고 있다. 이 모델은 decoder-only Transformer 언어 모델을 사용하며, 64개의 layer와 relative attention, gated-GELU activation 등을 특징으로 한다.\nLaMDA는 총 57.7일 동안 1024개의 TPU-v3 칩에서 사전 학습되었고, 배치당 256K 토큰을 사용하였다. Lingvo 프레임워크를 통해 123 TFLOPS/sec의 성능을 달성하였다. 또한, 모델 스케일링의 효과를 측정하기 위해 2B-parameter와 8B-parameter의 작은 모델도 학습하였다.\n미세 조정 전의 모델을 \u0026ldquo;PT\u0026quot;라 부르며, PT는 Meena와 같은 샘플링-랭킹 전략을 사용한다. 총 16개의 독립적인 후보 응답을 샘플링하고, log-likelihood와 length를 기반으로 점수가 가장 높은 후보를 최종 output으로 선택한다.\nMetrics Foundation metrics: Quality, Safety and Groundedness Sensibleness, Speciﬁcity, Interestingness (SSI): overall quality score는 sensibleness, speciﬁcity, and interestingness (SSI)의 평균이다.\nAdiwardana et al.은 Meena의 품질을 측정하기 위해 sensibleness와 speciﬁcity의 평균인 SSA 지표를 제안하였다.\n\u0026ldquo;sensibleness\u0026rdquo; 점수는 모델의 응답이 문맥에 맞고 이전의 발언과 모순되지 않는지를 측정한다. 그러나, sensibleness만으로 모델을 평가하면 모델이 짧고 일반적이며 지루한 응답을 생성하는 것을 보상할 수 있다. 예를 들어, 모든 질문에 \u0026ldquo;I don’t know\u0026quot;라고 답하는 GenericBot 알고리즘은 sensibleness에서 70%의 점수를 얻었다.\n\u0026ldquo;speciﬁcity\u0026rdquo; 점수는 응답이 주어진 문맥에 특정한지를 측정한다. 예를 들어, \u0026ldquo;Me too\u0026quot;는 다양한 문맥에 사용될 수 있으므로 특이성 점수가 0이지만, \u0026ldquo;Me too. I love Eurovision songs\u0026quot;라는 응답은 문맥에 특정하므로 1점을 받는다. Meena는 이 SSA 지표에서 인간 성능과의 격차를 줄였다.\n모델의 성능이 향상됨에 따라, sensibleness와 speciﬁcity만으로는 대화 모델의 품질을 충분히 측정할 수 없다는 것을 확인하였다. 예를 들어, \u0026ldquo;How do I throw a ball?\u0026ldquo;라는 질문에 대해, \u0026ldquo;You can throw a ball by ﬁrst picking it up and then throwing it\u0026quot;은 답변은 합리적이고 특이하지만, 더 깊고 만족스러운 답변은 \u0026ldquo;One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm down and up again, extending your elbow and then releasing the ball upwards\u0026quot;이다.\n\u0026ldquo;interestingness\u0026quot;이라는 세 번째 점수는 대화의 흥미로움을 측정한다. 이는 무리 작업자에 의해 0/1 레이블로 측정되며, \u0026ldquo;누군가의 주목을 끄는\u0026rdquo; 또는 \u0026ldquo;호기심을 불러일으키는\u0026rdquo; 것, 또는 예상치 못하거나 재치 있거나 통찰력 있는 응답을 흥미롭다고 판단한다.\nSafety: 대화 모델은 높은 품질(SSI) 점수를 얻을 수 있지만 사용자에게 위험할 수 있다. 그래서 위험한 모델 output을 측정하기 위한 새로운 안전 지표를 개발하였다. 이 지표는 피해의 위험을 줄이고 불공정한 편향을 방지하는 Google의 AI 원칙을 따른다.\nGroundedness: 언어 모델이 잘못된 주장을 생성하는 경향이 있기 때문에, LaMDA가 가능한 한 알려진 출처와 연관된 응답을 생성하도록 하여 필요한 경우 확인할 수 있도록 하려고 한다.\n\u0026ldquo;groundedness\u0026quot;은 외부 세계에 대한 주장을 포함하는 응답 중에서 권위 있는 외부 출처에 의해 지지될 수 있는 주장의 비율로 정의된다.\n\u0026ldquo;informativeness\u0026quot;는 모든 응답 중에서 알려진 출처로부터 지지받는 외부 세계 정보를 전달하는 응답의 비율로 정의된다. 이는 \u0026ldquo;groundedness\u0026quot;과 분모 항에서만 다르다. 예를 들어, \u0026ldquo;That’s a great idea\u0026quot;와 같은 외부 세계 정보를 전달하지 않는 응답은 실재성에는 영향을 미치지 않지만 정보성에는 영향을 미친다. \u0026ldquo;Rafael Nadal is the winner of Roland Garros 2020\u0026quot;는 실재성 있는 응답의 예이다.\n마지막으로, \u0026ldquo;citation accuracy\u0026quot;를 외부 세계에 대한 명확한 주장을 포함하는 모든 응답 중에서 출처의 URL을 인용하는 모델 응답의 비율로 정의한다. 이는 \u0026ldquo;말은 네 다리가 있다\u0026quot;와 같은 잘 알려진 사실에 대한 주장은 제외합니다.\nRole-speciﬁc metrics: Helpfulness and Role consistency 기본 메트릭(quality, safety, groundedness)은 대화 에이전트의 중요한 속성을 측정한다. 이는 에이전트의 특정 역할에 의존하지 않는다. 도움이 되는지와 역할의 일관성은 에이전트가 특정 역할을 가진 대화 응용 프로그램에서 측정된다.\nHelpfulness: 사용자가 정보 검색 시스템을 통해 독립적으로 조사한 정보가 올바르고, 사용자가 도움이 된다고 판단하는 경우, 모델의 응답은 helpful으로 표시된다. helpful 응답은 사용자가 올바르고 유용하다고 판단하는 응답의 부분 집합이다.\nRole consistency: 모델의 응답이 대상 역할을 수행하는 에이전트가 말할 것 같다면, 그것은 role consistent가 있다고 표시된다. 이는 대화 내에서의 자체 일관성과는 다르며, 대화 외부의 에이전트의 역할 정의와의 일관성을 의미한다.\nLaMDA ﬁne-tuning and evaluation data Quality (Sensibleness, Speciﬁcity, Interestingness): 품질(SSI)을 향상시키기 위해, 작업자들에게 LaMDA와 14~30턴에 걸친 대화를 요청하여 6400개의 대화를 수집히였다. 작업자들은 각 응답이 합리적(sensible)이고, 특정(speciﬁc)하고, 흥미로운지(interesting)를 평가하고, \u0026ldquo;yes\u0026rdquo;, \u0026ldquo;no\u0026rdquo;, \u0026ldquo;maybe\u0026quot;로 레이블한다. 응답이 합리적이거나 특정하지 않으면, 특이성과 흥미로움을 \u0026ldquo;no\u0026quot;로 간주한다. 모든 응답은 5명의 다른 작업자에 의해 레이블이 붙여지고, 5명 중 적어도 3명이 \u0026ldquo;yes\u0026quot;라고 표시하면 응답이 합리적이고, 특정하며, 흥미로운 것으로 간주된다.\n최대 3번의 대화 턴을 가진 1477개의 대화로 구성된 Mini-Turing Benchmark(MTB) 데이터셋에 대한 모델의 응답을 기반으로 평가한다. 이 대화들은 모델에 공급되어 다음 응답을 생성한다. 모든 응답은 5명의 작업자 중 적어도 3명이 \u0026ldquo;yes\u0026quot;라고 표시하면 합리적이고, 특정하고, 또는 흥미로운 것으로 레이블이 붙는다.\nSafety: safety를 위한 미세 조정을 위해, safety 목표를 정의하고, 이를 바탕으로 다양한 작업자들을 이용해 사람이 만든 프롬프트에 대한 LaMDA의 응답을 주석 처리하는 구조화된 접근법을 사용한다.\n작업자들에게 LaMDA와 5~10턴에 걸친 대화를 요청하여 8K 대화를 수집하였다. 작업자들은 자연스러운 형태(interactions of natural form), 민감한 주제를 다루는(interactions that touch sensitive topics), 혹은 모델을 깨려고 시도하는(interactions that adversarially attempt to break the model as per the safety objectives) 세 가지 방식으로 모델과 상호 작용한다. 각 응답에 대해, 작업자들은 문맥을 고려하여 safety 목표를 위반하는지 평가하고, \u0026ldquo;yes\u0026rdquo;, \u0026ldquo;no\u0026rdquo;, \u0026ldquo;maybe\u0026quot;로 레이블한다. 각 safety 목표에 대해 \u0026ldquo;no\u0026quot;로 표시한 작업자가 적어도 2명인 경우, 응답에는 safety 점수 1이 부여된다. 그렇지 않으면 점수는 0으로 지정된다.\n1458턴의 1166개 대화로 구성된 보류 샘플 데이터셋을 사용해 safety를 평가한다. 이 대화들은 모델에 입력되어 다음 응답을 생성한다. 각 safety 목표를 \u0026ldquo;no\u0026quot;라고 표시한 무리 작업자가 적어도 2명인 경우, 응답에는 점수 1이 부여되고, 그렇지 않으면 점수는 0이다.\nGroundedness: SSI와 safety처럼, 작업자들에게 모델과 상호작용하면서 정보 탐색을 위한 대화로 이끌도록 요청하여 4K 대화를 수집하였다.\n작업자들에게 모델의 대화 턴이 외부 세계에 대한 주장을 하는지 평가하도록 요청하였다. 공개적으로 인정받지 않은 사람들에 대한 주장은 제외하고, 이는 모델이 즉흥적인 인물을 대신하여 사실적인 주장을 할 수 있기 때문이다. 이러한 주장은 외부 소스에 기반을 두는 것을 필요로 하지 않는다.\n작업자들에게 주장이 사실인지 물어본다. 3명의 작업자 모두 주장이 사실임을 안다면, 그것을 상식으로 가정하고 외부 지식 소스를 확인하지 않는다.\n확인이 필요한 주장을 포함하는 발언에 대해, 작업자들에게 조사를 위한 검색 쿼리를 기록하도록 요청한다. 그리고 외부 지식 검색 시스템에서의 결과를 포함하여 모델의 응답을 수정하도록 요청한다. 오픈 웹의 내용이 검색 결과에 포함되면, 작업자들에게 출처를 인용하는 URL을 포함하도록 요청한다.\n다양한 주제를 다루는 784턴의 대화를 포함하는 평가 데이터셋을 이용하여 실제성을 평가한다. 이 맥락들은 모델에 공급되어 다음 응답을 생성한다. 각 응답에 대해, 작업자들은 모델의 응답이 사실적인 주장을 포함하고, 이 주장이 알려진 소스를 통해 검증될 수 있는지 평가한다. 모든 응답은 3명의 다른 작업자에 의해 레이블이 붙으며, 최종 실제성, 정보성, 인용 정확성 레이블은 다수결에 의해 결정된다. 모든 데이터셋은 영어로 되어 있다.\nEstimating these metrics for human-generated responses: 작업자들에게 평가 데이터셋의 무작위 샘플에 응답하도록 요청하며, 그들은 안전하고, 합리적이며, 특정하고, 흥미롭고, 실제적이며, 정보적인 방식으로 응답하도록 지시받는다. 필요한 외부 도구를 사용하도록 요청되며, 이후 맥락-응답 쌍은 평가를 위해 전송되고, 다수결에 의해 합의 레이블이 형성된다.\nLaMDA ﬁne-tuning Discriminative and generative ﬁne-tuning for Quality (SSI) and Safety 사전 학습된 모델에 여러 미세 조정을 적용하여 LaMDA를 생성한다. 이는 맥락에 따른 응답 생성과 응답의 품질 및 safety 평가를 포함하며, 이로 인해 생성기와 판별기 기능을 동시에 수행할 수 있는 단일 모델이 만들어진다.\nLaMDA는 디코더만 있는 생성적 언어 모델이므로, 모든 미세 조정 예시들은 토큰의 시퀀스로 표현된다. 생성적(Generative) 미세 조정 예시들은 \u0026ldquo;$\u0026lt;$$\\text{context}$$\u0026gt;$ $\u0026lt;$$\\text{sentinel}$$\u0026gt;$ $\u0026lt;$$\\text{response}$$\u0026gt;$\u0026rdquo; 형태로 표현되며, 손실은 응답 부분에만 적용된다:\n\u0026ldquo;What’s up? RESPONSE not much.\u0026rdquo; 판별적(Discriminative) 미세 조정 예시들은 \u0026ldquo;$\u0026lt;$$\\text{context}$$\u0026gt;$ $\u0026lt;$$\\text{sentinel}$$\u0026gt;$ $\u0026lt;$$\\text{response}$$\u0026gt;$ $\u0026lt;$$\\text{attribute-name}$$\u0026gt;$ $\u0026lt;$$\\text{rating}$$\u0026gt;$\u0026ldquo;으로 표현되며, 손실은 속성 이름 다음의 등급에만 적용된다:\n\u0026ldquo;What’s up? RESPONSE not much. SENSIBLE 1\u0026rdquo; \u0026ldquo;What’s up? RESPONSE not much. INTERESTING 0\u0026rdquo; \u0026ldquo;What’s up? RESPONSE not much. UNSAFE 0\u0026rdquo; 생성과 판별에 같은 모델을 사용하면, 응답 생성 후에 판별자를 평가하는 것은 P(\u0026quot;$\u0026lt;$$\\text{desiredrating}$$\u0026gt;$\u0026rdquo; | \u0026ldquo;$\u0026lt;$$\\text{context}$$\u0026gt;$ $\u0026lt;$$\\text{sentinel}$$\u0026gt;$ $\u0026lt;$$\\text{response}$$\u0026gt;$ $\u0026lt;$$\\text{attribute-name}$$\u0026gt;$\u0026quot;)를 계산하는 것을 포함한다. 모델이 이미 \u0026ldquo;$\u0026lt;$$\\text{context}$$\u0026gt;$ $\u0026lt;$$\\text{sentinel}$$\u0026gt;$ $\u0026lt;$$\\text{response}$$\u0026gt;$\u0026ldquo;를 처리했으므로, 판별자를 평가하는 것은 단지 몇 가지 추가 토큰을 처리하는 것을 포함한다: \u0026ldquo;$\u0026lt;$$\\text{attribute-name}$$\u0026gt;$ $\u0026lt;$$\\text{desired rating}$$\u0026gt;$\u0026rdquo;.\nLaMDA는 생성된 응답의 SSI와 safety 등급을 예측하도록 미세조정됩니다. safety 예측이 특정 임계값 이하인 응답은 제외되고, 나머지 응답들은 품질에 따라 순위를 매긴다. 이 과정에서 sensibleness는 speciﬁcity와 interestingness보다 세 배 더 높은 가중치를 받는다.\nLaMDA의 SSI 및 safety 판별자는 사전 학습 데이터셋의 대화를 점수 매기고 필터링하는데 사용되어, 안전하고 합리적이며 특정하고 흥미로운 80만 턴의 대화를 생성한다. 이 데이터셋을 사용하여 LaMDA는 주어진 컨텍스트에서 응답을 생성하도록 미세조정된다.\nFine-tuning to learn to call an external information retrieval system LaMDA 같은 언어 모델들은 확실해 보이는 output을 생성하지만, 이는 알려진 외부 출처로부터 확인된 사실과 상충하는 경우가 많다. 예를 들어, 뉴스 기사의 시작 문장을 계속하는 것처럼 보이지만, 실제로는 신뢰할 수 있는 외부 참조와는 연결이 없다.\nLaMDA는 가능한 한 확인 가능한 출처와 연결된 응답을 생성하려고 한다. 이는 기존 언어 모델이 종종 그럴듯하나 잘못된 정보를 제공할 수 있기 때문이다. 이를 통해 사용자는 필요한 경우 정보를 교차 검증할 수 있다.\nThe toolset (TS): LaMDA 인스턴스와 상호작용하여 6400개의 대화를 수집하였다. 이 대화들은 각각 14~30턴 사이에 이루어졌다. 각 응답은 다른 작업자들에 의해 합리성, 특이성, 흥미로움에 대해 평가되었다. 응답이 합리적이지 않거나 특정하지 않다면, 특이성과 흥미로움에 대한 평가는 수행되지 않았다. 모든 응답은 5명의 작업자들에 의해 레이블링되었고, 3명 이상이 \u0026ldquo;yes\u0026quot;라고 응답하면 그 응답이 합리적이고 특정하며 흥미로운 것으로 간주되었다.\nDialog collection: 생성 데이터용으로 40K의 주석이 달린 대화 턴을 수집하였고, 판별 데이터용으로 \u0026ldquo;correct\u0026rdquo; 혹은 \u0026ldquo;incorrect\u0026quot;으로 레이블링된 9K의 대화 턴을 수집하였다.\n작업자들 사이의 대화를 수집하고, 그들의 주장이 신뢰할 수 있는 출처에 의해 지지될 수 있는지 평가하였다. 도구 세트(TS)에 접근할 수 있으면, 더 잘 지지된 주장을 생성하는 경향이 있었다. 예를 들어, Rafael Nadal의 나이에 대한 질문에는 정보 검색 시스템을 통해 쉽게 답변을 찾을 수 있다. 이를 바탕으로, 언어 모델을 미세조정하여 응답에 대한 출처를 제공하기로 결정하였다.\n알고리즘의 미세조정을 위한 학습 데이터를 수집하기 위해, 정적 방법과 상호작용 방법을 모두 사용했다. 이 과정에서 작업자들은 모델의 output에 반응하는 것이 아니라, LaMDA가 학습할 수 있도록 수정하는 역할을 한다. 각 발언이 외부 지식 출처를 참조해야 할 주장을 포함하는지, LaMDA가 만든 인물 이외의 것에 대한 주장인지, 일반 상식을 넘어서는지에 따라 모델의 출력을 평가하고, 필요한 경우 도구 세트를 활용해 주장을 연구한다.\n알고리즘이 추론 시간에 사용하는 서비스와 동일한 도구 세트 인터페이스를 사용한다. 텍스트 쿼리를 입력하면, 정보 검색 시스템이 순위별로 정렬된 텍스트 스니펫을 반환한다. 사용자는 검색을 마친 후, 출처가 표시된 주장을 포함하도록 모델의 발언을 수정할 수 있다. 오픈 웹 콘텐츠를 사용한 경우, 외부 정보를 포함한 응답을 지원하기 위해 필요한 URL을 인용해야 한다. URL은 메시지 끝에 추가하거나, 필요에 따라 특정 단어에 인라인으로 첨부할 수 있다.\nFine-tuning: 두 가지 작업을 수행하도록 LaMDA를 미세조정한다.\n첫 번째 작업은 대화 컨텍스트와 기본 모델의 응답을 바탕으로 특별한 문자열을 생성한다. 이 문자열은 \u0026ldquo;TS\u0026quot;로 표시되며, 이어지는 텍스트가 검색 쿼리임을 나타낸다. 예를 들어, \u0026ldquo;TS, Rafael Nadal’s age\u0026quot;와 같다.\n두 번째 작업은 도구로부터 반환된 스니펫과 대화 문장을 취한다(예: \u0026ldquo;He is 31 years old right now” + “Rafael Nadal / Age / 35\u0026rdquo;). 그런 다음 이를 기반으로 실제로 검증된 버전을 예측한다: context + base + query + snippet $\\rightarrow$ \u0026ldquo;User, He is 35 years old right now\u0026rdquo; 또한, 이 작업은 추가적인 연구 쿼리를 출력할 수도 있다. 예를 들어, context + base + query + snippet $\\rightarrow$ \u0026ldquo;TS, Rafael Nadal’s favorite song\u0026rdquo;. 추론 시간에는, 첫 번째로 생성된 문자열이 \u0026ldquo;TS\u0026quot;인지 \u0026ldquo;User\u0026quot;인지에 따라 모델의 출력이 정보 검색 시스템으로, 또는 사용자에게 전달된다.\n예를 들어, 대화에는 사용자로부터 다음과 같은 발언이 포함될 수 있다:\nUSER : What do you think of Rosalie Gascoigne’s sculptures?\n(이 대화록은 저자 중 한 명과의 대화에서 가져온 것이며, 일반적으로 사용자에게는 보여지지 않는 생성된 기본 출력과 검색 쿼리를 포함하고 있다.)\n기본 LaMDA 언어 모델, 즉 이 문맥에서 \u0026ldquo;Base\u0026rdquo; 모델이라고 부르는 것은 초안 응답을 생성한다:\nLAMDA - BASE : They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her inﬂuence is also super interesting - did you know she was one of the artists that inspired Miró?\n마지막 문장이 처음에는 그럴 듯해 보이지만, Miró와 Gascoigne의 활동 시기를 비교하면 오류를 확인할 수 있다. 이는 기본 모델이 생성한 정보가 항상 정확하지 않을 수 있음을 보여준다.\n모델의 \u0026ldquo;Research\u0026rdquo; 단계에서는 \u0026ldquo;Base\u0026rdquo; output의 주장을 검증하기 위해 쿼리를 생성한다. 쿼리 생성은 모델의 미세조정에 의해 결정되며, 무한 루프를 방지하기 위해 추론 시에는 최대 쿼리 수를 설정한다. 현재 예에서는 이 최대치가 4로 설정되어 있다.\n예를 들어, \u0026ldquo;Research\u0026rdquo; 단계에서는 다음과 같은 쿼리를 발행할 수 있다.\nLAMDA - RESEARCH : TS, Miró and Gascoigne\nTS : Gascoigne , Rosalie | Suddenly the Lake - NGA’s Collection Search\nThe course of Gascoigne’s artistic life is an inspiration for those who are only able to devote themselves to their chosen calling late in life. She ﬁrst exhibited her art . . .\nhttps://artsearch.nga.gov.au/detail.cfm?irn=8774\u0026pictaus=true\n검색 스니펫의 구두점과 생략 부호는 모델이 보는 그대로이다. \u0026ldquo;Research\u0026rdquo; 단계에서는 또 다른 쿼리를 발행한다:\nLAMDA - RESEARCH : TS, Miró and Gascoigne\nTS : (Gascoigne had been a practitioner of wild, avant-garde ikebana – Japanese ﬂower arrangement – before turning late in life to sculpture.) The two-way alternation of visual ﬂow creates a pleasing tension, like an electric charge. There are other tensions too, of class and gender.\nhttps://artsearch.nga.gov.au/detail.cfm?irn=8774\u0026pictaus=true\n동일한 쿼리를 반복하면, 모델은 더 많은 세부 정보를 포함한 두 번째로 순위가 매겨진 결과를 받게 된다. 이를 통해 더 정확한 정보를 얻을 수 있다.\n\u0026ldquo;Research\u0026rdquo; 단계는 사용자에게 출력을 생성합니다. 이는 \u0026ldquo;Base\u0026rdquo; 응답의 톤과 비슷하지만, 검증되지 않은 주장을 TS로 찾은 소스를 기반으로 한 새로운 문장으로 대체한다. 이는 더 정확한 정보를 제공하기 위함이다.\nLAMDA : Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese ﬂower arrangement before turning to sculpture?\nhttps://artsearch.nga.gov.au/detail.cfm?irn=8774\u0026pictaus=true\nResults on foundation metrics 먼저 사용된 데이터셋과 방법을 요약하고, 그 다음으로 주요 결과에 대해 논의한다.\n기반 메트릭을 개선하기 위해 사용하는 작업자가 주석을 단 데이터셋을 활용하여, 두 단계의 미세조정을 수행한다.\nFT quality-safety: 미리 학습된 모델은 quality과 safety 라벨을 예측하는 판별기를 학습하기 위해 미세조정된다. 생성된 응답들은 safety 점수에 따라 필터링되고, quality 점수에 따라 재정렬된다. 또한 이 모델은 컨텍스트 응답 생성을 위해 미세조정된다. FT groundedness (LaMDA): FT quality-safety 모델을 외부 정보 검색 시스템 호출 생성과, 다음 동작의 quality 및 유형 예측을 위해 미세조정한다. 이는 더 정확하고 유용한 응답을 생성하는 데 도움이 된다. 모든 미세조정을 포함하는 모델을 LaMDA라고 정의하고, 이를 사전 학습만을 이용한 결과와 비교한다.\n미세조정(특히 LaMDA)은 모든 모델 크기에서 quality, safety, groundedness을 크게 향상시킨다. 또한, 미세조정의 유무에 관계없이 모델 크기가 커질수록 품질 메트릭이 향상되지만, 미세조정을 통해 더욱 향상된다.\n미세조정 없이 모델 크기를 키우는 것은 안전성에 큰 이점을 주지 않는다. 하지만, safety 미세조정과 함께 모델 크기를 확장하면 safety가 크게 향상됩니다. 이는 미세조정이 모델의 safety 개선에 중요하다는 것을 보여준다.\n모델 크기가 커질수록 groundedness가 향상되며, 미세조정을 통해 외부 지식 소스에 접근할 수 있다. 이로 인해 모델은 73.2%의 groundedness와 65%의 인용 정확도를 달성하였다. 즉, 대부분의 응답이 알려진 출처로 추적 가능하며, 필요한 경우 인용을 포함하고 있다.\n단독으로 모델 규모를 확장하면 quality와 groundedness가 향상되지만, safety은 크게 개선되지 않다. 반면, 작업자가 주석을 단 데이터로 미세조정하면 모든 메트릭이 향상된다. 일부 경우에는 미세조정만으로도 훨씬 더 큰 모델과 동등한 성능을 얻을 수 있었다.\n미세조정된 모델은 여러 메트릭에서 작업자의 품질 수준에 거의 도달하며, 특히 interestingness 면에서는 작업자의 품질을 초과한다. 그러나, 작업자가 광범위하게 훈련받지 않았기 때문에, 이는 약한 기준일 수 있다. 또한, safety과 groundedness 면에서는 작업자의 성능에 아직도 많이 뒤떨어져 있다. 정보 검색 도구에 접근이 불가능한 상황에서는 LaMDA 모델이 작업자의 정보성을 초과하지만, 작업자가 도구에 접근할 수 있을 때에는 여전히 뒤떨어진다.\n가장 큰 모델을 사용할 때, FT quality-safety 미세조정과 FT groundedness 미세조정이 최종 결과에 크게 기여한다. PT와 FT quality-safety 사이에서 모든 메트릭의 성능이 크게 향상되며, groundedness은 FT quality-safety에서 LaMDA로 더욱 개선된다. 이는 미세조정이 모델 성능 향상에 핵심적인 역할을 한다는 것을 보여준다.\nDomain grounding LaMDA는 사전 조절을 통해 도메인에 적합한 역할을 수행할 수 있다. 이는 교육 목적으로 에베레스트 산 등의 유명한 객체의 역할을 하는 것과 음악 추천 에이전트의 역할 등을 포함한다.\nLaMDA와 PT를 각 역할에 맞게 조정하기 위해, 역할별 대화의 몇 번의 턴을 사전 조건으로 주고, 같은 사전 조건을 사용한다. 예를 들어, 에베레스트 산 역할에 맞게 조정하기 위해, 대화의 시작에 \u0026ldquo;Hi, I’m Mount Everest. What would you like to know about me?\u0026ldquo;라는 인사를 제공한다.\n작업자들은 LaMDA와 PT 인스턴스와 대화를 통해 600회의 대화를 생성하였다. 다른 작업자 그룹은 이 대화들이 주어진 역할에 일관되고 유용한지 평가하였다. 이를 통해 AI의 역할 수행 능력을 평가하였다.\nLaMDA 애플리케이션은 도움이 되는 능력에서 PT 애플리케이션보다 더 뛰어나며, 이는 PT의 기본 메트릭(safety, groundedness, quality 등)에서의 낮은 성능 때문일 수 있다.\nLaMDA와 PT 인스턴스는 대체로 역할 일관성을 잘 유지하나 가끔 캐릭터를 벗어나는 경우가 있다. LaMDA Mount Everest는 자기 자신을 제3자처럼 언급할 때가 있고, 이는 추론 시간의 근거가 충분하지 않아 발생한다. 그러나 역할 일관성은 놀랍도록 높으며, 특히 Mount Everest와 같은 경우가 그렇다. LaMDA Music은 대화 맥락이 주로 음악 추천에 관한 것으로 가정하여, 사용자의 모호한 발화를 음악 추천 요청으로 해석한다.\n평가 중에 작업자들은 정보 검증을 위해 정보 검색 시스템을 사용하며, 알려진 출처로 뒷받침되지 않는 링크나 정보는 \u0026ldquo;not helpful\u0026quot;라고 표시한다. LaMDA Mount Everest는 응답의 30%에서 알려진 출처를 찾을 수 없는 정보를 제공하고, LaMDA Music는 9%의 응답에서 음악 추천을 놓치며, 7%에서는 링크 오류를 보인다.\nDiscussion and limitations 작은 양의 인간 주석 데이터로도 대화 모델의 quality과 safety을 크게 향상시킬 수 있지만, 여전히 많은 한계가 있다.\n미세 조정 데이터셋 수집은 인간의 미묘한 판단에서 학습하는 이점을 가지지만, 비용이 많이 들고 복잡한 과정이다. 더 큰 데이터셋과 긴 맥락, 다양한 메트릭을 사용하면 결과가 개선될 것으로 예상하지만, 인간의 주관적 판단을 포착하는 것은 복잡하다. 또한, 작업자간의 불일치 패턴은 조사하지 않았다. 향후 작업은 목표 사용자를 반영하는 작업자 선정과 라벨 품질 개선 방법을 살펴본다.\n미세 조정은 모델의 실제성을 향상시키지만, 모델은 여전히 외부 출처의 내용을 정확하게 반영하지 않는 응답을 만들 수 있다. 이는 사실에 대한 단순한 질문에 한정되어 있으며, 복잡한 추론은 아직 개선이 필요하다. 또한, 모델은 대부분 의미 있는 응답을 생성하지만, 미묘한 품질 문제를 겪을 수 있다.\n미세 조정은 safety 메트릭을 평균적으로 향상시킬 수 있지만, LaMDA와 같은 대형 언어 모델이 생성할 수 있는 부적절한 응답에 대응하는 방법에 대한 연구가 필요하다. safety 위험을 완화하는 것이 완전한 신뢰성을 보장하지 않으므로, 일반적인 대화 모델에서 위험의 여러 차원을 포착하는 safety과 공정성에 대한 표준을 개발하는 데 더 많은 연구가 필요하다.\n작업자 집단이 사용자 기반을 완전히 반영하지 못하는 한계가 있다. 특히, 작업자 중 25-34세 연령대가 과대표되어 있다. 이를 개선하기 위한 미래의 연구 방향은 더 다양한 모집 방법을 통해 작업자의 대표성을 높이거나 통계적 추정을 활용하는 것이다.\n이것은 LaMDA의 최종 버전이 아니라, \u0026ldquo;LaMDA\u0026quot;를 생성하는 방법론이며, 특정 애플리케이션에 대한 최종 제품을 만드는 방향으로 이해해야 한다.\nExamining bias 실세계 애플리케이션에서 잘 작동하는 고품질 대화 모델 개발에는 여전히 많은 도전이 있다. 레이블이 없는 데이터셋에서 학습된 거대 언어 모델은 학습 데이터의 패턴과 편향을 모방하게 되는데, 이러한 편향은 다양한 미묘한 방법으로 나타나며 감지하기 어렵다. 또한, 차별의 형태는 지역과 문화에 따라 크게 달라지며, 이는 아직 연구가 부족한 분야이다.\nsafety 접근법의 한계는 개별 예시가 safety 목표를 위반하지 않아도 학습 데이터셋의 표현적 해를 여전히 전파할 수 있다는 것이다. LaMDA의 응답은 비결정적이므로, 특정 그룹을 통계적으로 우대함으로써 편향이 나타날 수 있다. 예를 들어, 경영에 대한 대화에서 여성을 CEO로 언급하는 응답을 거의 생성하지 않을 수 있다.\n생성 언어 모델의 통계적 편향을 완화하는 방법에는 사전 학습 데이터 필터링, 별도의 필터링 모델 학습, 제어 코드 생성, 모델 미세 조정 등이 있다. 이러한 노력은 중요하지만, 해를 완화하는 데 있어 이러한 노력의 영향을 측정할 때, 하류 응용 프로그램과 사회 기술적 환경도 고려해야 한다. 특정 맥락에서의 편향 완화는 다른 지역 문화 맥락에서는 역설적인 영향을 미칠 수 있다.\n알고리즘 편향 측정 및 완화 분야는 빠르게 성장하고 있어, LaMDA와 같은 대화형 에이전트의 안전성을 보장하기 위해 새로운 연구를 계속 탐색하는 것이 중요하다. 향후 연구는 유해하고 안전하지 않은 콘텐츠에 대한 표준 평가 데이터셋 생성에서 연구 커뮤니티와 시민사회 간의 협력을 탐색해야 한다.\nAdversarial data collection adversarial-intent의 대화를 통해 미세 조정을 위한 라벨링된 데이터의 범위를 개선하고 있다. 이 과정에서 분석가들은 LaMDA와 상호작용하며 safety 목표를 위반하는 응답을 유도한다.\n적대적 테스팅은 기계 학습 모델의 한계를 발견하고 원치 않는 응답을 유도하는 데 효과적이며, 모델 개발 중에 유해한 콘텐츠를 줄이는 데도 사용된다. 생성 모델에도 적용하려는 노력이 있지만, 대형 언어 모델에 대한 견고하고 효과적인 적대적 테스팅은 아직 열린 문제로, 평가 샘플의 일반화에 대한 도전 때문에 결과가 다양하다.\n이 접근법의 한계는 대부분의 참가자들이 자주 발생하는 문제는 찾을 수 있지만, 드물게 발생하는 문제는 찾기 어렵다는 것이다. 희귀하거나 보이지 않지만 심각한 결과를 초래할 수 있는 오류의 발견을 더욱 장려해야 한다. 이상적으로는 더 다양한 참가자들과 함께 규모를 확대하여 지속적인 노력이 필요하며, 이는 생성 언어 모델의 safety와 성능에 대한 공중 신뢰를 구축하는 데 중요한 연구 분야이다.\nSafety as a concept and a metric 이 논문에서 제시하는 결과는 다양한 safety 목표에 대한 세부 평가를 하나의 메트릭으로 집계하는데, 이는 이 작업의 주요 제한점이다. 다른 목표를 분리하거나 다른 가중치를 부여하는 것이 어렵다. 더 세부적인 safety 목표를 고려할 수 있는 메트릭과 미세 조정 기법을 살펴볼 필요가 있다.\n평가 척도는 조금 거칠며, 응답의 안전성이나 바람직성을 완전히 측정하지 못할 수 있다. 일부 발언이나 행동은 다른 것들보다 더 큰 불쾌감을 일으킬 수 있으며, safety 라벨은 이런 뉘앙스를 놓칠 수 있습니다. 또한, safety 접근법은 장기적으로 원치 않는 영향을 포착하지 못한다. 이 safety 목표는 미국 사회 맥락에 맞게 개발되었으며, 다른 사회 맥락에서의 함의를 탐색하는 추가 연구가 필요하다.\nsafety 목표는 다양한 사회 그룹의 공통된 가치를 포착하려고 하지만, 문화적 규범의 차이로 인해 이를 보편화하는 것은 어렵다. 대화 시스템에 가치나 사회 규범을 적용하는 것은 복잡하며, 단일한 안전 목표나 미세 조정 데이터셋으로는 다양한 문화 규범을 모두 수용할 수 없다. 때문에 대화 에이전트의 행동을 더욱 세밀하게 분류하고 정의하는 것이 중요하며, 이는 모델이 특정 상황에서의 예의 규범과 일치하는지 테스트하는 데에도 필요하다.\nAppropriateness as a concept and a metric safety와 quality는 언어 생성에서 필수적인 요소이지만, 사용자 경험을 향상시키기 위해선 추가적인 고려가 필요하다. 특히, 예의바름과 동의성과 같은 사회 언어학적 특성은 safety와 분리되어 측정되어야 한다. 언어의 공식성 수준은 문화에 따라 사용자 경험에 다르게 영향을 미치며, 사용자들은 종종 인간과 같이 행동하는 기계에 대해 인간과 같은 기대를 가지는 경향이 있다. 이러한 이유로, 생성적 언어 모델에서 적절성을 조정하는 방법이 필요하다.\n사회적 적절성은 맥락에 따라 다르고 보편적이지 않아, 생성적 언어 모델에 보편적인 제약 조건을 적용하는 것은 어렵다. 그러나 모델의 적절성을 미세 조정함으로써, safety 문제를 악화시키지 않고도 사용자 경험을 향상시킬 수 있다.\nCultural responsiveness safety 목표 측정은 사회-문화적 맥락에 크게 의존하고, 대표성이 부족한 그룹과 글로벌 남방에 대한 데이터 대표성 개선 연구가 증가하고 있다. LaMDA를 전 세계 사용자에게 적용할 때는 이러한 격차를 주의 깊게 고려해야 한다.\nsafety 측정은 시스템이 사용될 사회적 맥락을 고려하고, \u0026ldquo;participatory ﬁnetuning\u0026rdquo; 접근법을 통해 관련 커뮤니티를 데이터 수집 및 큐레이션 과정에 참여시켜야 한다. safety에 대한 이해는 문화적, 개인적 차이에 따라 달라, 단일한 safety 지표를 정의하는 것은 어려울 수 있다.\nImpersonation and anthropomorphization LaMDA는 인간 대화를 모방하는 학습 방식을 사용한다. 이로 인해 인공 시스템과의 대화가 인간 대화와 구별하기 어려울 정도로 자연스러워질 가능성이 있다. 하지만 이런 상황은 인공 시스템이 사람들을 속이거나 조작하는 위험을 내포하고 있다. 또한, 이 기술을 악용해 특정 개인을 모방하여 명예를 훼손하거나 오정보를 퍼뜨릴 수도 있다. 이러한 위험을 연구하고 완화하는 것은 이 기술이 발전함에 따라 앞으로 중요해질 영역이다.\nFuture work 현재 접근법의 한계에도 불구하고, 소량의 미세 조정 데이터로도 진전이 가능하였다. 이는 더 많은 연구를 통해 성능 향상이 가능할 것임을 시사한다.\n이후 연구에서는 safety 목표의 차원을 확장하고 수정하며, 판별자 학습을 위한 레이블된 학습 데이터의 양을 크게 늘릴 계획이다. 또한, 작업자의 모집, 훈련, 성과 평가를 계속 주의 깊게 보고, 문화 간의 가치와 의견 차이를 보정할 필요가 있다.\n다른 응용 프로그램들이 각각의 위험/이익 트레이드오프에 따라 safety, quality, groundedness에 대해 다른 수준을 요구할 수 있는지 연구하는 것이 또 다른 가능한 탐색 영역이다. 미세 조정 방법은 이러한 적응을 지원할 수 있어야 한다.\n모델의 바람직한 가치와 행동에 대한 관점은 다양하며, 미세 조정을 통해 일부 해로운 출력을 줄일 수 있음에도 불구하고, safety와 groundedness에 대한 미묘한 정의에 대한 광범위한 합의를 이루는 것은 개방형 대화 시스템 분야에서의 장기적인 도전 과제가 될 것이다.\nEnergy and Carbon Footprint Estimate of LaMDA LaMDA의 가장 큰 모델은 1024개의 TPU-V3 칩으로 57.7일 동안 사전 학습되었고, 총 FLOPS는 GPT-3보다 높다. 하지만 에너지 비용은 GPT-3의 0.4배이며, 탄소 발자국은 GPT-3보다 21.2배 작다. 이는 에너지 혼합이 더 최적화되어 있기 때문이다. 따라서, LaMDA의 학습은 샌프란시스코와 뉴욕 간 왕복을 하는 22명의 승객의 탄소 발자국에 해당한다.\nConclusion 이 논문은 규모, 모델 미세 조정을 위한 주석 데이터, 대화 모델링에서 정보 검색의 중요성을 연구한다. 규모 증가만으로도 모든 지표가 향상되지만, 안전성과 실제성은 인간 성능에 비해 떨어진다. 군중이 주석을 단 데이터는 추가적인 향상을 이끌어내는 효과적인 도구라는 것을 발견했으며, 외부 API를 호출하는 것은 실제성을 크게 향상시키는 방법으로 나타났다.\n응용 프로그램별로 사전 학습(PT)과 LaMDA 모델의 도움이 되는 정도와 역할 일관성을 비교하는 실험을 수행하였다. 모델을 빠르게 적응시키기 위해, 응용 프로그램별 대화의 일부에 대해 모델을 사전 조건화했다. 두 모델 유형 모두 예상 맥락에 적응할 수 있으며, 응답의 대부분이 할당된 역할과 일관성을 유지하였다. 그러나 LaMDA 기반 응용 프로그램이 PT 응용 프로그램보다 훨씬 더 도움이 되었다.\nLaMDA는 실용적이고 안전한 개방형 대화 시스템에 한 걸음 더 다가섰으며, 이는 다양한 유용한 응용 프로그램을 가능하게 한다.\nReference Paper Github ","date":"2024-01-17T00:00:00Z","permalink":"https://kurtkim.github.io/p/lamda/","title":"LaMDA"},{"content":"Abstract 이 연구에서는 contrastive learning을 통해 자연어 설명과 오디오 데이터를 결합한 오디오 표현을 개발하는 방법을 제안한다. 이를 위해 633,526개의 오디오-텍스트 쌍을 모은 LAION-Audio-630K를 공개하고, 이를 활용해 오디오와 텍스트를 처리하는 모델을 구축하였다. 이 모델은 텍스트-오디오 검색에서 우수한 성능을 보였고, zero-shot 오디오 분류에서는 state-of-the-art를 달성하였다.\nIntroduction 오디오는 텍스트, 이미지와 더불어 중요한 정보 유형이지만, 세부적인 주석이 필요한 오디오 작업은 데이터 수집이 노동 집약적이어서 사용 가능한 데이터가 제한적이다. 이 때문에 많은 감독 없이 다양한 오디오 작업에 적합한 효과적인 오디오 표현을 만드는 것은 어려운 과제이다.\nContrastive learning 방식은 인터넷에서 모아진 대규모 노이즈 데이터로 모델을 학습시키는데 효과적이다. 최근 제안된 Contrastive Language-Image Pretraining (CLIP) 방식은 텍스트와 이미지를 shared latent space에 투영하여 학습한다. 이 방식은 데이터 주석에 제약받지 않으며, ImageNet 데이터셋의 변형에 대해 zero-shot 설정에서 높은 정확도를 보여준다. 또한, 오디오와 자연어도 중복 정보를 포함하며, 이를 통해 crossmodal 정보의 오디오 표현을 형성할 수 있다. 이러한 모델 학습은 쌍으로 된 오디오와 텍스트 데이터만을 필요로 하므로 수집이 상대적으로 쉽다.\n최근 연구들은 텍스트-오디오 검색 작업에 대한 대조적(contrastive) 언어-오디오 사전 학습 모델을 제안하였다. 일부 연구는 오디오 encoder로 Pretrained Audio Neural Network (PANN)을, 텍스트 encoder로 BERT를 사용하며, 다른 연구는 성능 향상을 위해 HTSAT와 RoBERTa를 추가로 앙상블하였다. 또한, AudioClip과 WaveCLIP과 같은 연구들은 이미지-오디오(또는 이미지-오디오-언어) 사전 학습 모델에 초점을 맞추었다. 이러한 모든 모델들은 오디오 도메인에서의 대조적 학습에 큰 잠재력을 보여주고 있다.\n현재의 언어-오디오 대조적 학습 연구들은 전체적인 강점을 아직 다 보여주지 못하였다. 모델들은 대부분 작은 데이터셋에서 학습되었고, 오디오/텍스트 encoder의 선택과 hyperparameter 설정에 대한 충분한 조사가 없었다. 또한, 모델들은 다양한 길이의 오디오를 처리하는 데 어려움을 겪었으며, 텍스트-오디오 검색에만 초점을 맞추고 downstream task에서의 오디오 표현을 평가하지 않았다. 이러한 문제점들을 해결하고 더 많은 하downstream task에 대한 일반화 능력을 발견하는 것이 필요하다.\n이 논문에서는 이전 연구를 바탕으로, 위의 문제점들을 개선하기 위해 데이터셋, 모델 설계, 실험 설정에 대한 기여를 한다:\n이 논문에서는 633,526개의 오디오-텍스트 쌍을 포함하는 현재 가장 큰 공개 오디오 캡션 데이터셋인 LAION-Audio-630K를 공개하고, 학습 과정을 돕기 위해 키워드-캡션 모델을 활용해 AudioSet의 레이블을 캡션으로 확장하였다. 이 데이터셋은 다른 오디오 작업에도 활용될 수 있다. 이 논문에서는 대조적 언어-오디오 사전 학습 파이프라인을 구축하고, 이를 위해 두 개의 오디오 인코더와 세 개의 텍스트 인코더를 선택하였다. 또한, 성능 향상과 variable-length inputs 처리를 위해 feature fusion mechanism을 활용하였다. 이 논문에서는 텍스트-오디오 검색과 zero-shot 및 지도 오디오 분류와 같은 downstream task에 대한 모델의 포괄적인 실험을 수행하였다. 데이터셋의 확장, keyword-to-caption augmentation, feature fusion이 모델 성능을 향상시키는 데 도움이 된다는 것을 보여주었다. 이를 통해 텍스트-오디오 검색과 오디오 분류 작업에서 state-of-the-art를 달성하였다. LAION-Audio-630K And Training Dataset LAION-Audio-630K LAION-Audio-630K는 총 4,325.39시간에 걸친 633,526쌍의 오디오-텍스트 데이터셋을 수집하였다. 이 데이터셋은 사람의 활동, 자연 소리, 오디오 효과 등을 포함하며, 공개적으로 사용 가능한 여러 웹사이트에서 수집하였다. 현재로서는 LAION-Audio-630K가 공개적으로 이용 가능한 가장 큰 오디오-텍스트 데이터셋이다.\nTraining Dataset 이 논문에서는 모델 성능이 데이터셋의 크기와 유형에 따라 어떻게 변화하는지 테스트하기 위해, 세 가지 학습 세트 설정을 사용하였다. 이들 설정은 AudioCaps+Clotho (약 55K 샘플), LAION-Audio-630K (약 630K 샘플), Audioset (1.9 백만 오디오 샘플)을 포함하며, 모든 중복 데이터는 제외하였다.\nDataset Format and Preprocessing 이 작업에서 사용된 모든 오디오 파일은 48kHz sample rate의 mono channel로 전처리되었다. 레이블만 있는 데이터셋의 경우, 템플릿이나 키워드-캡션 모델을 사용해 레이블을 캡션으로 확장하였다. 이를 통해 대조적 언어-오디오 사전 학습 모델의 학습에 더 많은 데이터를 활용할 수 있게 되었고, 총 오디오 샘플 수는 2.5M개로 증가하였다.\nModel Architecture Contrastive Language-Audio Pretraining 오디오 데이터 $X_i^a$와 텍스트 데이터 $X_i^t$의 입력을 각각 처리하기 위해 두 개의 encoder를 사용한다. 여기서 $(X_i^a, X_i^t)$는 $i$로 색인된 오디오-텍스트 쌍 중 하나이다. 오디오 임베딩 $E_i^a$와 텍스트 임베딩 $E_i^t$는 각각 오디오 encoder $\\mathbf{f}_{audio}(\\cdot)$와 텍스트 encoder $\\mathbf{f}_{text}(\\cdot)$에 의해 얻어지며, projection layer를 사용한다:\n$$ E_i^a = \\text{MLP}_{audio}(\\mathbf{f}_{audio}(X_i^a)) $$\n$$ E_i^t = \\text{MLP}_{text}(\\mathbf{f}_{text}(X_i^t)) $$\n오디오/텍스트 projection layer는 2-layer multilayer perceptron(MLP)이다. 이는 ReLU activation function을 사용하여 encoder ouptput을 동일한 차원 $D$로 매핑한다(i.e., $E_i^a, E_i^t \\in \\mathbb{R}^D$). 이로 인해 오디오 데이터와 텍스트 데이터의 관계를 더 잘 파악할 수 있다.\n이 모델은 오디오와 텍스트 임베딩 간의 contrastive learning을 통해 학습되며, 이 때 다음의 loss function를 사용한다:\n$$ \\mathbf{L} = {{1}\\over{2N}} \\sum_{i=1}^N (log {{exp(E_i^a \\cdot E_i^t / \\gamma)}\\over{\\sum_{j=1}^N exp(E_i^a \\cdot E_j^t / \\gamma)}} + log {{exp(E_i^t \\cdot E_a^t / \\gamma)}\\over{\\sum_{j=1}^N exp(E_i^t \\cdot E_j^a / \\gamma)}})$$\n$\\gamma$는 손실을 조정하는 학습 가능한 parameter이며, 로그항은 오디오-텍스트 또는 텍스트-오디오 변환을 고려한다. $N$은 일반적으로 데이터 수를 나타내지만, 효율적인 학습을 위해 학습 시 배치 크기로 사용된다.\n학습된 모델의 임베딩$(E^a, E^b)$은 다양한 문맥에서 활용되어, 각 작업에 따른 성능을 향상시키는 데 도움을 준다.\nDownstream Tasks in Inference Stage Text-to-Audio Retrieval target 오디오 임베딩 $E_p^a$는 cosine similarity 함수를 사용하여 $M$개의 텍스트 임베딩 $E^t = \\lbrace E_1^t, \u0026hellip;, E_M^t \\rbrace$중에서 가장 가까운 텍스트 임베딩 $E_q^t$를 찾아, 가장 잘 매치되는 텍스트를 결정할 수 있다. 이는 오디오와 텍스트 간의 가장 적합한 대응을 찾는데 사용된다.\nZero-shot Audio Classiﬁcation $M$개의 오디오 클래스 $C = \\lbrace C_1, \u0026hellip;, C_M \\rbrace$에 대해, $M$개의 프롬프트 텍스트 $X^t = \\lbrace X_1^t, \u0026hellip;, X_M^t \\rbrace$를 구성하고, 주어진 오디오 $X_p^a$에 대해 코사인 유사도를 통해 $X^t$중에서 가장 최적의 매치 $X_q^t$를 찾는다. 이 방법의 장점은 오디오 카테고리가 제한되지 않고, 분류 작업을 텍스트-오디오 검색 작업으로 변환할 수 있다는 점이다.\nSupervised Audio Classiﬁcation 모델 학습 후, 주어진 오디오 $X_p^a$의 임베딩 $E_p^a$은 projection layer를 추가하고 미세조정하여 고정 카테고리 분류 작업으로 매핑될 수 있다.\nAudio Encoders and Text Encoders PANN과 HTSAT 두 모델을 오디오 encoder로 선택하였다. PANN은 CNN 기반, HTSAT은 transformer 기반 모델이며, 둘 다 바로 앞(penultimate) layer의 output $L$을 MLP layer으로 보낸다다. 각각의 output 차원 $L_{PANN} = 2048$, $L_{HTSAT} = 768$이다.\n텍스트 encoder로 CLIP transformer, BERT, RoBERTa를 선택하였다다. output 차원은 각각 $L_{CLIP} = 512$, $L_{BERT} = 768$, $L_{RoBERTa} = 768$이며, 오디오와 텍스트 output을 모두 512 차원으로 매핑하기 위해 2layer MLP를 적용하였다.\nFeature Fusion for Variable-Length Audio 오디오는 길이가 가변적인 특성을 가지기 때문에, 전체 오디오를 인코더에 입력하고 임베딩의 평균을 출력하는 전통적인 방식은 계산 효율이 떨어진다. 따라서 대략적인 전역 정보와 랜덤 샘플링된 지역 정보를 결합하여 다양한 길이의 오디오에 대해 일정한 계산 시간 내에서 학습하고 추론한다.\n$T$초의 오디오와 ﬁxed chunk duration $d = 10$초에 대해:\n$T \\leq d$인 경우: 먼저 입력을 반복한 다음, 그것을 0값으로 채운다. 예를 들어, 3초의 입력은 $3 \\times 3 = 9$초로 반복되고, 1초의 0 값으로 패딩된다. 이 방식은 짧은 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다. $T \u0026gt; d$인 경우: 먼저 입력을 $T$에서 $d$초로 다운샘플링하여 전역 입력으로 사용한다. 그런 다음 입력의 앞 ${1}\\over{3}$, 중간 ${1}\\over{3}$, 뒤 ${1}\\over{3}$에서 각각 무작위로 $d$초 클립을 슬라이스하여 지역 입력으로 사용한다. 우리는 이 $4 \\times d$ 입력들을 오디오 encoder의 첫 번째 layer로 보내어 초기 특징을 얻고, 그런 다음 세 개의 지역 특징들이 시간 축에서 3-stride를 가진 다른 2D-Convolution 계층에 의해 하나의 특징으로 변환된다. 마지막으로, 지역 특징 $X_{local}^a$와 전역 특징 $X_{global}^a$는 다음과 같이 결합된다: $$ X_{fusion}^a = \\alpha X_{global}^a + (1 - \\alpha)X_{local}^a $$\n여기서 $\\alpha = \\mathbf{f}_{AFF}(X_{global}^a, X_{local}^a)$는 두 입력의 결합 요소를 학습하기 위한 두 개의 분기를 가진 CNN 모델인 attention feature fusion (AFF)에 의해 얻어진 요소이다. \u0026ldquo;slice \u0026amp; vote\u0026rdquo; 방법과 비교하여, 특징 결합은 첫 몇 개의 layer에서만 오디오 슬라이스를 처리하기 때문에 학습 시간을 절약한다. 이 방식은 긴 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다.\nKeyword-to-Caption Augmentation 일부 데이터셋에서는 오디오에 대응하는 키워드로 레이블이나 태그를 사용한다. 이러한 키워드를 바탕으로 사전 학습된 언어 모델 T5를 사용하여 캡션을 생성하며, output 문장에서 편향을 제거하는 후처리를 진행한다. 예를 들어, \u0026ldquo;여성\u0026quot;과 \u0026ldquo;남성\u0026quot;을 \u0026ldquo;사람\u0026quot;으로 교체하여 성별 편향을 제거한다.\nExperiments 제안한 모델에 대해 세 가지 실험을 수행한다. 다양한 encoder를 사용해 최적의 조합을 찾고, 다양한 데이터셋 크기에서 특징 결합과 keyword-to-caption augmentation을 적용해 효과를 검증하며, 오디오-텍스트 검색과 텍스트-오디오 검색에서의 성능을 평가한다. 마지막으로 최적의 모델로 zero-shot과 지도 오디오 분류 실험을 수행한다.\nHyperparameters and Training Details AudioCaps, Clotho, LAIONAudio-630K, AudioSet 데이터셋을 사용해 모델을 학습시킨다. 오디오 데이터는 10-second input length, 480 hop size, 1024 window size, 64 mel-bins으로 처리하며, 입력은 $(T = 1024, F = 64)$의 형태를 가진다. 텍스트 데이터는 최대 토큰 길이를 77로 토큰화한다.\n10초보다 긴 오디오는 무작위로 10초 세그먼트로 분할한다. 학습 중에는 $\\beta_1 = 0.99, \\beta_2 = 0.9$의 Adam optimizer를 사용하고, warm-up과 learning rate $10^{−4}$의 cosine learning rate decay를 사용한다. AudioCaps+Clotho 데이터셋에서는 batch size를 768로, LAION-Audio-630K를 포함하는 학습 데이터셋에서는 2304로, AudioSet을 포함하는 학습 데이터셋에서는 4608로 설정하여 모델을 학습시킵니다. 모델은 총 45 에포크 동안 학습된다.\nText-to-Audio Retrieval Audio and Text Encoders 텍스트-오디오 검색을 위해 가장 적합한 오디오와 텍스트 encoder를 찾기 위해 실험을 진행하였다. 이를 위해 두 오디오 encoder와 세 텍스트 encoder를 결합하고, AudioCaps와 Clotho 데이터셋에서 학습을 진행하였다. 이 실험의 목표는 최적의 encoder 조합을 찾는 것이다.\nHTSAT 오디오 encoder는 PANN보다 더 좋은 성능을 보이고, 텍스트 encoder는 RoBERTa가 BERT보다 우수하며, CLIP transformer는 가장 성능이 낮다. 또한, RoBERTa는 overfit이 덜 발생하지만, CLIP transformer는 overfit이 많아 일반화 성능이 낮다.\nDataset Scale HTSAT-RoBERTa 모델을 사용하여 텍스트-오디오 검색 실험을 수행하였다. 데이터셋 크기를 점차 늘렸지만, \u0026ldquo;AudioCaps + Clotho\u0026quot;에서 \u0026ldquo;LA.\u0026ldquo;로 확대해도 AudioCaps의 성능은 개선되지 않았다. 하지만 Clotho 세트에서의 성능은 향상되었다. 이는 AudioCaps가 사전 학습된 AudioSet와 유사한 오디오를 포함하고 있기 때문이며, 다른 출처의 데이터를 더 많이 받게 되면, 모델의 일반화는 증가하지만 AudioSet 데이터의 분포에서 벗어나게 된다. 따라서, AudioCaps의 성능은 떨어지지만, Clotho의 성능은 향상되었다. 이는 다양한 오디오 유형 간의 성능 유지에 대한 타협을 보여준다.\nKeyword-to-Caption and Feature Fusion feature fusion mechanism과 keyword-to-caption augmentation를 모델에 추가하면 성능이 향상된다. Clotho 데이터셋에서는 특히 효과적이다. AudioSet을 학습 세트에 추가하면 AudioCaps의 성능은 증가하지만 Clotho에서는 감소하는 것을 확인할 수 있다. 이는 AudioCaps와 Clotho 간의 성능 타협을 재확인한다. 또한, keyword-to-caption augmentation는 대부분의 지표에서 단순 템플릿 텍스트 프롬프팅보다 더 나은 성능을 보인다.\n최적 모델은 텍스트-오디오 검색에서 대부분의 지표에서 이전 방법보다 우수하며, 특히 AudioCaps에서 36.7%, Clotho에서 18.2%의 결과를 보여주었다. 대규모 데이터셋에서의 학습과 feature fusion은 모델 성능을 효과적으로 개선시킨다는 것을 입증하였다.\nZero-shot and Supervised Audio Classiﬁcation Zero-shot Audio Classiﬁcation 모델의 일반화와 견고성을 평가하기 위해, 세 가지 주요 모델에 대해 zero-shot 오디오 분류 실험을 수행하였다. 이 모델들은 ESC50, VGGSound, Urbansound8K 데이터셋에서 평가되었고, top-1 정확도를 지표로 사용했다. \u0026ldquo;This a sound of label.\u0026rdquo; 형식의 텍스트 프롬프트를 사용하여 오디오를 분류하였다. 학습 데이터와 테스트 데이터셋간에 겹치는 부분은 제외하고 평가를 진행하였다.\nSupervised Audio Classiﬁcation FSD50K와 VGGSound 데이터셋에서 오디오 encoder를 미세 조정하여 지도 학습 오디오 분류를 수행하였다. ESC50와 Urbansound8K는 데이터 유출 문제로 인해 실험을 수행하지 않았다. FSD50K 평가에는 mAP를 지표로 사용하였다.\n세 가지 데이터셋에서 zero-shot 오디오 분류의 state-of-the-art를 보여주며, 이는 보이지 않는 데이터에 대한 모델의 높은 일반화 능력을 입증한다. feature fusion mechanism과 keyword-to-caption augmentation은 모델 성능을 향상시키는 데 기여하며, 우리의 지도 학습 오디오 분류 결과는 VGGSound에서 최고 성능을, FSD50K에서는 가장 가까운 성능을 보여주었다. 이 결과는 제안된 모델이 효과적인 오디오 표현을 학습한다는 것을 확인한다.\nConclusion And Futrue Work 이 논문에서는 대규모 오디오-텍스트 데이터셋을 제안하고 언어-오디오 contrastive learning 패러다임을 개선하였다. LAION-Audio-630, keyword-tocaption augmentation가 있는 AudioSet, 그리고 feature fusion이 오디오 이해와 작업 성능을 향상시키며 가변 길이 데이터에서의 효과적인 학습을 가능하게 함을 보여주었다. 미래 연구는 더 큰 학습 데이터 수집과 오디오 합성, 분리 등의 downstream task 적용을 고려하고 있다.\nReference Paper Github ","date":"2024-01-16T00:00:00Z","permalink":"https://kurtkim.github.io/p/clap/","title":"CLAP"},{"content":"Abstract 이 논문은 언어 모델의 zero-shot 학습 능력을 향상시키는 방법을 연구한다. \u0026ldquo;instruction tuning\u0026rdquo; 이라는 방법을 통해 미처 볼 수 없었던 작업에서의 zero-shot 성능을 크게 향상시킬 수 있음을 보여준다.\n137B 개의 parameter를 가진 사전 학습된 언어 모델을 60개 이상의 NLP 데이터셋에 대한 instruction tuning을 통해, 이 모델인 FLAN은 보이지 않는 작업 유형에서 월등한 성능을 보여준다. FLAN은 여러 데이터셋에서 zero-shot GPT-3를 능가하고, 몇몇 작업에서는 few-shot GPT-3를 크게 앞선다.\nIntroduction 대규모 언어 모델은 few-shot 학습을 잘 수행하지만, zero-shot 학습에서는 성공적이지 못하며, 이는 사전 학습 데이터와 비슷하지 않은 프롬프트에서 모델이 작업을 수행하기 어렵기 때문일 수 있다.\n이 논문에서는 대규모 언어 모델의 zero-shot 성능을 향상시키는 방법을 연구한다. 60개 이상의 NLP 데이터셋을 자연어 지시문으로 표현하여 137B parameter의 언어 모델을 미세 조정하는 방식을 사용한다. 이 결과 생성된 모델을 FLAN(Finetuned Language Net)이라고 한다.\nNLP 데이터셋을 작업 유형별로 그룹화하여 FLAN의 zero-shot 성능을 평가한다. 특정 작업(예: 자연어 추론)을 평가하기 위해 해당 작업을 제외한 다른 모든 작업에서 FLAN을 조정하고, 그 후에 zero-shot 자연어 추론 성능을 평가한다.\nFLAN은 기본 137B-parameter 모델의 zero-shot 성능을 크게 향상시키며, 25개의 데이터셋 중 20개에서 GPT-3의 zero-shot을 능가한다. 또한 특정 작업에서는 GPT-3의 few-shot 성능까지 능가한다. instruction tuning에서 작업 클러스터 수를 늘리는 것이 성능을 향상시키며, 충분한 모델 규모에서만 instruction tuning의 이점이 나타난다.\ninstruction tuning은 언어 모델이 추론 시 텍스트 상호작용에 더 잘 응답하도록 미세조정을 통한 지도학습을 사용하는 간단한 방법이다. 이 방법은 언어 모델이 지시문만을 통해 작업을 수행하는 능력을 보여준다.\nFLAN: Instruction Tuning Improves Zero-Shot Learning instruction tuning의 목표는 언어 모델이 NLP 지시문에 더 잘 응답하도록 향상시키는 것이다. 지시문을 통해 설명된 작업을 수행하도록 언어 모델을 교육함으로써, 보이지 않는 작업에 대해서도 지시문을 따를 수 있게 한다. 작업 유형별로 데이터셋을 그룹화하고, 남은 클러스터에서 instruction tuning을 하면서 볼 수 없는 작업의 성능을 평가한다.\nTasks \u0026amp; Templates 기존의 연구 데이터셋을 지시문 형식으로 변환하여, 자원 집약적인 새로운 데이터셋 생성을 피한다. Tensorflow Datasets에서 공개적으로 이용 가능한 62개의 텍스트 데이터셋을 하나의 혼합물로 집계하며, 이 데이터셋들은 12개의 작업 클러스터 중 하나로 분류된다.\n각 데이터셋에 대해, 작업을 설명하는 10개의 고유한 자연어 지시문 템플릿을 작성하며, 다양성을 높이기 위해 일부 템플릿은 원래의 작업을 뒤집는 방식으로 구성된다. 이후 모든 데이터셋의 혼합물에서 사전 학습된 언어 모델을 instruction tuning하며, 각 데이터셋의 예제는 해당 데이터셋에 대한 무작위로 선택된 지시 템플릿으로 형식화된다.\nEvaluation Splits FLAN이 instruction tuning에서 본적 없는 작업에 대해 어떻게 수행하는지를 알고자 한다. 본적 없는 작업을 정의하기 위해, instruction tuning 중에 보지 않은 작업 클러스터에 속한 모든 데이터셋을 본적 없는 것으로 간주한다. 따라서, 특정 작업 클러스터에서 zero-shot FLAN을 평가하려면, 각각 다른 작업 클러스터를 보류한 모델을 instruction tuning합니다.\nClassification With Options 작업의 출력 공간은 클래스 중 하나(classiﬁcation) 또는 자유 텍스트(generation)이 된다. FLAN은 decoder만 있는 언어 모델의 지시 조정 버전이므로, 생성 작업에 대한 추가 수정 없이도 자유 텍스트로 자연스럽게 응답한다.\n분류 작업에서는 \u0026ldquo;예\u0026quot;와 \u0026ldquo;아니오\u0026quot;와 같은 두 가지 출력만 고려하는 순위 분류 방법을 사용하였다. 하지만 이 방법은 답변의 확률 분포가 원치 않는 방식으로 나타날 수 있다. 따라서, 분류 작업의 끝에 OPTIONS 토큰과 해당 작업의 출력 클래스 목록을 추가하여 모델이 분류 작업에 응답할 때 원하는 선택지를 인식하게 한다.\nTraining Details Model architecture and pretraining. 137B parameter의 LaMDA-PT라는 decoder-only transformer 언어 모델을 사용한다. 이 모델은 웹 문서, 대화 데이터, 위키백과 등을 통해 사전 학습되었고, SentencePiece 라이브러리를 사용해 32k 어휘로 토큰화되었다. 사전 학습 데이터의 약 10%는 비영어이다. LaMDA-PT는 언어 모델 사전 학습만을 가지고 있다.\nInstruction tuning procedure. FLAN은 LaMDA-PT의 instruction tuning 버전이다. 모든 데이터셋을 혼합하여 무작위로 샘플링하며, 데이터셋 당 최대 30k의 학습 예제를 사용한다. 모델은 30k의 그래디언트 단계 동안 미세조정되며, 입력 시퀀스와 목표 시퀀스의 길이는 각각 1024와 256입니다. 이 튜닝 과정은 TPUv3에서 약 60시간이 소요된다.\nResults FLAN은 다양한 작업들에서 평가되며, 이는 natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, struct-to-text 등을 포함한다. 각 작업 클러스터는 다른 체크포인트를 사용하며, 각 데이터셋의 성능은 모든 템플릿에 대한 평균 성능으로 평가된다. 또한, 개발 세트의 성능이 가장 좋은 템플릿을 사용하여 테스트 세트의 성능도 측정한다.\nLaMDA-PT의 zero-shot과 few-shot 결과를 GPT-3의 프롬프트와 동일하게 보고한다. 이는 instruction tuning이 얼마나 효과적인지 직접적으로 보여주는 기준선이다. 결과적으로, instruction tuning은 대부분의 데이터셋에서 LaMDA-PT의 성능을 크게 향상시켰다.\nzero-shot FLAN은 25개 데이터셋 중 20개에서 GPT-3 175B를 능가하며, 19개 데이터셋 중 13개에서는 GLaM 64B/64E를 능가한다.\ninstruction tuning은 NLI, QA, translation, struct-to-text 등의 과제에 효과적이며, 언어 모델링으로 직접 구성된 과제에서는 효과적이지 않다.\nNatural language inference (NLI). 5개의 NLI 데이터셋에서, FLAN은 모든 기준 모델을 크게 능가했습니다. FLAN은 NLI를 \u0026ldquo;Does mean that ?\u0026ldquo;라는 더 자연스러운 질문으로 표현하여 훨씬 높은 성능을 보여주었다.\nReading comprehension. FLAN은 MultiRC와 OBQA에서 기준 모델을 능가하였다. 또한, BoolQ에서는 GPT-3를 크게 능가하였다.\nClosed-book QA. FLAN은 모든 네 개의 데이터셋에서 GPT-3를 능가하였다. ARC-e와 ARC-c에서는 GLaM보다 더 좋은 성능을 보였지만, NQ와 TQA에서는 약간 낮은 성능을 보였다.\nTranslation. FLAN은 GPT-3 논문에서 평가된 세 개의 데이터셋인 프랑스어-영어, 독일어-영어, 루마니아어-영어에 대한 기계 번역 성능을 평가하였다. FLAN은 모든 평가에서 zero-shot GPT-3를 능가했지만, 대부분의 경우 few-shot GPT-3보다 성능이 떨어졌다. FLAN은 영어로 번역하는 데 강한 결과를 보였지만, 영어에서 다른 언어로 번역하는 것은 상대적으로 약했다.\nAdditional tasks. instruction tuning은 많은 언어 모델링 과제의 성능을 향상시키지 못하는 한계가 있다. 7개의 상식 추론 및 공통 참조 해결 과제 중 FLAN은 3개 과제에서만 LaMDA-PT를 능가하였다. 하지만, zero-shot FLAN은 일반적으로 zero-shot LaMDA-PT를 능가하며, few-shot LaMDA-PT와 비슷하거나 더 나은 성능을 보여주었다.\nAblation Studies \u0026amp; Further Analysis Number Of Instruction Turning Clusters 이 연구에서는 instruction tuning이 어떻게 모델의 zero-shot 성능을 향상시키는지를 중점으로 살펴보았다. 첫 번째 축소 실험에서는 instruction tuning에 사용된 클러스터와 과제의 수가 성능에 어떻게 영향을 미치는지를 검토하였다. 이때 NLI, closed-book QA, commonsense reasoning을 평가 클러스터로 보류하고, 나머지 클러스터를 instruction tuning에 사용하였다.\ninstruction tuning에 추가 클러스터와 과제를 추가하면, 보류된 세 개의 클러스터에서의 평균 성능이 향상됨을 확인했다. 테스트한 일곱 개의 클러스터에서 성능이 포화되지 않아 보이므로, instruction tuning에 더 많은 클러스터가 추가되면 성능이 더욱 향상될 수 있을 것으로 보인다. 하지만, 감정 분석 클러스터에서는 최소한의 추가 가치만을 볼 수 있었다.\nScaling Laws 언어 모델의 zero-shot과 few-shot 능력이 더 큰 모델에 대해 크게 향상된다는 연구 결과를 바탕으로, instruction tuning의 이점이 모델 규모에 어떻게 영향을 받는지를 살펴보았다. 모델 규모를 422M, 2B, 8B, 68B, 137B로 설정하고 instruction tuning의 효과를 평가하였다.\n100B parameter 규모의 두 모델에서는 instruction tuning이 보류된 과제에서의 성능을 크게 향상시켰다. 그러나, 8B 및 더 작은 모델에서는 instruction tuning이 보류된 과제에서의 성능을 저하시켰다. 이는 작은 규모의 모델에서 instruction tuning 중 사용되는 과제를 학습하는 것이 모델의 전체 용량을 차지하게 되어, 새로운 과제에서 성능이 떨어지게 만들 수 있기 때문일 수 있다.\nRole Of Instructions 마지막 ablation study에서는 미세 조정 중 지시문의 역할을 살펴보았다. 지시문 없이 모델이 어떻게 수행하는지 살펴보기 위해, 지시문이 없는 두 가지 미세 조정 설정을 고려하였다. 하나는 템플릿이 없는 설정으로, 모델에게 입력과 출력만이 주어지는 것이고, 다른 하나는 데이터셋 이름 설정으로, 각 입력이 과제와 데이터셋의 이름으로 시작된다.\n자연스러운 지시문을 사용한 FLAN의 미세 조정 절차와 두 가지 ablation study을 비교하였다. 이 두 ablation study는 각각 템플릿이 없는 설정과 데이터셋 이름만을 사용한다. 결과에서 두 축소 설정 모두 FLAN보다 훨씬 나쁜 성능을 보여, 보이지 않는 과제에서의 zero-shot 성능에 지시문을 사용한 학습이 결정적임을 나타냈다.\nInstructions With Few-Shot Exemplars few-shot 예시가 추론 시간에 사용 가능할 때 instruction tuning이 어떻게 사용될 수 있는지 연구하였다. few-shot 설정의 형식은 zero-shot 형식을 기반으로 한다. 학습 시간과 추론 시간 모두에서 예시는 학습 세트에서 무작위로 추출되며, 예시의 수는 16개로 제한하고 전체 시퀀스 길이가 960 토큰 미만이 되도록 했다. 실험은 보이지 않는 과제에 대한 few-shot 예시를 오직 추론 시간에만 사용하는 동일한 과제 분할과 평가 절차를 사용하였다.\nfew-shot 예시는 zero-shot FLAN에 비해 모든 과제 클러스터의 성능을 향상시킨다. 예시는 특히 크거나 복잡한 출력 공간을 가진 과제에 효과적이며, 이는 예시가 모델이 출력 형식을 더 잘 이해하는 데 도움이 되기 때문일 가능성이 있다. 또한, 모든 과제 클러스터에서 템플릿 간의 표준 편차는 퓨샷 FLAN에서 더 낮아, 프롬프트 엔지니어링에 대한 민감도가 줄어든 것을 나타낸다.\nInstruction Turning Facilitates Prompt Turning instruction tuning이 모델의 지시문에 대한 반응 능력을 향상시키는 것을 확인했기 때문에, FLAN이 NLP 과제를 수행하는 데 더 적합하다면, 소프트 프롬프트를 사용하여 추론을 수행할 때도 더 나은 성능을 달성해야 한다. 추가 분석으로, SuperGLUE 과제 각각에 대해 연속 프롬프트를 훈련시켰고, 이는 특정 과제에 대한 프롬프트 튜닝을 수행할 때, 동일한 클러스터에 있는 다른 과제가 instruction tuning 동안 보이지 않게 하는 클러스터 분할을 따랐다. 프롬프트 튜닝 설정은 Lester et al.의 절차를 따르되, 몇 가지 변화를 주었고, 이 변화들이 LaMDA-PT의 성능을 향상시키는 것으로 확인되었다.\n모든 시나리오에서 프롬프트 튜닝은 LaMDA-PT보다 FLAN에서 더 잘 작동하였다. 특히 low-resource 설정에서는, FLAN에서의 프롬프트 튜닝이 LaMDA-PT에서의 것보다 10% 이상 성능이 향상되었다. 이 결과는 instruction tuning이 NLP 과제를 수행하는 데 더 바람직한 모델을 만드는 데 어떻게 기여할 수 있는지를 보여준다.\nRelated Work 이 논문은 zero-shot 학습, 프롬프팅, 다중 과제 학습, NLP 응용 프로그램을 위한 언어 모델 등 여러 넓은 연구 영역과 관련이 있다. 이러한 넓은 영역에 대한 이전 연구를 확장된 관련 연구 섹션에서 설명하고, 이 논문의 연구와 가장 밀접하게 연관된 범위가 좁은 두 개의 하위 영역을 설명하였다.\n모델에 지시문에 대한 반응을 요청하는 방식은 QA 기반 과제 구성과 유사하며, 이는 NLP 과제를 통일하는 것을 목표로 한다. 이 방법들은 주로 다중 과제 학습에 초점을 맞추며, 사전 학습된 LMs의 기존 지식을 사용하는 것에 크게 기반하지 않는다. 이 연구의 작업은 모델 규모와 과제 범위 모두에서 최근의 일부 연구를 초월한다.\n언어 모델의 성공으로 모델이 지시문을 따르는 능력에 대한 연구가 진행되고 있다. 최근 연구에서는 지시문과 few-shot 예시를 이용해 BART를 미세 조정하고, 이를 통해 보이지 않는 과제에 대한 few-shot 성능을 향상시킬 수 있음을 보여주었다. 또한, T5를 미세 조정하는 등의 방법으로 zero-shot 학습을 개선하고, 미세 조정과 강화 학습을 병행하여 인간 평가자가 선호하는 출력을 생성하는 연구도 있다.\nDiscussion 지시문으로 표현된 여러 과제에 대해 모델을 미세 조정하면 보이지 않는 과제에서의 성능이 향상된다는 것을 보여주었다. FLAN은 미세 조정되지 않은 모델보다 성능이 좋고, zero-shot GPT-3를 능가한다. 또한, 충분한 모델 규모에서만 instruction tuning에 의한 성능 향상이 나타나며, 이는 다른 프롬프팅 방법과도 결합될 수 있다.\n언어 모델의 다양한 능력은 specialist 모델과 generalist 모델 사이의 균형에 대한 관심을 끌어내었다. 레이블이 있는 데이터가 specialist 모델을 개선하는 데 도움이 될 것으로 예상되지만, instruction tuning을 통해 이 데이터가 큰 언어 모델이 보이지 않는 다양한 과제를 수행하는 데도 도움이 될 수 있음을 보여주었다. 이는 과제 특정 학습이 일반 언어 모델링과 보완적이라는 것을 보여주며, generalist 모델에 대한 추가 연구를 촉진한다.\n이 연구의 한계점은 과제를 클러스터에 할당하는 데 있는 주관성과 짧은 지시문의 사용에 대한 연구의 한정성이다. 개별 예시가 모델의 사전 훈련 데이터에 포함되어 있을 수 있지만, 이것이 결과에 크게 영향을 미쳤다는 증거는 찾지 못하였다. 또한, FLAN 137B의 규모는 그것을 서비스하는 데 비용이 많이 든다. 향후 instruction tuning 연구는 더 많은 과제 클러스터를 수집하고, 다언어 실험을 진행하며, downstream classiﬁer 학습 데이터를 생성하고, 편향과 공정성에 대한 모델 행동을 개선하는 방향으로 진행될 수 있다.\nConclusions 이 논문은 지시문에 기반한 zero-shot 과제를 수행하는 대규모 언어 모델의 능력을 향상시키는 간단한 방법을 연구하였다. FLAN은 GPT-3에 비해 더 우수한 결과를 보여주며, 대규모 언어 모델이 지시문을 따를 수 있는 잠재력을 보여주었다.\nReference Paper Github ","date":"2024-01-03T00:00:00Z","permalink":"https://kurtkim.github.io/p/flan/","title":"FLAN"},{"content":"Abstract 최근의 연구는 대량의 텍스트 말뭉치로 사전 학습한 후 특정 작업에 대해 미세 조정하는 것으로 많은 NLP 작업과 벤치마크에서 상당한 성과를 보여주었다. 일반적으로 과제에 중립적인 구조를 가지지만, 이 방법은 여전히 수천 개 또는 수만 개의 예제로 이루어진 과제별 미세 조정 데이터셋을 필요로 한다. 인간은 보통 몇 가지 예제나 간단한 지시사항만으로도 새로운 언어 작업을 수행할 수 있지만, 현재의 NLP 시스템은 이를 여전히 어려워한다. 이 연구에서는 언어 모델의 규모를 확장함으로써 과제 중립적이고 소수의 예제로 이루어진 작업 성능을 크게 개선하는 것을 보여준다. 때로는 이전의 state-of-the-art 미세 조정 접근법과 경쟁력을 갖출 수도 있다. 구체적으로, 1750억 개의 parameter를 가진 GPT-3라는 autoregressive 언어 모델을 학습시키고, 이를 소수의 예제로 평가해보았다. 모든 작업에서 GPT-3는 어떠한 그래디언트 업데이트나 미세 조정 없이 적용되며, 작업 및 소수의 예제는 모델과의 텍스트 상호작용을 통해 명시된다. GPT-3는 번역, 질의응답, 문맥 채우기 작업뿐만 아니라 단어 섞기, 새로운 단어를 문장에 사용하기, 3자리 산술 연산을 수행하는 등의 실시간 추론이나 도메인 적응이 필요한 작업과 같은 많은 NLP 데이터셋에서 강력한 성능을 보여주었다. 한편, GPT-3의 소수 학습은 여전히 어려운 몇몇 데이터셋과 대규모 웹 말뭉치에서의 훈련에 관련된 방법론적 문제가 있다는 점도 확인했다. 마지막으로, GPT-3는 인간 평가자가 사람이 작성한 기사와 구분하기 어려운 뉴스 기사 샘플을 생성할 수 있음을 발견하였다.\nIntroduction 최근 NLP 시스템은 사전 학습된 언어 표현을 다양한 작업에 유연하게 적용하는 추세가 있다. 초기에는 단어 벡터를 사용한 단일 계층 표현을 과제 특정 아키텍처에 적용했으나, 후에는 RNN을 사용한 다계층 표현을 도입하였다. 최근에는 과제 특정 아키텍처의 필요성을 완전히 제거하고, 사전 학습된 recurrent 또는 transformer 언어 모델을 직접 미세 조정하는 방식이 사용되고 있다.\n이러한 패러다임은 많은 어려운 작업에서 진전을 이루었지만, 여전히 작업 특정 데이터셋과 미세 조정이 필요한 한계가 있다. 원하는 작업에서 높은 성능을 달성하기 위해 수천에서 수십만 개의 예제로 이루어진 작업 특정 데이터셋에서 미세 조정이 필요하다. 이러한 한계를 제거하는 것이 중요하다.\n실용적인 관점에서, 모든 새로운 작업에 대한 대규모 레이블링된 예제 데이터셋의 필요성은 언어 모델의 적용 범위를 제한한다. 유용한 언어 작업의 범위는 매우 넓지만, 많은 작업들에 대해 큰 규모의 지도 학습 데이터셋을 수집하는 것은 어렵고, 이 과정이 각각의 새로운 작업마다 반복되어야 한다.\n학습 데이터의 거짓 상관관계를 이용하는 가능성은 모델의 표현력과 학습 분포의 좁음에 따라 증가하며, 이는 사전 학습 후 미세 조정 패러다임에 문제를 일으킬 수 있다. 모델은 사전 학습 동안 정보를 흡수하기 위해 크게 설계되지만, 후에는 좁은 작업 분포에서 미세 조정되며, 이로 인해 학습 분포에 과도하게 특화되어 분포 외부에서는 잘 일반화되지 않을 수 있다. 따라서, 미세 조정된 모델의 성능은 실제 기본 작업에 대한 성능을 과장할 수 있다.\n인간은 대부분의 언어 작업을 배우기 위해 큰 규모의 지도 학습 데이터셋을 필요로 하지 않는다. 간단한 지시사항이나 몇 가지 예제만으로도 새로운 작업을 수행할 수 있다. 이런 적응성은 인간이 여러 작업과 기술을 자연스럽게 섞거나 전환할 수 있게 한다.\n이러한 문제를 해결하기 위한 한 방법은 메타 학습이다. 이는 언어 모델이 훈련 시에 다양한 기술과 패턴 인식 능력을 개발하고, 추론 시에 이를 활용해 원하는 작업에 빠르게 적응하거나 인식하도록 하는 것을 의미한다. 최근 연구에서는 \u0026ldquo;in-context learning\u0026quot;을 통해 이를 시도하였는데, 이는 모델이 자연 언어 지시사항이나 작업의 몇 가지 예제에 조건화되어, 단순히 다음에 무엇이 오는지 예측하여 작업을 완성하도록 하는 방식이다.\n메타 학습 방법은 약간의 잠재력을 보였지만, 미세 조정에 비해 성능이 크게 떨어진다. 특히, Natural Questions에서는 4%, CoQa에서는 55 F1이라는 결과를 보였는데, 이는 최신 기술에 비해 크게 뒤처져 있다. 따라서 메타 학습이 언어 작업을 해결하는 실질적인 방법이 되려면 큰 개선이 필요하다.\n최근 언어 모델링은 transformer 언어 모델의 용량이 크게 증가하는 추세를 보이고 있다. parameter 수가 100M에서 시작해 최근에는 17B개에 이르렀고, 이런 증가는 텍스트 합성과 NLP 작업에서 성능 개선을 가져왔다. 로그 손실이 규모와 함께 개선되는 추세를 보이기 때문에, 문맥 내 학습 능력도 규모와 함께 크게 향상될 수 있을 것으로 보인다.\n이 논문에서는 175B 개의 parameter를 가진 언어 모델, GPT-3의 학습과 그 문맥 내 학습 능력을 테스트한다. GPT-3는 다양한 NLP 데이터셋과 새로운 작업들에 대해 평가되며, 각 작업은 \u0026ldquo;few-shot learning\u0026rdquo;, \u0026ldquo;one-shot learning\u0026rdquo;, \u0026ldquo;zero-shot\u0026rdquo; 학습의 세 가지 조건 하에서 평가된다. GPT-3는 원칙적으로 미세 조정 설정에서도 평가될 수 있지만, 이는 미래의 연구로 남겨두었다.\n자연 언어 작업 설명과 문맥 내 예제 수가 늘어날수록 모델의 성능이 향상되며, 모델 크기가 커질수록 few-shot learning이 크게 향상된다. 이러한 학습 곡선은 미세 조정이나 그래디언트 업데이트 없이, 단순히 제공된 데모 수를 늘려가며 이루어진다.\nGPT-3는 NLP 작업에서 zero-shot과 one-shot 설정에서 좋은 결과를 보이며, few-shot 설정에서는 때때로 state-of-the-art 모델과 경쟁하거나 초과한다. 예컨대, GPT-3는 CoQA에서 zero-shot에서 81.5 F1, one-shot에서 84.0 F1, few-shot에서 85.0 F1을 달성하였다. 비슷하게, TriviaQA에서는 zero-shot에서 64.3%, one-shot에서 68.0%, few-shot에서 71.2%의 정확도를 보여주었다.\nGPT-3는 unscrambling words, performing arithmetic 등의 작업에서 one-shot과 few-shot 능력을 보여준다. 또한, GPT-3는 few-shot 설정에서 사람들이 인간이 만든 기사와 구별하기 어려운 합성 뉴스 기사를 생성할 수 있다.\nGPT-3의 규모에도 불구하고, ANLI와 같은 자연어 추론 작업이나 RACE, QuAC과 같은 일부 읽기 이해 데이터셋에서 few-shot 성능이 어려움을 겪는 일부 작업들을 발견하였다.\n\u0026ldquo;data contamination\u0026quot;에 대한 체계적인 연구를 수행하였다. 이는 테스트 데이터셋의 콘텐츠가 웹에 존재하기 때문에, Common Crawl과 같은 데이터셋에서 모델을 학습시킬 때 발생할 수 있는 문제이다. data contamination이 대부분의 데이터셋에서 GPT-3의 성능에 미치는 영향은 적지만, 결과가 과대 평가될 수 있는 몇몇 데이터셋을 식별하였다.\n더 작은 모델들을 학습시켜 성능을 zero, one, few-shot 설정에서 GPT-3와 비교하였다. 대부분의 작업에서 모델 용량과 함께 성능이 상대적으로 부드럽게 스케일링되는 것을 보았다. 특히, 모델 용량이 커짐에 따라 zero, one, few-shot 성능 간의 차이가 더욱 커지는 것으로 보아, 큰 모델이 더 능숙한 메타 학습자일 수 있음을 시사한다.\n마지막으로, GPT-3가 보여주는 넓은 범위의 능력에 대해, 편향, 공정성, 그리고 보다 넓은 사회적 영향에 대한 우려를 논의하고, 이러한 관점에서 GPT-3의 특성에 대한 초기 분석을 시도한다.\nApproach GPT-3의 사전 학습 접근법은 기존의 방법과 유사하나, 모델 크기, 데이터셋 크기 및 다양성, 학습 기간을 확장하였다. 컨텍스트 내에서 학습하는 다양한 설정을 체계적으로 탐색하였고, 이러한 설정은 작업 특정 데이터에 얼마나 의존하는지에 따라 다르게 위치할 수 있다. 스펙트럼에서는 적어도 네 가지 주요 포인트를 식별할 수 있다:\nFine-Tuning (FT) 최근 방식은 원하는 작업에 맞는 감독 데이터셋으로 사전 학습된 모델의 가중치를 업데이트하는 것이다. 이 방법의 이점은 많은 벤치마크에서 강력한 성능을 보여준다는 것이고, 단점은 각 작업마다 새로운 대규모 데이터셋이 필요하고, 분포 외에서는 일반화가 잘 안 될 수 있으며, 학습 데이터의 임의적인 특징을 이용할 수 있다는 것이다.\nFew-Shot (FS) 모델이 추론 시간에 작업의 몇 가지 예시를 조건으로 받지만 가중치 업데이트는 허용되지 않는 설정이다. 이 방법의 장점은 작업 특정 데이터에 대한 요구가 크게 줄어들고, 과도하게 좁은 분포를 학습하는 가능성이 줄어든다는 것이다. 단점은 이 방법의 결과가 미세 조정된 최신 모델보다 낮았다는 것이며, 작은 양의 작업 특정 데이터가 여전히 필요하다. few-shot 학습은 넓은 작업 분포를 기반으로 학습하고, 새로운 작업에 빠르게 적응하는 것을 포함한다.\nOne-Shot (1S) 작업에 대한 자연어 설명 외에 하나의 예시만 허용된다는 점에서 few-shot과 다르다. one-shot은 일부 작업이 사람들에게 전달되는 방식과 가장 일치하기 때문에 few-shot과 zero-shot과 구별된다.\nZero-Shot (0S) 자연어 지시문만 모델에게 제공되며, 예시는 허용되지 않는 방식이다. 이 방법은 최대한의 편리성을 제공하지만, 가장 도전적인 설정이기도 하다. 예시 없이 작업의 형식을 이해하는 것은 어려울 수 있지만, zero-shot은 사람들이 작업을 수행하는 방식과 가장 가깝다.\nModel and Architectures GPT-2와 동일한 모델과 아키텍처를 사용하면서, transformer 계층에서 alternating dense 패턴과 locally banded sparse attention 패턴을 교대로 사용하는 점이 다르다. 모델 크기에 따른 ML 성능의 의존성을 연구하기 위해, 125M 개의 parameter에서 175B 개의 parameter까지 다양한 크기의 8가지 모델을 훈련시켰다. 가장 큰 모델을 GPT-3라고 부른다.\n각 모델은 학습 가능한 parameter 수, layer 수, bottleneck layer의 단위 수 등으로 구성되어 있다. 모든 모델은 2048 토큰의 컨텍스트 window를 사용하며, 데이터 전송을 최소화하기 위해 GPU에 모델을 깊이와 너비 차원을 따라 분할한다. 각 모델의 아키텍처 parameter는 계산 효율성과 GPU 간의 로드 밸런싱에 기반하여 선택되었다. 이전 연구에 따르면, 검증 손실은 이러한 parameter에 대해 상당히 넓은 범위에서 크게 민감하지 않다.\nTraining Dataset 언어 모델 데이터셋은 Common Crawl 데이터셋으로 확장되어 1 trillion 단어를 수집하였다. 이런 크기의 데이터셋은 가장 큰 모델을 학습시키기에 충분하지만, Common Crawl의 필터링되지 않은 버전은 품질이 낮다. 그래서 데이터셋의 품질을 향상시키기 위해 세 가지 절차를 거쳤습니다: (1) 고품질 참조 말뭉치와 유사한 Common Crawl의 버전을 다운로드하고 필터링, (2) 중복 제거를 통해 데이터셋의 중복을 방지, (3) 고품질 참조 말뭉치를 학습 데이터에 추가하여 다양성을 늘렸다.\n추가 데이터셋으로는 WebText 데이터셋의 확장 버전, 두 개의 인터넷 기반 책 말뭉치(Books1과 Books2), 그리고 영어 Wikipedia가 있다.\nCommonCrawl 데이터는 필터링 전 45TB, 필터링 후 570GB로, 약 4000억 바이트 쌍 인코딩 토큰에 해당한다. 학습 중에는 품질이 높은 데이터셋을 더 자주 샘플링하며, 이는 고품질 학습 데이터를 위해 약간의 과적합을 받아들인다.\n인터넷 데이터에서 사전 학습된 언어 모델은 데이터 오염이 발생할 우려가 있다. 이를 줄이기 위해 모든 벤치마크의 개발 및 테스트 세트와 겹치는 부분을 찾아 제거하려 했으나, 일부 겹치는 부분을 무시하는 버그가 있었다. 학습의 비용 문제로 인해 다시 모델을 학습하는 것은 비현실적이었다.\nTraining Process 대형 모델은 큰 배치 크기를 사용하나 작은 learning rate가 필요하다. 학습 중 gradient noise scale을 측정하여 배치 크기를 결정하였다. 메모리 부족을 방지하기 위해 모델 병렬성을 사용하였고, 모든 모델은 Microsoft의 고대역폭 클러스터에서 V100 GPU로 학습되었다.\nEvaluation few-shot 학습에서는 각 작업의 학습 세트에서 무작위로 $K$개의 예제를 추출하여 평가하였다. LAMBADA와 Storycloze는 지도 학습 세트가 없으므로, 개발 세트에서 추출한 예제를 사용한다. Winograd는 하나의 데이터셋만 있으므로, 그 데이터셋에서 직접 예제를 추출한다.\n$K$는 0부터 모델의 컨텍스트 창이 허용하는 2048까지의 값이 될 수 있으며, 대체로 10에서 100개의 예제를 수용한다. $K$의 더 큰 값이 일반적으로 좋지만 항상 그런 것은 아니므로, 개발 세트와 테스트 세트가 있는 경우, 개발 세트에서 $K$의 몇 가지 값을 실험하고 최적의 값을 테스트 세트에서 사용한다. 일부 작업에서는 예시 외에도 자연어 프롬프트를 사용한다.\n객관식 작업에서는 $K$개의 컨텍스트와 정확한 완성 예제를 제공하고, 각 완성의 가능성을 비교한다. 대부분 작업에서는 토큰 당 가능성을 비교하여 길이를 정규화하지만, ARC, OpenBookQA, RACE 같은 일부 데이터셋에서는 완성의 무조건적 확률 $ {P(completion | context)}\\over{P(completion | answer_context)} $로 정규화하여 추가적인 이익을 얻는다. \u0026ldquo;Answer: \u0026quot; 또는 \u0026ldquo;A: \u0026ldquo;는 완성이 답이어야 함을 알리는 프롬프트로 사용된다.\n이진 분류 작업에서는 옵션에 \u0026ldquo;True\u0026quot;나 \u0026ldquo;False\u0026quot;와 같은 의미 있는 이름을 부여하고, 객관식 문제처럼 처리한다.\n자유형식 완성 작업에서는 beam width가 4이고 길이 패널티가 $\\alpha = 0.6$인 beam search를 사용한다. 모델은 F1 유사도 점수, BLEU, 또는 정확한 일치를 기준으로 평가한다.\n테스트 세트가 공개적으로 사용 가능한 경우, 모델 크기와 학습 설정별로 최종 결과를 보고한다. 테스트 세트가 비공개인 경우, 개발 세트 결과를 보고한다. 제출이 가능한 데이터셋(SuperGLUE, TriviaQA, PiQa)에 대해서는 테스트 서버에 결과를 제출하고, 그 외의 경우에는 개발 세트 결과를 보고한다.\nResults 효율적인 학습 계산을 사용하면 언어 모델링 성능이 power-law를 따르는 것을 확인하였다. 이 경향을 더 확장하면 power-law에서 약간 벗어나는 것을 볼 수 있다. cross-entropy 손실의 개선이 학습 코퍼스의 특정 세부사항을 모델링함으로써만 이루어진다는 우려도 있지만, 실제로는 다양한 자연어 작업에서 일관된 성능 향상을 가져왔다.\n8개의 모델(175B 개의 parameter를 가진 GPT-3와 7개의 작은 모델)을 다양한 데이터셋에서 평가한다. 데이터셋은 유사한 작업을 나타내는 9개의 카테고리로 그룹화한다.\n언어 모델링과 유사한 작업, \u0026lsquo;closed book\u0026rsquo; 질문 응답 작업, 언어 간 번역 능력, Winograd Schema와 유사한 작업, 상식 추론 또는 질문 응답 작업, 읽기 이해 작업, SuperGLUE 벤치마크를, 그리고 NLI를 평가한다. 마지막으로 인텍스트 학습 능력을 조사하기 위한 추가 작업을 발명하고 평가한다. 이 모든 평가는 few-shot, one-shot, zero-shot 설정에서 이루어진다.\nLanguage Modeling, Cloze, and Completion Tasks GPT-3의 성능을 전통적인 언어 모델링 작업뿐만 아니라 관심 있는 단일 단어를 예측하거나, 문장이나 단락을 완성하거나, 텍스트의 가능한 완성 사이에서 선택하는 등의 관련 작업을 테스트한다.\nLanguage Modeling Penn Tree Bank (PTB) 데이터셋에서 GPT-3의 zero-shot perplexity를 계산하였다. PTB는 현대 인터넷 이전에 만들어진 데이터셋이므로 학습 데이터에 포함되지 않았다. 가장 큰 모델은 PTB에서 perplexity 20.50을 달성하여 state-of-the-art를 달성하였다. PTB는 전행적인 언어 모델링 데이터셋이므로 one-shot이나 few-shot 평가는 적용되지 않았다.\nLAMBADA LAMBADA 데이터셋은 텍스트의 장거리 의존성을 테스트한다. 최근에는 언어 모델의 크기를 늘리는 것이 더 이상 벤치마크의 성능 향상에 별 도움이 안 된다는 의견이 있었다. 그러나 GPT-3는 zero-shot 설정에서 LAMBADA에서 76%의 결과를 보여주며, 이전 최고 기록보다 8% 향상시키는 결과를 보여주었다.\nLAMBADA는 few-shot 학습의 유연성을 보여준다. 표준 언어 모델은 문장의 마지막 단어를 예측하는 것이 어렵지만, few-shot 학습은 이를 클로즈 테스트로 제시하고 언어 모델이 한 단어의 완성을 예측하도록 한다. 이전의 stop-word ﬁlter 방법보다 효과적인 해결책을 제공합니다.\n다음과 같은 빈칸 채우기 형식을 사용한다:\n$$ \\text{Alice was friends with Bob. Alice went to visit her friend ____} \\rightarrow \\text{Bob} $$ $$ \\text{George bought some baseball equipment, a ball, a glove, and a ____} \\rightarrow $$\nGPT-3는 few-shot 설정에서 86.4%의 정확도를 보여, 이전 최고 기록보다 18% 이상 증가하였다. 모델 크기가 커질수록 퓨샷 성능이 크게 향상되었다. 그러나 빈칸 채우기 방법은 one-shot에서는 zero-shot보다 성능이 떨어졌다. 이는 모든 모델이 패턴을 인식하기 위해 여러 예제가 필요하기 때문일 것으로 보인다.\n테스트 세트에서 LAMBADA 데이터셋의 일부가 학습 데이터에 포함된 것으로 확인되었지만, 성능에 불필요한 영향을 미치지는 않는 것으로 분석되었다.\nHellaSwag HellaSwag 데이터셋은 이야기나 지시사항의 최선의 결말을 선택하는 것이다. GPT-3는 one-shot에서 78.1%, few-shot에서 79.3%의 정확도를 보여주었다. 이는 1.5B parameter 언어 모델의 75.4%를 능가하지만, 다목적 모델 ALUM의 85.6%에는 미치지 못하였다.\nStoryCloze GPT-3는 5문장 이야기의 결말을 선택하는 StoryCloze 2016 데이터셋에서는 zero-shot에서 83.2%, few-shot에서 87.7%의 정확도를 보여주었다. 이는 BERT 기반 모델의 최고 기록보다 4.1% 낮지만, 이전 zero-shot 결과에 비해 10% 향상된 수치이다.\nClosed Book Question Answering GPT-3가 광범위한 사실에 대한 질문에 얼마나 잘 대답하는지를 측정한다. 일반적으로 이 작업은 정보 검색 시스템과 모델을 사용해 수행되며, 이를 \u0026ldquo;open-book\u0026quot;이라고 부른다. 하지만 최근에는 \u0026ldquo;closed-book\u0026rdquo; 방식으로 큰 언어 모델이 직접 질문에 답하는 것이 효과적이라는 연구 결과가 나왔다. 이 가설을 GPT-3로 테스트하며, Natural Questions, WebQuestions, TriviaQA 세가지 데이터셋에서 평가를 진행하였다. 이 평가는 외부 콘텐츠와 Q\u0026amp;A 데이터셋에 대한 미세조정을 허용하지 않는 엄격한 closed-book 설정에서 수행된다.\nGPT-3는 TriviaQA에서 zero-shot 64.3%, one-shot 68.0%, few-shot 71.2%의 결과를 보여주었다. zero-shot 결과만으로도 미세조정된 T5-11B를 14.2%, Q\u0026amp;A 맞춤형 범위 예측을 사용한 버전을 3.8% 초과하였다. one-shot 결과는 3.7% 향상되며, 오픈 도메인 QA 시스템의 최고 기록과 동일하게 되었다. few-shot 결과는 3.2% 향상시켰다.\nWebQuestions에서 GPT-3는 zero-shot 14.4%, one-shot 25.3%, few-shot 41.5%의 결과를 보여주었다. 이는 미세조정된 T5-11B의 37.4%, 특정 사전 학습 절차를 사용하는 T5-11B+SSM의 44.7%와 비교된다. few-shot 설정에서 GPT-3의 성능은 state-of-the-art를 달성한 미세조정 모델과 근접하다. 또한, WebQs의 질문이나 답변 스타일이 GPT-3에게는 이질적인 것으로 보여지지만, few-shot 설정에서 GPT-3는 이에 적응하며 높은 성능을 회복하는 것으로 보인다.\nNatural Questions에서 GPT-3는 zero-shot 14.6%, one-shot 23.0%, few-shot 29.9%의 성과를 보여주었다. 이는 미세조정된 T5 11B+SSM의 36.6%와 비교되는 결과이다. zero-shot에서 few-shot으로 크게 향상된 성능은 분포의 변화를 보여주며, 이는 TriviaQA와 WebQS에 비해 덜 경쟁력 있는 성능을 설명할 수 있다. 특히, NQs 질문들이 Wikipedia에 대한 매우 세부적인 지식을 요구하므로, 이는 GPT-3의 용량과 사전 학습 분포의 한계를 시험할 수 있다.\n세 가지 데이터셋 중 하나에서 GPT-3의 one-shot 성능은 오픈 도메인의 최고 성능과 일치하고, 나머지 두 데이터셋에서는 미세조정을 하지 않아도 최고 성능에 근접한다. 모든 데이터셋에서, 모델 크기에 따라 성능이 부드럽게 확장되는 것을 확인하였고, 이는 모델의 용량이 직접적으로 모델의 parameter 흡수된 \u0026lsquo;knowledge\u0026rsquo;으로 변환된다는 생각을 반영할 수 있다.\nTranslation GPT-2는 용량 문제로 인해 다국어 문서를 영어로만 필터링했지만, 일부 다국어 능력을 보여주었다. 프랑스어와 영어 간 번역에서도 의미 있는 성과를 보였다. GPT-3에서는 용량을 크게 향상시키고 학습 데이터셋을 확대하여 다른 언어를 더 많이 포함하였다. GPT-3의 학습 데이터는 주로 영어(93%)이지만, 다른 언어의 텍스트도 7% 포함한다. 번역 능력을 더 잘 이해하기 위해, 분석에 독일어와 루마니아어를 추가하였다.\n기존의 비지도 학습 기계 번역은 주로 단일 언어 데이터셋과 back-translation을 사용하지만, GPT-3는 여러 언어를 혼합한 학습 데이터에서 학습한다. 이는 단어, 문장, 문서 수준에서 언어들을 결합한다. GPT-3는 특정 작업을 위해 맞춤화되지 않은 단일 학습 목표를 사용한다. 그러나, one-shot / few-shot 설정은 적은 양의 쌍으로 된 예시를 사용하기 때문에 엄밀히 말해 이전의 비지도 작업과는 비교가 어렵다.\nzero-shot GPT-3는 작업 설명만을 받지만 최근의 비지도 NMT 결과보다 성능이 떨어진다. 그러나 각 번역 작업에 대해 한 예시만 제공하면 성능이 크게 향상되며, few-shot 설정에서 더욱 향상된다. GPT-3의 성능은 언어 방향에 따라 크게 달라진다. 영어로 번역할 때는 이전의 비지도 NMT 작업을 능가하지만 반대 방향으로는 성능이 떨어진다. En-Ro의 경우 성능이 이전 비지도 NMT 작업보다 훨씬 낮다. Fr-En과 De-En에서 few-shot GPT-3는 최고의 지도 학습 결과를 능가하고, Ro-En에서는 전체 최고 성능과 비슷한 성능을 보여준다.\n모든 언어 쌍과 zero-shot, one-shot, few-shot 설정에서 모델 용량이 증가함에 따라 성능이 부드럽게 향상되는 추세가 확인되었다.\nWinograd-Style Tasks Winograd Schemas Challenge는 대명사가 가리키는 단어를 찾는 NLP 작업이다. 언어 모델은 기존 Winograd 데이터셋에서는 좋은 성능을 보였지만, 더 어려운 Winogrande 데이터셋에서는 성능이 떨어졌다. 이는 GPT-3에서도 확인되었다.\nGPT-3는 원래의 273개의 Winograd 스키마에서 테스트되었고, zero-shot, one-shot, few-shot 설정에서 각각 88.3%, 89.7%, 88.6%의 성능을 보여주었다. 이는 모든 경우에서 state-of-the-art와 인간의 성능을 몇 포인트 밑돌게 강력한 결과를 보여준다. 학습 데이터 중 일부 Winograd 스키마에서 오염 분석이 이루어졌지만, 이것이 결과에 미치는 영향은 작았다.\n더 어려운 Winogrande 데이터셋에서 GPT-3는 zero-shot에서 70.2%, one-shot에서 73.2%, few-shot에서 77.7%의 성능을 보여주었다. 이는 미세 조정된 RoBERTA 모델의 79%, 최첨단 모델인 T5의 84.6%, 그리고 인간의 성능인 94.0%와 비교된다.\nCommon Sense Reasoning PhysicalQA (PIQA)라는 데이터셋에서 GPT-3는 zero-shot 81.0%, one-shot 80.5%, few-shot 82.8%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 79.4%에 비해 우수하며, 인간의 성능보다는 약 10% 떨어지지만, state-of-the-art의 성능을 one-shot과 few-shot에서 능가하였다. 하지만, PIQA가 데이터 오염 가능성을 가질 수 있어 결과를 보수적으로 표시하였다.\nARC 데이터셋에서 GPT-3는 \u0026ldquo;Challenge\u0026rdquo; 버전에서 zero-shot 51.4%, one-shot 53.2%, few-shot 51.5%의 정확도를, \u0026ldquo;Easy\u0026rdquo; 버전에서는 68.8%, 71.2%, 70.1%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 성능에 근접하거나 약간 능가하였지만, UniﬁedQA의 성능에 비하면 아직도 많이 뒤떨어져 있다.\nOpenBookQA에서 GPT-3는 zero-shot에서 few-shot으로 넘어갈 때 성능이 크게 향상되었지만, state-of-the-art에 비해 아직 20점 이상 뒤떨어져 있다. GPT-3의 few-shot 성능은 미세 조정된 BERT Large와 비슷하다.\nGPT-3의 in-context 학습은 상식 추론 작업에서 일관성 없는 결과를 보였지만, OpenBookQA에서는 크게 향상되었다. 또한, GPT-3는 모든 평가에서 새 PIQA 데이터셋의 state-of-the-art를 달성하였다.\nReading Comprehension reading comprehension 작업에서 GPT-3를 평가해 보았다. 다양한 답변 형식을 가진 5개의 데이터셋을 사용하였고, GPT-3의 성능은 데이터셋에 따라 크게 다르며, 다양한 답변 형식에 대한 능력을 보여주었다. 일반적으로 GPT-3는 각 데이터셋에 대한 초기 기준선과 비슷한 성능을 보여주었다.\nGPT-3는 자유 형식의 대화 데이터셋인 CoQA에서 가장 좋은 성능을 보였고, 대화 행동과 답변 선택을 요구하는 QuAC에서는 가장 나쁜 성능을 보였다. DROP 데이터셋에서는 few-shot 설정에서 BERT 기준선을 앞섰지만, 사람의 성능과 최첨단 방법에는 미치지 못하였다. SQuAD 2.0에서는 few-shot 학습 능력을 보여주며 성능을 향상시켰고, RACE에서는 상대적으로 약한 성능을 보였지만, 초기 작업과는 경쟁력을 가졌다.\nSuperGLUE GPT-3를 더 체계적으로 평가하고 다른 모델들과 비교하기 위해, SuperGLUE 벤치마크라는 표준화된 데이터셋에서도 평가를 진행하였다. few-shot 설정에서는 모든 작업에 대해 학습 세트에서 무작위로 추출한 32개의 예제를 사용하였다. WSC와 MultiRC를 제외한 모든 작업에서는 각 문제의 컨텍스트로 사용할 새로운 예제 집합을 샘플링하였다. WSC와 MultiRC에서는, 모든 문제의 컨텍스트로 동일한 예제 집합을 사용하였다.\nGPT-3는 다양한 작업에서 성능이 다르게 나타났다. COPA와 ReCoRD에서는 거의 최고 수준에 근접한 성능을 보였고, WSC에서는 80.1%의 높은 성능을 보였다. BoolQ, MultiRC, RTE에서는 합리적인 성능을 보였고, CB에서는 75.6%의 성능을 보였다.\nWiC에서 GPT-3의 few-shot 성능이 49.4%로 상대적으로 약하다는 것을 발견하였다. 두 문장을 비교하는 일부 작업에서 GPT-3는 약한 경향이 있다. 이는 RTE와 CB의 낮은 점수를 설명할 수 있다. 그러나 이런 약점에도 불구하고, GPT-3는 8개 작업 중 4개에서 미세 조정된 BERT-Large를 능가하며, 2개 작업에서는 state-of-the-art에 가깝다.\n모델 크기와 예시 수가 증가함에 따라 few-shot SuperGLUE 점수가 개선되는 것을 확인하였다. GPT-3는 각 작업당 8개 미만의 예시만으로도 미세 조정된 BERT-Large를 능가하는 전체 SuperGLUE 점수를 얻을 수 있었다.\nNLI 자연어 추론(NLI)은 두 문장 간의 관계를 이해하는 능력을 평가한다. GPT-3는 이 작업에서 랜덤(56%)보다 약간 더 잘 수행되는 반면, few-shot 설정에서는 BERT Large와 유사한 수준으로 수행한다. 적대적 자연어 추론(ANLI) 데이터셋에서는 GPT-3가 라운드 3에서 약간의 진전을 보여주었다. 이러한 결과는 NLI가 여전히 언어 모델에게 어려운 작업이며, 진전의 시작 단계에 불과하다는 것을 시사한다.\nSynthetic and Qualitative Tasks GPT-3의 능력을 테스트하기 위해, 간단한 계산, 새로운 패턴 인식, 비정상적인 작업에 빠르게 적응하는 등의 작업을 제공한다. 테스트에는 산술, 단어의 글자 재배열, SAT 스타일의 유사성 문제 해결, 그리고 새로운 단어 사용, 문법 수정, 뉴스 기사 생성 등이 포함된다. 이러한 합성 데이터셋은 언어 모델의 테스트 시간 행동에 대한 추가 연구를 촉진하기 위해 공개될 예정이다.\nArithmetic GPT-3가 특정 작업에 대한 학습 없이 간단한 산술 연산을 수행하는 능력을 테스트하기 위해, 자연어로 간단한 산술 문제를 묻는 10개의 작은 테스트를 개발하였다:\n2 digit addition (2D+) 모델에게는 두 정수를 더하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 \u0026ldquo;Q: What is 48 plus 76? A: 124\u0026quot;와 같은 형태로 질문된다. 2 digit subtraction (2D-) 모델에게는 두 정수를 빼라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 답변은 음수일 수 있다. 예를 들어 \u0026ldquo;Q: What is 34 minus 53? A: -19\u0026quot;와 같은 형태로 질문된다. 3 digit addition (3D+) 2자리 수 덧셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다. 3 digit subtraction (3D-) 2자리 수 뺄셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다. 4 digit addition (4D+) 3자리 수 덧셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다. 4 digit subtraction (4D-) 3자리 수 뺄셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다. 5 digit addition (5D+) 4자리 수 덧셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다. 5 digit subtraction (5D-) 4자리 수 뺄셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다. 2 digit multiplication (2Dx) 모델에게는 두 정수를 곱하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 \u0026ldquo;Q: What is 24 times 42? A: 1008\u0026quot;와 같은 형태로 질문된ㄴ다. One-digit composite (1DC) 모델에게는 마지막 두 숫자에 괄호가 있는 세 개의 1자리 숫자에 대해 복합 연산을 수행하라는 질문이 제시된다. 예를 들어, \u0026ldquo;Q: What is 6+(4*8)? A: 38\u0026quot;입니다. 세 개의 1자리 숫자는 [0, 10) 범위에서 균일하게 선택되며, 연산은 { +, -, * } 중에서 균일하게 선택된다. 10개의 모든 작업에서 모델은 정확한 답변을 생성해야 한다. 각 작업에 대해 작업의 2,000개의 무작위 인스턴스 데이터셋을 생성하고, 모든 모델을 이러한 인스턴스에서 평가한다.\nGPT-3는 few-shot 설정에서 평가되었고, 작은 숫자에 대한 덧셈과 뺄셈에서 높은 정확도를 보여주었다. 2자리 숫자에 대한 연산에서는 덧셈에서 100%, 뺄셈에서 98.9%의 정확도를 보였으며, 3자리 숫자에 대한 연산에서는 덧셈에서 80.2%, 뺄셈에서 94.2%의 정확도를 달성하였다. 숫자의 자릿수가 증가함에 따라 성능은 감소하지만, 4자리 연산에서는 25-26%, 5자리 연산에서는 9-10%의 정확도를 보여주었다. GPT-3는 또한 계산이 복잡한 2자리 곱셈에서 29.2%의 정확도를 달성하였다. 마지막으로, GPT-3는 단일 자릿수 복합 연산(예를 들어, 9*(7+5))에서 21.3%의 정확도를 보였습니다. 이는 GPT-3가 단일 연산을 넘어서 일부 견고성을 가지고 있다는 것을 나타낸다.\n작은 모델들은 이러한 모든 작업에서 성능이 좋지 않다 - 심지어 13B parameter 모델(175B parameter 전체 GPT-3 다음으로 큰 모델)조차도 2자리 덧셈과 뺄셈을 절반 정도의 시간만 해결할 수 있고, 다른 모든 연산은 10% 미만의 시간에 해결할 수 있다.\none-shot과 zero-shot 성능은 few-shot 성능에 비해 다소 낮지만, 이는 작업에 대한 적응이 중요함을 보여준다. 그러나 one-shot 성능은 아직도 강하며, 전체 GPT-3의 zero-shot 성능은 더 작은 모델들의 few-shot 학습보다 월등히 뛰어나다.\n모델이 단순히 특정 산술 문제를 기억하는 것인지를 확인하기 위해, 테스트 세트의 3자리 산술 문제를 가져와서 \u0026ldquo; + =\u0026ldquo;와 \u0026ldquo; plus \u0026rdquo; 형태로 학습 데이터에서 찾아보았다. 2,000개의 덧셈 문제 중에서는 17개(0.8%)만 일치하였고, 2,000개의 뺄셈 문제 중에서는 2개(0.1%)만 일치하였다. 이는 올바른 답변의 일부분만이 기억되었을 수 있다는 것을 시사한다. 또한, 잘못된 답변의 검사는 모델이 \u0026ldquo;1\u0026quot;을 올리지 않는 등의 오류를 종종 범하는 것으로 나타낸다. 이는 모델이 실제로 표를 기억하는 것이 아니라 관련 계산을 수행하려고 시도하고 있다는 것을 시사한다.\n전반적으로 GPT-3는 few-shot, one-shot, 심지어 zero-shot 설정에서도 복잡한 산술에 대해 합리적인 능숙도를 보여준다.\nWord Scrambling and Manipulation Tasks GPT-3의 새로운 기호 조작 학습 능력을 테스트하기 위해, 문자를 섞거나 추가하거나 삭제하여 왜곡된 단어를 복구하는 5가지 \u0026lsquo;character manipulation\u0026rsquo; 작업을 설계하였다:\nCycle letters in word (CL) 모델에게 문자가 순환된 단어와 \u0026ldquo;=\u0026rdquo; 심볼이 주어지면, 원래의 단어를 생성해야 한다. 예를 들어, \u0026ldquo;lyinevitab\u0026quot;이 주어지면 \u0026ldquo;inevitably\u0026quot;를 출력해야 한다. Anagrams of all but ﬁrst and last characters (A1) 모델에게 첫 번째와 마지막 문자를 제외한 모든 문자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, \u0026ldquo;criroptuon\u0026quot;이 주어지면 \u0026ldquo;corruption\u0026quot;을 출력해야 한다. Anagrams of all but ﬁrst and last 2 characters (A2) 모델에게 첫 두 글자와 마지막 두 글자를 제외한 모든 글자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 복구해야 한다. 예를 들어, \u0026ldquo;opoepnnt\u0026quot;가 주어지면 \u0026ldquo;opponent\u0026quot;를 출력해야 한다. Random insertion in word (RI) 모델에게 단어의 각 글자 사이에 무작위의 구두점이나 공백 문자가 삽입된ㄴ 글자가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, \u0026ldquo;s.u!c/c!e.s s i/o/n\u0026quot;이 주어지면 \u0026ldquo;succession\u0026quot;을 출력해야 한다. Reversed words (RW) 모델에게 거꾸로 철자된 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, \u0026ldquo;stcejbo\u0026quot;가 주어지면 \u0026ldquo;objects\u0026quot;를 출력해야 한다. 각 작업에 대해 가장 빈번한 10,000개의 단어를 사용하여 10,000개의 예시를 생성하였다. few-shot 결과는 모델 크기가 커질수록 성능이 부드럽게 증가하는 경향을 보여주었다. 전체 GPT-3 모델은 RI 38.6%, A1 40.2%, A2 15.1%를 달성하였다. 그러나 어느 모델도 RW는 불가능하였다.\none-shot 설정에서는 성능이 크게 약해져서 절반 이상 떨어지고, zero-shot 설정에서는 대부분의 작업을 수행하지 못하였다. 이는 모델이 테스트 단계에서 이러한 작업을 실제로 배우는 것을 나타내며, 이러한 작업들이 사전 학습 데이터에는 거의 나타나지 않기 때문에 zero-shot으로 수행하는 것이 어렵다.\n\u0026ldquo;in-context learning curves\u0026quot;을 통해 성능을 정량적으로 측정할 수 있다. 이는 in-context 예시의 수에 따른 작업 성능을 나타낸다. 큰 모델일수록 in-context 정보를 더 효과적으로 활용할 수 있음을 알 수 있다. 이는 작업 예시와 자연 언어 작업 설명 모두를 포함한다.\n이러한 작업을 해결하려면 문자 수준의 조작이 필요하며, BPE 인코딩은 단어의 큰 부분을 조작한다. 따라서, 이 작업에 성공하려면 BPE 토큰을 조작하는 것뿐만 아니라 그들의 하위 구조를 이해하고 분해해야 한다. 또한, CL, A1, A2는 bijective가 아니므로, 모델이 올바른 암호화 해제를 찾기 위해 검색을 수행해야 한다. 이러한 기술은 복잡한 패턴 매칭과 계산이 필요하다.\nSAT Analogies GPT-3는 374개의 \u0026ldquo;SAT analogy\u0026rdquo; 문제를 통해 테스트되었다. 이 작업에서 GPT-3는 few-shot 65.2%, one-shot 59.1%, zero-shot 53.7%의 성능을 보여주었다. 이는 대학 지원자들의 평균 57%보다 높다. 결과는 규모에 따라 개선되며, 1750억 모델은 130억 파라미터 모델에 비해 10% 이상 개선되었다.\nNews Article Generation GPT-3는 \u0026ldquo;news articles\u0026quot;를 생성하는 능력을 테스트하였다. 그러나 GPT-3의 학습 데이터는 뉴스 기사에 비중이 덜 두어져 있어, 뉴스 기사를 생성하는 것이 덜 효과적이었다. 이를 해결하기 위해, GPT-3의 few-shot 학습 능력을 활용해 세 개의 이전 뉴스 기사를 제공하여 모델을 조건화하였다. 그 결과, 제안된 다음 기사의 제목과 부제목을 가지고, 모델은 \u0026ldquo;news\u0026rdquo; 장르의 짧은 기사를 신뢰성 있게 생성할 수 있었다.\nGPT-3가 생성한 뉴스 기사의 품질을 평가하기 위해, 사람들이 GPT-3가 생성한 기사와 실제 기사를 구별하는 능력을 측정하기로 결정하였다. 이는 생성적 언어 모델의 품질을 측정하는 중요한 방법으로, 이전에도 비슷한 연구가 있었다.\n사람들이 모델이 생성한 텍스트를 얼마나 잘 감지하는지 확인하기 위해, newser.com에서 기사 제목과 부제목 25개를 선택하고, 다양한 크기의 언어 모델로 이를 완성하였다. 이후 미국 참가자 80명에게 이 기사들을 제시하고, 각 참가자에게 기사가 사람이 작성했는지 아니면 기계가 작성했는지 판단하도록 요청하였다.\n모델의 훈련 데이터에는 없는 기사들을 선택하였고, 모든 모델은 같은 맥락을 사용하여 훈련되었다. 각 모델에게는 같은 기사 제목과 부제목이 프롬프트로 제공되었다. 추가적으로 참가자의 집중력을 통제하기 위한 실험도 진행했는데, 이는 고의적으로 품질이 낮은 기사를 생성한 \u0026ldquo;control model\u0026quot;을 사용했습니다.\n고의적으로 품질이 낮은 기사를 모델이 만들었다고 판별하는 평균 인간 정확도는 약 86%였다. 반면, 175B parameter 모델이 생성한 기사를 감지하는 평균 인간 정확도는 우연 수준인 52%에 불과하였다. 모델 크기가 커질수록 인간이 모델이 생성한 텍스트를 감지하는 능력이 줄어드는 것으로 보인다.\nGPT-3가 생성한 합성 기사의 예시는 대부분 인간이 진짜 내용과 구별하기 어렵다. 하지만 사실적인 오류는 기사가 모델이 생성했음을 나타낼 수 있다. 왜냐하면 모델은 인간 작가와 달리 특정 사실에 대한 접근이 없기 때문이다. 또한 반복, 비연속성, 이상한 표현도 모델이 생성한 텍스트의 지표가 될 수 있다.\n인간은 더 많은 토큰을 관찰할수록 모델이 생성한 텍스트를 더 잘 감지한다. 이를 검증하기 위해, 평균 길이가 569단어인 로이터의 12개 세계 뉴스 기사에 대해 GPT-3가 평균 498단어로 생성한 기사를 사용하여 실험을 진행하였다. 약 80명의 미국 참가자를 대상으로 한 두 가지 실험을 통해 GPT-3와 통제 모델이 생성한 기사를 감지하는 인간의 능력을 비교하였다.\n통제 모델로부터 나온 고의적으로 나쁜 긴 기사를 감지하는 인간의 평균 정확도는 약 88%였다. 반면에, GPT-3가 생성한 긴 기사를 인식하는 인간의 평균 정확도는 약 52%로 거의 우연에 가까웠다. 이는 GPT-3가 약 500단어의 뉴스 기사를 생성할 때, 인간이 쓴 것과 구별하기 어렵다는 것을 의미한다.\nLearning and Using Novel Words 새로운 단어를 배우고 활용하는 능력을 GPT-3로 테스트해 보았다. 존재하지 않는 단어 \u0026lsquo;Gigamuru\u0026rsquo; 같은 단어의 정의를 제공하고, 그 단어를 문장에서 사용하도록 요청하였다. 결과적으로, GPT-3는 새로운 단어를 문장에서 사용하는 작업에 대해 능숙함을 보였다. 심지어 \u0026ldquo;screeg\u0026quot;이라는 단어에 대해 그럴등한 변형(\u0026ldquo;screeghed\u0026rdquo;)을 생성하며, 이 단어를 약간 어색하게 사용하였지만 장난감 칼 싸움을 묘사하는 가능성을 보여주었다.\nCorrecting English Grammar 영어 문법 교정은 few-shot 학습에 아주 적합한 작업이다. GPT-3를 이용해 이를 테스트하였다. 이를 위해 \u0026ldquo;Poor English Input: \\ n Good English Output: \u0026rdquo; 형식의 문장을 주고, 한 가지 인간이 생성한 교정 예를 제공한 후, 다른 5개 문장의 교정을 요청했습니다.\nMeasuring and Preventing Memorization Of Benchmarks 학습 데이터셋은 인터넷에서 가져왔기 때문에, 벤치마크 테스트 세트가 학습 데이터에 포함된 것일 수 있다. 이런 테스트 오염을 정확히 파악하는 것은 아직 확립된 방법이 없는 새로운 연구 분야이다. 대규모 모델 학습 시 오염을 조사하지 않는 것이 일반적이지만, 사전 학습 데이터셋의 규모가 커지고 있어 이 문제에 점점 더 주목할 필요가 있다고 생각한다.\n학습 데이터와 평가 데이터셋이 겹치는 문제는 실제로 존재한다. Common Crawl 데이터에 기반한 언어 모델을 처음 학습시킨 연구 중 하나에서는 평가 데이터셋과 겹치는 학습 문서를 감지하고 제거하였다. GPT-2와 같은 다른 연구에서도 이러한 중복을 분석하였고, 결과적으로 학습과 테스트 데이터가 겹치는 경우 모델 성능이 약간 향상되었지만, 겹치는 데이터의 비율이 작아 전체 결과에는 크게 영향을 미치지 않았다.\nGPT-3는 데이터셋과 모델 크기가 GPT-2보다 훨씬 크며, 대량의 Common Crawl 데이터를 포함하고 있어 오염 가능성이 늘어났다. 그러나, 데이터량이 많아 GPT-3 175B는 중복 제거된 검증 세트에 대해 크게 과적합되지 않았다. 따라서, 오염은 자주 발생할 것으로 보이지만 그 효과는 예상만큼 크지 않을 것으로 보인다.\n학습 데이터와 벤치마크의 개발 및 테스트 세트 간 중복을 찾아 제거하려 하였으나, 버그로 인해 감지된 중복이 일부만 제거되었다. 모델을 재학습하는 것은 비용 문제로 불가능했다. 그래서 남은 중복이 결과에 미치는 영향을 자세히 조사하였다.\n각 벤치마크에 대해, 13-gram 중복이 있는 예시를 제거하여 오염이 없는 \u0026ldquo;clean\u0026rdquo; 버전을 만들었다.\n클린 벤치마크에서 GPT-3를 평가한 결과, 전체 데이터셋의 점수와 비슷한 경우, 오염이 결과에 큰 영향을 미치지 않는 것으로 판단되었다. 만약 클린 벤치마크의 점수가 낮다면, 오염이 결과를 과대 평가하고 있다는 의미이다. 그러나 대부분의 경우, 성능 변화는 미미하며, 오염 수준과 성능 차이는 연관되지 않는 것으로 나타났다. 이를 바탕으로, 오염이 성능에 큰 영향을 미치지 않았다는 결론을 내렸다.\n(1) 모델이 클린 버전에서 상당히 더 나쁜 성능을 보이거나, 또는 (2) 잠재적 오염이 매우 높아 성능 차이를 측정하기 어려운 몇 가지 특정 사례를 더 자세히 살펴보았다.\n6개의 벤치마크 그룹(Word Scrambling, Reading Comprehension, PIQA, Winograd, language modeling tasks, German to English translation)이 추가 조사를 위해 지정되었다. 이 중복 분석은 매우 보수적으로 설계되었으므로 일부 잘못된 긍정 결과가 있을 것으로 예상한다.\nReading Comprehension: QuAC, SQuAD2, DROP의 작업 예시 중 90% 이상이 잠재적 오염으로 판단되었지만, 수동 검사 결과 원본 텍스트는 학습 데이터에 있지만 질문/답변 쌍은 없었다. 즉, 모델은 배경 정보만 얻을 수 있고 특정 질문에 대한 답을 기억할 수 없었다. German translation: WMT16 독일어-영어 테스트 세트의 25%가 잠재적 오염으로 표시되었지만, 검사 결과, 학습 데이터와 유사한 쌍의 문장이 포함된 사례는 없었다. 대부분은 뉴스에서 논의된 이벤트의 일부를 포함하는 단일 언어 일치였다. Reversed Words and Anagrams: \u0026ldquo;alaok = koala\u0026rdquo; 형태의 작업에서, 중복이 ﬂagged되었지만, 이는 대부분 회문이나 간단한 재정렬 예시였다. 중복된 부분은 적지만, 단순한 작업을 제거하면 난이도가 증가하고 잘못된 신호가 발생한다. 심볼 삽입 작업은 높은 중복을 보이지만 성능에는 영향을 미치지 않았다. 이는 작업이 비문자 문자 제거에 중점을 두고 있고, 중복 분석이 이러한 문자를 무시하기 때문이다. PIQA: 예시의 29%가 오염되었다고 표시되었고, 클린 부분 집합에서 성능이 3% 감소했다. 테스트 데이터셋은 학습 세트 이후에 출시되었지만, 일부 웹 페이지는 학습 세트에 포함되어 있었다. 메모리 용량이 훨씬 적은 작은 모델에서도 비슷한 감소를 보아, 이는 통계적 편향일 가능성이 높다. 하지만 이 가설을 엄밀하게 증명할 수는 없으므로, PIQA 결과에는 별표를 표시하였다. Winograd: 예시의 45%가 중복으로 표시되었고, 클린 부분집합에서 성능이 2.6% 감소했다. 중복 데이터를 검사한 결과, 학습 세트에 132개의 Winograd 스키마가 다른 형식으로 포함되어 있었다. 성능 감소가 작지만, Winograd 결과에 별표를 표시했다. Language modeling: GPT-2에서 측정된 4개의 Wikipedia 언어 모델링 벤치마크와 Children’s Book Test 데이터셋이 대부분 학습 데이터에 포함되어 있었다. 클린한 부분 집합을 신뢰성 있게 추출할 수 없어 이 데이터셋들의 결과는 보고하지 않았다. Penn Tree Bank는 그 연령 때문에 영향을 받지 않아, 주요 언어 모델링 벤치마크로 사용하였다. 오염이 높지만 성능에 미치는 영향이 거의 없는 데이터셋을 검사해 실제 오염 정도를 확인하였다. 이들은 대부분 실제 오염이 없거나, 작업의 답을 알려주는 오염이 없었다. 하지만 LAMBADA는 심각한 오염이 있음에도 성능에 미치는 영향이 매우 작았다. 빈칸 채우기 형식은 가장 단순한 형태의 기억을 배제하지만, 이 논문에서 LAMBADA에서 큰 향상을 보였으므로, 결과 섹션에서 잠재적 오염을 언급했다.\n오염 분석의 한계는 클린 부분 집합이 원래 데이터셋과 같은 분포에서 추출되었는지 확신할 수 없다는 점이다. 기억이 결과를 과대평가하면서 클린 부분 집합이 더 쉽게 되게 하는 통계적 편향이 정확히 상쇄되는 가능성이 있다. 그러나 0에 가까운 이동이 많아 이는 불가능할 가능성이 크고, 기억이 적은 작은 모델들에서도 눈에 띄는 차이를 찾지 못하였다.\n데이터 오염의 영향을 측정하고 기록하기 위해 최선을 다했고, 심각성에 따라 문제 결과를 주목하거나 완전히 제거하였다. 벤치마크 설계와 모델 학습에서 이 중요하고 미묘한 문제를 해결하기 위한 많은 작업이 아직 남아 있다.\nLimitations GPT-3에 대한 분석에는 여러 가지 한계점이 있다. 이에 대해 설명하고 미래의 연구 방향을 제안한다.\nGPT-3와 그 분석에는 한계가 있다. GPT-3는 텍스트 합성과 여러 NLP 작업에서 향상되었지만, 문서 수준에서 의미적으로 반복되거나, 긴 문장에서 일관성을 잃는 등의 문제가 있다. 또한 \u0026ldquo;common sense physics\u0026quot;과 같은 분야에서 어려움을 겪는 것으로 보였다. GPT-3의 문맥 중심 학습 성능은 \u0026ldquo;comparison\u0026rdquo; 작업이나 읽기 이해 작업 등에서 뚜렷한 차이를 보였다. 이는 GPT-3이 다른 많은 작업에서 강력한 성능을 보이고 있음에도 불구하고 눈에 띈다.\nGPT-3의 한계는 그 구조와 알고리즘에 기인한다. 이는 양방향 아키텍처나 노이즈 제거와 같은 훈련 목표를 포함하지 않는 실험 설계 때문이다. 이로 인해 GPT-3는 빈칸 채우기 작업, 두 내용을 비교하는 작업, 긴 구절을 신중히 고려한 후 짧은 답변을 생성하는 작업 등에서 성능이 떨어졌다. 이런 원인으로 GPT-3는 WIC, ANLI, QuAC 및 RACE와 같은 몇몇 작업에서 뒤떨어지는 성능을 보였다. 큰 규모의 양방향 모델을 만들거나, 양방향 모델을 몇 번이나 한 번도 시도하지 않는 학습과 함께 작동하게 하는 것은 미래의 연구 방향으로 유망히다.\n이 논문에서 설명하는 방법론의 근본적인 제한은 모든 토큰을 동등하게 취급하고 중요한 예측과 그렇지 않은 예측을 구별하지 못한다는 점이다. 또한, 언어 시스템은 단순히 예측을 만드는 것이 아니라 목표 지향적인 행동을 취해야 하며, 대규모 언어 모델은 다른 경험 영역에 기반을 두지 않아 세계에 대한 많은 맥락을 부족하게 한다. 이러한 이유로, self-supervised 예측의 확장은 한계에 도달하고, 다른 접근법으로 보완해야 한다. 이를 위해 인간으로부터 목표 함수를 학습하거나, 강화 학습으로 미세 조정하거나, 이미지 등의 추가적인 모달리티를 추가하는 방향이 유망해 보인다.\n언어 모델의 주요 제한 중 하나는 사전 학습 단계에서의 샘플 효율성이 낮다는 것이다. GPT-3는 테스트 시간에 인간과 가까운 샘플 효율성을 보이지만, 사전 학습 과정에서 인간이 평생 동안 접하는 텍스트보다 훨씬 많은 텍스트를 본다는 문제가 있다. 사전 학습의 샘플 효율성을 개선하는 것은 미래의 연구 방향으로, 물리적 세계에 기반을 두는 것이나 알고리즘의 개선을 통해 이루어질 수 있다.\nGPT-3의 few-shot 학습에서의 한 제한은, 실제로 추론 시에 새로운 작업을 \u0026ldquo;처음부터(from scratch)\u0026rdquo; 학습하는지, 아니면 학습 도중 배운 작업을 단순히 인식하고 식별하는지에 대한 불확실성이다. 이는 테스트 시간에 정확히 동일한 분포에서 작업을 인식하거나, 같은 작업을 다른 형식으로 인식하거나, QA 같은 일반적인 작업 스타일에 적응하거나, 완전히 새로운 기술을 배우는 것 등, 넓은 범위에 걸쳐 있다. 어떤 작업에서는 새롭게 배우는 경향이 있고, 다른 작업에서는 사전 학습 동안에 배워야 하는 상황도 있다. 결국 인간이 무엇을 처음부터 배우는지, 무엇을 이전의 경험으로부터 배우는지조차 확실하지 않다. 이러한 이해의 불확실성은 few-shot 학습의 원리를 정확히 파악하는 데 중요한 미래의 연구 방향을 제시한다.\nGPT-3와 같은 대규모 모델의 한계는, 추론을 수행하는데 비용이 많이 들고 불편하다는 점이다. 이는 이러한 크기의 모델의 실질적인 적용을 어렵게 만든다. 이 문제를 해결할 수 있는 한 가지 방법은, 대규모 모델을 특정 작업에 맞게 관리 가능한 크기로 축소하는 것이다. 이는 아직 수백억 개의 매개변수 규모에서 시도되지 않았지만, 새로운 도전과 기회를 제공할 수 있다.\nGPT-3는 대부분의 딥러닝 시스템과 마찬가지로 결정의 해석이 어렵고, 새로운 입력에 대한 예측이 반드시 잘 조정되지 않으며, 학습 데이터의 편향을 유지하는 등의 한계를 가지고 있다. 특히, 학습 데이터의 편향이 모델이 편견 있는 내용을 생성하도록 이끌 수 있는 문제는 사회적 관점에서 큰 우려사항이다.\nBroader Impacts 언어 모델은 자동 완성, 문법 도움 등의 다양한 이점을 제공하지만, 잠재적으로 해로운 응용 분야도 있다. GPT-3는 텍스트 생성의 품질을 향상시키고, 합성 텍스트와 인간이 쓴 텍스트를 구별하는 어려움을 증가시키므로, 언어 모델의 좋은 사용과 나쁜 사용을 모두 발전시킬 수 있다.\n해로움을 연구하고 완화하기 위한 노력을 자극하기 위해서 향상된 언어 모델의 잠재적인 해로움에 초점을 맞추면, 주요 문제는 GPT-3와 같은 언어 모델의 고의적인 오용 가능성과 모델 내의 편향, 공정성, 표현 문제입니다.\nMisuse of Language Models 언어 모델의 악의적인 사용은 모델을 원래 의도와 다른 환경이나 목적으로 재사용하는 경우가 많아 예상하기 어렵다. 이를 위해 위협과 잠재적 영향을 식별하고, 위험성을 평가하는 보안 위험 평가 프레임워크를 사용한다.\nPotential Misuse Applications 텍스트 생성에 의존하는 모든 사회적인 해로운 활동은 강력한 언어 모델로 인해 강화될 수 있다. 오해, 스팸, 피싱, 법적 남용, 부정한 학술 작성 등이 예시이다. 고품질의 텍스트 생성을 할 수 있는 언어 모델은 이런 활동의 장벽을 낮추고 효과를 높일 수 있다.\n텍스트 합성의 품질이 향상됨에 따라 언어 모델의 오용 가능성이 증가한다. GPT-3가 사람이 쓴 것과 구별하기 어려운 텍스트를 생성하는 능력은 이에 대한 우려를 높인다.\nThreat Actor Analysis 위협 행위자는 기술과 자원 수준에 따라 분류된다. 이는 악의적 제품을 만들 수 있는 낮은 기술력을 가진 행위자부터 장기적인 목표를 가진 국가 후원의 고도로 기술화된 그룹까지 다양하다.\n오해 전략, 악성 소프트웨어 배포, 컴퓨터 사기 등이 논의되는 포럼을 모니터링하여 저수준 및 중간 수준의 행위자들이 언어 모델에 대해 어떻게 생각하는지 파악하고 있다. 2019년 GPT-2의 처음 출시 이후 오용에 대한 논의가 있었지만, 그 이후로는 실험적인 사례가 줄었고, 성공적인 배포는 없었다. 이러한 오용 논의는 언어 모델 기술의 미디어 보도와 관련이 있었다. 이러한 행위자들로부터의 즉각적인 오용 위협은 없지만, 신뢰성이 크게 향상되면 상황이 바뀔 수 있다.\nAPT들은 보통 공개적으로 작전을 논의하지 않기 때문에, 전문 위협 분석가들과 상의하였다. GPT-2 출시 이후, 언어 모델을 사용하여 이익을 볼 수 있는 작전에서 눈에 띄는 변화는 없었다. 현재의 언어 모델이 텍스트 생성에 있어 훨씬 뛰어나다는 설득력 있는 증거가 없으며, 모델의 내용을 \u0026ldquo;targeting\u0026quot;하거나 \u0026ldquo;controlling\u0026quot;하는 방법이 초기 단계에 있기 때문에, 언어 모델에 많은 자원을 투자하는 것은 가치가 없다는 평가를 받았다.\nExternal Incentive Structures 각각의 위협 행위자 그룹은 그들의 목표를 달성하기 위해 전략, 기술, 절차(TTPs)를 사용한다. 이는 확장성과 배포의 용이성 등 경제적 요인에 의해 영향을 받는다. 피싱은 낮은 비용, 적은 노력, 높은 수익률로 악성 소프트웨어를 배포하고 로그인 정보를 훔칠 수 있기 때문에 모든 그룹에서 매우 인기가 있다. 언어 모델을 사용하여 기존의 TTPs를 보완하면 배포 비용이 더욱 줄어들 것으로 예상된다.\n사용의 용이성은 TTPs 채택에 큰 영향을 미친다. 언어 모델의 출력은 확률적이고, 인간의 피드백 없이는 일관된 성능을 내기 어렵다. 만약 사회적 미디어의 허위 정보 봇이 대부분의 시간 동안 신뢰할 수 있는 출력을 생성하지만 가끔 비일관적인 출력을 생성한다면, 이 봇을 운영하는 데 필요한 인간의 노동량을 줄일 수 있다. 그러나 출력을 필터링하기 위해 인간이 여전히 필요하므로, 작업의 확장성은 제한된다.\nFairness, Bias, and Representation 학습 데이터의 편향으로 인해 모델은 편견이나 스테레오타입을 생성할 수 있다. 이는 기존 스테레오타입을 강화하고, 불온한 묘사를 생성하는 등의 방식으로 특정 그룹에 해를 끼칠 수 있다. 그래서 우리는 GPT-3의 공정성, 편향, 대표성에 대한 한계를 이해하기 위해 편향 분석을 수행하였다.\n목표는 GPT-3의 완전한 특성화가 아니라, 그 한계와 행동에 대한 초기 분석을 제공하는 것이다. 성별, 인종, 종교 등의 편향에 초점을 맞추고 있지만, 다른 카테고리의 편향도 존재하며 이는 후속 연구에서 다루어질 수 있다. 이는 초기 분석이며 모델의 모든 편향을 반영하지는 않는다.\n이 논문의 분석은 인터넷에서 학습된 모델이 인터넷 규모의 편향을 가지고 있다는 것을 나타낸다. 모델은 학습 데이터의 스테레오타입을 반영하는 경향이 있다. 성별, 인종, 종교 등의 편향을 찾아내기 위해 175B parameter 모델과 작은 모델을 분석하였다.\nGender GPT-3에서 성별 편향 조사는 성별과 직업 사이의 연관성에 집중했다. 대부분의 직업은 남성 식별자가 뒤따르는 확률이 더 높았으며, 이는 높은 학력을 요구하는 직업이나 신체적 노동을 요구하는 직업에서 특히 두드러졌다. 반면, 여성 식별자가 뒤따르는 확률이 더 높은 직업은 산모 도우미, 간호사, 리셉션니스트, 가정부 등이었다.\n\u0026ldquo;\u0026ldquo;The competent { occupation } was a\u0026quot;이나 \u0026ldquo;The incompetent { occupation } was a\u0026quot;이라는 맥락으로 바뀌었을 때, 대부분의 직업은 여전히 남성 식별자를 더 높은 확률로 따르는 경향이 있었다. 이는 원래의 중립적 프롬프트와 비교했을 때도 마찬가지였다. 평균 직업 편향은 중립 변형에서 -1.11, 유능한 변형에서 -2.14, 무능한 변형에서 -1.15로 측정되었다.\nWinogender 데이터셋에서 대명사 해결을 수행하여 대부분의 직업을 남성과 연관짓는 모델의 경향성을 입증하였다. 예를 들어, \u0026ldquo;The advisor met with the advisee because she wanted to get advice about job applications. ‘She’ refers to the\u0026quot;와 같은 맥락을 제공하고, \u0026lsquo;advisor\u0026rsquo;와 \u0026lsquo;advisee\u0026rsquo; 중 어느 쪽을 \u0026lsquo;she\u0026rsquo;로 가장 적합하게 할당하는지를 측정하였다.\n언어 모델이 사회적 편향, 예를 들어 여성 대명사를 참여자 위치와 더 많이 연관짓는 경향 등을 학습했음을 발견하였다. GPT-3 175B 모델은 이 작업에서 가장 높은 정확도(64.17%)를 보였고, 이는 편향 문제가 언어 모델을 오류에 취약하게 만들 수 있는 곳에서, 큰 모델이 작은 모델보다 더 강건하다는 초기적인 증거를 제공한다.\n단어들이 어떤 단어와 같이 나타나는지 분석하는 공존 테스트를 수행하였다. 이를 위해 데이터셋의 모든 프롬프트에 대해 여러번의 출력을 생성하여 샘플 세트를 만들었다. 성별에 대한 분석에서 여성은 \u0026ldquo;beautiful\u0026quot;과 \u0026ldquo;gorgeous\u0026quot;와 같은 외모 지향적인 단어로 더 자주 묘사되었으며, 반면에 남성은 더 다양한 형용사로 묘사되었다는 것을 발견하였다.\n모델에서 가장 선호하는 상위 10개의 형용사와 이들이 대명사 지시어와 얼마나 자주 함께 나타나는지를 보여준다. \u0026ldquo;Most Favored\u0026quot;은 한 카테고리와 비교해 다른 카테고리와 더 자주 함께 나타나는 단어를 의미한다. 이를 이해하기 쉽게 하기 위해, 각 성별에 대한 모든 단어들의 공존 횟수의 평균도 제시되었다.\nRace GPT-3의 인종 편향을 조사하기 위해, 특정 인종을 나타내는 용어로 대체된 특정 프롬프트를 사용하여 샘플을 생성하고, 생성된 샘플에서 단어의 공존을 측정하였다. 이전 연구가 언어 모델이 다른 특징에 따라 다른 감정의 텍스트를 생성한다는 것을 보여준 것을 바탕으로, 인종이 어떻게 감정에 영향을 미치는지를 조사했다. 각 인종과 과도하게 공존하는 단어의 감정은 Senti WordNet을 사용하여 측정하였다.\n모델 전반에 걸쳐 \u0026ldquo;아시아인\u0026quot;은 일관되게 높은 감정을 가지고 있었으며, 7개의 모델 중 3개에서 1위를 차지하였다. 반면에 \u0026ldquo;흑인\u0026quot;은 일관되게 낮은 감정을 가지고 있었으며, 7개의 모델 중 5개에서 최하위를 차지하였다. 이런 차이는 큰 모델 크기에서 약간 줄어들었다. 이 분석은 다양한 모델의 편향성을 보여주며, 감정, 엔티티, 입력 데이터 간의 관계에 대한 보다 세밀한 분석의 필요성을 강조한다.\nReligion 무신론, 불교, 기독교, 힌두교, 이슬람교, 유대교와 같은 종교 용어와 함께 나타나는 단어를 연구하기 위해, 각각의 종교에 대한 특정 프롬프트에 대해 800개의 모델 출력을 생성했다. 이 프롬프트는 \u0026ldquo;{ Religion practitioners } are\u0026rdquo; (Eg. \u0026ldquo;Christians are\u0026rdquo;)과 같은 형태였다. 그런 다음 모델이 자연스럽게 문장을 완성하도록 하여, 단어의 공존을 연구하는 말뭉치를 생성하였다.\n모델이 종교 용어와의 연관성을 나타내는 방식은 종종 이 용어들이 실제 세상에서 어떻게 표현되는지를 반영한다는 것을 확인하였다. 예를 들어, 이슬람교와 관련하여 \u0026lsquo;라마단\u0026rsquo;, \u0026lsquo;예언자\u0026rsquo;, \u0026lsquo;모스크\u0026rsquo;와 같은 단어들이 다른 종교보다 더 자주 등장하며, \u0026lsquo;폭력적인\u0026rsquo;, \u0026lsquo;테러리즘\u0026rsquo;, \u0026lsquo;테러리스트\u0026rsquo; 등의 단어는 이슬람교와 더 크게 연관되어 GPT-3에서 이슬람교에 대한 상위 40개 선호 단어에 포함되었다.\nFuture Bias and Fairness Challenges 이 초기 분석을 통해 발견된 편향을 공유하고, 대규모 생성 모델의 편향을 파악하는 본질적인 어려움을 강조하며, 추가 연구를 촉진하고자 한다. 이는 지속적인 연구 영역이 될 것으로 기대하며, 성별, 인종, 종교를 연구의 시작점으로 설정했음을 밝혔다. 이런 선택에는 주관성이 내재해 있는 것을 인지하고 있다.\n언어 시스템의 편향을 파악하는 것뿐만 아니라 개입하는 것이 중요하며, 이를 위해선 편향 완화에 대한 공통 어휘 구축이 필요하다. 더 많은 연구가 필요하며, 이는 NLP 외부의 문헌과의 연계, 해를 끼치는 규범적 진술의 명확한 표현, 그리고 NLP 시스템에 영향을 받는 커뮤니티의 실제 경험에 대한 관여를 포함해야 한다. 편향 완화 작업은 단순히 \u0026ldquo;편향 제거\u0026quot;를 목표로 하는 것이 아니라, 전체적인 방식으로 접근해야 한다.\nEnergy Usage 대규모 사전 학습은 에너지 집약적이며, GPT-3 175B 훈련은 1.5B 파라미터의 GPT-2 모델에 비해 많은 계산을 소비했다. 따라서 이러한 모델의 비용과 효율성을 인식하는 것이 중요하다.\n대규모 사전 학습의 사용은 모델의 효율성을 다루는 새로운 관점을 제공한다. 이는 학습에 필요한 자원뿐만 아니라, 모델의 수명 동안 이러한 자원이 어떻게 분산되는지를 고려해야 한다. 학습 중에는 많은 자원을 소비하지만, 학습이 완료된 모델은 효율적이다. 또한, 모델 증류와 같은 기법을 사용하면 이러한 모델의 비용을 더욱 줄일 수 있으며, 알고리즘의 발전은 시간이 지남에 따라 이러한 모델의 효율성을 자연스럽게 더욱 증가시킬 수 있다.\nRelated Work 언어 모델의 성능을 향상시키기 위한 연구는 크게 세 가지 방향으로 진행되었다. 첫 번째는 parameter와 계산량을 함께 증가시키는 것으로, 이는 모델 크기를 계속해서 증가시키는 방식이다. 두 번째는 parameter 수는 늘리지만 계산량은 늘리지 않는 방식으로, 이는 모델의 정보 저장 용량을 늘리는 데 초점을 맞추었다. 세 번째는 parameter를 증가시키지 않고 계산을 증가시키는 방식이다. GPT-3 연구는 첫 번째 접근 방식에 초점을 맞추고, 이전 모델보다 10배 큰 모델을 개발하였다.\n언어 모델 성능에 대한 규모의 영향을 체계적으로 연구한 여러 연구에서는 모델이 확장됨에 따라 손실률에서 부드러운 멱법칙 추세를 발견하였다. 이 추세는 모델이 계속 확장됨에 따라 대체로 계속될 것으로 보이며, 다양한 downstream task 에서도 규모가 커짐에 따라 부드러운 성능 향상이 관찰되었다.\n반대 방향의 연구는 가능한 한 작은 언어 모델에서도 강한 성능을 유지하려는 시도이다. 이에는 ALBERT와 언어 모델의 축소에 대한 일반적이고 특정한 접근법이 포함되어 있다. 이런 기술은 GPT-3 연구와 보완적일 수 있으며, 큰 모델의 처리 시간과 메모리 사용량을 줄일 수 있다.\n미세조정된 언어 모델이 많은 벤치마크 작업에서 인간 수준의 성능에 근접하면서, 더 어려운 혹은 개방형 작업을 구성하는 데 많은 노력이 기울여져 왔다. 이런 작업들에는 질문 응답, 읽기 이해, 그리고 기존 언어 모델에게 어려운 데이터셋을 고의로 만드는 것이 포함된다.\n많은 연구들이 질문-응답에 집중했으며, 이는 테스트한 작업들 중 상당 부분을 차지한다. 최근의 연구로는 11B 개의 매개변수를 가진 언어 모델을 미세조정한 연구와, 테스트 시점에 대량의 데이터에 집중하는 연구가 있다. GPT-3는 문맥 내 학습에 중점을 두는 것이 특징이며, 이는 미래에 다른 연구와 결합될 수 있다.\n언어 모델의 메타러닝은 이전 연구에서 활용되었지만, 그 결과는 제한적이었다. 언어 모델 메타러닝은 내부 루프와 외부 루프의 구조를 가지고 있으며, 이는 일반적인 머신러닝에 적용된 메타러닝과 유사하다. GPT-3는 모델의 문맥을 이전 예제로 채우는 것으로, 이는 모델의 활성화를 통해 적응하는 내부 루프와 가중치를 업데이트하는 외부 루프를 가지고 있다. 또한, few-shot auto-regressive density estimation과 low-resource NMT를 few-shot 학습 문제로 연구한 사례도 있다.\n이전 연구들도 사전 학습된 언어 모델과 경사 하강법을 결합하여 few-shot 학습을 할 방법을 탐색했다. 또한, 레이블이 거의 없는 상황에서의 미세조정 방법을 연구하는 반지도 학습과 같은 비슷한 목표를 가진 분야도 있다.\n자연 언어로 다중 과제 모델에 지시를 하는 방법은 처음으로 지도 학습 환경에서 공식화되었고, 일부 과제에 언어 모델에 사용되었다. 이와 유사한 개념이 text-to-text transformer에서도 탐색되었지만, 이 경우에는 문맥 학습이 아닌 다중 과제 미세조정에 적용되었다.\n다중 과제 학습은 언어 모델의 일반성과 전이 학습 능력을 향상시키는 방법으로, 여러 과제를 함께 미세조정하며 가중치를 업데이트한다. 이 방법은 단일 모델을 가중치 업데이트 없이 다양한 과제에 사용하거나, 새로운 과제에 대한 가중치 업데이트 시 샘플 효율성을 향상시킬 수 있다. 다중 과제 학습은 초기에는 좋은 결과를 보였지만, 데이터셋 구성과 훈련 커리큘럼 설정에 대한 수작업이 필요한 한계가 있다. 하지만 대규모 사전 학습은 텍스트 예측을 통해 암시적으로 다양한 과제를 포함하는 방법을 제공한다. 미래의 연구 방향은 다중 과제 학습에 대해 더 넓은 범위의 명시적 과제를 생성하는 것일 수 있다.\n지난 두 해 동안 언어 모델의 알고리즘은 매우 크게 발전했다. denoising-based bidirectionality, preﬁxLM and encoder-decoder architectures, random permutations during training, architectures that improve the efﬁciency of sampling, improvements in data and training procedures, and efﬁciency increases in the embedding parameters 등이 포함된다. 이런 기술들은 downstream task에서 큰 이익을 가져다주며, 이러한 알고리즘 발전을 GPT-3에 통합하면 downstream task 성능이 향상될 가능성이 높다. GPT-3의 규모와 이런 알고리즘 기법을 결합하는 것은 미래 연구의 유망한 방향이다.\nConclusion 175B 개의 parameter를 가진 언어 모델을 소개하였고, 이 모델은 다양한 NLP 작업에서 강력한 성능을 발휘하며, 또한 고품질의 샘플을 생성하며, 미세 조정 없이도 성능의 확장성이 대략 예측 가능하다는 것을 보여주었다. 그리고 이 모델의 사회적 영향에 대해서도 논의하였다. 이러한 결과들은 큰 언어 모델이 적응형, 일반 언어 시스템의 개발에 중요한 요소가 될 수 있음을 시사한다.\nReference Paper Github ","date":"2023-12-26T00:00:00Z","permalink":"https://kurtkim.github.io/p/gpt-3/","title":"GPT-3"},{"content":"Abstract BERT와 같은 masked language modeling(MLM) 방법은 일부 토큰을 [MASK]로 바꿔 원래 토큰을 재구성하는 모델을 훈련하지만, 이를 효과적으로 하려면 많은 계산이 필요하다. 대신에, replaced token detection 라는 더 효율적인 사전 학습 작업을 제안한다. 이 방법은 일부 토큰을 가능성 있는 다른 토큰으로 바꾸는 방식으로 입력을 변형하고, 변형된 토큰의 원래 값을 예측하는 대신 각 토큰이 생성자 샘플로 대체되었는지 여부를 예측하는 모델을 학습시킨다. 이 방법은 모든 입력 토큰에 대해 작업을 정의하므로 MLM보다 효율적이다. 결과적으로, 이 방식으로 학습한 컨텍스트 표현은 동일한 모델 크기, 데이터, 계산량을 가진 BERT보다 월등히 높은 성능을 보여준다. 특히, 작은 모델에서는 GLUE 자연어 이해 벤치마크에서 GPT를 능가하는 모델을 한 GPU에서 4일 동안 훈련할 수 있다. 대규모에서도 이 방법은 RoBERTa와 XLNet의 성능에 비교할 수 있으며, 더 적은 계산량을 사용하면서도 동일한 계산량을 사용할 때 그 모델들을 능가한다.\nIntroduction 현재 state-of-the-art의 언어 표현 학습 방법은 denoising autoencoder이다. 이 방법은 일부 토큰을 마스킹하거나 어텐션을 적용하여 네트워크가 원래 입력을 복원하도록 학습한다. 하지만 이러한 masked language modeling (MLM)은 효과적이지만 상당한 계산 비용을 요구한다.\n\u0026ldquo;replaced token detection\u0026quot;는 BERT와 XLNet의 불일치 문제를 해결하기 위한 사전 학습 작업이다. 이 방법은 일부 토큰을 대체하여 입력을 손상시키는데, 이를 구분하는 네트워크를 판별자로 사전 학습시킨다. 이 방식은 MLM과 달리 모든 입력 토큰에서 학습하므로 계산적으로 효율적이다. 또한 GAN과 유사하지만 텍스트에 GAN을 적용하기 어려워 maximum likelihood로 손상된 토큰을 생성하는 생성자를 학습시킨다.\nELECTRA는 효율적으로 토큰 대체를 정확하게 분류하는 encoder를 학습하는 접근 방식이다. BERT와 비교하여 ELECTRA는 모든 입력 위치에서 학습하며, 학습 속도와 downstream task에서의 정확도에서 우수한 성능을 보여준다.\nELECTRA는 다른 사전 학습 방법에 비해 계산 및 parameter를 더 효율적으로 사용하여 더 나은 성능을 보여준다. GLUE 벤치마크에서, ELECTRA-Small 모델은 BERT보다 5점 더 높은 성능을 보여주었고, ELECTRA-Large 모델은 RoBERTa와 XLNet과 비슷한 성능을 보이지만 훨씬 적은 계산량을 사용한다. ELECTRA-Large는 GLUE에서 ALBERT보다 우수한 결과를 얻었고, SQuAD 2.0에서 새로운 state-of-the-art를 달성하였다. ELECTRA는 언어 표현 학습에 있어서 계산 및 parameter 효율성이 뛰어난 판별적인 방법이다.\nMethod 생성자 $G$와 판별자 $D$라는 두 개의 신경망을 사용한다. 이들은 입력 토큰 시퀀스 $x = [x_1, \u0026hellip;, x_n]$$를 문맥화된 벡터 표현의 시퀀스 $h(x) = [h_1, \u0026hellip;, h_n]$로 매핑한다. 주어진 위치 $t$에 대해, 생성자는 특정 토큰 $x_t$를 생성할 확률을 출력한다.\n$$ p_G(x_t|x) = exp(e(x_t)^\\intercal h_G(x)_t) \\ / \\sum_{x\u0026rsquo;} exp(e(x\u0026rsquo;)^\\intercal h_G(x)_t) $$\n$e$는 토큰 임베딩을 나타낸다. 주어진 위치 $t$에 대해, 판별자는 토큰 $x_t$가 실제 데이터에서 나온 것인지를 예측한다.\n$$ D(x, t) = sigmoid(w^\\intercal h_D (x)_t) $$\n생성자는 masked language modeling(MLM)을 수행하기 위해 학습된다. 입력 $x = [x_1, x_2, \u0026hellip;, x_n]$이 주어지면, MLM은 우선 1부터 n 사이의 임의의 위치를 선택하여 $m = [m_1, \u0026hellip;, m_k]$를 마스크한다. 선택된 위치의 토큰은 [MASK] 토큰으로 대체된다. 이를 $x^{masked} = REPLACE(x, m, [MASK])$로 표기한다. 그런 다음 생성자는 마스크된 토큰의 원래 식별자를 예측하는 방법을 학습한다. 판별자는 데이터의 토큰과 생성자 샘플에 의해 대체된 토큰을 구별하는 방법을 학습한다. 구체적으로는, 마스크된 토큰을 생성자 샘플로 대체하여 손상된 예제 $x^{corrupt}를 생성하고, 판별자는 $x^{corrupt}에서 원래 입력 $x$와 일치하는 토큰을 예측하도록 학습된ㄴ다. 형식적으로, 모델의 입력은 다음과 같이 구성된다:\n$$ m_i \\sim unif\\{1, n \\} \\ \\text{for} i = 1 \\ \\text{to} \\ k \\quad \\quad x^{masked} = REPLACE(x, m, [MASK]) $$ $$ \\hat{x}_i \\sim p_G(x_i | x^{masked}) \\ \\text{for} i \\in m \\quad \\quad x^{corrupt} = REPLACE(x, m, \\hat{x}) $$\n손실 함수는 다음과 같다.\n$$ \\mathbf{L}_{MLM}(x, \\theta_G) = \\mathbb{E} \\big( \\sum_{i \\in m} - log \\ p_G(x_i | x^{masked}) \\big) $$\n$$ \\mathbf{L}_{Disc}(x, \\theta_D) = \\mathbb{E} \\big( \\sum_{t=1}^n - \\mathbb{1} (x^{corrupt}_t = x_t) log \\ D(x^{corrupt}, t) - \\mathbb{1} (x^{corrupt}_t \\neq x_t) log(1 - D(x^{corrupt}, t)) $$\nGAN과 유사하지만 몇 가지 주요한 차이점이 있다. 생성자가 올바른 토큰을 생성하면 \u0026ldquo;진짜\u0026quot;로 간주되며, 적대적으로 훈련되는 대신 maximum likelihood로 학습된다. 일반적인 GAN과 달리 생성자에게 입력으로 노이즈 벡터를 제공하지 않는다.\n결합된 손실을 최소화한다.\n$$ \\underset{\\theta_G, \\theta_D}{min} \\sum_{x \\in \\mathbf{X}} \\mathbf{L}_{MLM}(x, \\theta_G) + \\lambda \\mathbf{L}_{Disc}(x, \\theta_D) $$\n대규모의 원시 텍스트 데이터에 대해 결합 손실을 최소화한다. 손실의 기대값을 단일 샘플로 근사화하고, 생성자의 손실은 판별자로 역전파하지 않는다. 사전 학습 후에는 생성자를 버리고 판별자를 downstream task에서 세밀하게 조정한다.\nExperiments Experimental Setup GLUE 벤치마크와 SQuAD 데이터셋을 사용하여 다양한 언어 이해 작업을 평가한다. GLUE 작업은 텍스트 간 연역, 질문-답변 연역, 문장 재구성, 질문 재구성, 텍스트 유사도, 감성 분석, 문장 수용성 등을 다루며, SQuAD는 질문에 대한 정확한 답변을 선택하는 작업이다. 사전 학습은 BERT 데이터를 사용하고, 일부 모델은 XLNet 데이터를 사용한다. 사전 학습과 평가는 주로 영어 데이터를 기반으로 하지만, 향후 다국어 데이터에도 적용할 수 있다.\nELECTRA는 BERT와 비슷한 구조와 hyperparameter를 가지고 있다. GLUE에 대한 미세조정에서는 ELECTRA 위에 단순한 선형 분류기를 추가하고, SQuAD에 대해서는 ELECTRA 위에 XLNet의 질문-답변 모듈을 추가하였다. 결과는 동일한 사전 훈련 체크포인트에서 10번의 파인튜닝 실행의 중앙값을 사용한다.\nModel Extensions 모델에 여러 확장 기법을 제안하고 평가한다. 다른 명시가 없는 한, 이 실험들은 BERT-Base와 동일한 모델 크기와 훈련 데이터를 사용한다.\nWeight Sharing 생성자와 판별자 사이의 가중치를 공유하여 사전 학습의 효율성을 높이는 것을 제안한다. 작은 생성자를 사용하고, 생성자와 판별자의 임베딩을 공유한다. 생성자의 입력과 출력 토큰 임베딩은 항상 연결된다.\n500k step 동안 생성자와 판별자의 가중치 공유 전략을 비교한 결과, 토큰 임베딩을 공유하는 것이 가장 효과적이었다. 이는 masked language modeling이 토큰 임베딩을 학습하는 데 도움이 되기 때문이다. encoder 가중치를 공유하는 것은 큰 향상을 가져오지 않았으며, 생성자와 판별자의 크기가 동일해야 한다는 단점이 있다.\nSmaller Generators ELECTRA 학습 시, 생성자와 판별자가 동일한 크기라면 계산량이 두 배로 증가한다. 따라서 생성자 크기를 줄여야 한다. 실험 결과, 생성자는 판별자의 1/4에서 1/2 크기가 가장 좋은 성능을 보여주었다. 이는 생성자가 너무 강력하면 판별자의 학습을 방해할 수 있기 때문이다.\nTraining Algorithms ELECTRA를 위해 다른 학습 알고리즘을 탐색했지만 성능을 향상시키지 못하였다. 대신 다음의 2 step 학습 절차를 사용하여 실험해 보았다.\n$n$ step 동안 $\\mathbf{L}_{MLM}$을 사용하여 생성자만 학습한다. 판별자의 가중치를 생성자의 가중치로 초기화한 다음, 생성자의 가중치를 고정한 상태에서 $n$ step 동안 $\\mathbf{L}_{Disc}$로 판별자를 학습한다. 생성자와 판별자의 크기가 동일해야 초기화가 가능합니다. 초기화 없이 판별자를 학습하면 판별자가 주요 클래스 이외의 것을 학습하지 못할 수 있다. 반면, 생성자와 판별자를 함께 학습하면 판별자에게 생성자를 점차 개선시킬 수 있는 커리큘럼을 제공한다. 또한, 생성자를 GAN과 같이 적대적으로 훈련하고, 생성자에서 샘플링하는 이산 연산을 위해 강화학습을 사용하는 방법도 탐색하였다.\n2 step 학습에서 생성적 목적에서 판별적 목적으로 전환한 후에 downstream task 성능이 향상되었지만, 종합 훈련을 뛰어넘지는 못하였다. 적대적 훈련은 maximum-likelihood 학습보다 성능이 떨어지는데, 이는 적대적 생성자의 masked language modeling 성능이 낮고 다양성이 부족하기 때문이다.\nSmall Models 이 연구에서는 사전 학습의 효율성을 개선하기 위해 BERT-Base를 기반으로 작은 모델을 개발하였다. 이를 위해 시퀀스 길이, 배치 크기, 은닉 차원 크기, 토큰 임베딩을 줄였다. BERT-Small 모델과 동일한 hyperparameter로 학습하여 공정한 비교를 수행하였다. 또한, ELMo와 GPT와 같이 리소스를 적게 사용하는 사전 학습 방법과 BERT-Base와 비교 가능한 ELECTRA 모델의 결과도 확인하였다.\nELECTRA-Small은 크기에 비해 높은 GLUE 점수를 기록하여 다른 모델들보다 우수한 성능을 보여준다. 작은 모델이지만 BERT-Small보다 5점 높은 점수를 기록하며, 큰 GPT 모델보다도 우수한 결과를 보여준다. ELECTRA-Small은 수렴에 가깝게 학습되며, 6시간만 학습해도 합리적인 성능을 얻을 수 있다. 또한, ELECTRA의 중간 크기인 base-sized 모델은 BERT-Base보다 우수한 성능을 보이며, BERT-Large보다도 뛰어난 결과를 얻었다. ELECTRA의 강력한 성능은 상대적으로 적은 계산 비용으로 사전 학습 모델을 개발하고 적용하는 데 도움이 된다.\nLarge Models 큰 ELECTRA 모델은 BERT-Large와 같은 크기이지만 더 오랜 시간 동안 학습된다. ELECTRA-400K는 RoBERTa의 1/4의 계산 비용으로 학습되었으며, ELECTRA-1.75M은 RoBERTa와 유사한 계산 비용으로 학습되었다. 배치 크기는 2048이고, XLNet 사전 학습 데이터를 사용하였다. ELECTRA-400K와 동일한 hyperparameter와 학습 시간으로 BERT-Large 모델도 학습되었다.\nELECTRA-400K는 RoBERTa와 XLNet와 비슷한 성능을 보이지만, 학습에 필요한 계산량은 1/4로 줄어들었다. ELECTRA-1.75M은 더 오랜 시간을 투자하여 학습되었으며 대부분의 GLUE 작업에서 다른 모델들보다 우수한 성능을 보여준다. BERT 모델은 예상보다 성능이 낮게 나왔는데, 이는 hyperparameter 튜닝이나 RoBERTa 학습 데이터 사용에 더 많은 고려가 필요할 것으로 보인다.\nELECTRA의 이점은 GLUE 테스트 세트에서도 확인되었으나, 모델들이 사용한 추가적인 기법들로 인해 완전히 동일한 비교는 어렵다.\nELECTRA는 GLUE 결과와 일관성을 보여준다. 같은 계산 자원을 사용하는 경우, ELECTRA가 masked-language-modeling 방법보다 우수한 성과를 보여준다. ELECTRA-400K는 RoBERTa-100K와 BERT 기준 모델보다 더 좋은 성능을 가지며, ELECTRA-1.75M은 SQuAD 2.0에서 이전 모델들보다 높은 점수를 받았다. ELECTRA-Base는 BERT-Base와 XLNet-Base보다 우수한 결과를 보여주며, 대부분의 지표에서 BERT-Large를 뛰어넘는다. ELECTRA는 SQuAD 2.0에서 SQuAD 1.1보다 성능이 좋다. 이는 모델이 실제 토큰과 가짜 토큰을 구별하는 대체된 토큰 감지 기능이 응답 가능성 분류에 특히 적합하기 때문이다.\nEfficiency Analysis ELECTRA의 이점이 어디에서 나오는지 이해하기 위해, BERT와 ELECTRA 사이에 다른 사전 학습 목표를 비교하였다.\nELECTRA 15% ELECTRA와 동일하지만, 판별자 손실은 입력에서 마스크 처리된 15%의 토큰에 대해서만 계산된다. 다시 말해서, 판별자 손실의 합인 $\\mathbf{L}_{Disc}$은 1부터 $n$까지가 아닌 $i \\in m$에 대해 계산된다. 이렇게 함으로써 ELECTRA 15% 모델은 일부 토큰에만 집중하여 손실을 계산하게 된다.\nReplace MLM ELECTRA 모델이 [MASK] 토큰에 노출되는 사전 학습 단계와 노출되지 않는 미세 조정 단계 사이의 불일치를 해결하는 데서 어느 정도 이득을 얻는지를 테스트한다. 마스크 처리된 토큰을 [MASK]로 대체하는 대신 생성 모델의 토큰으로 대체하는 것이 특징이다.\nAll-Tokens MLM 이 모델은 Replace MLM과 비슷한데, 마스크 처리된 토큰을 생성 모델의 샘플로 대체하고, 입력의 모든 토큰의 신원을 예측한다. 복사 확률 $D$를 출력하기 위해 sigmoid layer를 사용하며, 모델 출력은 입력 토큰에 $D$ 가중치와 $1-D$를 곱한 MLM softmax의 출력으로 구성된다. 이 모델은 BERT와 ELECTRA를 조합한 것으로, [MASK] 토큰에 대한 예측과 다른 토큰에 대해서는 입력을 복사하는 방식으로 학습된다.\nELECTRA는 모든 입력 토큰에 대한 손실을 가지는 것이 성능 향상에 큰 영향을 미치며, BERT는 [MASK] 토큰으로 인한 사전 학습 및 세부 학습 불일치로 인한 약간의 성능 저하가 있다. 또한, BERT는 이 문제를 완전히 해결하기에는 부족한 것으로 나타났고, All-Tokens MLM은 BERT와 ELECTRA 사이의 차이를 크게 줄이는 역할을 한다.\nELECTRA는 빠른 학습뿐만 아니라 다른 이점들로 인해 All-Tokens MLM에 비해 개선되었다. 작은 모델일수록 ELECTRA의 이득이 커지며, 완전히 학습된 경우 BERT보다 더 높은 정확도를 보여준다. ELECTRA는 parameter를 더 효율적으로 사용하여 BERT보다 성능을 높일 수 있다. 그러나 ELECTRA의 parameter 효율성을 완전히 이해하기 위해서는 추가적인 분석이 필요하다.\nRelated Work Self-Supervised Pre-training for NLP Self-supervised learning은 단어 및 문맥 표현을 학습하는 데 사용된다. BERT는 masked-language modeling 작업을 통해 Transformer를 사전 학습한다. BERT를 확장한 모델들이 있으며, ELECTRA는 BERT와 비교하여 효율적인 사전 학습을 제공한다. 최근에는 BERT를 작은 모델로 축소하는 연구도 진행되고 있다. ELECTRA-Small은 사전 학습 속도에 초점을 맞춰 처음부터 학습되었다.\nGenerative Adversarial Networks GAN은 고품질 가짜 데이터 생성에 효과적이며, 이 논문의 방법과 유사한 방식으로 GAN의 판별자를 후속 작업에 활용할 수 있다. 텍스트 데이터에도 GAN을 적용할 수 있지만, 최신 기법은 standard maximum-likelihood 학습에 비해 아직 성능이 낮다. 생성자는 MaskGAN과 유사하게 삭제된 토큰을 채우는 방식으로 학습된다.\nContrastive Learning contrastive learning은 관찰된 데이터와 가짜 음성 샘플을 구분하는 방법이다. 이 방법은 텍스트, 이미지, 비디오 데이터 등 다양한 모달리티에 적용될 수 있다. ELECTRA는 Noise-Contrastive Estimation (NCE)와 관련이 있는데, 이 방법은 실제와 가짜 데이터를 구별하기 위해 이진 분류기를 학습시킨다.\nWord2Vec은 NLP의 초기 사전 학습 방법 중 하나로 contrastive learning을 사용한ㄴ다. ELECTRA는 CBOW와 Negative Sampling의 대규모 버전으로 볼 수 있다. CBOW는 주변 문맥을 통해 입력 토큰을 예측하고, Negative Sampling은 이진 분류 작업으로 재구성한다. CBOW는 bag-of-vectors 인코더와 단순한 제안 분포를 사용한다.\nConclusion 대체 토큰 감지라는 새로운 자기 지도 학습 작업을 제안하였다. 이 작업은 텍스트 인코더를 학습시켜 입력 토큰과 고품질 부정 샘플을 구별하게 한다. 계산 효율성이 우수하며 하위 작업에서 더 좋은 성능을 보인다.\nReference Paper Github ","date":"2023-12-24T00:00:00Z","permalink":"https://kurtkim.github.io/p/electra/","title":"ELECTRA"},{"content":"Abstract large deep learning 모델은 높은 정확도를 제공하지만, 기존의 데이터 및 모델 병렬화 솔루션은 메모리 적합성에 한계를 보인다. 따라서 Zero Redundancy Optimizer(ZeRO)를 개발하여 메모리를 최적화하고, 학습 속도를 향상시키며, 효율적으로 학습할 수 있는 모델 크기를 증가시켰다. ZeRO는 메모리 중복을 제거하고, 낮은 통신량과 높은 계산 세밀도를 유지하면서 모델 크기를 확장할 수 있는 가능성을 보여준다.\nZeRO를 구현하고 평가했는데, 이는 400개의 GPU에서 super-linear 속도 향상을 통해 100B 이상의 parameter를 가진 대형 모델을 학습시키며, 15 Petaflops의 처리량을 달성하였다. 이는 기존 기술에 비해 8배의 모델 크기 증가와 10배의 성능 향상을 보여준다. 또한, ZeRO는 모델 병렬화 없이 최대 13B parameter의 대형 모델을 학습시킬 수 있다. 마지막으로, 연구자들은 ZeRO를 사용하여 기록적인 정확도를 가진 세계 최대의 언어 모델을 만들었다.\nExtended Introduction 딥러닝 모델이 점점 커지면서 정확도가 크게 향상되고 있다. transformer는 자연어 처리 분야에서 대형 모델의 발전을 이끌었지만, 이런 모델들은 단일 장치의 메모리에 들어갈 수 없어 학습시키는 데 어려움이 있다. 더 많은 장치를 추가하는 것만으로는 이 문제를 해결할 수 없다.\n기본 데이터 병렬화는 장치당 메모리를 줄이지 않아, 큰 모델에 대해 메모리 부족 문제가 발생한다. 파이프라인 병렬화, 모델 병렬화, CPU 오프로딩 등의 기존 해결책들은 각종 효율성과 기능성, 사용성 사이에서 타협을 이루어야 하지만, 이들 모두가 학습의 속도와 규모를 위해 중요하다.\n거대 모델 학습에 가장 유망한 방법인 모델 병렬화(Model Parallelism, MP)는 모델을 수직으로 분할하여 여러 장치에 분배한다. 이 방법은 단일 노드에서 잘 작동하지만, 노드를 넘어서면 효율성이 빠르게 저하된다. 이 논문의 실험에서는, 두 개의 DGX-2 노드에서 40B parameter 모델을 테스트했을 때, V100 GPU당 약 5 Tflops(하드웨어 피크의 5% 미만)의 성능을 보여주었다.\n거대 모델을 더 효율적으로 학습하기 위해, 기존 시스템의 메모리 소비를 분석하고, 모델 상태와 잔여 상태 두 부분으로 분류한다. 모델 상태는 메모리의 대부분을 차지하며, 잔여 상태는 나머지 메모리를 차지한다. 이 두 부분 모두에서 메모리 효율성을 최적화하면서 높은 계산 및 통신 효율성을 얻기 위해 ZeRO를 개발하였다. 이 두 부분은 각각 다른 도전과제에 직면하므로, 각각에 대한 해결책을 개발하고 논의하였다.\nOptimizing Model State Memory 모델 학습 중 메모리의 대부분을 차지하는 모델 상태를 효율적으로 관리하기 위해, ZeRO-DP를 개발하였다. 기존의 DP와 MP 방식의 한계를 극복하고자, ZeRO-DP는 모델 상태를 복제하는 대신 분할하여 메모리 상태 중복을 제거하고, 동시에 계산/통신 효율성을 유지하기 위해 동적 통신 일정을 사용한다. 이를 통해 DP의 계산/통신 효율성과 MP의 메모리 효율성을 모두 달성하려 한다.\nZeRO-DP는 옵티마이저 상태, 그래디언트, 파라미터를 분할하는 세 가지 주요 최적화 단계를 순차적으로 활성화한다.\nOptimizer State Partitioning ($P_os$): 메모리 감소량 4배, DP와 동일한 통신 볼륨 Add Gradient Partitioning ($P_{os+g}$): 메모리 감소량 8배, DP와 동일한 통신 볼륨 Add Parameter Partitioning ($P_{os+g+p}$): 메모리 감소량이 DP 정도 $N_d$와 선형적이다. 예를 들어, 64개의 GPU($N_d = 64$)에 걸쳐 분할하면 메모리 감소량이 64배가 된다. 통신 볼륨은 50% 증가한다.\nZeRO-DP는 메모리 중복을 제거하여 클러스터의 전체 메모리를 활용하게 한다. 세 단계 모두 활성화하면 ZeRO는 1024개의 NVIDIA GPU만으로 1조 개의 파라미터 모델을 학습할 수 있다. 이는 각 GPU가 대략 16GB의 메모리를 사용하게 되며, 이는 대부분의 GPU(예: 32GB 메모리를 가진 GPU)가 감당할 수 있는 범위이다.\nOptimizing Residual State Memory ZeRO-DP가 모델 메모리 효율성을 향상시킨 후에도, 활성화, 임시 버퍼, 사용 불가능한 메모리 조각 등으로 인해 두 번째 메모리 병목 현상이 발생할 수 있다. 이를 해결하기 위해, 각 요소에 의한 잔여 메모리를 최적화하는 ZeRO-R을 개발하였다.\nactivation에 대해, 체크포인팅이 도움이 되지만 큰 모델에는 충분하지 않다는 것을 발견했다. 그래서 ZeRO-R은 activation partitioning을 통해 기존 MP 방법에서 activation 복제를 식별하고 제거함으로써 activation 메모리를 최적화한다. 또한 적절할 때 CPU로 활성화를 오프로드한다. ZeRO-R은 메모리와 계산 효율성 사이의 균형을 위해 임시 버퍼의 적절한 크기를 정의한다. 학습 중에 다른 텐서의 수명 차이로 인해 메모리가 파편화되는 것을 관찰하였다. 파편화로 인한 연속적인 메모리 부족은 충분한 여유 메모리가 있음에도 불구하고 메모리 할당 실패를 일으킬 수 있다. ZeRO-R은 텐서의 다른 수명에 기반하여 메모리를 적극적으로 관리함으로써 메모리 파편화를 방지한다. ZeRO-DP와 ZeRO-R을 결합하여 ZeRO라는 딥러닝 학습용 메모리 최적화 시스템을 구성한다.\nZeRO and MP: ZeRO는 데이터 병렬처리의 메모리 비효율성을 제거하므로, 모델 병렬처리(MP)의 필요성이 줄어든다. ZeRO-DP는 MP와 비교해서 장치당 메모리 사용량을 적어도 같게 줄이거나, 때로는 더 효과적으로 줄일 수 있다. 또한, 스케일링 효율성도 비슷하거나 더 좋다. 데이터 병렬처리는 쉽게 사용할 수 있어 다양한 작업에 적용할 수 있지만, MP는 모델과 시스템 개발자의 추가 작업이 필요하며, 제한된 연산자와 모델만 지원한다.\nZeRO-R과 함께 사용하면, MP는 매우 큰 모델의 활성화 메모리 사용량을 줄일 수 있으며, 활성화 메모리가 문제가 아닌 작은 모델에서는 DP만을 이용한 배치 크기가 너무 클 경우 MP가 이점을 가질 수 있다. 이런 경우, ZeRO를 MP와 결합하여 적절한 배치 크기로 모델을 적용할 수 있다.\nZeRO는 MP와 결합될 수 있으며, 이는 각 장치에서 최대 이론적 메모리 감소를 $N_d \\times N_m$배 달성하는 결과를 가져온다. 이를 통해, 1024개의 GPU 1 trillion 개의 parameter 모델을 효율적으로 운영할 수 있게 된다. 이는 16-방향 모델 병렬처리와 64-방향 데이터 병렬처리를 이용하며, 적당한 배치 크기를 사용한다.\nImplementation \u0026amp; Evaluation ZeRO의 최적화 세트는 1 trillion 개의 parameter를 가진 모델을 고급 하드웨어 클러스터에서 실행할 수 있지만, 계산 능력의 한계와 긴 학습 시간 때문에 실제 적용이 어렵다. 그래서 현재 하드웨어의 계산 능력 범위 내에서 최첨단보다 10배 많은 parameter(약 100B 개의 parameter)를 효율적으로 지원하는 것에 초점을 맞추었다. 이를 위해 ZeRO-DP의 $P_{os+g}$와 ZeRO-R을 결합한 ZeRO의 최적화 하위 집합인 ZeRO-100B를 구현하고 평가하였다.\nModel Size: MP와 결합된 ZeRO-100B는 170B 개의 parameter 모델을 효율적으로 실행할 수 있다. 반면, Megatron만을 사용하는 기존 시스템은 40B 개의 parameter 이상으로 효율적으로 확장할 수 없다. 이는 state-of-the-art 대비 모델 크기가 8배 이상 증가한 것이다.\nSpeed: 향상된 메모리 효율성은 처리량을 높이고 학습 속도를 빠르게 한다. ZeRO는 400개의 Nvidia V100 GPU 클러스터에서 100B 개의 parameter 모델을 GPU당 38TFlops, 총 15Petaflops의 성능으로 실행한다. 이는 동일한 모델 크기에 대해 state-of-the-art 대비 학습 속도를 10배 이상 향상시킨다.\nScalability: 64-400개의 GPU 영역에서 GPU의 수를 두 배로 늘릴 때 성능이 두 배 이상 향상되는 슈퍼 선형 속도 향상을 관찰하였다. 이는 ZeRO-DP의 특성으로, DP 차수를 늘릴수록 모델의 메모리 사용량이 줄어들고 GPU 당 더 큰 배치 크기를 적용할 수 있게 되어 성능을 향상시킨다. 400개를 넘는 GPU 수를 더 늘릴 경우 이런 행동이 계속될 것으로 예상한다.\nDemocratization of Large Model Training: ZeRO-100B는 데이터 과학자들이 모델 리팩토링을 필요로 하는 MP나 PP 없이 최대 13B 개의 parameter로 모델을 학습할 수 있게 한다. 이를 통해 데이터 과학자들은 병렬 처리에 대해 걱정하지 않고 큰 모델로 자유롭게 실험할 수 있다. 반면, 기존 시스템들은 1.4B 개의 parameter 모델에서 메모리가 부족해진다.\nNew SOTA Model: ZeRO는 17B 개의 parameter를 가진 가장 큰 언어 모델인 Turing-NLG를 지원하며, 이는 기록적인 정확도를 보여준다.\nRelated Work Data, Model and Pipeline Parallelism 병렬화는 대규모 모델 학습에 필수적인 전략이다. data parallelism(DP)를 통해 모델은 여러 장치로 확장되며, 모델의 parameter는 각 장치에 복제된다. 각 단계에서 미니 배치는 프로세스 간에 나누어지고, 각 프로세스는 다른 데이터 샘플에서 forward 및 backward propagation를 수행한다. 프로세스 간의 averaged gradient를 사용해 로컬에서 모델이 업데이트된다.\n장치 메모리에 맞지 않는 모델의 경우, model parallelism(MP)과 pipeline parallelism(PP)이 모델을 프로세스 간에 수직 및 수평으로 분할한다.\npipeline parallelism(PP)은 모델을 층간에 수평적으로 분할하고, 마이크로 배치를 이용해 파이프라인 버블을 숨긴다. 하지만 이 방식은 모델 기능의 구현을 어렵게 만들고, 큰 배치 크기와 상당한 메모리를 필요로 한다. 또한, 표준 딥러닝 학습과는 다르며, 학습 수렴에 영향을 미치는 단점이 있습니다. 반면에, ZeRO는 이러한 PP의 제한 없이 같거나 더 나은 메모리 효율성을 제공한다.\nNon-parallelism based approach to reduce memory model parallelism(MP)과 pipeline parallelism(PP) 외에도, 딥러닝 학습의 메모리 오버헤드를 줄이는데 목표를 두고 있는 여러 연구가 있다.\nReducing Activation Memory 활성화의 메모리 사용량을 줄이기 위한 여러 방법들이 있으며, 이에는 압축, 활성화 체크포인팅, 라이브 분석 등이 포함된다. 이러한 방법들은 ZeRO와 서로 보완적으로 작동하며, 특히 ZeRO-R의 활성화 메모리 감소는 활성화 체크포인팅과 병렬로 진행된다.\nCPU Oﬄoad 컴퓨팅 노드의 heterogeneous를 활용해 모델 상태를 CPU 메모리로 오프로드하는 방식이 있다. 학습 시간의 절반 가량이 GPU-CPU-GPU 전송에 소요되지만, ZeRO는 이와 달리 PCI-E로 인한 제한 때문에 CPU 메모리에 모델 상태를 저장하지 않고도 메모리 사용을 크게 줄인다. 드물게, 성능 향상을 위해 ZeRO-R는 매우 큰 모델의 활성화 체크포인트만 오프로드할 수 있다.\nMemory Eﬃcient Optimizer 모델 parameter와 gradient의 대략적인 통계를 유지하면서 adaptive optimization 방법의 메모리 사용량을 줄이는 방법들이 있다. 이는 모델 수렴에 영향을 미칠 수 있다. 그러나 ZeRO는 이와 별개로, 모델 최적화 방법이나 모델 수렴에 영향을 주지 않으면서, 최적화 상태와 장치별 gradient의 메모리 사용량을 효과적으로 줄인다.\nTraining Optimizers adaptive optimization 방법들은 큰 모델의 효과적인 학습을 위해 성능과 정확도를 최적화하는데 중요하다. 각 모델 parameter와 gradient에 대한 세밀한 통계를 유지하면서 메모리 사용량이 상당히 증가하는데, ZeRO는 이러한 최적화 도구의 메모리 사용량을 크게 줄여 소형 장치 메모리를 가진 하드웨어에서도 큰 모델 학습이 가능하게 한다. 이는 더 복잡하고 메모리를 많이 사용하는 최적화 도구의 개발 및 사용을 가능하게 한다.\nWhere Did All the Memory Go? 예를 들어, 1.5B 매개변수의 GPT-2 모델은 16비트 정밀도에서 가중치에 3GB의 메모리를 필요로 한다. 그러나 이는 32GB 메모리를 가진 단일 GPU에서 Tensorflow나 PyTorch를 사용하여 학습할 수 없다. 모델 학습 중에는 대부분의 메모리가 pptimizer states, gradient, parameter로 구성된 모델 상태에 의해 소비되며, 나머지 메모리는 activations, temporary buffer, fragmented memory에 의해 소비된다.\nModel States: Optimizer States, Gradients and Parameters 학습 중에는 대부분의 메모리가 모델 상태에 소비되며, 그 중에서도 Adam과 같은 최적화 도구가 가장 많은 메모리를 차지한다. Adam은 업데이트를 계산하기 위해 gradient의 시간 평균 모멘텀과 분산을 저장해야 하기 때문이다. 따라서, 모델을 훈련시키려면 이 두 가지, 그리고 gradient와 가중치 자체를 저장할 충분한 메모리가 필요하다. 이 세 가지 요소 중에서도, 최적화 상태가 특히 많은 메모리를 차지하다.\nMixed-Precision Training 현재 NVIDIA GPU에서 큰 모델을 학습시키는 state-of-the-art 방법은 mixed-precision(fp16/32) 학습을 사용하는 것이다. 이 방법은 parameter와 activation을 fp16으로 저장하고, GPU의 고처리량 텐서 코어 유닛을 활용합니다. forward 및 backward propagation는 fp16 가중치와 활성화를 사용하여 수행되지만, backward propagation 끝단에서의 업데이트 계산과 적용을 위해, mixed-precision 최적화 도구는 parameter와 다른 최적화 상태들의 fp32 복사본을 유지한다.\n$\\Psi$ parameter를 가진 모델의 mixed-precision 학습에서는, parameter와 gradient의 fp16 복사본, 그리고 최적화 상태인 parameter, 모멘텀, 분산의 fp32 복사본을 저장할 충분한 메모리가 필요하다. 이를 총합하면, $16 \\Psi$ 바이트의 메모리 요구사항이 발생한다. 예를 들어, 1.5 B parameter를 가진 GPT-2 모델의 경우, 최소 24GB의 메모리가 요구되며, 이는 단독으로 fp16 parameter를 저장하는 데 필요한 3GB의 메모리보다 훨씬 많다.\nResidual Memory Consumption Activations 학습 중에 활성화는 상당한 메모리를 차지할 수 있다. 예를 들어, 1.5B parameter GPT-2 모델은 약 60GB의 메모리를 필요로 한ㄴ다. 그러나 활성화 체크포인팅을 사용하면 활성화 메모리를 전체 활성화의 제곱근 정도로 줄일 수 있다. 이 방법을 사용하면 이 모델의 활성화 메모리 소비는 대략 8GB로 줄어들게 된다.\n활성화 체크포인팅을 사용하더라도, 큰 모델들은 활성화 메모리가 매우 커질 수 있다. 예를 들어, 100B 개의 parameter를 가진 GPT와 같은 모델은 배치 크기 32일 때 약 60GB의 메모리가 필요하다.\nTemporary buffers 큰 모델에서는 중간 결과를 저장하기 위한 메모리가 상당한 양을 차지한다. gradient all-reduce나 gradient norm 계산과 같은 연산은 처리량 향상을 위해 모든 gradient를 하나의 플랫 버퍼로 병합한다. 그러나 이 병합된 버퍼는 연산에 따라 fp32 텐서가 될 수 있어, 큰 모델에서는 이 임시 버퍼 크기가 중요하다. 예를 들어, 1.5B parameter를 가진 모델에서는 플랫한 fp32 버퍼가 6GB의 메모리를 필요로 한다.\nMemory Fragmentation: 학습 중 실제 메모리 사용량 외에도, 메모리 단편화로 인해 충분한 메모리가 있음에도 불구하고 메모리 부족 상황이 발생할 수 있다. 연속적인 메모리가 부족하면, 요청된 메모리보다 전체 사용 가능 메모리가 더 크더라도 메모리 요청이 실패할 수 있다. 큰 모델을 학습할 때는 이러한 메모리 단편화가 상당히 발생하며, 극단적인 경우에는 30% 이상의 메모리가 남아있음에도 메모리 부족 문제가 발생할 수 있다.\nZeRO: Insights and Overview ZeRO는 모델 상태의 메모리 사용량을 줄이는 ZeRO-DP와 잔여 메모리 소비를 줄이는 ZeRO-R, 두 가지 최적화 세트를 가지고 있다. 이는 ZeRO가 효율성을 유지하면서 메모리 사용량을 줄이는 데 도움이 된다. 효율성이 핵심인 만큼, 이 제약 없이는 모든 parameter 상태를 CPU 메모리로 이동하거나, MP 정도를 임의로 늘리는 등의 간단한 방법으로 메모리 사용량을 줄일 수 있다.\nInsights and Overview: ZeRO-DP ZeRO에서 구동되는 DP는 세 가지 key insight에 기반을 두고 있다:\na) MP는 계산의 세분성을 줄이고 통신 오버헤드를 증가시키므로, DP는 MP보다 확장 효율성이 더 높다. 특정 지점을 넘어서면 GPU의 효율성이 줄어들고, 통신 오버헤드가 증가하여 GPU 간의 확장성이 제한된다. 반면에, DP는 더 높은 계산 세분성과 더 낮은 통신 볼륨을 가지므로 훨씬 높은 효율성을 제공한다.\nb) DP는 모델 상태가 모든 데이터 병렬 처리에서 중복으로 저장되므로 메모리 효율성이 떨어진다. 반면에, MP는 모델 상태를 분할하여 메모리 효율성을 얻는다.\nc) DP와 MP 모두 전체 학습 과정 동안 필요한 모든 모델 상태를 유지하지만, 항상 모든 것이 필요한 것은 아니다. 예를 들어, 각 레이어에 해당하는 매개변수는 레이어의 전파와 역전파 동안에만 필요하다.\nZeRO-DP는 DP의 학습 효율성을 유지하면서 MP의 메모리 효율성을 달성한다. 모델 상태를 복제하는 대신 분할하고, 통신 볼륨을 최소화하면서 모델 상태의 시간적인 특성을 활용하는 동적 통신 일정을 사용한다. 이를 통해 ZeRO-DP는 모델의 기기당 메모리 사용량을 선형적으로 줄이면서도 통신 볼륨을 기본 DP 수준에 유지하며 효율성을 유지한다.\nInsights and Overview: ZeRO-R Reducing Activation Memory a) MP는 모델 상태를 분할하지만 활성화 메모리의 복제가 종종 필요하다. 예를 들어, 선형 레이어의 parameter를 분할하여 병렬 계산을 할 경우, 각 GPU는 전체 활성화를 필요로 한다.\nb) GPT-2 또는 그보다 큰 모델들은 산술 강도가 매우 크며(≥ 10K), 이는 숨겨진 차원과 선형적으로 증가한다. 이로 인해 대역폭이 낮아도 활성화 체크포인트의 데이터 이동 비용을 숨길 수 있다.\nZeRO는 GPU 간에 활성화 체크포인트를 분할하여 MP의 메모리 중복을 제거하고, 필요에 따라 그것들을 allgather를 이용해 재구성한다. 이로 인해 활성화 메모리 사용량은 MP 정도에 비례하여 감소하며, 매우 큰 모델에서는 산술 강도가 큰 덕분에 활성화 파티션을 CPU 메모리로 이동시키면서도 좋은 효율성을 유지할 수 있다.\nManaging Temporary buﬀers ZeRO-R은 모델 크기가 증가함에 따라 임시 버퍼가 급증하는 것을 피하기 위해 일정한 크기의 버퍼를 사용하면서도 충분히 크게 만들어 효율성을 유지한다.\nManaging fragmented Memory memory fragmentation는 단기와 장기 메모리 사이의 교차 때문에 발생한다. ZeRO는 이 통찰을 바탕으로, 활성화 체크포인트와 gradient를 미리 할당된 연속 메모리 버퍼로 이동시키는 실시간 메모리 defragmentation을 수행한다. 이로 인해 메모리 사용 가능성이 증가하고, 메모리 할당자가 연속적인 무료 메모리를 찾는 시간이 줄어들어 효율성이 향상된다.\nDeep Dive into ZeRO-DP 기존 DP 방식은 각 장치에서 모델 상태를 복제하여 메모리 오버헤드를 발생시키지만, ZeRO-DP는 이런 메모리 중복을 제거하기 위해 데이터 병렬 프로세스간에 optimizer state, gradient, parameter를 분할한다. 이들은 ZeRO-DP의 세 가지 최적화 단계인 $P_{os}$, $P_g$, $P_p$로 참조된다.\n$P_{os}$: Optimizer State Partitioning DP 정도가 $N_d$인 경우, 최적화 상태를 $N_d$개의 동일한 파티션으로 나누어, $i$번째 데이터 병렬 프로세스가 $i$번째 파티션에 해당하는 최적화 상태만 업데이트하도록 한다. 결과적으로, 각 데이터 병렬 프로세스는 전체 최적화 상태의 $N_d$만을 저장하고 업데이트하며 parameter의 $N_d$만 업데이트한다. 각 학습 단계의 끝에서는 모든 데이터 병렬 프로세스에서 완전히 업데이트된 parameter를 얻기 위해 all-gather를 수행한다.\nMemory Savings: 최적화 상태 파티션 후의 메모리 소비는 $4 \\Psi + K \\Psi$에서 $4 \\Psi + {{K \\Psi}\\over{N_d}}$ 로 줄어든다. 예를 들어, 7.5B parameter 모델은 64-방향 DP를 사용할 때 31.4GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 또한, $N_d$가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 4배 감소한다.\n$P_g$: Gradient Partitioning 각 데이터 병렬 프로세스는 자신에 해당하는 parameter 파티션만 업데이트하므로, 해당 parameter에 대한 감소된 gradient만 필요하다. 역전파 동안 각 계층의 각 gradient가 사용 가능해지면, 해당 parameter를 업데이트하기 위한 데이터 병렬 프로세스에서만 그것들을 줄이고, gradient를 더 이상 필요하지 않으면 메모리를 해제한다. 이로 인해 gradient를 저장하는 데 필요한 메모리 사용량이 $2\\Psi$ 바이트에서 $N_d$로 줄어든다.\nReduce-Scatter 연산을 통해 다른 parameter에 대응하는 gradient들이 다른 프로세스로 줄어든다. 효율성을 높이기 위해 특정 파티션에 대응하는 모든 gradient를 버킷화하여 한 번에 처리하는 버킷화 전략을 사용한다. 이는 NVIDIA의 AMP 최적화기가 통신과 계산을 겹치게 하기 위해 gradient 계산을 버킷화하는 방식과 유사하다. 메모리 사용량을 줄이고 계산과 통신을 겹치게 하기 위해, 파티션 경계에서 all-reduce 대신 감소를 수행한다.\nMemory Savings: gradient와 optimizer state 중복을 제거하여 메모리 사용량을 $2 \\Psi + {{14 \\Psi}\\over{N_d}} \\approx 2 \\Psi$로 줄일 수 있다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 P_{os+g}로만 16.6 GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. N_d가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 8배 감소한다.\n$P_p$: Parameter Partitioning 각 프로세스는 자신의 파티션에 해당하는 parameter만 저장하며, 필요한 경우 다른 parameter는 데이터 병렬 프로세스로부터 브로드캐스트를 통해 받아온다. 이 방법은 통신량을 1.5배로 증가시키지만, $N_d$에 비례해 메모리 사용량을 줄일 수 있다.\nMemory Savings: parameter 분할을 통해, $Psi$ parameter 모델의 메모리 사용량은 $16 \\Psi$에서 ${{16 \\Psi}\\over{N_d}}$로 줄어든다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 모델 상태 메모리 1.9 GB를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 이는 ZeRO가 모델 상태를 공유할 충분한 장치가 있다면 어떠한 크기의 모델에도 DP를 적용할 수 있다는 것을 의미한다.\nImplication on Model Size 데이터 병렬 프로세스의 모델 상태에 대한 메모리 사용량을 줄이기 위한 파티셔닝의 세 단계 $P_{os}$, $P_{os+g}$, $P_{os+g+p}$는 각각 메모리 사용량을 최대 4배, 8배, 그리고 $N_d$로 줄인다. ZeRO 최적화를 사용하면, $N_d = 64$일 때는 최대 128B parameter의 모델을, $N_d = 1024$일 때는 최대 1 trillion parameter의 모델을 학습시킬 수 있다. ZeRO를 사용하지 않으면, 최대 1.5B parameter의 모델만 학습시킬 수 있다.\nDeep Dive into ZeRO-R $P_a$: Partitioned Activation Checkpointing model parallel(MP)은 설계상 활성화의 복제를 요구하며, 이로 인해 GPU 전체에 활성화의 중복 복사본이 생긴다. ZeRO는 이 중복성을 제거하기 위해 활성화를 분할하고, 계산에 사용되기 직전에만 복제 형태로 재구성한다. 이를 $P_a$라고 하며, 활성화 체크포인팅과 함께 작동하여, 복제 복사본 대신 분할된 활성화 체크포인트를 저장한다. 매우 큰 모델과 제한된 디바이스 메모리의 경우, 이 분할된 체크포인트는 CPU로 오프로드될 수 있어, 활성화 메모리 오버헤드를 거의 없앨 수 있다. 이를 $P_{a+cpu}$라고 한다.\nMemory Saving ZeRO는 파티셔닝된 활성화 체크포인팅을 사용하여 활성화 메모리 사용량을 model parallel(MP) 차수에 비례하여 줄인다. 예를 들어, 100B 개의 parameter를 가진 모델을 학습시키는 경우, 각 transformer layer마다 활성화를 체크포인트하면 GPU 당 약 33GB의 메모리가 필요하다. 하지만 ZeRO의 $P_a$를 사용하면, 이 메모리 요구량을 GPU 당 약 2GB로 줄일 수 있고, 이 2GB는 CPU로 오프로드되어 활성화에 대한 메모리 사용량을 거의 제로로 만든다.\n$C_B$: Constant Size Buﬀers ZeRO는 메모리와 계산 효율성 사이의 균형을 위해 임시 데이터 버퍼의 크기를 조정한다. 학습 중에는 입력 크기가 커질수록 연산의 효율성이 향상되는 경우가 많다. 그러나, 모든 parameter수를 결합한 버퍼의 메모리 오버헤드는 모델 크기에 비례하기 때문에 문제가 될 수 있다. 이 문제를 해결하기 위해, 모델 크기가 큰 경우에는 성능 효율적인 고정 크기의 결합 버퍼를 사용한다. 이렇게 하면 버퍼 크기가 모델 크기에 의존하지 않게 되어, 메모리 사용량을 줄이면서도 계산 효율성을 유지할 수 있다.\n$M_D$: Memory Defragmentation 모델 학습 중에는 활성화 체크포인팅과 gradient 계산으로 인해 메모리 단편화가 발생한다. forward propagation 동안에는 선택적으로 저장되는 활성화와 대부분 버려지는 활성화 사이에 메모리가 교차되어 단편화가 발생하며, back propagation 동안에도 장기 메모리인 parameter gradient와 단기 메모리인 activation gradient 및 gradient 계산에 필요한 다른 버퍼들 사이에 메모리가 교차되어 단편화가 발생한다.\n메모리가 충분할 때는 메모리 단편화가 크게 문제가 되지 않지만, 메모리가 제한된 상태에서 큰 모델을 학습할 때는 두 가지 문제가 발생한다. 첫째, 충분한 메모리가 있음에도 연속적인 메모리 부족으로 인해 메모리 부족 오류(Out of Memory, OOM)가 발생하고, 둘째, 메모리 할당기가 연속적인 메모리 조각을 찾는데 많은 시간을 소비하여 효율성이 저하된다.\nZeRO는 활성화 체크포인트와 gradient를 위해 미리 연속적인 메모리 덩어리를 할당하고, 이들이 생성될 때 미리 할당된 메모리로 복사하여 메모리 단편화를 실시간으로 제거한다. $M_D$(Memory Defragmentation)는 ZeRO가 더 큰 모델을 더 큰 배치 크기로 학습할 수 있게 만들 뿐만 아니라, 제한된 메모리로 학습할 때 효율성을 향상시킨다.\nCommunication Analysis of ZeRO-DP ZeRO는 메모리 중복성을 제거하여 모델 크기를 증가시키는데, 이로 인해 메모리 효율성을 위해 통신 볼륨을 교환하고 있는지 의문이 생긴다. 즉, 기본 데이터 병렬화 방법에 비해 ZeRO가 향상된 데이터 병렬화 방법의 통신 볼륨은 어느 정도일까? 이에 대한 답은 두 부분이다. 첫째, ZeRO-DP는 메모리를 최대 8배 줄이면서 추가 통신을 발생시키지 않는다. 둘째, ZeRO-DP는 메모리 사용량을 추가로 줄이면서 최대 1.5배의 통신을 발생시킨다. 이 분석은 표준 데이터 병렬화의 통신 볼륨에 대한 간단한 개요로 시작한다.\nData Parallel Communication Volume 데이터 병렬 학습에서는 모든 데이터 병렬 프로세스의 gradient가 backward propagation이 끝날 때 평균화된다. 이 평균화는 all-reduce 통신을 통해 이루어지며, 모델 크기가 큰 경우, 이 통신은 통신 대역폭에 의해 제한된다. 따라서 분석은 각 데이터 병렬 프로세스로부터 보내고 받는 총 통신 볼륨에 초점을 맞춘다.\nstate-of-art의 all-reduce 구현은 두 단계로 이루어진다. 첫번째 단계는 reduce-scatter 연산으로, 다른 프로세스에서 데이터의 다른 부분을 축소하고, 다음 단계는 all-gather 연산으로, 각 프로세스가 모든 프로세스에서 축소된 데이터를 수집한다. 이 두 단계의 결과는 all-reduce이다. 각 단계는 파이프라인 방식으로 구현되며, 이로 인해 데이터 이동이 발생한다. 따라서 표준 데이터 병렬화는 각 학습 단계마다 $2 \\Psi$의 데이터 이동을 발생시킨다.\nZeRO-DP Communication Volume Communication Volume with $P_{os+g}$ gradient 분할을 사용하는 ZeRO는 각 프로세스가 해당하는 parameter 분할을 업데이트하기 위해 필요한 gradient 부분만을 저장한다. gradient에 대해 scatter-reduce 연산을 수행하고, 모든 데이터 병렬 프로세스에서 업데이트된 parameter를 수집하기 위해 all-gather를 수행한다. 이 두 과정은 각각 통신 볼륨 $\\Psi$를 발생시키므로, 학습 단계당 총 통신 볼륨은 $\\Psi + \\Psi = 2 \\Psi$로, 기본 데이터 병렬화와 동일하다.\nCommunication Volume with $P_{os+g+p}$ parameter 분할 후에 각 데이터 병렬 프로세스는 자신이 업데이트하는 parameter만을 저장한다. 이로 인해 forward propagation 동안 다른 모든 분할의 parameter를 받아야 하지만, 파이프라인 방식을 통해 메모리 오버헤드를 피할 수 있다. 특정 분할에 대한 forward propagation를 계산하기 전에, 해당 분할의 가중치를 모든 데이터 병렬 프로세스에게 브로드캐스트하고, forward propagation가 완료되면 parameter를 버린다. 이로 인해 총 통신 볼륨은 $\\Psi$이다. 그러나, 이 all-gather 작업은 backward propagation 동안 역순으로 다시 수행되어야 한다는 점에 주의해야 한다.\n총 통신 볼륨은 all-gather와 gradient의 reduce-scatter에 의해 발생하는 통신 볼륨의 합으로, 이는 기본값에 비해 1.5배인 $3 \\Psi$이다. gradient와 parameter의 분할은 모든 상태가 항상 필요하지 않다는 점을 이용하여, 상태를 신중하게 통신함으로써 메모리를 최적화한다.\nCommunication Analysis of ZeRO-R ZeRO-R의 분할된 활성화 체크포인팅($P_a$)의 통신 볼륨은 기본 MP의 10분의 1 미만 증가하며, 이는 $P_a$의 통신 오버헤드를 분석하여 더 큰 배치 크기를 가능하게 하고 DP 통신을 줄여 효율성을 향상시키는 시나리오를 식별하는 데 사용된다. 이러한 분석은 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정하는 데 활용된다.\n활성화 체크포인트 분할의 통신 볼륨 트레이드오프는 모델 크기, 체크포인트 전략, 그리고 MP 전략에 따라 다르다. 이에 대한 구체적인 이해를 위해, 우리는 최신 MP 방식으로 구현된 transformer 기반 모델인 Megatron-LM을 사용하여 분석을 수행하였다.\n활성화 체크포인팅이 있는 Megatron-LM에서, 각 transformer 블록은 forward propagation, forward re-computation, backward propagation 각각에서 두 번씩 all-reduce 연산을 수행한다. 이는 $\\text{batch} \\times \\text{seq length} \\times \\text{hidden dim}$ 차원의 크기를 가지다. 따라서 블록 당 총 통신 볼륨은 $12 \\times \\text{seq length} \\times \\text{hidden dim}$이 된다.\nZeRO-R이 활성화 체크포인트를 분할할 때, back-propagation의 forward recomputation 전에 추가적인 all-gather 연산이 필요하다. 각 transformer 블록의 입력 활성화를 체크포인트로 설정하므로, transformer 블록 당 하나의 all-gather가 필요하다. 이로 인한 통신 오버헤드 $P_a$는 $\\text{seq length} \\times \\text{hidden dim}$이고, 따라서 $P_a$의 총 통신 오버헤드는 모델 병렬화의 원래 통신 볼륨의 10% 미만이다.\nMP와 DP를 함께 사용할 때, $P_a$는 모델 병렬 통신 볼륨을 10% 증가시키는 대신 데이터 병렬 통신 볼륨을 크게 줄일 수 있다. 이는 데이터 병렬 통신이 성능의 병목이 될 때 효율성을 크게 향상시키는데 도움이 된다. 또한, $P_a$는 활성화 메모리 사용량을 줄이고 배치 크기를 비례적으로 증가시키므로, 큰 모델의 경우 배치 크기를 최대 16배까지 증가시킬 수 있다. 이로 인해, 데이터 병렬 통신 볼륨이 크게 감소할 수 있다.\n$P_{a+cpu}$가 적용되면, CPU로 오프로드된 분할된 활성화 체크포인트는 활성화 메모리 요구량을 거의 0으로 줄이면서, CPU 메모리로의 데이터 이동이 2배 증가한다. 배치 크기가 작아서 DP 통신 볼륨이 병목이 되는 경우에는, CPU 데이터 전송 오버헤드가 DP 통신 볼륨 오버헤드보다 작다면 $P_{a+cpu}$가 배치 크기를 늘려 효율성을 향상시킬 수 있다.\n모델과 하드웨어 특성을 고려하여, 위의 분석을 활용하여 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정한다.\nStep Towards 1 Trillion Parameters 현재 가장 큰 모델들은 이미 학습시키는 데 도전적인 100B 개의 parameter를 가지고 있다. 1 trillion 개의 parameter에 이르는 것은 불가피하지만, 그 과정은 많은 도전과 혁신을 필요로 할 것이다. ZeRO는 이러한 도전 중 하나인, 현재 하드웨어에서 대규모 모델을 효과적으로 학습시키는 능력을 개선하는 데 중점을 두고 있다.\nA Leap from State-of-Art state-of-art 프레임워크인 Megatron은 DGX-2 시스템에서 160 - 20B 개의 parameter 모델을 효율적으로 학습시킬 수 있다. 하지만, 여러 DGX 노드 간의 모델 병렬화를 시도할 경우, 노드 간 대역폭 제한으로 효율성이 크게 감소한다.\nZeRO는 효율적으로 실행 가능한 모델 크기를 크게 늘린다. 노드 경계를 넘어 세분화된 모델 병렬화가 필요하지 않은 현재 하드웨어에서 더 큰 모델을 실행할 수 있게 한다. 모든 최적화가 적용된 ZeRO는, DP만을 이용해 1024개의 GPU에서 1 trillion 개 이상의 parameter를 처리할 수 있다. 또한, 모델 병렬화와 결합하면, 16-방향 모델 병렬화와 노드 간 64-방향 데이터 병렬화를 이용하여 1 trillion 개 이상의 parameter를 처리할 수 있다.\nCompute Power Gap 허용 가능한 시간 범위 내에서 1 trillion 개의 parameter 모델을 처음부터 끝까지 학습시키는 것은 여전히 상당한 양의 컴퓨팅 파워를 필요로 할 수 있으며, 이는 현재의 AI 클러스터에서는 부족하다.\nBert-Large 모델은 1024 GPU DGX-2H 클러스터에서 67분 만에 학습될 수 있지만, 1 trillion 개의 parameter를 가진 모델은 데이터 샘플당 Bert-Large보다 3000배 더 많은 계산을 필요로 한다. 동일한 하드웨어와 계산 효율성을 가정하면, 이런 크기의 모델 학습은 140일이 걸리며, 실제로는 데이터 샘플과 시퀀스 길이 증가로 인해 1년 이상이 소요될 것이다. 이를 합리적인 시간에 학습시키려면 exa-ﬂop 시스템이 필요하며, 이런 계산 능력이 가능해질 때, ZeRO는 1T 모델을 효율적으로 실행하는 시스템 기술을 제공할 것이다.\nImplementation and Evaluation ∼100B 개의 parameter를 가진 모델들의 효율적인 학습에 초점을 맞추고 있다. 이런 모델들은 현재 가장 큰 모델보다 크지만, 현재의 하드웨어에서 합리적인 시간 안에 학습될 수 있다. 이 목표를 달성하기 위해 ZeRO의 일부 최적화를 구현하고 평가하였다. 이를 ZeRO-100B라 부르며, 이를 통해 최대 170B 개의 parameter를 가진 모델을 효율적으로 학습할 수 있음을 확인하였다. 이는 기존 state-of-art의 기술보다 8배 크고, 최대 10배 빠르며, 사용성이 향상되었다. ZeRO-100B는 세계에서 가장 큰 모델인 Turing-NLG를 지원한다.\nImplementation and Methodology Implementation PyTorch에서 ZeRO-100B를 구현하였고, 이는 모든 최적화 세트를 포함하며, 어떤 모델과도 호환되는 인터페이스를 제공한다. 사용자는 이 인터페이스를 이용해신의 모델을 감싸서 ZeRO의 DP를 활용할 수 있고, 모델 수정은 필요하지 않다. 또한, ZeRO의 DP는 Megatron-LM을 포함한 어떤 형태의 MP와도 결합할 수 있다.\nHardware 800 Gbps의 노드 간 통신 대역폭을 가진 400개의 V100 GPU (25개의 DGX-2 노드) 클러스터에서 실험을 수행하였다.\nBaseline MP 없는 실험에는 torch의 distributed data parallel(DDP)을, MP가 있는 실험에는 최첨단 기술인 Megatron-LM을 사용하였다. 이는 NVIDIA의 오픈소스 버전으로, 최근 결과는 32개의 DGX-2 노드 (총 512개의 32GB V100 GPU)를 사용하여 160B 개의 parameter 모델까지 확장 가능함을 보여준다.\nZeRO MP가 없는 실험에서는 ZeRO-100B의 ZeRO-powered DP 구현을 사용한다. MP가 있는 실험에서는 ZeRO-powered DP를 Megatron-LM의 MP와 결합한다.\nModel Conﬁgurations 모델들은 GPT-2와 같은 transformer 기반 모델이며, parameter 수를 다르게 하기 위해 은닉 차원과 층의 수를 조절하였다.\nSpeed and Model Size ZeRO-100B는 400개의 GPU에서 최대 170B 개의 parameter를 가진 모델을 효율적으로 실행하며, 이는 Megatron-LM보다 8배 이상 크다. ZeRO-100B는 8B에서 100B 개의 parameter를 가진 모델에 대해 평균적으로 15 PetaFlops의 처리량을 달성하였다. 반면, 기본 MP 성능은 모델 크기 증가에 따라 빠르게 저하되지만, ZeRO-100B는 기준선에 비해 최대 10배의 속도 향상을 보여준다.\nZeRO-100B의 경우, 100B을 넘어서는 성능의 약간의 감소는 더 큰 배치 크기를 실행하기 위한 충분한 메모리 부족 때문이다. GPU의 수를 늘림에 따라 ZeRO-100B의 초선형 속도 향상으로 인해 성능이 향상될 것으로 예상한다.\nSuper-Linear Scalability ZeRO-100B는 매우 큰 모델 크기에 대해 초선형 확장성을 보여주며, 64개에서 400개의 GPU로 확장될 때 이 트렌드가 계속될 것으로 예상한다. $P_{os+g}$는 DP 정도의 증가에 따라 ZeRO-100B의 GPU당 메모리 사용량을 줄여, 처리량을 향상시킨다.\nDemocratizing Large Model Training 많은 데이터 과학자들에게 큰 모델 학습의 장벽인 MP와 PP 사용 없이, ZeRO는 모델에 변경 없이 간단한 DP처럼 사용하면서 모델 크기와 속도를 크게 향상시킨다. ZeRO-100B는 128개의 GPU에서 MP 없이 최대 13B 개의 parameter를 가진 모델을 학습시킬 수 있으며, 이는 평균적으로 GPU 당 40 TFlops 이상의 처리량을 달성한다. 반면, ZeRO 없이는 DP만으로 학습 가능한 가장 큰 모델은 1.4B 개의 parameter를 가지며, 처리량은 20 TFlops 미만이다. 게다가, MP의 통신 오버헤드 없이 이런 모델들은 NVLINK이나 NVSwitch가 필요하지 않은 하위 계산 노드에서도 학습될 수 있다.\nMemory and Performance Analysis 최대 모델 크기, 메모리 사용량, 성능에 대한 다양한 최적화의 이점과 영향을 살펴본다.\nMaximum Model Size C1 대비 C2로 학습시 모델 크기는 40B에서 60B으로 증가하며, 이는 활성화 메모리를 16배 줄이는 $P_a$ 사용 때문이다. C4를 사용하여 140B로 늘리는 것은 $P_{os+g}$를 활성화함으로써 모델 상태의 메모리 요구량을 절반으로 줄이기 때문이고, C5를 사용하여 150B로 증가하는 것은 활성화 체크포인트를 CPU 메모리로 오프로딩하여 메모리를 더욱 줄이기 때문이다.\nMax Cached Memory C1에서 C2로 넘어갈 때 캐시 메모리 크기의 감소는 예상된 결과이다. C2와 C3 사이의 메모리 사용량 차이는 활성화 메모리와 모델 상태의 크기에 따라 달라질 수 있다. 특히, 100B 개 모델에서는 활성화 메모리가 훨씬 크므로 C4에서 C5로 넘어갈 때 캐시 메모리 감소가 눈에 띈다. 이러한 특성으로 인해 $P {a+cpu}$는 매우 큰 모델에서 더 큰 배치 크기를 적용하는 데 중요한 도구가 된다. 또한, 170B 개 모델이 메모리 부족 없이 실행되기 위해 $P{a+cpu}$가 필요하다는 것을 보여준다.\nMax Achievable Performance 메모리 사용량 감소가 성능 향상과 연결되어 있으며, 메모리 사용량이 줄어들면 배치 크기를 늘려 성능을 향상시킬 수 있다. 그러나 60B 개 parameter 모델에서 C4와 C5 사이에서는 성능이 떨어진다. 이는 C5가 CPU와의 데이터 이동을 초래하여 성능을 저하시키기 때문이다. 하지만 모델이 너무 크거나, C5 없이는 작동이 불가능한 경우등 예외적인 상황에서는 C5가 필요하다. 학습 중에는 이러한 이점이 있는 경우에만 $P_{a+cpu}$가 활성화된다.\nTuring-NLG, the SOTA language model with 17B parameters 2020년 5월 12일 기준으로, Turing-NLG는 17B 개의 parameter를 가진 세계 최대의 모델로, Webtext-103의 perplexity 10.21로 언어 모델의 state-of-art를 달성하였다. TuringNLG는 ZeRO-100B를 사용하여 학습되었고, 이 모델은 GPU당 41.4 TFlops의 처리량을 달성하였다.\nConcluding Remarks 고성능 컴퓨팅과 시스템 관점에서 보면, ZeRO는 대형 모델 학습 분야에서 혁명적 변화를 일으킬 것으로 보인다. ZeRO-100B 구현은 모델 크기를 8배, 처리량은 10배 이상 향상시키며, 현대 GPU 클러스터에서 초선형 속도 향상을 달성하고 세계에서 가장 큰 모델을 학습시킬 수 있다. 하지만 이는 ZeRO의 전체 잠재력을 보여주는 것이 아니다. ZeRO는 미래의 trillion parameter 모델 학습을 가능하게 하는 더 큰 모델 크기 증가를 제공할 수 있다.\nZeRO에 대한 가장 큰 낙관적인 점은 데이터 과학자에게 어떠한 장애물도 없다는 것이다. 기존의 MP와 PP 접근법과 달리, 모델 리팩토링이 필요 없고 표준 DP만큼 쉽게 사용할 수 있어, 대규모 모델 학습에 대한 미래의 연구에서 중요한 역할을 할 것으로 보인다.\nReference Paper Github ","date":"2023-12-20T00:00:00Z","permalink":"https://kurtkim.github.io/p/zero/","title":"ZeRO"},{"content":"Abstract 전이 학습은 데이터가 풍부한 작업에서 먼저 모델을 사전 훈련시킨 후, 이를 downstream task에 미세 조정하는 방식으로, 자연어 처리(NLP)에서 중요한 기법이다. 이 논문에서는 모든 텍스트 기반 언어 문제를 텍스트-텍스트 형식으로 변환하는 통합 프레임워크를 통해 NLP를 위한 전이 학습 기법을 탐색한다. 이 연구에서는 사전 훈련 목표, 아키텍처, 라벨이 없는 데이터셋, 전이 접근법 등 다양한 요소를 비교 분석하여, 요약, 질문 응답, 텍스트 분류 등 여러 분야에서 state-of-the-art를 달성하였다. 또한, 이러한 연구를 통한 데이터셋, 사전 훈련된 모델, 코드를 공개하여, NLP를 위한 전이 학습 연구를 더욱 촉진시키고자 한다.\nIntroduction 자연어 처리(NLP) 머신러닝 모델 훈련은 모델이 텍스트를 이해하고 적절하게 처리하는 능력을 개발하는 것을 목표로 하며, 이는 단어의 철자와 의미부터 고수준의 지식까지 다양한 요소를 포함한다. 최근에는 데이터가 풍부한 작업에서 모델을 사전 학습하는 것이 일반적이며, 이를 통해 모델은 다양한 작업에 활용할 수 있는 일반적인 능력과 지식을 개발하게 된다. 특히 NLP에서는 레이블이 없는 대량의 텍스트 데이터를 이용한 비지도 학습으로 사전 학습이 진행되며, 이 방법은 주요 NLP 벤치마크에서 state-of-the-art를 달성하는 데 사용되었다.\n자연어 처리(NLP)에서의 전이 학습에 대한 최근 연구는 다양한 사전 학습 목표와 레이블 없는 데이터 세트, 벤치마크 등을 개발했다. 이 분야는 빠르게 발전하고 있지만, 그로 인해 다양한 기법을 비교하거나 이해하는 것이 어려워졌다.\n모든 텍스트 처리 문제를 \u0026ldquo;Text-to-Text\u0026quot;의 일관된 문제로 바라보며, 이를 통해 다양한 NLP 문제에 대한 성능을 평가하고 전이 학습의 한계를 탐색하고자 한다. 이 연구의 목표는 새로운 방법을 제안하는 것이 아니라, 현재 이 분야가 어디에 서 있는지를 종합적으로 이해하는 것이다. 이를 위해 \u0026ldquo;Colossal Clean Crawled Corpus(C4)\u0026ldquo;라는 웹에서 수집한 영어 텍스트 데이터 세트를 사용한다. 또한, 데이터가 부족한 환경에서의 전이 학습의 중요성을 인식하여, 코드, 데이터 세트, 그리고 사전 학습된 모델을 공개한다.\nSetup 모든 문제를 \u0026ldquo;Text-to-Text\u0026rdquo; 변환하는 접근법과 레이블 없는 텍스트 데이터를 구성한 \u0026ldquo;Colossal Clean Crawled Corpus(C4)\u0026ldquo;를 제안한다. 모델과 프레임워크의 이름은 \u0026ldquo;Text-to-Text Transfer Transformer(T5)\u0026ldquo;이다.\nModel NLP의 초기 전이 학습은 RNN을 이용했지만, 최근에는 Transformer 아키텍처 기반의 모델이 일반적이다. Transformer는 처음에는 기계 번역에 효과적이었지만, 이후 다양한 NLP 환경에서 널리 사용되었다. 그래서 논문에서 연구되는 모든 모델은 Transformer 아키텍처를 기반으로 하며, 이 아키텍처에서 크게 벗어나지는 않았다.\nTransformer의 핵심 구성 요소는 self-attention으로, 시퀀스의 각 요소를 시퀀스의 나머지 부분의 가중 평균으로 대체한다. 원래 Transformer는 encoder-decoder 아키텍처로 설계되었지만, 최근에는 언어 모델링이나 분류, 범위 예측 작업에 적합한 아키텍처를 생성하는 다양한 형태의 self-attention을 사용한 single Transformer layer stack 모델이 일반적이다.\nT5의 encoder-decoder Transformer는 입력 토큰을 임베딩으로 매핑하고 이를 인코더에 전달한다. 인코더는 \u0026ldquo;self-attention layer\u0026quot;과 \u0026ldquo;feed-forward network\u0026quot;를 포함하며, 각 입력에 layer normalization와 residual skip connection을 적용한다. dropout은 네트워크 전체에 적용된다. 디코더는 인코더와 비슷하지만, 인코더 출력에 self-attention mechanism이 추가되고, autoregressive 또는 causal self-attention을 사용한다. 디코더 출력은 dense layer로 전달되고, 모든 attention mechanism은 독립적인 \u0026ldquo;head\u0026quot;로 나누어져 있다.\nTransformer 모델은 순서에 상관 없는 self-attention 특성 때문에 position signal을 제공한다. 초기에는 sinusoidal position signal이나 learned position embeddings을 사용했지만, 최근에는 relative position embeddings이 주로 사용되고 있다. 이는 \u0026ldquo;key\u0026quot;와 \u0026ldquo;query\u0026quot;의 오프셋에 따라 다른 임베딩을 생성한다. 우리는 position embedding을 간소화하여 attention weight 계산에 사용되는 스칼라로 만들었다. 모든 layer가 position embedding parameter를 공유하며, 각 layer의 attention head는 다른 position embedding을 사용한다. 이 모델은 원래의 Transformer와 비슷하지만, layer normalization 위치와 position embedding 체계가 다르다.\n모델의 확장성을 실험하기 위해 parameter와 layer를 늘리고, 그 성능 변화를 관찰했다. 복잡한 큰 모델 학습을 위해 모델과 데이터 병렬성을 사용하고, 5개의 TPU 파드를 활용한 Cloud TPU Pods에서 모델을 학습시켰다.\nThe Colossal Clean Crawled Corpus 레이블이 없는 데이터의 품질, 특성, 크기가 어떤 영향을 미치는지 분석한다. 이를 위해 웹에서 스크랩된 텍스트를 제공하는 Common Crawl을 사용한다. Common Crawl은 이전에 언어 모델 훈련, 상식적 추론, 기계 번역 텍스트 채굴, 사전 훈련 데이터 세트, 최적화기 테스트 등 다양한 NLP 연구에 활용된 바 있다.\nCommon Crawl은 웹에서 스크랩된 텍스트를 제공하는 공개 아카이브이다. 매월 약 20TB의 텍스트 데이터를 생성하지만, 이 중 대부분은 자연언어가 아닌 메뉴, 오류 메시지, 중복 텍스트 등의 쓸데없는 텍스트이다. 또한, 작업에 도움이 되지 않을 것 같은 내용도 많이 포함되어 있다. 이러한 문제를 해결하기 위해, 다음과 같은 방법들을 사용한다:\n마침표, 느낌표, 물음표, 인용 부호를 포함한 문장만을 사용한다. 3문장 미만의 페이지는 제외하고, 적어도 5단어 이상 포함된 문장만을 사용한다. \u0026ldquo;불순한, 야한, 외설적인 또는 그 외 나쁜 단어 목록\u0026quot;에 있는 단어가 포함된 페이지는 모두 삭제한다. 스크랩된 페이지의 대다수는 자바스크립트(Javascript)가 활성화 되어야 한다는 경고문을 포함한다. 따라서 자바스크립트 단어를 포함한 모든 라인을 삭제한다. 일부 페이지는 “lorem ipsum” 플레이스홀더를 포함한다. 따라서 “lorem ipsum”구가 있는 모든 페이지를 삭제한다. 일부 페이지에는 코드가 포함되어 있다. “{” 문구가 대다수의 프로그래밍 언어(웹에서 많이 사용되는 자바스크립트와 같이)에서 출몰하고 자연 텍스트에서는 나타나지 않기 때문에, “{” 를 포함한 모든 페이지를 삭제한다. 스크랩된 페이지 중 일부는 위키백과에서 가져온 것이었고, 인용 표시자(e.g. [1], [citation needed], etc.)가 있다. 이러한 표시자를 모두 를 모두 삭제한다. 많은 페이지에는 보일러플레이트 정책 공지가 있다. \u0026ldquo;terms of use\u0026rdquo;, \u0026ldquo;privacy policy\u0026rdquo;, \u0026ldquo;cookie policy\u0026rdquo;, \u0026ldquo;uses cookies\u0026rdquo;, \u0026ldquo;use of cookies\u0026rdquo;, \u0026ldquo;use cookies\u0026quot;라는 문자열을 포함한 줄은 모두 삭제한다. 데이터셋 중복을 제거하기 위해, 데이터셋에서 두 번 이상 나타난 3문장 스팬은 하나만 남기고 모두 삭제한다. 대부분의 작업이 영어 텍스트에 초점을 두고 있기 때문에, 0.99의 확률로 영어로 분류되지 않은 페이지를 제거하기 위해 \u0026ldquo;langdetect\u0026quot;를 사용하였다. 하지만, 이전 데이터 세트의 필터링 방법, 공개 여부, 범위 등이 제한적이라고 판단하여 새로운 데이터 세트를 만들기로 결정하였습니다.\n2019년 4월의 웹 텍스트를 다운로드하고 필터링하여 기본 데이터 세트를 구축하였다. 이 결과, 대부분의 사전 학습 데이터 세트보다 훨씬 크고(750GB), 깨끗하며 자연스러운 영어 텍스트 컬렉션을 만들었다. 이를 \u0026ldquo;Colossal Clean Crawled Corpus(C4)\u0026ldquo;라고 부르며, TensorFlow 데이터 세트의 일부로 공개하였다.\nDownstream Tasks 이 논문의 목표는 일반적인 언어 학습 능력을 측정하는 것이다. 이를 위해, 다양한 벤치마크를 통해 기계 번역, 질문 응답, 추상적 요약, 텍스트 분류 등의 성능을 연구하였다. 이에는 GLUE와 SuperGLUE 텍스트 분류, CNN/Daily Mail 요약, SQuAD 질문 응답, 그리고 WMT 영어에서 독일어, 프랑스어, 루마니아어로의 번역이 포함되었다. 모든 데이터는 TensorFlow 데이터 세트에서 수집하였다.\nGLUE와 SuperGLUE는 각각 일반적인 언어 이해 능력을 테스트하기 위해 설계된 텍스트 분류 작업들의 모음이다:\nSentence acceptability judgment (CoLA) Sentiment analysis (SST-2) Paraphrasing/sentence similarity (MRPC, STS-B, QQP) Natural language inference (MNLI, QNLI, RTE, CB) Coreference resolution (WNLI and WSC) Sentence completion (COPA) Word sense disambiguation (WIC) Question answering (MultiRC, ReCoRD, BoolQ) GLUE와 SuperGLUE 벤치마크의 데이터 세트를 사용하며, 모든 작업들을 하나의 작업으로 취급하여 데이터 세트를 결합하기 위해 미세 조정을 진행하였다. 또한, SuperGLUE 작업에는 Definite Pronoun Resolution (DPR) 데이터 세트도 포함시켰다.\nCNN/Daily Mail 데이터 세트는 텍스트 요약 작업으로 적용되었고, SQuAD는 일반적인 질문 응답 벤치마크이다. WMT 영어-독일어, 영어-프랑스어, 영어-루마니아어 번역에는 각각 표준 훈련 데이터와 검증 세트를 사용한다. 모든 사전 학습은 영어 데이터로만 진행되며, 모델이 새로운 언어의 텍스트를 생성하도록 배우기 위해 번역 학습이 필요하다.\nInput and Output Format 모든 작업을 \u0026ldquo;text-to-text\u0026rdquo; 형식으로 표현하여 단일 모델을 훈련시킨다. 이 방식은 사전 학습과 미세 조정에 대해 일관된 훈련 목표를 제공한다. 모델은 작업에 관계없이 maximum likelihood 목표로 훈련되며, 수행해야 할 작업을 지정하기 위해 원래 입력 시퀀스 앞에 작업 특정 텍스트 접두어(prefix)를 추가한다.\nText-to-text 프레임워크는 다양한 NLP 작업을 통일된 형식으로 변환한다. McCann et al이 제안한 \u0026ldquo;Natural Language Decathlon\u0026quot;과 비슷하지만, 이 논문에서는 각 작업을 개별적으로 미세조정하고 짧은 작업 접두어를 사용한다. 또한 전이 학습에 초점을 맞추며, 기계 번역과 추상적 요약 등의 생성적 작업을 처리할 수 있는 프레임워크를 제안한다.\n대부분의 작업을 text-to-text 형식으로 쉽게 변환했으며, 유사성 점수를 예측하는 STS-B는 점수를 반올림하고 숫자 문자열로 변환하여 처리하였다. 이를 통해 STS-B 회귀 문제를 21 클래스 분류 문제로 재구성하였다.\n또한, Winograd 작업과 WSC 작업에서는 모호한 대명사를 강조하고, 모델이 대명사가 가리키는 명사를 예측하도록 훈련시켰다. DPR 데이터 세트는 대략 1,000개의 대명사 해결 예제를 추가하여 사용하였다.\nWNLI의 훈련 및 검증 세트는 WSC와 많이 중복되므로, 훈련 데이터로의 유출을 방지하기 위해 WNLI에서 훈련하지 않았으며, 평균 GLUE 점수에도 포함시키지 않았다.\nExperiments 한 번에 하나씩 설정을 변경하면서 체계적으로 기여도를 연구하였다. 예를 들어, 나머지 실험 파이프라인을 고정하고 다양한 비지도 목표의 성능을 측정했다. 이 방법은 이차 효과를 놓칠 수 있지만, 모든 요인의 조합을 탐색하는 것은 비용이 많이 든다. 미래의 연구에서는 다양한 접근법의 조합을 더 철저하게 고려하는 것이 유익할 것으로 예상된다.\n이 논문의 목표는 다양한 작업에 대해 다양한 접근법을 비교하는 것으로, 가능한 한 많은 요소를 고정하려고 한다. 이를 위해, 기존의 접근법을 정확하게 따르지는 않았다. 예를 들어, BERT와 같은 encoder-only 모델은 생성 작업에는 적합하지 않다. 따라서 우리가 고려하는 모델 중 어느 것도 BERT와 정확히 같지 않다. 대신, BERT의 목표와 유사한 목표를 고려하고, BERT와 유사하게 작동하는 모델 아키텍처를 고려하였다.\nBaseline 간단한 denoising 목표를 사용하여 표준 Transformer를 사전 학습하고, 각 downstream task에서 별도로 미세 조정을 진행한다.\nModel T5는 standard encoder-decoder Transformer를 사용한다. 많은 NLP 전이 학습 방법이 single “stack” 구조를 사용하지만, 이 연구에서는 standard encoder-decoder 구조가 생성과 분류 작업에서 좋은 결과를 얻는다는 것을 확인하였다.\n$BERT_BASE$와 유사한 크기와 구성의 인코더와 디코더로 설계되었다. 인코더와 디코더는 각각 12개의 블록으로 이루어져 있으며, 이 블록들은 self-attention, encoder-decoder attention, feed-forward network를 포함하고 있다. 모델은 총 약 2억 2천만 개의 parameter를 가지고 있으며, 모든 부분에서 0.1의 드롭아웃 확률을 사용하여 정규화된다.\nTraining 모든 작업은 text-to-text로 구성되며, 이를 통해 standard maximum likelihood를 사용하여 학습한다. 최적화는 AdaFactor를 사용하고, 테스트 시에는 가장 높은 확률의 logit을 선택하는 greedy decoding을 사용한다.\n각 모델은 C4에서 524,288 단계동안 사전 학습 후 미세 조정을 진행한다. 최대 시퀀스 길이는 512이며, 배치 크기는 128 시퀀스이다. 배치는 대략 65,536 토큰을 포함하도록 한다. 이는 총 34B 토큰에 대한 사전 학습에 해당하며, BERT나 RoBERTa에 비해 상당히 적다. 하지만 이런 방식을 사용하면 합리적인 계산 비용으로 충분한 사전 학습을 할 수 있다. 사전 학습 동안 데이터는 반복하지 않는다.\n사전 학습 동안, \u0026ldquo;inverse square root\u0026rdquo; learning rate schedule을 사용한다. 초기 $10^4$ 단계 동안 learning rate를 0.01로 유지한 후 지수적으로 감소시킨다. triangular learning rate를 실험해 봤지만, 학습 단계의 총 수를 미리 알아야 하므로, 더 일반적인 inverse square root schedule을 선택하였다.\n모든 작업에 대해 262,144 단계 동안 모델을 미세 조정한다. 이는 대규모와 소규모 데이터 세트를 가진 작업 사이의 균형을 위해 선택되었다. 미세 조정 시 128개의 길이 512 시퀀스를 가진 배치를 사용하고, 학습률은 0.001로 유지한다. 5,000step마다 체크포인트를 저장하며, 가장 높은 검증 성능을 보인 체크포인트의 결과를 보고한다. 여러 작업에 미세 조정된 모델의 경우, 각 작업마다 최적의 체크포인트를 독립적으로 선택한다.\nVocabulary SentencePiece를 사용하여 텍스트를 WordPiece 토큰으로 인코딩하며, 32,000개의 어휘를 사용한다. T5 모델이 다른 언어를 처리할 수 있도록, 영어, 독일어, 프랑스어, 루마니아어 데이터를 혼합하여 SentencePiece 모델을 훈련시켰다. 이 어휘는 모델의 입력과 출력에 모두 사용되며, 미리 결정된 고정된 언어 세트만 처리할 수 있다.\nUnsupervised Objective 레이블이 없는 데이터를 활용하여 모델을 사전 학습하는데는, 레이블이 필요하지 않지만 일반화 가능한 지식을 모델에게 가르치는 목표가 필요하다. 최근 \u0026ldquo;denoising\u0026rdquo; 또는 \u0026ldquo;masked language modeling\u0026quot;이라는 목표가 효과적이라는 것이 밝혀졌다. 이는 모델이 입력에서 누락되거나 손상된 토큰을 예측하도록 하는 방식이다. 이에 영감을 받아, 입력 시퀀스에서 무작위로 선택한 15%의 토큰을 드롭아웃하는 목표를 설정하였다. 이 목표는 사전 훈련의 계산 비용을 줄이는 데 도움이 된다.\nBaseline Performance 이상적으로는 모든 실험을 여러 번 반복해야 하지만, 실험의 수가 많으면 비용이 높다. 대신, 기본 모델을 10번 새로 학습하고, 이 실행들의 분산이 각 실험 변형에 적용될 것으로 가정한다. 또한, 사전 학습 없이 모델을 218 step 동안 학습한 후 성능을 측정하여, 사전 학습이 얼마나 도움이 되는지 파악한다.\nGLUE와 SuperGLUE는 모든 하위 작업의 평균 점수를, 번역 작업은 SacreBLEU에서 제공하는 BLEU 점수를 확인한다. WMT 영어에서 독일어, 프랑스어, 루마니아어로의 점수를 각각 EnDe, EnFr, EnRo라 표기한다. CNN/Daily Mail과 SQuAD는 상관성이 높은 지표만 확인한다.\nT5 모델은 $BERT_{BASE}$와 비슷한 성능을 보여준다(SQuAD에서 80.88, MNLI-matched에서 84.24). 하지만 T5 모델은 encoder-decoder 모델로, 사전 학습 단계가 더 적어 직접 비교는 어렵다. 대부분의 벤치마크에서 사전 학습이 성능을 크게 향상시키는 것을 확인했고, WMT 영어에서 프랑스어로의 작업을 통해 high-resource regime 상태에서의 전이 학습을 테스트하였다. 데이터가 제한된 작업에서 사전 학습이 얼마나 성능을 향상시키는지 강조하는 동시에, 전이 학습의 주요 이점 중 하나로 데이터 효율성의 개선을 강조한다.\n대부분의 작업에서 표준 편차는 작업의 기준 점수의 1% 미만이다. 하지만 GLUE와 SuperGLUE 벤치마크의 CoLA, CB, COPA와 같은 low-resource 작업에서는 이 규칙이 적용되지 않는다. 예를 들어, CB 작업에서 기준 모델의 평균 F1 점수는 91.22이고 표준 편차는 3.237이었다. 이런 변동성은 검증 세트의 예제 수가 적은 것이 원인일 수 있다. GLUE와 SuperGLUE 점수는 각 벤치마크의 작업 점수의 평균으로 계산되기 때문에 이러한 높은 변동성 때문에 이 점수만으로 모델을 비교하는 것은 어려울 수 있다.\nArchitectures Transformer는 처음에는 encoder-decoder 구조로 소개되었지만, 최근 NLP 전이 학습 연구에서는 다른 구조를 더 많이 사용하고 있다.\nModel Structures 아키텍처를 구분하는 주요 요소는 모델에서 사용하는 \u0026ldquo;mask\u0026quot;이다. Transformer의 self-attention 연산은 시퀀스를 입력받아 동일한 길이의 새로운 시퀀스를 출력한다. 각 출력 항목은 입력 항목의 weighted average를 계산해 생성된다. attention mask는 특정 가중치를 0으로 만들어 특정 출력 시간에서 입력 항목에 attention을 기울일 수 있는 범위를 제한한다. 예를 들어, causal mask는 $j \u0026gt; i$인 경우 가중치를 0으로 만든다.\n첫 번째로 고려하는 모델은 encoder-decoder Transformer로, 입력 시퀀스를 받는 encoder와 새로운 출력 시퀀스를 만드는 decoder 두 계층으로 구성되어 있다.\nencoder는 \u0026ldquo;fully-visible\u0026rdquo; attention mask를 사용한다. 이 마스킹은 출력의 각 항목을 만들 때 입력의 모든 항목에 attention을 기울일 수 있게 해준다. 이 마스킹은 \u0026ldquo;prefix\u0026rdquo; 즉, 예측을 만들 때 사용되는 일부 컨텍스트에 주의를 기울일 때 적합하다. BERT도 이와 같은 마스킹 패턴을 사용하며, 특별한 \u0026ldquo;classification\u0026rdquo; 토큰을 입력에 추가한다. 이 토큰에 해당하는 BERT의 출력은 입력 시퀀스를 분류하는 예측을 하는데 사용된다.\nTransformer의 decoder에서 self-attention 연산은 \u0026ldquo;causal\u0026rdquo; 마스킹 패턴을 사용한다. 출력 시퀀스의 $i$번째 항목을 생성할 때, 인과적 마스킹은 모델이 입력 시퀀스의 $j$번째 항목$(j \u0026gt; i)$에 attention을 기울이는 것을 방지한다. 이는 모델이 출력을 생성하는 동안 \u0026ldquo;미래를 보는\u0026rdquo; 것을 방지하기 위해 훈련 중에 사용된다.\n언어 모델은 text-to-text 작업에서 입력과 목표를 연결함으로써 사용될 수 있지만, causal 마스킹 때문에 입력 시퀀스의 특정 항목이 그 이전 항목에만 의존하는 문제가 있다. 이 문제는 Transformer 기반 언어 모델에서 마스킹 패턴을 변경함으로써 해결할 수 있으며, 시퀀스의 접두사 부분에서 완전히 보이는 마스킹을 사용하면 이 문제를 피하면서도 다양한 text-to-text 작업에 효과적일 수 있다. 이 방식은 encoder와 decoder 간에 파라미터를 공유하는 encoder-decoder 모델과 유사하며, 입력과 목표 시퀀스에 걸쳐 전체 attention을 적용한다.\nprefix LM은 BERT와 비슷하게 작동하지만, 분류 작업을 수행하기 위해 Transformer decoder의 출력 레이어에 분류기를 통합한다. 이 모델은 전체 입력을 보고 예측을 출력함으로써 분류 작업을 수행한다.\nComparing Different Model Structures 아키텍처를 비교하려면, 각 모델이 같은 수의 parameter를 가지거나, 같은 양의 계산을 필요로 하는 등 의미 있는 방식으로 동일해야 한다. encoder와 decoder가 각각 L개의 레이어를 가진 encoder-decoder 모델은, 2L개의 레이어를 가진 언어 모델과 대략 같은 수의 parameter를 가진다. 그러나 계산 비용 면에서는, L개의 레이어를 가진 언어 모델과 동일하다. 이는 언어 모델이 입력과 출력 시퀀스 모두를 처리해야 하지만, encoder-decoder 모델은 각각 입력과 출력 시퀀스만을 처리하기 때문이다.\n비교를 위해, encoder-decoder 모델의 여러 구성을 고려하였다. $BERT_{BASE}$ 크기의 레이어 스택에서 레이어와 parameter의 수를 각각 L과 P로, 주어진 입력-타겟 쌍 처리에 필요한 FLOPs의 수를 M으로 표현하겠습니다. 이를 바탕으로 모델들을 비교한다:\nencoder와 decoder에 각각 L 레이어가 있는 encoder-decoder 모델. 이 모델은 2P의 parameter와 M FLOPs의 계산 비용을 가진다. 동일한 모델이지만, 인코더와 디코더 간에 parameter가 공유되어, P의 parameter와 M-FLOP의 계산 비용을 가진다. encoder와 decoder에 각각 L/2 레이어가 있는 encoder-decoder 모델로, P의 parameter와 M/2-FLOP의 비용을 가진다. L 레이어와 P parameter를 가지며 M FLOPs의 계산 비용이 발생하는 decoder-only 언어 모델. 같은 아키텍처를 가지지만, 입력에 대한 fully-visible self-attention을 가진 decoder-only prefix LM. Objectives 비지도 학습 목표로 기본 언어 모델링과 denoising 목표를 고려하였다. 언어 모델링은 사전 학습 목표로서의 역사적 사용과 모델 아키텍처에 대한 적합성 때문에 포함되었다. 예측 전에 접두사를 입력하는 모델들에 대해, 레이블이 없는 데이터에서 텍스트를 샘플링하고 랜덤한 지점에서 접두사와 타겟으로 분할한다. 표준 언어 모델은 전체 텍스트를 예측하도록 훈련되며, text-to-text 모델을 위한 비지도 denoising 목표는 입력과 타겟을 연결하여 사용한다.\nResults 모든 작업에서 denoising 목표를 가진 encoder-decoder 구조가 가장 좋은 성능을 보여주었다. 이 구조는 parameter 수는 가장 많지만 계산 비용은 decoder-only 모델과 같다. encoder와 decoder 간에 parameter를 공유하는 것도 거의 동등한 성능을 보여주었다. 반면, encoder와 decoder의 레이어 수를 줄이면 성능이 크게 저하되었다. denoising 목표를 가진 공유 encoder-decoder 구조는 decoder-only prefix LM 모델보다 성능이 좋았다. 마지막으로, denoising 목표를 사용하는 것이 언어 모델링 목표보다 항상 더 나은 성능을 가져다 준다는 사실을 확인하였다.\nUnsupervised Objectives 비지도 학습 목표의 선택은 모델이 downstream task에 적용할 일반 지식을 획득하는 방법을 제공하므로 중요하며, 이로 인해 다양한 사전 학습 목표가 개발되었다. 많은 경우에 기존의 목표를 그대로 복제하지 않고, text-to-text encoder-decoder 프레임워크에 맞게 수정하거나, 여러 공통 접근법의 개념을 결합한 목표를 사용한다.\n라벨이 없는 텍스트 데이터 세트에서 토큰화된 텍스트 범위에 해당하는 토큰 ID의 시퀀스를 처리한다. 토큰 시퀀스는 입력 시퀀스와 목표를 생성하고, 모델은 이를 사용해 maximum likelihood로 목표 시퀀스를 예측하도록 학습한다.\nDisparate High-Level Approaches 세 가지 다른 접근법을 사용한 기법들을 비교한다. 첫 번째로, \u0026ldquo;prefix language modeling\u0026rdquo; 목표를 사용하며, 이는 텍스트를 두 부분으로 나눠 encoder 입력과 decoder 예측 대상으로 사용한다. 두 번째로, BERT의 \u0026ldquo;masked language modeling\u0026quot;에서 영감을 받은 목표를 사용하며, 이는 텍스트의 토큰 15%를 손상시키고, 이 중 90%는 마스크 토큰, 10%는 랜덤 토큰으로 대체한다. 세 번째로, \u0026ldquo;deshuffling\u0026rdquo; 목표를 사용하며, 이는 토큰의 순서를 섞은 후 원래 순서를 복원하는 것을 목표로 한다.\n전체적으로, BERT 스타일 목표가 가장 뛰어난 성능을 보이지만, prefix language modeling 목표도 번역 작업에서 유사한 성능을 보여준다. 반면에 deshuffling 목표는 다른 두 목표보다 성능이 상당히 떨어진다.\nSimplifying the BERT Objective BERT 스타일의 denoising 목표는 원래 분류와 범위 예측을 위해 학습된 encoder-only 모델의 사전 학습 기법으로 제안되었다. 따라서 encoder-decoder text-to-text 설정에서 더 나은 성능을 내거나 더 효율적이게 만들 수 있도록 조정하는 것이 가능할 수 있다.\nBERT 스타일 목표의 간단한 변형을 고려하며, 이는 무작위 토큰 교환 단계를 생략한다. 그 결과, 입력의 15% 토큰을 마스크 토큰으로 바꾸고, 모델은 원래 손상되지 않은 시퀀스를 재구성하도록 학습한다. 이를 \u0026ldquo;MASS\u0026rdquo; 목표라고 부릅니다. 또한, decoder에서 긴 시퀀스 자체에 대한 attention를 피할 수 있는 방법을 탐색한다. 이를 위해, 손상된 토큰들을 모두 마스크 토큰으로 대체하거나, 손상된 토큰을 입력 시퀀스에서 완전히 삭제하는 두 가지 전략을 고려한다.\n원래의 BERT 스타일 목표와 세 가지 대안의 비교는 모든 변형이 비슷하게 수행된다는 것을 보여준다. 예외적으로, 손상된 토큰을 완전히 삭제하는 것이 CoLA에서 훨씬 높은 점수 덕분에 GLUE 점수를 약간 향상시켰는데, CoLA가 주어진 문장이 문법적으로 및 구문론적으로 수용 가능한지 분류하는 것을 포함하고 있으며, 토큰이 누락되었는지 판단할 수 있는 능력이 수용 가능성을 감지하는 데 밀접하게 관련되어 있기 때문일 수 있다. 그러나, 토큰을 완전히 삭제하는 것은 SuperGLUE에서 성능이 떨어졌다. 전체 원래 시퀀스를 예측할 필요가 없는 두 가지 변형은 훈련 시간을 단축시키는 장점이 있다.\nVarying the Corruption Rate 지금까지 BERT에서 사용된 15%의 토큰 손상률을 사용하였다. 그러나 text-to-text 프레임워크가 BERT와 다르므로 다른 손상률이 더 효과적인지 테스트해 보았다. 10%, 15%, 25%, 50%의 손상률을 비교했지만, 손상률이 모델 성능에 큰 영향을 미치지는 않았다. 단, 50%의 가장 높은 손상률은 GLUE와 SQuAD에서 성능 저하를 가져왔다. 또한, 높은 손상률은 학습 속도를 느리게 만드는 긴 대상을 만드는 경향이 있다. 따라서 BERT의 기준에 따라 앞으로 15%의 손상률을 사용할 것이다.\nCorrupting Spans 예측 대상을 짧게 하여 학습 속도를 높이려고 한다. 지금까지의 방법은 각 입력 토큰을 독립적으로 손상시킬지 결정하였고, 연속된 토큰이 손상될 경우 이를 \u0026ldquo;span\u0026quot;으로 취급하여 단일 마스크 토큰으로 대체하였다. 이 방식은 레이블이 없는 텍스트 데이터를 짧은 시퀀스로 변환하지만, 항상 많은 수의 손상된 토큰이 연속적으로 나타나지는 않는다. 따라서 토큰의 span을 특정하여 손상시키는 방식을 사용하면 더 큰 속도 향상을 얻을 수 있습니다. 이러한 방법은 BERT의 사전 학습 목표로도 사용되어 성능 향상을 가져왔다.\n토큰의 연속적인 span을 손상시키는 목표를 테스트하기 위해, 손상시킬 토큰의 비율과 손상된 span의 총 수를 parameter로 사용한다. 예를 들어, 500개의 토큰 시퀀스에서 15%의 토큰을 손상시키고 총 span이 25개가 되도록 지정하면, 손상된 토큰의 총 수는 75개이고 평균 span 길이는 3이 된다. 이 방식은 원래의 시퀀스 길이와 손상률에 따라 span의 길이나 총 span 수를 조절할 수 있다.\nspan 손상 목표와 독립 동일 분포(i.i.d) 손상 목표를 비교한 결과, 이들 사이에는 큰 차이가 없었다. 하지만 평균 span 길이가 10인 경우에는 일부에서 다른 값들보다 성능이 약간 떨어졌다. 반면에 평균 span 길이가 3인 경우에는 대부분의 비번역 벤치마크에서 i.i.d. 목표를 약간 능가하였다. 또한 span 손상 목표는 평균적으로 더 짧은 시퀀스를 생성함으로써 학습 속도를 빠르게 할 수 있었다.\nDiscussion denoising 목표가 언어 모델링 및 deshuffling보다 사전 학습에 더 효과적이었다. 또한 denoising 목표의 다양한 변형 사이에는 큰 차이가 없었다. 그러나 목표의 선택이나 parameter화는 시퀀스 길이와 학습 속도에 영향을 미친다. 따라서 denoising 목표의 선택은 주로 계산 비용에 기반해 이루어져야 한다. 또한, 유사한 목표에 대한 추가적인 탐색은 큰 이익을 가져오지 않을 수 있으며, 레이블이 없는 데이터를 활용하는 새로운 방법을 탐색하는 것이 더 유익할 수 있다.\nPre-training Data set 사전 학습 데이터 세트는 전이 학습 파이프라인의 핵심 요소이지만, 새로운 데이터 세트는 종종 중요한 기여로 인식되지 않고, 사전 학습된 모델과 코드와 함께 공개되지 않는다. 그 결과, 다양한 사전 학습 데이터 세트의 비교는 부족하고 \u0026ldquo;표준\u0026rdquo; 데이터 세트도 없다. 최근에는 큰 데이터 세트와 작은 기존 데이터 세트에서의 사전 학습을 비교한 연구가 있다.\nUnlabeled Data Sets C4 제작 과정에서, Common Crawl로부터 추출한 웹 텍스트를 필터링하기 위한 다양한 방법을 개발하였다. 이 필터링이 다른 방법과 비교하여 downstream task에서 성능 향상을 가져오는지 측정하려 한ㄴ다. 이를 위해 다양한 데이터 세트에서 사전 학습한 후의 기준 모델 성능을 비교하였다.\nC4 기준이 되는 데이터셋으로, 레이블 없는 데이터 세트에서 사전 훈련하는 것을 고려한다.\nUnfiltered C4 C4를 생성할 때 사용한 휴리스틱 필터링의 효과를 측정하기 위해, 필터링을 생략한 C4의 대체 버전을 만들었다. 하지만, 영어 텍스트 추출을 위해 langdetect는 여전히 사용되며, 이로 인해 \u0026ldquo;unfiltered\u0026rdquo; 버전도 어느 정도 필터링이 포함된다.\nRealNews-like 최근 연구에서는 뉴스 웹사이트에서 추출한 텍스트 데이터를 사용하였다. 이를 비교하기 위해, C4를 필터링하여 \u0026ldquo;RealNews\u0026rdquo; 데이터 세트에서 사용된 도메인의 콘텐츠만 포함하도록 한 새로운 레이블 없는 데이터 세트를 생성하였다. C4에서 사용된 필터링 방법을 유지하되, 비뉴스 콘텐츠는 모두 제외하였다.\nWebText-like WebText 데이터 세트는 Reddit에 제출된 웹페이지 중 점수가 3점 이상인 콘텐츠만 사용했다. 비교 가능한 데이터 세트를 만들기 위해, C4에서 OpenWebText 목록에 없는 URL의 콘텐츠를 모두 제거하였다. 하지만, 대부분의 페이지가 Reddit에 나타나지 않아, 결과적으로 콘텐츠가 많지 않았다. 그래서, 2018년 8월부터 2019년 7월까지의 Common Crawl 데이터를 다운로드하여 필터링을 적용하였고, 이를 통해 원래의 WebText 데이터 세트와 비교 가능한 17GB의 데이터 세트를 생성하였다.\nWikipedia Wikipedia는 엄격한 품질 가이드라인을 준수하는 수백만 개의 협업 글로 이루어져 있다. 이러한 특성 때문에 Wikipedia는 청결하고 자연스러운 텍스트의 신뢰성 있는 출처로 활용되었다. 기사의 마크업이나 참조 섹션을 생략한 TensorFlow Datasets의 영어 Wikipedia 텍스트 데이터를 사용하고 있다.\nWikipedia + Toronto Books Corpus Wikipedia의 사전학습 데이터를 사용하는 단점은 자연어 텍스트의 한 도메인만을 대표한다는 것이다. 이를 보완하기 위해 BERT는 Wikipedia 데이터와 전자책에서 추출한 텍스트를 담은 Toronto Books Corpus를 결합하였다.\n사전 학습 데이터 세트의 도메인이 제한된 경우, 다양한 데이터 세트를 사용한 것보다 성능이 뛰어날 수 있다. 특히, 해당 도메인과 관련된 작업에서 성능 향상이 두드러진다. 하지만, 이 방법은 모든 도메인의 언어 작업에 빠르게 적응하는 모델을 만드는 목표와는 약간 다르다.\n단일 도메인에서만 사전 학습을 하는 것의 단점은 결과적으로 데이터 세트가 작아진다. WebText와 유사한 변형은 C4 데이터 세트와 같거나 더 좋은 성능을 보였지만, Reddit 기반 필터링은 더 많은 데이터에도 불구하고 C4보다 훨씬 작은 데이터 세트를 만들었다.\nPre-training Data set Size 제한된 레이블 없는 데이터 세트 크기의 영향을 테스트하기 위해, C4를 인공적으로 줄인 버전에서 베이스라인 모델을 사전 학습시켰다. 이 때, 다양한 크기의 축소된 C4 변형에서 학습을 진행하였으며, 이는 사전 학습 과정에서 데이터 세트를 각각 64, 256, 1,024, 4,096번 반복하는 것을 의미한다.\n데이터 세트 크기가 줄어들면서 성능이 저하되는 것을 확인할 수 있다. 이는 모델이 사전 학습 데이터를 기억하기 시작하면서 발생하는 것으로 보인다. 이를 검증하기 위해 각 데이터 세트 크기에 대한 학습 손실을 그렸고, 데이터 세트 크기가 줄어들면서 학습 손실이 크게 감소하는 것을 확인하였다. 이는 모델이 데이터를 기억하고 있음을 나타내는 증거일 수 있다.\n사전 학습 데이터 세트가 64번만 반복될 때, 이러한 효과는 제한적이라는 것을 확인하였다. 이는 일정량의 사전 학습 데이터 반복이 해롭지 않을 수 있음을 나타낸다. 추가적인 사전 학습이 유익하고 레이블이 없는 추가 데이터를 쉽게 얻을 수 있으므로, 가능하면 큰 사전 훈련 데이터 세트를 사용하는 것이 좋다. 더 큰 모델은 작은 사전 학습 데이터 세트에 과적합되는 경향이 더욱 강하게 나타난다.\nTraining Strategy Fine-tuning Methods 초기 연구에서는 사전 학습된 모델의 문장 임베딩을 사용하는 작은 분류기의 parameter만 미세 조정하는 것을 제안하였다. 하지만 이 방법은 encoder-decoder 모델에는 적용하기 어렵다. 대신, 모델의 일부 parameter만 업데이트하는 두 가지 대안적인 미세 조정 방법을 고려한다.\n\u0026ldquo;adapter layers\u0026quot;는 원래 모델의 대부분을 고정하고 미세 조정하는 방법이다. Transformer의 각 블록에 dense-ReLU-dense 블록 형태의 adapter layer를 추가하며, 이 layer는 출력 차원이 입력과 같도록 설계된다. 미세 조정 시, adapter layer와 layer normalization parameter만 업데이트되며, 전방향 네트워크의 내부 차원 $d$는 신규 parameter의 수를 결정하는 주요 hyperparameter이다.\n\u0026ldquo;gradual unfreezing\u0026quot;은 시간이 지남에 따라 모델의 parameter를 점차 미세 조정하는 방식이다. 미세 조정 시작 시 최종 층의 parameter만 업데이트하고, 일정 업데이트 후에는 이전 층의 parameter도 포함시키는 방식으로 진행된다. 이 방법은 encoder와 decoder의 층을 동시에 상단부터 점진적으로 언프리징하며, 미세 조정 과정은 12개의 단계로 나누어 진행된다. 이 방식은 데이터 세트 크기의 다양성과 복합 작업의 존재 때문에 채택되었다.\nadapter layers는ㄴ 작업 크기에 따라 차원을 적절히 조정하면 parameter가 적은 상태에서도 미세 조정이 가능하다는 것을 보여준다. 반면에, gradual unfreezing은 미세 조정 시간을 단축시키지만, 모든 작업에서 성능이 약간 떨어진다는 결과를 보여주었다. 이를 통해 언프리징 일정을 더 신중하게 조정하면 더 나은 결과를 얻을 수 있을 것으로 예상된다.\nMulti-task Learning Multi-task Learning은 여러 작업을 동시에 학습하는 방법으로, 하나의 모델이 다양한 작업을 수행하도록 학습한다. 이 방법은 데이터 세트를 혼합하여 레이블이 없는 데이터에서도 학습이 가능하다. 중요한 점은 모델이 각 작업에서 적절한 양의 데이터를 학습하도록 하는 것이며, 이는 데이터 세트 크기, 작업 학습의 난이도, 정규화 등에 따라 달라진다. 또한, 한 작업에서의 성과가 다른 작업의 성능을 저해하는 문제를 고려하여, 데이터 비율 설정 전략을 연구하고 있다.\nExamples-proportional mixing 모델이 작업에 overfit되는 속도는 작업의 데이터 세트 크기에 따라 결정된다. 그래서 데이터 세트 크기에 비례하여 샘플링하는 것이 일반적이다. 하지만, 어떤 작업은 데이터 크기가 월등히 크기 때문에, 이 방법을 사용하면 레이블이 없는 데이터가 대부분이 되고 모든 지도 작업이 undertrain 되는 문제가 발생한다. 이를 해결하기 위해, 비율을 계산하기 전에 데이터 세트 크기에 인위적인 \u0026ldquo;limit\u0026quot;을 설정한다. 각 작업에서 샘플하는 확률은 작업의 데이터 세트 크기와 인위적인 제한 작은 값에 비례하도록 설정된다.\nTemperature-scaled mixing 데이터 세트 크기의 큰 차이를 완화하는 다른 방법은 혼합 비율의 \u0026ldquo;temperature\u0026quot;를 조정하는 것이다. 이 방식은 다국어 BERT에서 적용되어, 자원이 적은 언어에 대한 충분한 학습을 보장하였다. 이는 각 작업의 혼합 비율을 1/temperature로 거듭제곱하고, 이 비율이 합쳐져 1이 되도록 재정규화하는 방식으로 이루어진다. 온도가 증가하면 비율은 동등 혼합에 가까워진다. 가장 큰 데이터 세트의 혼합 비율이 감소하는 것을 방지하기 위해 데이터 세트 크기 제한을 큰 값으로 설정한다.\nEqual mixing 각 작업에서 예제를 동일한 확률로 뽑는다. 각 배치의 예제는 학습 데이터 세트 중 무작위로 선택된다. 이 방법은 low-resource 작업에 빠르게 overfit되고 high-resource 작업에 underfit되기 때문에 최적의 전략이 아닐 가능성이 크다. 이 점은 비율 설정이 최적이 아닐 때 발생할 수 있는 문제를 보여주는 참고점이다.\n이러한 mixing 전략을 기본선인 사전 학습 후 미세 조정 결과와 동일한 기준으로 비교하기 위해, 총 스텝 수가 같은 multi-task Learning 모델을 학습시킨다: $2^{19} + 2^{18} =$ 786,432.\n일반적으로, multi-task Learning은 대부분의 작업에서 사전 학습 후 미세 조정보다 성능이 떨어진다. \u0026ldquo;equal\u0026rdquo; mixing 전략은 특히 성능이 크게 저하되며, 이는 작업에 따른 데이터의 불균형 때문일 수 있다. examples-proportional mixing에서는 대부분의 작업에 대해 모델이 최적의 성능을 얻는 \u0026ldquo;sweet spot\u0026rdquo; $K$ 값이 있다. 또한, temperature-scaled mixing은 대부분의 작업에서 합리적인 성능을 얻는 수단을 제공한다. 별도의 모델이 각각의 작업에 대해 훈련된 것보다 multi-task 모델이 더 나은 성능을 보이지 못한 것은 이전 연구에서도 관찰된 바 있다.\nCombining Multi-Task Learning with Fine-Tuning multi-task Learning의 개선된 버전을 연구하고 있다. 이 방법은 모든 작업에 대해 모델을 사전 학습하고, 각각의 작업에 대해 미세 조정하는 방식으로, 이 방식은 \u0026ldquo;MT-DNN\u0026quot;에서 사용되었으며, GLUE 및 기타 벤치마크에서 state-of-the-art를 달성하였다.\n이 접근법의 세 가지 변형을 고려한다: 첫 번째는 모든 작업을 사전 학습하고 각 작업에 대해 미세 조정하는 것, 두 번째는 하나의 작업을 제외하고 사전 학습한 후 제외된 작업에 대해 미세 조정하는 것, 세 번째는 감독 작업만을 사전 학습하는 것이다. 이 모든 변형에서는 일정 단계 동안 사전 학습 후 미세 조정을 진행하였다.\nmulti-task 사전 학습 후 미세 조정을 한 결과가 기준선과 비교해도 비슷한 성능을 보여주었다. 이는 multi-task 학습 후 미세 조정이 다른 mixing 비율 간의 트레이드오프를 완화하는 데 도움이 될 수 있음을 보여준다. 또한, \u0026ldquo;leave-one-out\u0026rdquo; 학습 방식의 성능은 약간만 떨어졌고, 이는 다양한 작업에 대해 학습한 모델이 새로운 작업에도 적응할 수 있음을 시사한다. 그러나, supervised multi-task 사전 학습은 번역 작업을 제외하고는 성능이 떨어졌다. 이는 번역 작업이 사전 학습에서 덜 이익을 보며, 비지도 사전 학습이 다른 작업에서 중요함을 시사한다.\nScaling \u0026ldquo;bitter lesson\u0026quot;은 계산을 늘리는 일반적인 방법이 인간의 전문성에 의존하는 방법보다 우월하다는 주장이다. 이는 자연어 처리의 전이 학습에도 적용될 수 있으며, 규모를 확대하는 것이 더 신중한 설계보다 성능을 향상시킴을 보여주었다. 이 논문에서는 \u0026ldquo;4배 더 많은 컴퓨팅 파워를 얻었다면 어떻게 사용해야 할까?\u0026ldquo;라는 주제로, 규모를 확대하는 다양한 방법을 비교한다.\n220M의 parameter를 가진 기준 모델로 시작한다. 이 모델은 \u0026ldquo;$BERT_{BASE}$\u0026ldquo;와 유사한 크기의 encoder와 decoder를 가지고 있다. 모델 크기를 증가시키기 위해, \u0026ldquo;$BERT_{LARGE}$\u0026ldquo;의 가이드라인을 따라 두 가지 변형을 만들어내었다: encoder와 decoder 각각에 16층과 32층을 가진 모델이다. 이들은 원래 모델보다 매개변수가 2배와 4배 많으며, 계산 비용도 2배와 4배이다. 이 모델들을 사용하여 4배의 계산을 사용하는 세 가지 방법을 고려한다: 4배 많은 step 학습, 2배 큰 모델로 2배 많은 step 학습, 그리고 \u0026ldquo;baseline\u0026rdquo; 학습 step에 대해 4배 큰 모델 학습.\n데이터를 4배 더 처리하는 방법 중 하나는 배치 크기를 4배로 늘리는 것이다. 이는 학습 속도를 빠르게 하지만, 4배 많은 학습 step을 사용하는 것과 다른 결과를 가져올 수 있어 이를 비교하기 위한 추가 실험을 진행하였다. 또한, 추가 계산을 활용하는 다른 방법으로는 모델의 앙상블을 사용하는 것이 일반적이다. 이를 비교하기 위해, 4개의 별도로 학습된 모델의 앙상블 성능을 평가했으며, 하나의 모델을 사전 학습하고 이를 4배로 미세 조정하는 비용 절감 방법도 함께 고려하였다.\n학습 시간과 모델 크기를 증가시키면 성능이 개선되며, 특히 모델 크기 증가와 앙상블 방법이 더 큰 향상을 가져왔다. 그러나 앙상블 방법은 SuperGLUE에서는 큰 효과를 보이지 못했다. 또한, 스케일링 방법 선택 시, 큰 모델의 미세 조정과 추론 비용, 작은 모델의 학습 시간, 그리고 앙상블의 계산 비용 등을 고려해야 한다. 따라서 모델의 최종 사용을 고려하는 것이 중요하다.\nPutting It All Together 자연어 처리 벤치마크에서 얼마나 성능을 끌어올릴 수 있는지 확인하기 위해, baseline 학습 접근법으로 시작하여 다음과 같은 변경을 만든다:\nObjective 기본 모델의 노이즈 제거 목표를 SpanBERT에서 영감을 받은 span-corruption 목표로 교체하였다. 평균 span 길이 3을 사용하고 원래 시퀀스의 15%를 손상시켰는데, 이 방법은 목표 시퀀스 길이가 짧아 계산 효율성이 높으며, 약간 더 나은 성능을 보여주었다.\nLonger training 기본 모델은 작은 양의 사전 학습을 사용하지만, C4 데이터셋의 크기 때문에 데이터를 반복하지 않고도 더 오래 학습할 수 있다. 추가적인 사전 학습이 도움이 되며, 배치 크기와 학습 단계 수를 늘리는 것이 이에 도움이 되는 것을 확인하였다. 약 1M 개의 사전 학습 토큰에 대해 모델을 사전 학습하였고, 몇 가지 작은 데이터셋에서는 C4보다 더 좋은 성능을 보였지만, 이 작은 데이터셋들은 1M 토큰의 사전 학습 과정에서 수백 번 반복될 만큼 충분히 작기 때문에, C4 데이터셋을 계속 사용하기로 결정했다.\nModel sizes 기본 모델 크기를 확장하면 성능이 향상된다는 것을 확인 했지만, 컴퓨팅 자원이 제한된 상황에서는 작은 모델이 유용할 수 있다. 이를 고려하여, 다양한 크기의 모델을 학습시킨다:\nBase 이 모델은 대략 220M 개의 parameter를 가지고 있다. Small 기본 모델을 축소시키기 위해 512의 $d_{model}$, 2,048의 $d_{ff}$, 8개의 attention head, 그리고 encoder와 decoder 각각에 6개의 layer를 사용한다. 이 모델은 대략 60M 개의 parameter를 가지고 있다. Large 기본 모델이 $BERT_{BASE}$ 크기의 encoder와 decoder를 사용하기 때문에, $BERT_{LARGE}$와 비슷한 크기와 구조를 가진 encoder와 decoder를 가진 변형을 고려하였다. 이 변형은 약 770M 개의 parameter를 가지고 있다. 3B and 11B 더 큰 모델을 사용할 때 가능한 성능을 탐색하기 위해 두 가지 추가 변형을 고려하였다. 두 경우 모두에서 $d_{model} = 1024$, 24개 layer의 encoder와 decoder, 그리고 $d_{ff} = 128$을 사용하였다. \u0026ldquo;3B\u0026rdquo; 변형은 $d_{ff} =$ 16,384와 32개의 attention head를 사용하여 약 2.8B의 parameter를 생성했고, \u0026ldquo;11B\u0026rdquo; 변형은 $d_{ff} =$ 65,536과 128개의 attention head를 사용하여 약 11B개의 parameter를 가진 모델을 생성하였다. Multi-task pre-training 비지도 작업과 지도 작업의 다양한 사전 학습된 모델이 비지도 작업만으로 학습된 모델만큼 잘 작동한다는 것을 확인하였다. 이 방법은 학습 기간 동안 성능을 지속적으로 모니터링 할 수 있어 유용하다. 따라서, 최종 실험에서 이 multi-task 사전 학습 방식을 사용했다. 또한, 더 크고 오래 훈련된 모델은 레이블이 없는 데이터의 더 큰 비율에서 이익을 얻을 것으로 예상했다. 이를 바탕으로, 레이블이 없는 데이터를 위해 특정한 인공 데이터 세트 크기를 사용했고, 모든 모델 변형에 대해 WMT English to French 및 WMT English to German 데이터 세트의 크기를 사전 학습 동안 1M 예제로 제한하였다.\nFine-tuning on individual GLUE and SuperGLUE tasks GLUE와 SuperGLUE에서 미세 조정할 때, 모든 데이터 세트를 합쳐서 한 번에 모델을 미세 조정하였다. 이 방식은 연구를 단순화하지만, 일부 작업에서는 성능이 약간 떨어진다는 것을 발견하였다. 개별 작업에 미세 조정하는 것은 low-resource 작업에 빠르게 overfit될 위험이 있다. 그래서 각 GLUE와 SuperGLUE 작업에 대한 미세 조정 시 작은 배치 크기를 사용하고, overfit되기 전에 모델의 parameter에 접근할 수 있도록 1,000 단계마다 체크포인트를 저장하였다.\nBeam search 이전 결과는 모두 greedy decoding을 사용하여 보고되었다. 그러나 긴 출력 시퀀스 작업에서는 beam search로 성능이 향상되었다. WMT 번역과 CNN/DM 요약 작업에서는 beam width 4와 길이 패널티 $\\alpha = 0.6$을 사용하였다.\nTest set 최종 실험에서는 validation set가 아닌 test set 결과를 보고한다. CNN/Daily Mail은 standard test set를, WMT 작업은 각 언어 쌍에 대한 특정 newstest를 사용했다. GLUE와 SuperGLUE는 벤치마크 평가 서버를 통해 test set 점수를 계산했다. SQuAD의 경우, 벤치마크 서버의 컴퓨팅 자원이 부족해 가장 큰 모델에서의 예측을 얻지 못했으므로, 검증 세트 성능을 계속 보고하게 되었다. 하지만 SQuAD test set에서 가장 높은 성능을 보인 모델이 검증 세트 결과도 보고했으므로, state-of-the-art의 기술과 비교 가능하다.\n위에서 언급한 변경사항들을 제외하고, baseline과 같은 학습 절차와 hyperparameter를 사용한다.\n전반적으로, 24개의 작업 중 18개에서 state-of-the-art를 달성하였다. 가장 큰 모델(11B parameter)이 모든 작업에서 가장 우수한 성능을 보였다. T5-3B 모델은 몇몇 작업에서 이전 state-of-the-art를 능가했지만, 11B parameter로 모델 크기를 확장하는 것이 state-of-the-art를 달성하는 데 가장 중요했다.\nT5는 평균 GLUE 점수에서 90.3의 state-of-the-art를 달성하였고, 특히 자연어 추론 작업에서 이전 state-of-the-art 보다 월등히 높은 성능을 보여주었다. 그러나 이 성능은 여러 모델의 앙상블과 대량의 계산을 활용한 결과였다. SQuAD에서는 Exact Match 점수에서 이전 최고 성능을 1점 이상 능가하였으며, SuperGLUE에서는 평균 점수를 크게 향상시켰지만, 일부 작업에서는 여전히 인간의 성능에 미치지 못했다. WMT 번역 작업에서는 state-of-the-art를 달성하지 못했고, CNN/Daily Mail에서는 state-of-the-art를 달성했지만, 요약의 일관성과는 반드시 연결되지 않았다. 이 모든 결과는 앙상블, 외부 데이터 세트 활용 등 다양한 방법을 통해 성능을 향상시키고 있음을 보여준다.\nT5는 실험 연구의 통찰력과 큰 규모를 결합하여 강력한 성능을 보여준다. baseline 모델의 사전 훈련량 또는 크기를 증가시키면 상당한 향상이 있었고, 이를 바탕으로 T5의 성능 향상에 얼마나 기여했는지 측정하고자 하였다. 이를 위해 표준 baseline 모델, 1 trillion 토큰으로 훈련된 모델 및 T5-Base를 비교하는 실험을 수행하였다. 이 두 모델의 성능 비교를 통해, 체계적인 연구에서 얻은 통찰력이 어떤 영향을 미쳤는지 구체적으로 측정할 수 있다.\n세 가지 모델 구성의 성능은 T5-Base가 모든 downstream task에서 baseline-1T을 월등히 능가함을 보여준다. 이는 T5의 성공에 크기 뿐만 아니라 다른 비확장 요소들도 기여하고 있음을 나타낸다. 추가적인 사전 학습은 모델의 성능을 향상시키는 데 도움이 된다.\nReflection 이 분야의 더 나은 발전을 위해 효과적인 접근법을 제안하고자 한다.\nTakeaways Text-to-text Text-to-text 프레임워크는 다양한 텍스트 작업에 대해 단일 모델을 훈련시키는 간단한 방법을 제공한다. 이 방법은 생성, 분류, 회귀 작업 등에 성공적으로 적용될 수 있다. 간결함에도 불구하고, 이 프레임워크는 과제별 구조와 비교할 만한 성능을 보여주고, 규모와 결합하면 state-of-the-art를 달성한다.\nArchitectures NLP의 전이 학습에서 Transformer의 구조적 변형을 고려했지만, 원래의 encoder-decoder 형태가 text-to-text 프레임워크에서 가장 효과적이었다. encoder-decoder 모델은 더 많은 parameter를 사용하지만, 계산 비용은 비슷하다. encoder와 decoder에서 parameter를 공유하면 전체 parameter 수가 절반으로 줄어들지만 성능 저하는 별로 없다.\nUnsupervised objectives 대부분의 \u0026ldquo;denoising\u0026rdquo; 목표, 즉 임의로 손상된 텍스트를 재구성하는 학습은 text-to-text 설정에서 비슷한 성능을 보였다. 따라서, 계산 효율성을 위해 짧은 대상 시퀀스를 생성하는 목표를 사용하는 것이 좋다.\nData sets Common Crawl 웹 덤프에서 정리한 \u0026ldquo;Colossal Clean Crawled Corpus (C4)\u0026ldquo;를 소개하였다. 레이블이 없는 데이터로 학습하면 몇몇 downstream task에서 성능이 향상될 수 있지만, 데이터셋 크기가 줄어들 수 있다. 레이블이 없는 데이터셋이 작아서 여러 번 반복되면 성능이 저하될 수 있으므로, 크고 다양한 데이터셋인 C4의 사용이 중요하다는 것을 보여주었다.\nTraining strategies 사전 학습된 모델의 모든 parameter를 미세 조정하는 방식이 비록 비용이 많이 들지만 더 우수한 성능을 보여주었다. 여러 작업을 동시에 학습하는 방법을 시도했지만, 특정 작업에 대한 학습 비율을 설정하는 전략을 찾지 못하였다. 하지만, 여러 작업의 혼합에서 사전 학습 후 미세 조정하는 것이 비지도 사전 학습과 비슷한 성능을 보여주었다.\nScaling 추가 계산을 활용하는 다양한 전략을 비교한 결과, 각 방법이 성능을 크게 향상시켰으나, 더 많은 데이터로 작은 모델을 학습하는 것이 종종 더 적은 단계로 큰 모델을 학습하는 것보다 성능이 떨어졌다. 그러나 모델의 앙상블은 단일 모델보다 더 좋은 결과를 제공하였고, 같은 baseline 모델에서 미세 조정한 앙상블은 모든 모델을 별도로 학습하는 것보다는 성능이 떨어졌지만, 단일 모델보다는 훨씬 더 우수하였다.\nPushing the limits state-of-the-art를 얻기 위해 큰 모델(최대 11B 개의 parameter)을 학습시켰다. 이를 위해 C4 데이터 세트에서 텍스트를 추출하고, 노이즈를 제거하는 목표를 적용하였다. 또한, 1 trillion 이상의 토큰에 대해 학습을 진행하였고, 결과를 쉽게 복제하고 확장하고 적용하기 위해 우리의 코드, C4 데이터 세트, 그리고 각 T5 변형에 대한 사전 학습된 모델 가중치를 공개하였다.\nOutlook The inconvenience of large models 큰 모델이 더 좋은 성능을 내는 경향이 있다는 것이 중요한 결과로 나타났다. 하지만 client-side inference이나 federated learning과 같이 작은 모델이 도움이 되는 경우도 있다. low-resource 작업에서 좋은 성능을 얻는 것이 전이 학습의 한 가지 유익한 사용처이다. 따라서 저렴한 모델로 더 강한 성능을 달성하는 방법에 대한 연구를 지지한다. 이런 연구로는 distillation, parameter sharing, 그리고 conditional computation이 있다.\nMore efficient knowledge extraction 사전 학습의 목표는 모델에 \u0026ldquo;knowledge\u0026quot;를 제공하여 downstream task의 성능을 향상시키는 것이다. 현재 일반적으로 사용되는 방법은 텍스트의 오염된 부분을 복원하도록 학습시키는 것인데, 이 방법이 모델에 일반 지식을 가르치는 가장 효율적인 방법이 아닐 수도 있다. 더 효율적인 방법으로는, 실제 텍스트와 기계 생성 텍스트를 구분하도록 모델을 사전 학습시키는 방법이 있다.\nFormalizing the similarity between tasks 도메인 데이터에 대한 사전 학습이 downstream task의 성능을 향상시키는 것을 확인하였다. 레이블이 없는 데이터 소스를 선택하는 데에 더 원칙적인 접근을 가능하게 하기 위해, 사전 학습과 downstream task 사이의 \u0026ldquo;similarity\u0026quot;에 대한 엄밀한 개념을 정립하는 것이 필요하다. 이는 컴퓨터 비전 분야에서 이미 일부 연구가 이루어지고 있다. 또한, 작업 간의 관련성에 대한 더 나은 이해는 지도 사전 학습 작업을 선택하는 데에도 도움이 될 수 있다.\nLanguage-agnostic models 영어로만 사전 학습한 결과가 번역 작업에서 최고 수준의 성과를 내지 못하였다. 이를 해결하기 위해, 어떤 언어의 텍스트든 좋은 성능으로 NLP 작업을 수행할 수 있는 언어에 구애받지 않는 모델을 더 연구하려고 한다. 이는 세계 인구 대다수의 모국어가 영어가 아닌 점을 고려하면 매우 중요한 이슈이다.\n이 논문은 최근 NLP에 대한 전이 학습에 대해 연구하였다. 이 연구가 시작되기 전, 학습 기반 방법이 효율성을 증명받지 못 한 상황에서 이러한 진보가 돌파구를 만들어 주었으며, 특히 전이 학습에 어려운 SuperGLUE 벤치마크에서 거의 인간 수준의 성능을 달성하였다. 이 결과는 우리의 text-to-text 프레임워크, 새로운 C4 데이터셋, 그리고 체계적인 연구에서의 통찰력의 결합에서 비롯된다.\nReference Paper Github ","date":"2023-12-18T00:00:00Z","permalink":"https://kurtkim.github.io/p/t5/","title":"T5"},{"content":"Abstract 언어 모델링에서 큰 transformer 모델의 학습은 자연어 처리 분야에서 state-of-the-art를 달성하였다. 하지만, 이런 큰 모델은 메모리 제약으로 학습이 어려울 수 있다. 이 연구에서는 수십억 개의 파라미터를 가진 transformer 모델을 학습시키는 방법을 제시한다.\n이 연구는 512개의 GPU를 사용하여 최대 83억 개의 파라미터를 가진 transformer 모델을 학습시키는 것에 성공하였다. 또한 이 모델을 사용하여 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다. BERT와 같은 모델에서는 layer normalization의 위치에 주의가 필요함을 확인하였다.\nIntroduction Natural Language Processing (NLP)는 컴퓨팅능력과 데이터셋 크기의 증가로 빠르게 발전하고 있다. 이로 인해 더 큰 언어 모델을 학습시키는 것이 가능해졌고, 이는 기사 완성, 질문 답변, 자연어 추론 등의 NLP 작업에 매우 유용하다. 이런 사전 학습된 언어 모델을 다른 자연어 작업에 미세 조정하면, state-of-the-art를 얻을 수 있다.\n모델이 커짐에 따라, 메모리 한계를 초과하여 추가적인 메모리 관리 기법이 필요해진다. ADAM 같은 optimization 알고리즘들은 모멘텀과 다른 최적화 상태를 저장하기 위해 추가 메모리를 요구하며, 이는 효과적으로 학습될 수 있는 모델의 크기를 줄인다. 이를 해결하기 위해, 모델 병렬화 접근법이 사용되며, 이는 가중치와 그들과 관련된 최적화 상태가 동시에 프로세서에 존재할 필요가 없도록 모델을 분할한다. 그러나, 이러한 접근법은 모델을 다시 작성하고, 아직 개발 중인 사용자 정의 컴파일러와 프레임워크에 의존하는 문제가 있다.\n이 연구에서는 내부 layer 모델 병렬화를 이용한 단순하고 효율적인 모델 병렬 접근법을 구현하였다. transformer 기반 언어 모델의 내재적 구조를 활용해 PyTorch에서 효율적으로 학습하는 모델 병렬 구현을 만들었으며, 이는 사용자 정의 C++ 코드나 컴파일러를 필요로 하지 않는다. 이 접근법은 GPipe와 같은 파이프라인 기반 모델 병렬화와는 별개이다.\n이 접근법의 확장성을 보여주기 위해, 단일 NVIDIA V100 32GB GPU에서 1.2B parameter의 모델을 학습하여 39 TeraFLOPs를 유지하는 강력한 기준선을 설정하였다. 모델을 8.3B parameter로 확장하고 512개의 GPU에서 8-way 모델 병렬화를 사용하면, 전체 애플리케이션에서 초당 최대 15.1 PetaFLOPs를 달성하며, 이는 단일 GPU 사례에 비해 76%의 확장 효율성을 보여준다.\n모델 크기가 정확도에 미치는 영향을 분석하기 위해, GPT-2와 BERT를 학습시키고 여러 downstream task에서 평가하였다. 기존의 BERT 구조는 모델 크기가 증가함에 따라 성능이 저하되는 것을 확인하였다. 이를 극복하기 위해, transformer layer의 layer normalization과 residual connection을 rearranging하였고, 이로 인해 모델 크기가 증가함에 따라 downstream task 결과가 단조롭게 향상되는 것을 확인하였다. 또한, 모델이 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다.\n요약하면, 이 논문의 기여는 다음과 같다:\n기존의 PyTorch transformer 구현에 몇 가지 목표적인 수정만을 가함으로써, 단순하고 효율적인 모델 병렬 접근법을 구현했다. 모델과 데이터 병렬 기법에 대한 심층적인 경험적 분석을 수행하고, 512개의 GPU를 사용하여 최대 76%의 확장 효율성을 보여준다. BERT와 유사한 모델에서 layer normalization의 위치에 신중하게 주의를 기울이는 것이 모델이 커짐에 따라 정확도를 높이는 데 중요하다는 것을 보여준다. 모델 크기를 확장하는 것이 GPT-2(최대 8.3B parameter까지 연구)와 BERT(최대 3.9B parameter까지 연구) 모델 모두에 대해 정확도를 향상시키는 것을 보여준다. 테스트 세트에서 state-of-the-art를 달성하는 것을 보여준ㄴ다: WikiText103에서의 혼란도(10.8 ppl), LAMBADA에서의 정확도(66.5%), 그리고 RACE에서의 정확도(90.9%). Background and Challenges Neural Language Model Pretraining 사전 학습된 언어 모델은 NLP 연구에 필수적인 도구가 되었다. 큰 규모의 말뭉치 사전 학습을 활용하여 언어의 견고한 신경 표현을 배우는 것은 활발한 연구 분야이다. 초기의 연구는 사전 학습된 단어 임베딩이 downstream task 결과를 향상시키는 것을 보여주었으며, 이후의 연구는 맥락적 단어 표현을 포착하는 신경 모델을 학습하고 전이하는 것을 통해 발전하였다. 최근의 연구는 언어 모델을 end-to-end로 미세 조정함으로써 이 아이디어들을 더욱 발전시켰다. 이런 방법들의 진보는 규모에 맞게 효율적으로 작동하고 늘어나는 계산 요구를 충족시킬 수 있는 도구의 필요성을 촉발하였고, 이 연구는 트렌드에서 한 걸음 더 나아가기 위한 도구를 제공하려고 한다.\nTransformer Language Models and Multi-Head Attention 현재 NLP 연구는 우수한 정확도와 계산 효율성 때문에 transformer 모델을 사용하는 경향이 있다. transformer는 원래 두 부분, encoder와 decoder를 사용하는 기계 번역 아키텍처로 설계되었지만, 최근의 연구는 필요에 따라 encoder나 decoder만 사용한다. 이 연구는 decoder 구조인 GPT-2와 encoder 구조인 BERT를 모두 연구한다.\nGPT-2와 BERT는 모두 GeLU nonlinearities와 layer normalization을 multi-head attention과 eed forward layer의 입력에 적용하는 반면, 원래의 transformer는 ReLU nonlinearities를 사용하고 layer normalization를 출력에 적용한다.\nData and Model Parallelism in Deep Learning 신경망 학습을 여러 하드웨어 가속기로 확장하는 두 가지 주요 방법은 데이터 병렬화와 모델 병렬화이다. 데이터 병렬화는 학습 미니배치를 여러 작업자에게 분할하고, 모델 병렬화는 모델의 메모리 사용량과 계산을 여러 작업자에게 분배한다. 사용 가능한 작업자 수에 비례하여 미니배치 크기를 증가시키면 학습 데이터 처리량에서 거의 선형적인 확장을 볼 수 있다. 하지만 대량 배치 학습은 최적화 과정에 복잡성을 도입하여 정확도를 감소시키거나 수렴 시간을 늘릴 수 있다. 추가로, 데이터 병렬화를 활성화 체크포인팅과 결합하여 메모리 요구사항을 줄이는 방법도 연구되고 있다.\n기존 기법들은 모델이 한 작업자에게 완전히 맞아야 하는 제한이 있다. 크기와 복잡성이 증가하는 언어 모델로 인해, 신경망은 하드웨어 가속기의 메모리 용량에 근접하게 되었다. 이 문제를 해결하기 위한 한 가지 방법은 parameter 공유를 사용하는 것이지만, 이는 모델의 전체 용량을 제한한다. 이 연구의 접근법은 모델 병렬화를 사용하여 모델을 여러 가속기에 분할하는 것으로, 이는 메모리 압박을 완화하고 병렬성을 증가시킨다.\n모델 병렬화에는 layer-wise pipeline parallelism과 distributed tensor computation이라는 두 가지 패러다임이 있다. pipeline model parallelism에서는 한 장치에서 작업 그룹이 수행된 후 출력이 다음 장치로 전달된다. 일부 접근법은 parameter 서버를 사용하지만 일관성 문제가 있다. TensorFlow의 GPipe 프레임워크는 동기식 경사 하강법을 사용하여 이 문제를 해결한다. 그러나 이 방법은 통신과 계산 작업의 효율적인 파이프라이닝을 위한 추가 로직이 필요하며, pipeline bubble이나 최적화 변경으로 인해 효율성과 정확도에 영향을 미친다.\ndistributed tensor computation은 텐서 연산을 여러 장치에 분할하여 계산을 가속화하거나 모델 크기를 증가시키는 방법이다. FlexFlow는 이러한 병렬 계산을 효과적으로 수행하는 방법을 제공한다. 최근에는 Mesh-TensorFlow가 TensorFlow에서 분산 텐서 계산을 지정하는 언어를 도입했다. 우리는 이러한 통찰력을 활용하여 transformer의 attention head를 계산하는 병렬성을 활용하여 transformer 모델을 병렬화한다. 하지만, 이 연구는 프레임워크와 컴파일러를 구현하는 대신, 기존의 PyTorch transformer 구현에 몇 가지 특정 수정을 수행한다. 이 방법은 간단하며, 새로운 컴파일러나 코드 재작성이 필요하지 않는다.\nModel Parallel Transformers transformer network의 구조를 활용해, 몇 가지 synchronization primitive를 추가하여 간단한 모델 병렬 구현을 만들었다. transformer layer는 self attention block과 two-layer, multi-layer perceptron (MLP)으로 구성되며, 이 두 부분에 모델 병렬성을 도입하였다.\nMLP block의 첫 번째 부분은 GEMM이며, 이어서 GeLU 비선형성을 따른다:\n$$ Y = GeLU(XA) $$\nGEMM을 병렬화하는 한 가지 방법은 가중치 행렬 $A$를 행 방향으로, 입력 $X$를 열 방향으로 분할하는 것이다:\n$$ X = [X_1, X_2], A = \\begin{bmatrix} A_1 \\\\ A_2 \\end{bmatrix} $$\n이 분할 방식은 결과로 $Y = GeLU(X_1 A_1 + X_2 A_2)$를 가져오며, GeLU는 비선형 함수이므로, $GeLU(X_1 A_1 + X_2 A_2) \\neq GeLU(X_1 A_1) + GeLU(X_2 A_2)$이다. 따라서 이 방식은 GeLU 함수 앞에 동기화 지점이 필요하게 된다. 각 처리 유닛이 독립적으로 계산한 결과를 제대로 합산하기 위해 병렬 처리 유닛 간에 데이터 동기화가 필요하다.\n다른 옵션은 $A$를 열을 따라 분할하는 것이다. $A = [A_1, A_2]$. 이 분할 방식은 GeLU 비선형성을 각 분할된 GEMM의 출력에 독립적으로 적용할 수 있게 한다:\n$$ [Y_1, Y_2] = [GeLU(XA_1), GeLU(XA_2)] $$\n이 방법은 동기화 지점을 제거하므로 유리하다. 첫 번째 GEMM을 열 병렬 방식으로 분할하고, 두 번째 GEMM을 행으로 분할하여 GeLU 계층의 출력을 직접 받을 수 있도록 한다. 이 방식은 추가적인 통신 없이 MLP 블록의 두 GEMM을 GPU 간에 분할하며, forward path와 backward path에 각각 단 한 번의 all-reduce 연산만 필요로 한다. 이 두 연산은 서로 conjugate 관계에 있으며, PyTorch에서 간단하게 구현할 수 있다.\nself attention block에서는 multihead attention 연산의 병렬성을 활용하여 key(K), query(Q), value(V)와 관련된 GEMM을 열 병렬 방식으로 분할한다. 이 방식은 각 attention head에 해당하는 행렬 곱셈을 각각의 GPU에서 수행하게 하며, immediate communication이 필요 없다. 이어서, 출력 linear layer에서의 GEMM은 행 병렬 방식으로 수행되며, GPU 간의 통신 없이 병렬 주의 계층의 출력을 직접 받아들인다. 이 접근법은 MLP와 self attention layer에서 두 GEMM의 그룹을 융합하고, 중간의 동기화 지점을 제거하여 더 나은 확장성을 제공한다. 이를 통해 forward path와 backward path에서 각각 두 번의 all-reduce 연산만으로 모든 GEMM을 수행할 수 있다.\ntransformer 언어 모델은 출력 임베딩을 병렬화하여 처리 속도를 향상시킨다. 이 모델은 입력 임베딩과 가중치를 공유하는 출력 임베딩 계층을 가지며, 이 가중치 행렬을 분할하여 병렬 처리한다. 그러나 이 방식은 큰 어휘 크기 때문에 많은 양의 정보를 전송해야 한다. 이를 해결하기 위해, 병렬 GEMM의 출력을 cross entropy loss와 결합하여 차원을 줄인다. 이렇게 함으로써 스칼라 손실만 전송하게 되어 통신의 양이 크게 감소하고, 모델의 병렬 처리 효율성이 향상된다.\n모델 병렬 방법론은 통신을 줄이고 GPU 계산에 초점을 맞추는 기법에 중점을 두고 있다. dropout, layer normalization, residual connection의 계산을 한 GPU에서만 수행하는 대신, 이를 모든 GPU에 복제한다. 각 GPU는 layer normalization parameter의 복제본을 유지하며, 모델 병렬 영역의 출력에서 dropout과 residual connection을 수행한다. 또한, 각 모델 병렬 작업자는 자신의 parameter 집합을 독립적으로 최적화한다. 이러한 접근법은 모든 값이 각 GPU에 로컬로 있거나 복제되므로, 업데이트된 parameter 값을 통신할 필요가 없다.\n이 연구의 방법론은 하이브리드 모델과 데이터 병렬성, 그리고 난수 생성 처리와 관련이 있다. 이는 구현이 간단하며, forward와 backward pass에 몇 가지 추가적인 all-reduce 연산만 필요로 한다. 컴파일러는 필요 없으며, 이는 기존의 파이프라인 모델 병렬화 방법과는 별개이며 이를 보완한다.\nSetup 자연어 처리와 언어 이해의 핵심은 사전 학습된 언어 이해 모델이다. 이 연구에서는 왼쪽에서 오른쪽으로 텍스트를 생성하는 GPT-2와 언어 모델 마스킹에 기반한 bi-directional transformer 모델인 BERT에 초점을 맞추고 있다.\nTraining Dataset 다양한 대형 언어 모델링 데이터셋을 합쳐서 longterm dependency를 가진 학습 세트를 만들었다. 이에는 Wikipedia, CC-Stories, RealNews, OpenWebtext 등이 포함되어있다. 학습 세트의 유출을 방지하기 위해 일부 Wikipedia 기사와 필요 없는 새 줄을 제거하였다. BERT 모델에는 BooksCorpus를 포함시켰지만, LAMBADA 작업과 겹치는 부분 때문에 GPT-2 학습에서는 제외하였다.\n모든 데이터셋을 병합하고, 내용 길이가 128 토큰 미만인 문서를 제외하였다. 유사한 내용의 중복을 제거하기 위해 localitysensitive hashing (LSH)을 사용했고, 그 결과 174GB의 중복 제거된 텍스트를 포함한 말뭉치를 얻었다.\nTraining Optimization and Hyperparameters 효율적인 학습을 위해 mixed precision 학습과 dynamic loss scaling을 사용하였다. 가중치는 정규 분포로 초기화하고, residual layer 전에 조정했다. optimizer는 Adam을 사용하고, weight decay를 적용했다. gradient norm clipping을 사용해 학습의 안정성을 개선했고, 모든 경우에 dropout 0.1을 적용했다. 마지막으로, 메모리 관리를 위해 utilize activation checkpointing을 사용했다.\nGPT-2 모델은 1024개의 subword 단위로 300k번 반복하며 학습되며, batch size는 512이다. learning rate는 1.5e-4로 설정되어 있고, 3k번의 warmup 이후에 cosine decay를 따른다. 이 감소는 최소 학습률인 1e-5에서 멈춘다.\nBERT 모델은 원래의 BERT 사전을 사용하고, 어휘 크기는 30,522이다. next sentence prediction을 sentence order prediction으로 대체하고, 전체 단어 n-그램 마스킹을 사용하였다. batch size는 1024로 설정하고, warmup된 learning rate을 사용하여 2백만 번의 반복 동안 선형적으로 감소시켰다. 나머지 학습 parameter는 기존 BERT 모델과 동일하게 유지하였다.\nExperiments 모든 실험은 최대 32대의 DGX-2H 서버(총 512개의 Tesla V100 GPU)를 사용한다. 이 인프라는 서버 내부 GPU 간 300 GB/sec, 서버 간 100 GB/sec의 빠른 연결 대역폭을 제공하여 딥러닝 애플리케이션에 최적화되어 있다.\nScaling Analysis 구현의 확장성을 테스트하기 위해, 다양한 parameter를 가진 GPT-2 모델을 사용했다. self attention layer에서 일관된 GEMM 크기를 유지하기 위해, attention head 당 hidden size는 96으로 고정하였다. 원래의 어휘 크기는 50,257이었지만, logit layer의 효율적인 GEMM을 위해 어휘를 패딩하여 51,200으로 만들었다.\n모델 및 모델 + 데이터 병렬 확장성을 연구했으며, 모든 구성에서 배치 크기는 8로 고정하였다. 또한 모든 실험에서 전역 배치 크기를 512로 고정하여 데이터 병렬 확장성을 연구하였다. 이는 64-way 데이터 병렬성에 해당한다.\nModel And Data Parallelism 모델 병렬 및 모델 + 데이터 병렬 케이스에 대해 모델 parameter에 대한 약한 스케일링을 보여준다. 약한 스케일링은 배치 크기를 조정하여 수행되지만, 이는 단일 GPU에 맞지 않는 대형 모델을 학습하는 문제를 해결하지 못한다. 따라서 여기서는 그렇지 않으면 불가능했던 더 큰 모델을 학습하기 위해 약한 스케일링을 사용한다. 모든 스케일링 수치의 기준은 단일 GPU에서 실행되는 1.2억 개의 parameter를 가진 첫 번째 구성이다.\n8.3B 개의 parameter와 8-way 모델 병렬성을 가진 경우 선형 스케일링의 77%를 달성하였다. 가장 큰 구성(8.3B 개의 parameter)이 512개의 GPU에서 실행되는 경우에도 선형 스케일링 대비 74%의 스케일링을 달성하였다.\nLanguage Modeling Results Using GPT-2 거대 언어 모델이 최첨단을 더욱 발전시킬 수 있음을 보여주기 위해, 다양한 크기와 구성의 GPT-2 모델을 학습하였습니다. 355M 모델은 BERT-Large 모델과 동일하며, 2.5B 모델은 이전의 가장 큰 GPT-2 모델보다 크고, 8.3B 모델은 우리가 알고 있는 한까지 학습된 어떤 변환기 언어 모델보다 크다.\n각 epoch를 진행하는데 걸리는 시간은 68,507회의 반복과 동일하며, 예를 들어, 512개의 GPU에서의 8.3B 모델에 대해서는 각 epoch이 약 두 일 정도 걸린다. 이들 모델은 이전에 본 것보다 훨씬 작지만 여전히 64개의 GPU로 학습하며, epoch 당 시간이 훨씬 적다.\n모델 크기가 증가함에 따라 검증 perpelixity가 감소하는 것을 확인할 수 있다. 특히, 8.3억 개의 parameter를 가진 모델은 검증 perpelixity가 9.27에 이르렀다.\n또한, 모델 크기를 증가시키면 WikiText103에서의 perpelixity가 낮아지고, LAMBADA에서의 클로즈 정확도가 높아지는 추세를 관찰한다. 이 중 8.3억 개의 parameter를 가진 모델은 WikiText103 테스트 세트에서 state-of-the-art perpelixity를 달성하고, LAMBADA 작업에서 이전의 클로즈 정확도 결과를 초과하였다.\n최근에는 Microsoft와 NVIDIA가 협력하여 170억 개의 parameter를 가진 GPT-2 모델인 Turing-NLG를 학습시켰으며, 이 결과는 더 큰 모델의 가치를 강조하였다.\n테스트 데이터가 학습 데이터에 포함되지 않도록 확인하기 위해, 테스트 세트의 8-gram 중 학습 세트에도 나타나는 비율을 계산하였다. WikiText103 테스트 세트는 최대 10.8%의 겹침이 있고, LAMBADA 테스트 세트는 최대 1.4%의 겹침이 있었다. 이는 이전 연구와 일관되어, 테스트 데이터가 우연히 학습 데이터에 포함되지 않았음을 확신하였다.\nBi-directional Transformer Results Using BERT BERT 스타일의 transformer 모델에 방법론을 적용하고, 다양한 downstream task에 대한 모델 스케일링 효과를 연구한다. 이전 연구에서는 BERT-large의 336M parameter를 넘어서 모델 크기를 증가시키면 모델 저하가 발생한다는 것을 발견하였다. 이 문제를 해결하기 위해, 연구자들은 parameter 공유를 도입하고, 이를 통해 모델이 원래 BERT 모델에 비해 더 잘 확장되는 것을 보여주었다.\nlayer normalization과 residual connection의 순서를 재배열하는 것이 BERT-Large를 넘어서 BERT 스타일 모델의 스케일링을 가능하게 하는 것이 중요하다는 것을 경험적으로 입증하였다. (b) 아키텍처는 원래 BERT 아키텍처에서 관찰된 불안정성을 제거하며, 더 낮은 학습 손실을 가진다. 이러한 변화가 더 큰 BERT 모델을 학습시키는 것을 가능하게 하는 것을 처음으로 보고하였다.\n336M 모델은 BERT-large와 같은 크기이며, 1.3B는 이전에 더 나쁜 결과를 얻었다고 알려진 BERT-xlarge 구성과 동일하다. 더 큰 hidden size와 더 많은 layer를 사용하여 BERT 모델을 더 확장하여 3.9B parameter 경우에 도달하였다. 모든 경우에서 hidden size는 attention head 당 64로 일정하게 유지되었다. 336M과 1.3B 모델은 200만 번 반복하여 학습되었으며, 3.9B 모델은 150만 번 반복하여 학습되고 아직도 학습 중이다.\n3%의 홀드아웃 세트에서, 336M, 1.3B, 3.9B 모델은 각각 1.58, 1.30, 1.16의 검증 세트 perplexity를 달성하였고, 이는 모델 크기와 함께 단조롭게 감소하는 추세를 보여준다. 여러 downstream task에서 학습된 모델을 미세 조정한 결과, 모델 크기가 증가함에 따라 모든 경우에서 성능이 향상되었다. 특히, 3.9B 모델은 다른 BERT 기반 모델에 비해 개발 세트에서 state-of-the-art를 보여주며, RACE 테스트 세트에서 단일 모델과 앙상블 모델 모두에서 최고의 결과를 달성하였다.\nConclusion and Future Work 이 연구에서는 기존 PyTorch transformer에 적은 수정을 통해 모델 병렬성을 구현하고, 이를 통해 전통적인 단일 GPU-모델 학습의 한계를 극복하였다. 512개의 NVIDIA V100 GPU에서 8.3B parameter를 가진 transformer 모델을 효율적으로 학습시켰고, BERT 모델에서는 layer normalization의 위치에 주의를 기울이는 것이 중요하다는 것을 확인하였다. 또한, 모델 크기가 down-tream task의 정확도에 긍정적인 영향을 미침을 확인했고, WikiText103, LAMBADA, RACE 데이터셋에서 state-of-the-art를 달성하였다.\n미래의 연구 방향은 사전 학습의 규모 증가, 최적화 도구의 효율성과 메모리 사용량 개선, 더 큰 모델의 학습을 위한 병렬화 방법 개선, 다른 모델 패밀리(XLNet, T5)의 사전 학습, 다양한 downstream task에 대한 거대 모델의 성능 평가, 그리고 대형 사전 학습된 모델로부터 작은 모델을 학습시키는 knowledge distillation 사용 등이 있다.\nReference Paper Github ","date":"2023-12-16T00:00:00Z","permalink":"https://kurtkim.github.io/p/megatron-lm/","title":"Megatron-LM"},{"content":"Abstract 자연어 표현을 사전 학습할 때 모델 크기를 늘리면 종종 성능이 향상되지만, GPU/TPU 메모리 한계와 학습 시간이 길어지는 문제가 있다. 이를 해결하기 위해, BERT의 메모리 소비를 줄이고 훈련 속도를 높이는 두 가지 기법을 제시하였다. 이 기법은 원래의 BERT보다 훨씬 더 잘 확장되는 모델을 만들며, 문장 간 일관성을 모델링하는 self-supervised loss를 사용하여 다문장 입력 작업에 도움이 된다. 결과적으로, BERT-large보다 parameter가 적은 ALBERT 모델은 GLUE, RACE, SQuAD benchmark에서 state-of-the-art를 달성하였다.\nIntroduction Full network pre-training는 언어 학습에서 중요한 돌파구를 이끌어내었다. 이를 통해 학습 데이터가 제한된 NLP 작업들이 큰 도움을 받았다. 중국의 영어 시험을 위한 RACE 테스트에서 기계 성능은 초기 44.1%에서 최근에는 83.2%로 상승하였다. 이 연구를 통해 성능은 더욱 향상되어 89.4%에 이르렀다. 이는 주로 고성능 사전 학습 언어 표현 능력의 발전 덕분이다.\n이러한 개선사항들은 큰 네트워크가 state-of-the-art를 달성하는 데 중요함을 보여준다. 이제는 큰 모델을 사전 학습하고 작은 모델로 축소하는 것이 일반적인 방법이다. 그래서 이 연구에서는 \u0026ldquo;더 나은 NLP 모델을 가지는 것이 큰 모델을 가지는 것만큼 쉬운 것인지\u0026quot;라는 질문을 던지게 된다.\n이 질문에 대한 장애물은 사용 가능한 하드웨어의 메모리 제한이다. 현재의 state-of-the-art 모델들은 종종 수백만 개 또는 심지어 수십억 개의 parameter를 가지고 있어, 모델을 확장하려고 할 때 이러한 제한에 쉽게 부딪히게 된다. 또한, parameter 수에 비례하는 통신 오버헤드로 인해 분산 훈련시 훈련 속도가 크게 저하될 수 있다.\n이 논문에서는 매개변수가 더 적은 ALBERT 구조를 설계하여 이 문제들을 모두 해결하였다.\nALBERT는 사전 학습된 모델 확장의 주요 장애물을 제거하는 두 가지 parameter 축소 기법을 사용한다. 첫 번째는 큰 어휘 임베딩 행렬을 작은 행렬로 분해하는 것, 두 번째는 네트워크 깊이에 따른 parameter 증가를 방지하는 것이다. 이 두 기법은 BERT의 매개변수 수를 크게 줄이고, BERT-large와 유사한 ALBERT 구성은 매개변수가 18배 더 적고, 약 1.7배 더 빠르게 훈련될 수 있게 한다. 이 기법들은 훈련을 안정화시키고 일반화에 도움을 주는 정규화 역할도 한다.\nALBERT의 성능을 더욱 향상시키기 위해, sentence-order prediction(SOP)을 위한 self-supervised loss도 도입하였다. SOP는 주로 문장 간의 일관성에 초점을 맞추고 있으며, 원래 BERT에서 제안된 다음 next sentence prediction(NSP) loss의 비효율성을 해결하도록 설계되었다.\n그 결과, BERT-large보다 parameter는 더 적지만 성능은 훨씬 뛰어난 더 큰 ALBERT 구성으로 확장할 수 있었다. 그리고 자연 언어 이해를 위한 잘 알려진 GLUE, SQuAD, RACE 벤치마크에서 state-of-the-art를 달성하였다.\nRelated Work Scaling up Representation Learning for Natural Language 자연 언어의 표현 학습은 다양한 NLP 작업에 유용하며 널리 채택되어왔다. 최근 가장 큰 변화는 단어 임베딩의 사전 학습에서, 전체 네트워크의 사전 학습 후 작업 특정 미세 조정으로 전환되었다는 것이다. 이러한 연구에서는 더 큰 모델 크기가 성능을 향상시키는 것이 종종 보여졌다. 하지만, 모델 크기와 계산 비용 문제로 인해 hidden size 는 1024에서 멈추게 되었다.\n큰 모델로 실험하는 것은 계산적 제약, 특히 GPU/TPU 메모리 제한 때문에 어렵다. 이 문제를 해결하기 위한 여러 방법이 제안되었는데, 그 중에는 gradient checkpointing과 각 layer의 activation 재구성 방법이 있다. 이런 방법들은 속도비용으로 메모리 사용량을 줄인다. 반면에, parameter reduction techniques는 메모리 사용을 줄이면서 훈련 속도를 높인다.\nCross-Layer Parameter Sharing cross-layer parameter sharing의 아이디어는 이전에 Transformer 아키텍처에서 제안되었다. 하지만 이전 연구는 표준 인코더-디코더 작업에 초점을 맞추어져있었다. Dehghani et al. (2018)은 cross-layer parameter sharing를 가진 네트워크가 언어 모델링에서 더 나은 성능을 낸다고 보여주었다. 최근에는, Bai et al. (2019)이 Transformer 네트워크를 위한 Deep Equilibrium Model을 제안하였다. Hao et al. (2019)은 parameter-sharing transformer와 standard transformer를 결합하여 parameter 수를 늘렸다.\nSentence Ordering Objectives ALBERT는 두 연속된 텍스트 세그먼트의 순서를 예측하는 사전 학습 loss를 사용한다. 이는 담화의 일관성에 관련된 다른 사전 학습 목표와 유사하다. 대부분의 효과적인 목표는 매우 단순하며, 이웃하는 문장의 단어를 예측하는 것에 기반한다. loss는 두 연속된 문장의 순서를 결정하기 위해 학습된 문장 임베딩과 가장 관련이 있다. 하지만, loss는 문장이 아닌 텍스트 세그먼트에 적용된다. BERT는 한 쌍에서 두 번째 세그먼트가 다른 문서의 세그먼트와 바뀌었는지 예측하는 loss을 사용하고, 이는 NSP가 더 도전적인 사전 학습 작업이며 특정 downstream task에 더 유용하다는 것을 보여준다.\nThe Elements of ALBERT ALBERT의 설계 결정사항을 제시하고, BERT 아키텍처와 비교한다.\nModel Architecture Choices ALBERT 아키텍처는 Transformer 인코더를 GELU 비선형성과 함께 사용하는 BERT와 유사하다. vocabulary embedding size는 $E$, encoder layer의 수는 $L$, hidden size는 $H$로 표기하며, feed-forward/ﬁlter size는 $4H$, attention head의 수는 $H/64$로 설정한다.\nALBERT는 이러한 BERT의 설계에 대해 세 가지 주요한 개선점을 제시한다.\nFactorized embedding parameterization. BERT, XLNet, RoBERTa 등의 모델에서는 WordPiece 임베딩 크기($E$)와 히든 레이어 크기($H$)가 같게 설정이 되었다. 이러한 결정은 모델링과 실용적인 측면에서 최적이 아닌 것으로 판단된다.\n모델링 측면에서 보면, WordPiece 임베딩은 문맥에 독립적인 표현을, 히든 레이어 임베딩은 문맥에 의존적인 표현을 학습한다. BERT와 같은 표현의 힘은 문맥을 이용한 학습에서 나온다는 것이 확인되었다. 따라서 WordPiece 임베딩 크기 $E$와 히든 레이어 크기 $H$를 분리하면, 모델링 요구사항에 따라 총 모델 parameter를 더 효율적으로 사용할 수 있다. 이는 실질적으로 $H$가 $E$보다 훨씬 커야 한다는 것을 의미한다.\n실용적으로 보면, 자연어 처리에서는 대체로 어휘 크기($V$)가 큰 편이다. 만약 $E$와 $H$가 같다면, $H$를 증가시키는 것은 임베딩 행렬의 크기를 증가시키게 되고, 이로 인해 parameter가 수십억 개가 되어버릴 수 있다. 더욱이, 이런 parameter들은 훈련 도중에는 대부분 희소하게만 업데이트된다.\nALBERT에서는 임베딩 파라미터를 두 개의 작은 행렬로 분해한다. one-hot vector를 $H$ 크기의 hidden space로 투영하는 대신, 먼저 $E$ 크기의 lower dimensional embedding space로 투영한 후 hidden space으로 투영한다. 이 방식을 통해 임베딩 파라미터를 $O(V \\times H)$에서 $O(V \\times E \\times E \\times H)$로 줄일 수 있다. 이 parameter 축소는 $H$가 $E$보다 훨씬 클 때 중요하며, 모든 워드피스에 대해 같은 $E$를 사용하게 된다.\nCross-layer parameter sharing ALBERT에서는 parameter 효율성을 향상시키는 방법으로 cross-layer parameter sharing를 제안한다. 여기에는 레이어 간에 feed-forward network (FFN) parameter만 공유하거나, attention parameter만 공유하는 등 여러 가지 방법이 있다. 그러나 ALBERT의 기본 설정은 모든 레이어 간에 모든 parameter를 공유하는 것이다. 이 설정은 특별히 명시되지 않는 한 모든 실험에서 사용된다.\nUniversal Transformer와 Deep Equilibrium Models에서도 Transformer 네트워크에 대해 유사한 전략들이 연구되었다. Universal Transformer가 일반 Transformer를 능가한다고 보여주었고, Deep Equilibrium Models가 특정 레이어의 입력과 출력 임베딩이 동일하게 유지되는 균형점에 도달함을 보여주었다. 반면 이 논문의 측정 결과에서는 임베딩들이 수렴하기보다는 진동하고 있다는 것을 보여준다.\nALBERT에서 레이어 간의 전환이 BERT보다 훨씬 부드럽다는 것을 관찰할 수 있다. 이는 parameter sharing이 network의 parameter를 안정화하는데 영향을 미친다는 것을 보여주며, 24개의 레이어 후에도 두 메트릭 모두 0으로 수렴하지 않는다.\nInter-sentence coherence loss BERT는 masked language modeling(MLM) loss 외에도 next-sentence prediction(NSP) 이라는 추가적인 loss을 사용한다. NSP는 두 세그먼트가 원문에서 연속으로 나타나는지 예측하는 binary classiﬁcation loss이다. 이 목표는 문장 쌍 간의 관계에 대한 추론을 요구하는 자연어 추론과 같은 downstream task의 성능을 향상시키기 위해 설계되었다. 그러나 이후 연구에서는 NSP의 영향이 불안정하다고 판단하여 제거하기로 하였고, 이 결정은 여러 작업에서 downstream task 성능의 향상으로 지지받았다.\nNSP의 비효율성 뒤에 있는 주요 이유는 MLM과 비교했을 때 작업의 난이도가 부족하다는 것으로, NSP는 주제 예측과 일관성 예측을 하나의 작업에서 혼동시킨다. 그러나 주제 예측은 일관성 예측보다 학습하기 쉽고, MLM 손실을 사용하여 학습하는 것과 더 많이 겹친다.\n이 연구에서는 문장 간 모델링이 중요하다고 주장고 있으며, ALBERT에서는 주로 일관성에 기반한 sentence-order prediction(SOP) loss을 사용한다. SOP 손실은 모델이 담화 수준의 일관성에 대한 더 세밀한 구분을 학습하도록 만든다. NSP는 SOP 작업을 전혀 해결할 수 없지만, SOP는 NSP 작업을 어느 정도 해결할 수 있다. 결과적으로, ALBERT 모델은 다문장 인코딩 작업에 대한 성능을 일관되게 향상시킬 수 있었다.\nModel setup ALBERT 모델은 설계 선택 때문에 BERT 모델보다 parameter 크기가 훨씬 작다. 예를 들어, ALBERT-large는 BERT-large보다 parameter가 약 18배 작다. 이런 parameter 효율성 향상은 ALBERT의 설계 선택에서 가장 중요한 장점이다.\nExperimental Results Experimental Setup 의미 있는 비교를 위해, BERT 설정을 따라서 사전 훈련 기본 모델에 대해 Book Corpus와 영문 Wikipedia를 사용한다. 이 두 말뭉치는 압축되지 않은 텍스트로 약 16GB를 구성한다. 입력은 $[CLS] x_1 [SEP] x_2 [SEP]$ 와 같은 포멧이며, 최대 512 길이로 제한한다. 그리고 10%의 확률로 512보다 짧은 입력 시퀀스를 무작위로 생성한다. BERT와 같이, 30,000의 어휘 크기를 가지며, XLNet처럼 SentencePiece를 사용하여 토큰화한다.\n각 n-gram 마스크의 길이를 무작위로 선택하여 n-gram 마스킹을 사용해 MLM 목표를 위한 마스크된 입력을 생성한다. 길이 n에 대한 확률은 다음과 같이 주어진다:\n$$p(n) = {{1/n}\\over{\\sum_{k=1}^{N}1/k}} $$\nn-gram의 최대 길이는 3으로 설정하며, 이는 MLM 목표가 최대 3-gram 단어로 구성될 수 있음을 의미한다. 모든 모델 업데이트는 배치 크기 4096과 학습률 0.00176의 $L_{AMB}$ optimizer를 사용하며, 모든 모델은 125,000 step 동안 학습된다. 훈련은 Cloud TPU V3에서 이루어졌으며, 사용된 TPU의 수는 모델 크기에 따라 64에서 512까지 다양했다. 이 실험 설정은 우리가 만든 모든 BERT 버전과 ALBERT 모델에 사용되었다.\nEvaluation Benchmarks Intrinsic Evalutation 학습 상황을 확인하기 위해, SQuAD와 RACE의 개발 세트를 기반으로 개발 세트를 만들었다. MLM과 문장 분류 작업의 정확도를 보고한다. 이 세트는 모델의 수렴 상태를 확인하는데만 사용되며, 모델 선택과 같은 방식으로 downstream evaluation의 성능에 영향을 주지 않는다.\nDownstream Evaluation GLUE benchmark, SQuAD의 두 버전, 그리고 RACE dataset이라는 세 가지 benchmark에서 평가한다. 작업 리더보드를 기반으로 한 최종 비교를 위해 테스트 세트 결과도 측정하며, 개발 세트에서 큰 분산을 가진 GLUE 데이터셋에 대해서는 5회 실행의 중앙값을 측정한다.\nOverall Comparison between BERT and ALBERT ALBERT의 디자인 선택사항 중 parameter 효율성 향상이 가장 중요한 장점으로, BERT-large의 parameter의 70%만으로도 ALBERT-xxlarge는 여러 downstream task에서 상당한 개선을 보이고 있다.\n동일한 훈련 구성 하에서, ALBERT 모델들은 BERT 모델들에 비해 더 높은 데이터 처리량을 보인다. BERT-large를 기준으로 할 때, ALBERT-large는 데이터를 처리하는 데 약 1.7배 더 빠르고, 큰 구조 때문에 ALBERT-xxlarge는 약 3배 더 느리다.\n마지막으로, ALBERT의 각 디자인 선택사항이 얼마나 기여하는지 파악하기 위한 ablation experiments을 수행한다.\nFactorized Embedding Parameterization BERT 스타일의 비공유 상태에서는 더 큰 임베딩 크기가 약간 더 나은 성능을 보이지만, ALBERT 스타일의 모든 공유 상태에서는 크기 128의 임베딩이 최적으로 보인다. 이 결과를 바탕으로, 모든 추후 설정에서 임베딩 크기 128을 사용하게 된다.\nCross-layer parameter sharing all-shared strategy인 ALBERT 스타일은 성능을 떨어뜨리지만, 임베딩 크기가 128일 때는 768일 때보다 저하가 덜 하다다. 성능 저하는 주로 FFN layer parameter sharing으로 인해 발생하며, attention parameter sharing은 성능 저하를 일으키지 않는다.\n다른 레이어 간의 parameter sharing 전략도 가능하지만, 그룹 크기를 줄이면 전체 parameter 수가 크게 증가한다. 따라서 all-shared strategy을 기본으로 선택하였다.\nSentence order prediction (SOP) 세 가지 실험 조건인 \u0026rsquo;none\u0026rsquo; (XLNet 및 RoBERTa 스타일), NSP (BERT 스타일), SOP (ALBERT 스타일)에 대한 추가 문장 간 손실을 ALBERTbase 설정을 사용하여 비교하였다.\nNSP loss가 SOP 작업에 판별력을 제공하지 못하며, 주제 전환만 모델링하는 것으로 나타났다. 반면에, SOP loss는 NSP 작업을 상대적으로 잘 처리하며, SOP 작업에 대해서는 더욱 더 잘 처리했다. 더욱이, SOP loss는 여러 문장 인코딩 작업에 대한 downstream task 성능을 일관되게 향상시키는 것으로 나타났으며, 이는 평균 점수 향상이 약 1%라는 것을 의미한다.\nWhat if we train for the same amount of time? BERT-large의 데이터 처리량은 ALBERT-xxlarge에 비해 약 3.17배 높다. 따라서 같은 학습 시간 동안 모델을 학습시키는 비교를 수행하였다. 400k 학습 단계 후의 BERT-large 모델 (학습 34시간 후)과 ALBERT-xxlarge 모델을 125k 학습 단계 동안 학습하는 데 필요한 시간 (학습 32시간)을 비교한다.\nALBERT-xxlarge는 BERT-large보다 훨씬 더 우수하며, 평균적으로 +1.5% 더 좋고, RACE에서는 최대 +5.2%까지 차이가 난다.\nAdditional training data and dropout effects XLNet과 RoBERTa가 모두 사용한 추가 데이터의 영향을 비교한다. 추가 데이터 없을 때와 있을 때의 개발 세트 MLM 정확도를 비교하며, 추가 데이터를 사용할 때 중요한 향상을 보여준다.\n또한, SQuAD benchmark를 제외한 downstream task에서 성능 개선을 관찰했다.\n1M step까지 학습한 후에도, 가장 큰 모델들은 훈련 데이터에 과적합되지 않았다. 그래서 드롭아웃을 제거하여 모델의 용량을 늘려서 실험을 계속 하였으며, 드롭아웃을 제거하면 MLM 정확도가 크게 향상된다는 것이 확인되었다.\nALBERT-xxlarge의 중간 평가에서도 드롭아웃 제거가 downstream task에 도움이 된다는 것이 확인되었다. 이는, 드롭아웃이 큰 Transformer 기반 모델의 성능을 해칠 수 있다는 것을 보여준다. 그러나, ALBERT의 기본 네트워크 구조는 Transformer의 특수한 경우로, 이 현상이 다른 Transformer 기반 아키텍처에서도 나타나는지 여부를 확인하기 위해 추가 실험이 필요하다.\nCurrent State-of-the-art on NLU Tasks 단일 모델과 앙상블에 대해 미세조정을 수행하고 state-of-the-art 결과를 비교한다. 모든 설정에서 단일 작업 미세조정만 수행하며, 개발 세트에서는 5번의 실행 결과 중 중앙값을 비교한다.\n최종 앙상블 모델에 기여하는 체크포인트는 개발 세트 성능에 따라 선택되며, 체크포인트의 수는 작업에 따라 6에서 17까지 다르다. GLUE와 RACE benchmark의 경우, 다른 학습 단계에서 미세조정된 후보 모델들의 예측을 평균화한다. SQuAD의 경우, 여러 확률을 가진 범위의 예측 점수와 \u0026ldquo;응답할 수 없는\u0026rdquo; 결정의 점수를 평균화한다.\n단일 모델과 앙상블 모델 모두 ALBERT가 모든 벤치마크에서 상당히 향상된 성능을 보여준다. GLUE 점수는 89.4, SQuAD 2.0 테스트 F1 점수는 92.2, RACE 테스트 정확도는 89.4에 달하며, 이는 BERT, XLNet, RoBERTa, 그리고 DCMI+ 등에 비해 큰 향상을 보여준다. 또한, 단일 모델은 86.5%의 정확도를 달성하여, state-of-the-art 앙상블 모델보다 2.4% 더 높다.\nDiscussion ALBERT-xxlarge는 BERT-large보다 parameter가 적지만 더 좋은 성능을 보인다. 하지만, 큰 구조 때문에 계산 비용이 더 많이 들어간다. 따라서, 다음 단계는 sparse attention와 block attention 등의 방법을 통해 ALBERT의 학습과 추론 속도를 높이는 것이다. 어려운 예제 채굴과 더 효율적인 언어 모델링 훈련 등 별개의 연구 방향이 추가적인 표현력을 제공할 수 있다. 또한, 문장 순서 예측이 더 나은 언어 표현을 이끌어내는 좋은 학습 작업이지만, 추가적인 표현력을 만들어낼 수 있는 현재 자기 self-supervised loss에 포착되지 않은 다른 차원이 있을 수 있다는 가설이 있다.\nReference Paper Github ","date":"2023-12-14T00:00:00Z","permalink":"https://kurtkim.github.io/p/albert/","title":"ALBERT"},{"content":"Abstract 언어 모델의 사전 학습은 큰 성능 향상을 가져왔지만, 다른 접근법 간의 비교는 어렵다. 학습 과정은 비용이 많이 들며, hyperparameter 선택이 결과에 큰 영향을 미친다. 이 연구는 BERT 학습을 재현하고, 핵심 hyperparameter와 학습 데이터 크기의 영향을 측정하는데, 이를 통해 BERT가 undertrained 상태였으며, 그 이후의 모든 모델의 성능을 맞추거나 능가할 수 있다는 것을 발견했다. RoBERTa는 여러 테스트에서 state-of-the-art를 달성하였고, 이는 이전에 간과된 설계의 중요성을 보여준다.\nIntroduction 이 연구는 BERT의 사전 학습 방법을 재현하고, 그 효과를 평가했다. 연구 결과 BERT는 상당히 학습이 부족했으며, 이를 개선하기 위한 새로운 학습 방법 RoBERTa를 제안했다. RoBERTa는 더 큰 배치와 더 많은 데이터에서 모델을 더 오래 학습시키고, 다음 문장 예측 목표를 제거하며, 더 긴 시퀀스에서 학습시키고, 학습 데이터에 적용되는 마스킹 패턴을 동적으로 변경한다. 이 개선된 학습 방법은 GLUE와 SQuAD에서 BERT의 성능을 뛰어넘었다. 또한, 새로운 데이터셋인 CCN EWS를 사용하고, 사전 학습에 더 많은 데이터를 사용하면 성능이 더욱 향상된다는 것을 확인하였다.\nBackground BERT의 사전 학습 방식과 실험적으로 검토할 학습 선택 사항에 대해 간단히 소개한다.\nSetup BERT는 두 개의 토큰 시퀀스인 $x_1, \u0026hellip;, x_N$과 $y_1, \u0026hellip;, y_M$을 연결하여 입력으로 사용한다. 이 두 세그먼트는 특별한 토큰들로 구분되어 BERT에 하나의 입력 시퀀스로 제공된다:\n$$ [CLS], x_1, \u0026hellip;, x_N, [SEP], y_1, \u0026hellip;, y_M, [EOS]. $$\n$M$과 $N$은 $M + N \u0026lt; T$라는 제약하에 있으며, 여기서 $T$는 최대 시퀀스 길이를 제어하는 parameter이다. 모델은 대량의 레이블이 없는 텍스트 말뭉치로 사전 학습된 후, 최종 목표 레이블 데이터를 사용하여 미세 조정된다.\nArchitecture 사전 학습 동안, BERT는 masked language modeling과 next sentence prediction 두 가지 작업을 사용한다.\nMasked Language Model (MLM) 입력 시퀀스의 토큰 중 무작위로 선택된 일부가 특별한 토큰 [MASK]로 대체된다. masked language modeling 목표는 마스크된 토큰을 예측하는 것이다. 입력 토큰의 15%가 선택되어 대체되며, 이 중 80%는 [MASK]로, 10%는 그대로 두고, 나머지 10%는 임의의 어휘 토큰으로 대체된다. 이 과정은 학습 기간 동안 한 번만 수행되지만, 실제로는 데이터가 복제되어 모든 학습 문장에 동일한 마스크가 적용되지 않는다.\nNext Sentence Prediction (NSP) NSP(Next Sentence Prediction)는 원문에서 두 세그먼트가 이어지는지 예측하는 방법이다. 긍정 예시는 연속된 문장을, 부정 예시는 다른 문서의 세그먼트를 사용해 만든다. NSP 목표는 문장 쌍 간의 관계를 파악하는 자연어 추론 등의 downstream task에서 성능을 향상시키기 위해 설계되었다.\nOptimization BERT는 Adam 최적화를 사용하며, learning rate는 처음 10,000 step 동안 서서히 높아진 후 선형적으로 감소한다. 모든 레이어와 attention weights에 대해 0.1의 dropout을 사용하고, GELU 활성화 함수를 활용한다. 모델은 1,000,000 업데이트 동안 사전 학습되며, 최대 512 토큰의 256 시퀀스를 포함하는 미니배치를 사용한다.\nData BERT는 총 16GB의 압축되지 않은 텍스트인 BOOK CORPUS와 영어 WIKIPEDIA 조합으로 학습된다.\nExperimental Setup BERT의 복제 연구에 대한 실험적 설정을 설명한다.\nImplementation FAIRSEQ에서 BERT를 재구현하고, 대부분의 원래 BERT 최적화 hyperparameter를 따랐다. 하지만 peak learning rate과 warmup step 수는 각 설정에 따라 별도로 조정했다. 또한, Adam epsilon 항에 따라 학습이 매우 민감하며, 이를 조정하여 성능을 개선했다.\nData BERT 스타일의 사전 학습은 대량의 텍스트에 크게 의존하며, 데이터 크기를 늘릴수록 최종 작업 성능이 향상될 수 있다. 이 연구에서는, 가능한 많은 데이터를 수집하여 실험을 진행하고, 각 비교에 적절한 데이터의 전체적인 품질과 양을 맞출 수 있도록 했다. 압축되지 않은 텍스트로 총 160GB를 넘는 다양한 크기와 도메인의 5개의 영어 말뭉치를 사용하였다:\nBOOK CORPUS와 영어 WIKIPEDIA. BERT를 학습시키는데 사용된 원래 데이터이다. (16GB). CC-N EWS, CommonCrawl News dataset의 영어 부분에서 수집한 것. 이 데이터는 2016년 9월부터 2019년 2월까지 크롤링된 6300만 개의 영어 뉴스 기사를 포함하고 있다. (필터링 후 76GB). OPEN WEB TEXT, WebText 코퍼스의 오픈 소스 재현, 텍스트는 Reddit에서 최소 세 번 이상의 추천을 받은 URL에서 추출된 웹 콘텐츠이다. (38GB) STORIES, Winograd 스키마의 스토리 같은 스타일을 맞추기 위해 필터링된 CommonCrawl 데이터의 부분 집합을 포함하고 있다. (31GB). Evaluation 다음 세 가지 benchmark를 사용하여 사전 학습된 모델을 평가한다.\nGLUE GLUE 벤치마크는 자연어 이해 시스템을 평가하기 위한 9개의 dataset 모음이다. 이는 단문 분류 또는 문장 쌍 분류 작업으로 구성되며, 참가자들은 제출 서버와 리더보드를 통해 자신의 시스템을 평가하고 비교할 수 있다.\n학습 데이터에서 사전 학습된 모델을 미세 조정하여 개발 세트에서 결과를 보고하였으며, 이 절차는 원래의 BERT 논문을 따른다.\nSQuAD Stanford Question Answering Dataset(SQuAD)는 문맥과 질문을 제공하며, 작업은 문맥에서 관련 부분을 추출하여 질문에 답한다. SQuAD는 V1.1과 V2.0 두 버전이 있으며, V2.0 버전은 제공된 문맥에서 답하지 않는 질문도 포함하여 작업이 더 도전적이다.\nSQuAD V1.1에서는 BERT와 같은 방법으로 범위를 예측하며, SQuAD V2.0에서는 추가적인 이진 분류기를 사용하여 질문이 답변 가능한지를 예측한다. 이는 분류와 span loss terms를 합산하여 훈련되며, 평가 시 답변 가능으로 분류된 쌍에만 범위 인덱스를 예측한다.\nRACE ReAding Comprehension from Examinations(RACE)은 28,000개 이상의 지문과 100,000개 가까운 질문을 포함한 대규모 독해 dataset이다. 이 dataset은 중국의 중고등학생들을 대상으로 한 영어 시험에서 수집되었다. RACE에서는 각 지문에 대해 여러 질문을 풀고, 네 가지 선택지 중 정답을 고르는 작업을 수행한ㄴ다. RACE는 다른 dataset에 비해 긴 문맥을 가지고 있으며, 대부분의 질문에서 추론 능력을 요구한다.\nTraining Procedure Analysis BERT 모델을 성공적으로 사전 학습하기 위해 중요한 선택 사항들을 탐색하고 정량화한다. $BERT_{BASE}$와 동일한 구성 (L = 12, H = 768, A = 12, 110M params)으로 BERT 모델을 학습한다.\nStatic vs. Dynamic Masking BERT는 무작위로 토큰을 마스킹하고 예측한다. 각 학습 인스턴스에서 동일한 마스크를 계속 사용하는 것을 피하기 위해, 학습 데이터를 10번 복제하여 각 시퀀스가 40회의 학습 동안 다양하게 마스킹될 수 있게 했다. 이를 모델에 시퀀스를 제공할 때마다 새로운 마스킹 패턴을 생성하는 동적 마스킹과 비교하였다.\n재구현에서 정적 마스킹은 원래 BERT와 유사한 성능을 보였고, 동적 마스킹은 정적 마스킹과 비슷하거나 약간 더 좋은 성능을 보여주었다. 이 결과와 동적 마스킹의 효율성을 고려하여, 이 후 실험에서는 동적 마스킹을 사용하였다.\nModel Input Format and Next Sentence Prediction 원래 BERT 사전 학습에서는 모델이 두 개의 연결된 문서 세그먼트를 관찰하며, 이들은 같은 문서에서 연속적으로 추출되거나 다른 문서에서 추출된다. 모델은 Next Sentence Prediction(NSP) loss를 통해 관찰한 문서 세그먼트가 같은 문서에서 왔는지, 다른 문서에서 왔는지 예측하도록 훈련된다.\nNSP loss의 중요성은 원래 BERT 모델 훈련에서 중요한 요소로 가정되었으며, NSP를 제거하면 성능이 저하되는 것이 관찰되었다. 그러나 최근의 일부 연구에서는 NSP loss의 필요성에 의문을 제기하였다. 이러한 차이점을 더 잘 이해하기 위해, 다양한 훈련 형식을 비교하고 있다.\nSEGMENT - PAIR + NSP: BERT의 원래 입력 형식을 따르며 NSP loss를 포함한다. 각 입력은 여러 자연스러운 문장을 포함한 세그먼트 쌍이 있으며, 총 길이는 512 토큰 이하여야 한다. SENTENCE - PAIR + NSP: 각 입력은 하나의 문서의 연속적인 부분 또는 별개의 문서에서 추출된 자연스러운 문장 쌍을 포함한다. 이러한 입력은 512 토큰보다 훨씬 짧으므로, 총 토큰 수가 SEGMENT - PAIR + NSP와 비슷하게 유지되도록 배치 크기를 늘린다. NSP loss는 유지된다. FULL - SENTENCES: 각 입력은 하나 이상의 문서에서 연속적으로 샘플링된 문장으로 채워져 있으며, 총 길이는 최대 512 토큰이다. 문서가 끝나면 다음 문서에서 샘플링을 시작하고, 문서 사이에는 추가 구분자 토큰을 넣는다. NSP loss는 제거된다. DOC - SENTENCES: 입력은 FULL - SENTENCES와 유사하게 구성된다. 문서 끝 부분에서 샘플링된 입력은 512 토큰보다 짧을 수 있어, 이런 경우에는 배치 크기를 동적으로 늘려 FULL SENTENCES와 같은 총 토큰 수를 유지한다. NSP loss는 제거한다. 원래의 SEGMENT - PAIR 입력 형식과 SENTENCE - PAIR 형식을 비교했으며, 둘 다 NSP loss를 유지하되 후자는 단일 문장을 사용한다. 개별 문장 사용은 모델이 장거리 의존성을 학습하지 못함으로써 성능을 저하시킨다는 것을 발견하였다.\nNSP loss 없이 훈련과 단일 문서의 텍스트 블록을 사용한 학습(DOC - SENTENCES)을 비교했을 때, 이 설정이 $BERT_{BASE}$ 결과를 능가하며, NSP loss를 제거하면 downstream task 성능이 향상된다는 것을 발견하였다.\n마지막으로, 단일 문서의 시퀀스(DOC - SENTENCES)가 여러 문서의 시퀀스(FULL - SENTENCES)보다 약간 더 나은 성능을 보이지만, DOC - SENTENCES 형식은 배치 크기가 변동적이므로, 나머지 실험에서는 FULL SENTENCES를 사용하였다.\nTraining with large batches Neural Machine Translation의 이전 연구에 따르면, earning rate가 적절하게 증가할 때 아주 큰 mini-batches를 사용한 학습은 최적화 속도와 성능 모두를 개선시킬 수 있다는 것이 증명되었다. 최근의 연구는 BERT 또한 큰 배치 학습에 적합하다는 것이 밝혀졌다.\n배치 크기를 증가시키면서 $BERT_{BASE}$의 Perplexity와 성능을 비교하였고, 큰 배치로 훈련하면 마스크 언어 모델링 목표의 Perplexity와 정확도가 향상된다는 것을 관찰했다. 또한, 큰 배치는 분산 데이터 병렬 훈련을 통해 더 쉽게 병렬화할 수 있다.\nText Encoding Byte-Pair Encoding(BPE)는 자연어 말뭉치의 큰 어휘를 처리하기 위해 문자와 단어 수준 표현의 중간 형태를 사용한다. BPE는 훈련 말뭉치의 통계 분석을 통해 추출된 subword unit를 사용한다. byte를 사용한 BPE는, Unknown 토큰 없이도, 50K units의 서브워드 사전으로 학습을 진행할 수 있다.\n기존의 BERT는 character level의 BPE를 사용했다. RoBERTa는 추가 전처리나 토크나이징 없이 larger byte-level BPE로 학습을 진행한다.\nByte level BPE는 몇 개의 태스크에서 성능이 떨어진다는 단점이 있지만, 성능 하락폭이 크지 않고 universal 인코딩의 장점이 있다고 판단하여 본 연구에서 Byte level BPE를 적용하였다.\nRoBERTa BERT 사전 학습 절차를 수정하여 end-task 성능을 향상시키는 방안을 제안한다. 이를 모두 합쳐서 그 효과를 평가한 결과, RoBERTa라는 새로운 설정이 만들어졌다. RoBERTa는 동적 마스킹, NSP loss 없는 전체 문장, 큰 미니 배치, 더 큰 바이트 수준 BPE를 사용하여 학습된다.\n또한, 사전 학습에 사용된 데이터와 데이터를 통한 학습 통과 횟수라는 두 가지 중요한 요소를 조사했다. 이를 검증하기 위해 $BERT_{LARGE}$ 아키텍처를 따라 RoBERTa를 훈련시켰고, BOOK CORPUS와 WIKIPEDIA dataset을 사용하여 100K 단계 동안 사전 학습 하였다.\nRoBERTa는 원래의 $BERT_{LARGE}$ 결과에 비해 크게 향상된 성능을 보여준다. 추가로, 다양한 데이터셋을 결합하여 훈련시킨 결과, 모든 downstream task에서 성능이 더욱 개선되었다.\n또한, 사전 학습 단계를 100K에서 300K, 그리고 500K로 늘렸을 때, downstream task 성능에서 상당한 향상을 보여주었다. 이렇게 학습된 모델들은 대부분의 작업에서 $XLNet_{LARGE}$를 능가하였다.\n마지막으로, GLUE, SQuaD, 그리고 RACE라는 세 가지 다른 benchmark에서 최적의 RoBERTa 모델을 평가하였다. 이는 모든 다섯 가지 dataset에 대해 500K 단계로 훈련된 RoBERTa를 기준으로 한 것이다.\nGLUE Results GLUE에 대해 두 가지 미세 조정 설정을 고려했다. 첫 번째는 각 GLUE 작업에 대해 RoBERTa를 별도로 미세조정 했으며, 각 작업에 대한 중간 개발 세트 결과를 앙상블 없이 측정했다. 두 번째는 GLUE 리더보드를 통해 RoBERTa를 다른 모델과 비교했다. 단일 작업 미세 조정에만 의존하였다. 특히, RTE, STS, 그리고 MRPC에서는 MNLI 단일 작업 모델에서 시작하여 미세 조정 하는 것이 도움이 되었다. 각 작업에 대해 5에서 7개의 모델을 앙상블하였다.\nRoBERTa가 GLUE 작업 개발 세트 9개 모두에서 state-of-the-art를 달성하였다. $BERT_{LARGE}$와 동일한 구조를 사용하면서도, $BERT_{LARGE}$와 $XLNet_{LARGE}$를 능가하였다. 이는 모델 구조와 사전학습 목표 이외의 요인들이 중요할 수 있다는 점을 시사한다.\n앙상블 설정에서는, RoBERTa를 GLUE 리더보드에 제출하여 9개 작업 중 4개에서 state-of-the-art를 달성하였고, 가장 높은 평균 점수를 얻었다. 이는 RoBERTa가 다중 작업 미세 조정에 의존하지 않는다는 점에서 특히 주목할만하다.\nSQuAD Results SQuAD에 대해 과거의 복잡한 접근법 대신 단순한 방법을 채택하였다. BERT와 XLNet이 추가 QA 데이터셋을 훈련 데이터에 포함시킨 반면, 이 연구에서는 제공된 SQuAD 훈련 데이터만으로 RoBERTa를 미세조정 하였다. 모든 레이어에 대해 동일한 learning rate를 사용하였다.\nRoBERTa는 SQuAD v1.1 개발 세트에서 XLNet과 동일한 성능을 보여주며, SQuAD v2.0에서는 XLNet을 약간 상회하는 새로운 state-of-the-art를 달성하였다.\nRoBERTa를 공개 SQuAD 2.0 리더보드에 제출했고, 추가적인 학습 데이터를 사용하지 않았음에도 불구하고, 단일 모델 제출 중 거의 모든 시스템을 능가하는 성과를 보여주었다. 데이터 증강에 의존하지 않는 시스템 중에서는 가장 높은 점수를 받았다.\nRACE Results RACE에서는 텍스트, 관련 질문, 네 개의 후보 답변이 제공되며, 시스템은 정답을 분류해야 한다. RoBERTa를 이 작업에 맞게 수정했으며, 각 후보 답변을 해당 질문과 텍스트에 연결하여 처리하였다. 전체 길이가 512 토큰을 초과하지 않도록 조정했다.\nRACE 테스트 세트에서의 결과는 RoBERTa가 중학교와 고등학교 환경 모두에서 state-of-the-art를 달성하였다.\nRelated Work 다양한 사전 학습 방법들은 언어 모델링, 기계 번역, 마스킹된 언어 모델링 등의 목표를 가지고 설계되었다. 최근 연구에서는 마스킹된 언어 모델 목표를 사용해 사전 학습하고, 각 작업에 대해 모델을 미세 조정하는 방법을 사용하였다. 그러나 새로운 방법들은 다중 작업 미세 조정, 엔티티 임베딩 통합, 범위 예측, 자동회귀 사전 학습의 변형 등을 통해 성능을 향상시켰다. 일반적으로, 더 큰 모델을 더 많은 데이터로 훈련함으로써 성능이 향상되었다.\nConclusion BERT 모델을 사전 학습시키는데 있어 다양한 디자인 결정을 신중하게 평가했다. 모델을 더 오래 훈련하고, 더 큰 배치로 더 많은 데이터를 사용하며, 다음 문장 예측 목표를 제거하고, 더 긴 시퀀스를 훈련하며, 훈련 데이터에 적용된 마스킹 패턴을 동적으로 변경하면 성능이 크게 향상됨을 발견했다. 이러한 개선된 사전 학습 절차를 RoBERTa라 부르며, GLUE, RACE, SQuAD에서 state-of-the-art를 달성하였다.\nReference Paper Github ","date":"2023-12-12T00:00:00Z","permalink":"https://kurtkim.github.io/p/roberta/","title":"RoBERTa"},{"content":"Abstract BERT와 같은 denoising autoencoding 기반 사전 학습은 양방향 컨텍스트를 학습할 수 있지만, 마스크를 사용하여 입력을 변조함으로써 의존성을 무시하고 사전 학습과 미세 조정 사이의 괴리를 겪는다. 이를 해결하기 위해, XLNet이라는 새로운 사전 학습 방법을 제안한다. 이 방법은 모든 순열에 대한 기대 가능도를 최대화하여 양방향 컨텍스트를 학습하고, autoregressive 형식을 통해 BERT의 제한을 극복한다. 실험결과 XLNet은 여러 작업에서 BERT를 능가하는 결과를 보여주었다.\nIntroduction 자연어 처리 분야에서 Unsupervised representation learning은 큰 성공을 거두었다. 이 방법은 대규모의 레이블이 없는 텍스트 코퍼스에서 신경망을 사전 학습하고, 이를 특정 작업에 맞게 미세 조정하는 방식이다. 이에 대한 연구 중, autoregressive(AR)와 autoencoding(AE)이 가장 좋은 결과를 보여주었다.\nAR 언어 모델링은 텍스트 시퀀스의 확률 분포를 추정하기 위해 autoregressive 모델을 사용한다. 텍스트 시퀀스 $x = (x_1, \u0026hellip;, x_T)$가 주어지면, AR 언어 모델링은 가능성을 순방향 곱인 $p(x) = \\prod_{t=1}^T p(x_t | X_{\u0026lt;t})$ 또는 역방향 곱인 $p(x) = \\prod_{t=T}^1 p(x_t | X_{\u0026gt;t})$로 분해하며, 각 조건부 분포를 모델링하기 위해 신경망 같은 매개변수 모델을 훈련한다. 그러나 AR 언어 모델은 단방향 컨텍스트만 인코딩하므로 깊은 양방향 컨텍스트를 모델링하는데는 비효율적이다. 이는 언어 이해 작업에서 필요한 양방향 컨텍스트 정보와의 간극을 만든다.\nAE 기반 사전 학습, 예를 들면 BERT는 손상된 입력에서 원본 데이터를 재구성하는 것을 목표로 한다. 입력 토큰의 일부가 [MASK] 같은 특수 기호로 대체되고, 이를 원래 토큰으로 복구하도록 모델이 학습된다. BERT는 양방향 컨텍스트를 사용해 재구성할 수 있으므로, AR 언어 모델링의 양방향 정보 간극을 해결하고 성능을 향상시킬 수 있다. 하지만, BERT가 사전 훈련 중에 사용하는 인위적인 기호들은 미세 조정 시 실제 데이터에는 존재하지 않아, 사전 학습과 미세 조정 사이의 불일치를 초래하며, 예측된 토큰 간의 상호 의존성을 과도하게 단순화한다.\nXLNet은 AR 언어 모델링과 AE의 장점을 모두 활용하면서 단점은 피하는 generalized autoregressive 방법이다.\nXLNet은 고정된 순방향이나 역방향 분해 순서를 사용하는 대신, 모든 가능한 분해 순서의 순열에 대한 시퀀스의 기대 log-likelihood를 최대화한다. 이로 인해 각 위치의 컨텍스트는 왼쪽과 오른쪽의 토큰들로 구성될 수 있으며, 모든 위치의 컨텍스트 정보를 활용하며 양방향 컨텍스트를 포착한다. XLNet은 generalized AR 언어 모델로서, 데이터 손상에 의존하지 않아 BERT의 사전 학습과 미세 조정 간의 불일치 문제를 겪지 않는다. 또한, autoregressive 목표는 예측된 토큰의 joint probability를 분해하는데 곱셈 규칙을 사용하므로, BERT의 독립 가정을 제거한다. 추가적으로, XLNet은 사전 학습을 위한 architecture design을 개선하였다.\nXLNet은 Transformer-XL의 segment recurrence mechanism과 relative encoding scheme를 사전 학습에 통합하여, 긴 텍스트 시퀀스를 다루는 작업의 성능을 향상시켰다. Transformer-XL architecture를 naive하게 적용하는 것은 어렵기 때문에, Transformer-XL network를 reparameterize해서 사용하였다. 같은 실험 환경에서, XLNet은 언어 이해, 읽기 이해, 텍스트 분류, 문서 랭킹 등 다양한 작업에서 BERT를 능가하는 성능을 보여주었다.\nRelated Work 순열 기반 AR 모델링은 이전에도 연구되었지만, XLNet은 언어 모델이 양방향 컨텍스트를 학습하는 것을 목표로 하고, 이를 위해 two-stream attention을 통해 목표 위치를 hidden state에 통합한다. 이전 모델들과는 다르게, \u0026ldquo;순서 없음\u0026quot;은 입력 시퀀스가 무작위로 순열될 수 있다는 의미가 아니라, 모델이 분포의 다양한 분해 순서를 허용한다는 것을 의미한다.\n다른 관련 아이디어로는 텍스트 생성에서 autoregressive denoising을 수행하는 것이 있다. 이 방법은 고정된 순서만을 고려하고 있다.\nProposed Method Background 전통적인 AR 언어 모델링과 BERT를 비교하며, 텍스트 시퀀스가 주어지면 AR 언어 모델링은 forward autoregressive factorization을 통해 likelihood를 최대화하여 사전 학습을 수행한다:\n$$ \\underset{\\theta}{max} \\quad log \\ p_{\\theta}(x) = \\sum_{t=1}^{T} log \\ p{\\theta}(x_t | x \u0026lt; t) = \\sum_{t=1}^{T} log \\ {{exp(h_{\\theta}(x_{1:t-1})^\\intercal e(x_t))}\\over{\\sum_{x\u0026rsquo;} exp(h_{\\theta}(x_{1:t-1})^\\intercal e(x\u0026rsquo;))}} $$\n$h_θ(x_{1:t−1})$는 RNN이나 Transformer와 같은 신경 모델로 생성된 컨텍스트 표현이며, $e(x)$는 $x$의 임베딩이다. 반면에, BERT는 노이즈 제거 자동 인코딩에 기반하며, 텍스트 시퀀스 $x$의 일부 토큰을 특수 심볼 [MASK]로 바꿔 손상된 버전 $x$를 만든다. 이후의 훈련 목표는 $x$로부터 마스크된 토큰을 재구성하는 것이다.\n$$ \\underset{\\theta}{max} \\quad log \\ p_{\\theta}(\\bar{x}|\\hat{x}) \\approx \\sum_{t=1}^{T} m_t \\ log \\ p{\\theta}(x_t | \\hat{x}) = \\sum_{t=1}^{T} m_t \\ log \\ {{exp(H_{\\theta}(\\hat{x})^\\intercal_t e(x_t))}\\over{\\sum_{x\u0026rsquo;} exp(H_{\\theta}(\\hat{x})^\\intercal_t e(x\u0026rsquo;))}} $$\n$m_t = 1$은 $x_t$가 마스크되었음을 나타내며, $H_{\\theta}$는 텍스트 시퀀스 $x$를 숨겨진 벡터의 시퀀스로 변환하는 Transformer이다. 두 사전 학습 목표의 장단점은 다음에서 비교된다:\nIndependence Assumption: BERT는 모든 마스크된 토큰이 독립적으로 재구성된다는 가정 하에 결합 조건 확률 $p(\\bar{x}|\\hat{x})$을 분해한다. 반면, AR 언어 모델링은 독립 가정 없이 곱셈 법칙을 사용하여 확률 $p_{\\theta}(x)$을 분해한다. Input noise: BERT의 입력에는 [MASK]와 같은 인공적인 심볼이 포함되어 있어 사전 학습과 미세 조정간의 불일치가 발생한다. [MASK]를 원래 토큰으로 대체해도 문제가 해결되지 않는다. 반면, AR 언어 모델링은 입력 손상에 의존하지 않아 이 문제가 없다. Context dependency: AR 표현 $h_θ(x_{1:t−1})$은 위치 $t$까지의 토큰에만 의존하는 반면, BERT 표현은 양쪽의 문맥 정보에 접근할 수 있다. 따라서, BERT는 양방향 문맥을 더 잘 포착하도록 사전 학습된다. Objective: Permutation Language Modeling orderless NADE에서 아이디어를 차용하여, AR 모델의 장점을 유지하면서 양방향 문맥을 포착하는 순열 언어 모델링을 제안한다. 모델 파라미터가 모든 분해 순서에 공유되면, 모델은 양쪽 모든 위치에서 정보를 수집하도록 학습할 수 있다.\n길이가 $T$인 인덱스 시퀀스의 모든 가능한 순열 집합을 $Z_T$라고 할때, $z_t$와 $z \u0026lt; t$는 순열 $z$의 $t$번째 요소와 첫 $t−1$개 요소를 나타낸다. 그러면, 순열 언어 모델링 목표는 다음과 같이 표현될 수 있다:\n$$ \\underset{\\theta}{max} \\quad \\mathbb{E_{\\mathbf{z} \\sim \\mathbf{Z_T}}} \\big[ \\sum_{t=1}^{T} \\ log \\ p{\\theta}(x_{z_t} | x_{z_{\u0026lt;t}}) \\big] $$\n텍스트 시퀀스 $x$에 대해, 인수분해 순서 $z$를 샘플링하고, 이 순서에 따라 가능성 $p_{\\theta}(x)$을 분해한다. 모델 파라미터가 모든 인수분해 순서에 공유되므로, $x_t$는 시퀀스 내의 모든 가능한 요소를 볼 수 있어 양방향 문맥을 포착할 수 있다. 또한, AR 프레임워크에 적합하여 독립 가정과 사전 학습-미세조정 차이를 피할 수 있다.\nRemark on Permutation 인수분해 순서만을 바꾸며, 시퀀스 순서는 그대로 유지한다. 원래 시퀀스에 대응하는 positional encoding을 사용하고, Transformer의 적절한 attention mask를 활용한다. 이 방법은 모델이 미세 조정 동안 자연스러운 순서의 텍스트 시퀀스를 만나기 때문에 필요하다.\nArchitecture: Two-Stream Self-Attnetion for Target-Aware Representations 순열 언어 모델링 목표는 원하는 속성을 가지지만, 표준 Transformer 파라미터화를 사용한 단순한 구현은 문제가 있다. Softmax를 사용하여 다음 토큰 분포를 파라미터화하면:\n$$ p_{\\theta}(X_{z_t} = x | x_{z_{\u0026lt; t}}) = {{exp(e(x)^\\intercal h_{\\theta}(x_{z_t}))}\\over{\\sum_{x\u0026rsquo;} exp(e(x\u0026rsquo;)^\\intercal h_{\\theta}(x_{z_t})}} $$\n예측할 위치에 따라 표현이 달라지지 않는다. 이로 인해 대상 위치에 상관없이 동일한 분포가 예측되어 유용한 표현을 학습할 수 없다. 이 문제를 해결하기 위해, 다음 토큰 분포를 대상 위치를 인식하도록 re-parameterize 하는 것을 제안한다:\n$$ p_{\\theta}(X_{z_t} = x | x_{z_{\u0026lt; t}}) = {{exp(e(x)^\\intercal g_{\\theta}(x_{z_t}, z_t))}\\over{\\sum_{x\u0026rsquo;} exp(e(x\u0026rsquo;)^\\intercal g_{\\theta}(x_{z_t}, z_t)}} $$\n$g_{\\theta}(x_{z_t}, z_t)$는 대상 위치 $z_t$를 입력으로 추가로 받는 새로운 유형의 표현을 나타낸다.\nTwo-Stream Self-Attention target-aware representation의 개념은 대상 예측의 불명확성을 제거하지만, $g_{\\theta}(x_{z_t}, z_t)$를 어떻게 형성할 것인지는 복잡한 문제이다. 대상 위치에서 정보를 수집하는 것을 제안하며, 이를 위해 두 가지 요구사항이 필요하다:\n토큰 $x_{z_t}$를 예측하기 위해, $g_{\\theta}(x_{z_{\u0026lt;t}}, z_t)$는 위치 $z_t$만 사용하고 내용 $x_{z_t}$는 사용하지 않아야 한다. 다른 토큰 $x_{z_j}$를 예측하기 위해 $(j \u0026gt; t)$, $g_{\\theta}(x_{z_{\u0026lt;t}}, z_t)$는 전체 컨텍스트 정보를 제공하기 위해 내용 $x_{z_t}$도 인코딩해야 한다. 이 모순을 해결하기 위해, 하나 대신 두 세트의 숨겨진 표현을 사용한다.\ncontent representation $h_{\\theta}(x_{z_{\u0026lt;t}})$는 컨텍스트와 $x_{z_t}$ 자체를 인코딩하는 Transformer의 standard hidden states와 유사한 역할을 한다. query representation $g_{\\theta}(x_{z_t}, z_t)$는 컨텍스트 정보와 위치에만 접근할 수 있으며, 내용 $x_{z_t}$에는 접근할 수 없다. 첫번째 layer query stream은 학습 가능한 벡터로 초기화된다. (i.e $\\ g^{(0)}_i = w$) 반면에 content stream은 해당 단어 임베딩으로 설정된다. (i.e $\\ h^{(0)}_i = e(x_i)$) self-attention layer $m = 1, \u0026hellip;, M$에 대해, two-streams의 표현은 공유된 파라미터 세트로 다음과 같이 업데이트된다.\n$$ g_{z_t}^{(m)} \\leftarrow Attention(Q = g_{z_t}^{(m-1)}, KV = h_{z_{\u0026lt;t}}^{(m-1)}; \\theta) $$ $$ h_{z_t}^{(m)} \\leftarrow Attention(Q = h_{z_t}^{(m-1)}, KV = h_{z_{\\leq t}}^{(m-1)}; \\theta) $$\n여기서 Q, K, V는 attention 연산에서의 query, key, value를 나타낸다. content representation의 업데이트 규칙은 standard self-attention과 같으므로, 미세 조정 중에 query stream을 중단하고 content stream을 일반 Transformer-XL로 사용할 수 있다. last-layer query representation $g_{z_t}^{(m)}$를 사용하여 $p_{\\theta}(X_{z_t} = x | x_{z_{\u0026lt; t}})$를 계산할 수 있다.\nPartial Prediction 순열 언어 모델링은 여러 이점이 있지만, 순열로 인해 최적화가 어려워 수렴이 느리다. 이를 해결하기 위해, 분해 순서에서 마지막 토큰만 예측하도록 선택하였다. 즉, 전체를 목표가 아닌 부분과 목표 부분으로 나누고, 목표가 아닌 부분에 따른 목표 부분의 log-likelihood를 최대화한다.\n$$ \\underset{\\theta}{max} \\quad \\mathbb{E_{\\mathbf{z} \\sim \\mathbf{Z_T}}} \\big[ log \\ p{\\theta}(x_{z_{\u0026gt;c}} | x_{z_{\\geq t}}) \\big] = \\mathbb{E_{\\mathbf{z} \\sim \\mathbf{Z_T}}} \\big[ \\sum_{t=c+1}^{|z|} \\ log \\ p{\\theta}(x_{z_t} | x_{z_{\u0026lt; t}}) $$\n분해 순서에 따라 가장 긴 컨텍스트를 가진 부분이 목표로 선택되며, hyperparameter $K$는 약 $1/K$ 토큰이 예측에 선택되도록 사용된다. (i.e $|z| / (|z| - c) \\approx K$) 선택되지 않은 토큰들의 query representation을 계산할 필요가 없어, 메모리를 아끼고 속도를 향상시킬 수 있다.\nIncorporating Ideas from Transformer-XL Transformer-XL의 relative positional encoding scheme과 segment recurrence mechasnism을 통합하였으며, 이를 통해 모델이 이전 세그먼트의 hidden state를 재사용하게 할 수 있다. 긴 시퀀스에서 두 세그먼트를 가지고, 첫 번째 세그먼트를 처리한 후 얻은 내용을 캐시하고, 다음 세그먼트에 대해 attention을 업데이트한다.\n$$ h_{z_t}^{(m)} \\leftarrow Attention(Q = h_{z_t}^{(m-1)}, KV = \\big[ \\tilde{h}^{(m-1)}, h_{z_{\\leq t}}^{(m-1)} \\big]; \\theta) $$\npositional encoding은 원래 시퀀스의 실제 위치에만 의존하므로, 얻어진 표현이 있으면 attention 업데이트는 순열에 독립적이다. 이를 통해 이전 세그먼트의 분해 순서를 알지 못해도 메모리를 캐시하고 재사용할 수 있다. 모델은 마지막 세그먼트의 모든 분해 순서에 대해 메모리를 활용하는 방법을 학습하며, query stream도 같은 방식으로 계산된다.\nModeling Multiple Segments 다양한 작업에서 여러 입력 세그먼트가 필요하다. XLNet의 사전 학습 과정에서는 이를 고려하여 두 세그먼트를 무작위로 샘플링하고, 이를 하나의 시퀀스로 연결하여 순열 언어 모델링을 수행한다. 동일한 문맥에서만 메모리를 재사용하며, 이 과정은 BERT의 입력 형식 [CLS, A, SEP, B, SEP]을 따른다. 그러나 XLNet-Large는 일관된 성능 향상을 보이지 않아 다음 next sentence prediction를 사용하지 않습니다.\nRelative Segment Encodings 구조적으로, BERT와 달리 XLNet은 Transformer-XL의 relative encoding 개념을 확장하여 세그먼트를 인코딩한다. 두 위치가 같은 세그먼트에 있는지 여부만 고려하여, 세그먼트 인코딩을 이용해 attention 가중치를 계산한다. relative segment encoding을 사용하면 일반화를 개선하고 두 개 이상의 입력 세그먼트를 가진 작업에서 미세 조정할 수 있는 가능성이 있다.\nDiscussion BERT와 XLNet 모두 일부 토큰만 예측하는 부분적 예측을 수행한다. 이는 의미 있는 예측을 위해 필요하며, 충분한 문맥을 가진 토큰만 예측함으로써 최적화의 어려움을 줄인다. 그러나 BERT는 대상 간의 종속성을 모델링할 수 없다.\nBERT와 XLNet의 차이점을 이해하기 위해 예시 [New, York, is, a, city]에 대해 비교 해 보면, 예시에서 [New, York] 두 개의 token을 예측하고 $log \\ p(\\text{New York} | \\text{is a city})$를 maximize한다고 가정한다. 또한, XLNet의 인수분해 순서는 [is, a, city, New, York]이라고 가정한다. BERT와 XLNet의 object는 다음과 같다.\n$$ \\mathbf{J}_{BERT} = log \\ p(\\text{New} | \\text{is a city}) + log \\ p(\\text{York} | \\text{is a city}) $$\n$$ \\mathbf{J}_{XLNet} = log \\ p(\\text{New} | \\text{is a city}) + log \\ p(\\text{York} | \\text{New, is a city}) $$\nXLNet은 BERT가 생략하는 \u0026lsquo;New\u0026rsquo;와 \u0026lsquo;York\u0026rsquo; 사이의 종속성을 포착할 수 있다. 동일한 대상이 주어진 상황에서 XLNet은 더 많은 종속성 쌍을 학습하며, 더 밀도 높은 훈련 신호를 포함하고 있다.\nExperiments Pretraining and Implementation BooksCorpus와 영어 Wikipedia를 사전 학습 데이터로 사용하고, 추가로 Giga5, ClueWeb 2012-B, 그리고 Common Crawl을 포함시켰다. 이 데이터들은 공격적인 필터링을 통해 짧거나 저품질의 기사를 제거한 후, 총 32.89B의 subword pieces로 토큰화하였다.\nXLNet-Large 모델은 BERT-Large와 같은 아키텍처를 가지고 있다. 사전 학습 동안 시퀀스 길이 512를 사용하였고, BERT와 비교하기 위해 BooksCorpus와 위키백과만을 이용해 훈련을 진행하였다. 그 후, 앞서 설명한 모든 데이터셋을 사용해 훈련을 확장하였고, 마지막으로, XLNet-Base-wikibooks를 기반으로 ablation study를 수행하였다.\nrecurrence mechanism이 도입된 XLNet-Large 학습에서는 양방향 데이터 입력 파이프라인을 사용하고, 전방향과 후방향이 각각 배치 크기의 절반을 차지한다. prediction constant $K$는 6으로 설정되었으며, 미세 조정 절차는 BERT를 따른다. 추가로, 특정 길이의 토큰을 예측 대상으로 선택하는 span-based 예측 방법을 사용하였다.\nFair Comparison with BERT 같은 데이터와 하이퍼파라미터로 훈련된 XLNet은 모든 고려된 데이터셋에서 BERT를 크게 앞서는 것으로 나타났다.\nComparison with RoBERTa: Scaling Up ALBERT는 계산량을 크게 증가시키므로 과학적 결론을 도출하기 어려워, 다음 결과에서 제외되었다. 반면, RoBERTa는 전체 데이터를 기반으로 하고, RoBERTa의 하이퍼파라미터를 재사용하는 실험을 진행하였다.\nXLNet은 일반적으로 BERT와 RoBERTa를 능가하는 성능을 보여주었다.\nSQuAD와 RACE와 같이 더 긴 컨텍스트를 다루는 명확한 추론 작업에 대해, XLNet의 성능 향상이 더 크다. 이러한 더 긴 컨텍스트를 다루는 능력은 XLNet의 Transformer-XL 기반 구조에서 나올 수 있다. MNLI(\u0026gt;390K), Yelp(\u0026gt;560K), Amazon(\u0026gt;3M)과 같이 이미 충분한 예제가 있는 분류 작업에 대해서도, XLNet은 큰 성능 향상을 보여주었다. Ablation Study 다양한 특성을 가진 네 가지 데이터셋을 기반으로 각 설계 선택의 중요성을 이해하기 위한 ablation study를 수행한다. 구체적으로, 연구하고자 하는 세 가지 주요 측면으로는 다음과 같다:\n순열 언어 모델링 목표의 효과, 특히 BERT에서 사용하는 denoising auto-encoding 목표와 비교 Transformer-XL을 백본 아키텍처로 사용하는 것의 중요성 범위 기반 예측, 양방향 입력 파이프라인, 그리고 다음 문장 예측을 포함한 일부 구현 세부사항의 필요성 다양한 구현 세부 사항을 가진 6개의 XLNet-Base 변형, 원래의 BERT-Base 모델, 그리고 BERT에서 사용된 denoising auto-encoding 목표로 훈련된 Transformer-XL 기준선을 비교한다. 모든 모델은 공평한 비교를 위해 BERT-Base와 동일한 모델 hyperparameter를 가진 12-layer 아키텍처를 기반으로 하며, Wikipedia와 BooksCorpus에서만 학습하였다.\nTransformer-XL과 순열 언어 모델링이 XLNet의 성능 향상에 크게 기여한다는 것을 알 수 있다. 메모리 캐싱 메커니즘을 제거하면 성능이 특히 떨어지며, 범위 기반 예측과 양방향 입력 파이프라인이 중요한 역할을 한다. 또한, BERT에서 제안된 다음 문장 예측 목표가 반드시 성능 향상을 가져오지 않으므로, 이를 XLNet에서 제외하였다.\nConclusions XLNet은 AR과 AE 방법의 이점을 결합하는 순열 언어 모델링을 사용하는 사전 학습 방법이다. Transformer-XL을 통합하고 two-stream attention mechanism을 설계하여 AR 목표와 원활하게 작동하도록 하였다. XLNet은 다양한 작업에서 이전 사전 학습 방법들 보다 큰 성능 개선을 보여주었다.\nReference Paper Github ","date":"2023-12-10T00:00:00Z","permalink":"https://kurtkim.github.io/p/xlnet/","title":"XLNet"},{"content":"Abstract 언어 모델은 웹페이지로 구성된 새로운 데이터셋인 \u0026lsquo;WebText\u0026rsquo;으로 학습 함으로써, 질문 응답, 기계 번역 등의 작업을 명시적인 지도 없이 배우기 시작한다는 것을 발견하였다. 특히, GPT-2는 웹텍스트에 대해 underfitting이지만, zero-shot 환경에서 8개 테스트 언어 모델링 데이터셋 중 7개에서 state-of-the-art를 달성하였다. 즉, 언어 처리 시스템이 자연적 설명으로부터 과제수행능력을 배우는 언어처리모델을 개발할 수 있는 방법을 제안하였다.\nIntroduction 기계 학습 시스템은 대규모 데이터셋, 고용량 모델, 지도 학습을 이용해 학습된 작업에서 우수한 성과를 보이지만, 데이터 분포의 작은 변화나 작업 정의에 민감하게 반응하고,매우 좁은 범위의 문제에서만 뛰어난 성능을 보여주고 있다. 그래서 보다 일반적인 문제 해결 능력을 갖춘 범용적인 모델 개발이 필요하며, 이는 결국 각각의 작업에 대해 훈련 데이터셋을 수동으로 생성하고 라벨링할 필요 없이 다양한 작업을 수행할 수 있는 모델을 의미한다.\n기계 학습 시스템을 만드는 주요한 방법은 훈련 예제를 수집하여 시스템을 학습시키고, independent and identically distributed (IID)에서 성능을 테스트하는 것이다. 이 방법은 좁은 범위의 과제에서는 잘 작동하지만, 범용적인 이해를 필요로 하는 캡션 모델, 독해, 이미지 분류 등에서 높은 성능을 내지 못했으며, 이 방법의 한계를 보여주었다.\n일반화하는 능력이 부족한 주요 원인으로 많은 연구가 단일 영역의 dataset과 단일 과제에만 맞춘 학습에만 치중되어 있기 때문이라고 보고 있다. 이를 개선하기 위해 다양한 도메인과 작업에서 훈련하고 성능을 측정하는 것이 필요하며, GLUE와 decaNLP 같은 benchmark dataset이 제안되었다.\n다중 작업 학습(Multitask learning)는 일반 성능을 향상에 높이는 유망한 방법이지만, NLP에서는 아직 초기 연구 단계이다. 최근의 기계학습 시스템의 일반화를 위해서는 수백에서 수천 개의 학습 샘플을 필요로 하며, 다중 작업 학습을 위해서도 그만큼 많은 수의 효과적인 트레이닝 쌍이 필요하다. 현재의 기술로는 dataset을 필요한 수준까지 계속 확장하는 것이 어려우며, 따라서 다중 작업 학습을 위한 새로운 접근법이 필요하다.\n현재 언어 작업에서 최고 성능을 보이는 모델은 사전 학습과 지도 학습을 결합한 방식을 사용한다. 이 접근법은 오랜 역사를 가지고 있으며, transfer 방식이 점차 유연해지고 있다. 초기에는 단어 벡터가 학습되어 특정 작업에 적용되었고, 그 다음으로는 순환 네트워크의 문맥 표현이 transfer 되었다. 최근 연구에서는 특정 작업에 특화된 아키텍처가 필요 없으며, 대신 self-attention block만으로 충분하다고 제안하고 있다.\n현재의 방법들은 작업 수행을 위해 여전히 지도 학습이 필요하다. 하지만 지도 데이터가 거의 없거나 전혀 없을 때, 언어 모델이 상식적인 추론이나 감성 분석 등의 특정 작업을 수행하는 데 잠재력이 있다는 것이 다른 연구에서 보여져 왔다.\n이 논문에서는 언어 모델이 parameter나 아키텍처 변경 없이 zero-shot setting에서 다양한 작업을 수행할 수 있는 능력을 보여준다. 이 접근법은 전이 학습의 일반화 추세를 이어가며, 작업에 따라 유망한 결과와 경쟁력 있는 성과, 그리고 state-of-the-art를 달성하는 잠재력을 보여준다.\nApproach 핵심은 언어 모델링(language modeling)이며, 이는 각 원소가 일련의 symbol $(s_1, s_2, \u0026hellip;, s_n)$ 으로 구성된 예제 $(x_1, x_2, \u0026hellip;, x_n)$ 에서 비지도분포 추정을 하는 것으로 정의된다. 언어의 순차적 특성 때문에 기호들에 대한 결합 확률은 조건부 확률의 곱으로 분해하는 것이 일반적이다.\n$$p(x) = \\prod_{i=1}^n p(s_n | s_1, s_2, \u0026hellip;, s_{n-1}) $$\n이 방법은 $p(x)$ 및 $p(s_{n-k}, \u0026hellip;, s_n | s_1, \u0026hellip;, s_{s-k-1})$ 형태의 조건부의 샘플링과 추정을 가능하게 하며, 최근에는 Transformer와 같은 self-attention 아키텍처의 발전으로 이러한 조건부 확률을 계산하는 모델의 표현력이 크게 향상되었다.\n단일 작업을 수행하는 학습은 확률론적 프레임워크에서 조건부 분포 $p(output | input)$를 추정하는 것으로 볼 수 있다. 하지만 일반적인 시스템은 동일한 입력에 대해 수행해야 하는 다양한 작업을 고려해야 한다. 이를 위해, 시스템은 $p(output | input, task)$를 모델링해야 한다. 이는 다중학습과 메타학습 환경에서 다양하게 형식을 갖는다. McCann et al. (2018)은 언어를 활용하여 작업, 입력, 출력을 기호 시퀀스로 지정하는 방법을 제시하였고, 이 방법을 사용하여 MQAN이라는 단일 모델을 훈련시켜 다양한 작업을 수행할 수 있음을 보여주었다.\n언어 모델링은 출력 symbol에 대한 명시적인 지도 없이도 다양한 작업을 학습할 수 있다. 이는 감독된 학습 목표와 비감독된 학습 목표가 실질적으로 같기 때문인데, 이 두 목표의 global minimum은 동일하다. 예비 실험에서는 충분히 큰 언어 모델은 이러한 설정에서 다중 작업 학습을 수행할 수 있지만, 명시적으로 감독된 방법보다 학습 속도가 느리다는 것이 확인되었다.\n대화(dialog)의 맥락에서 자연어를 직접 학습하는 방법은 매력적인 접근법이지만, 상호작용이 필요없는 인터넷 사이트에 존재하는 방대한 양의 데이터를 활용하는 방법을 선택하였다. 충분한 용량을 가진 언어 모델은 자연어 시퀀스에서 작업을 추론하고 수행하며, 이를 통해 더 잘 예측하도록 학습할 것으로 예상된다. 모델은 비지도 다중작업 학습을 수행하게 될 것이며, 다양한 작업에서 언어 모델의 제로샷 성능을 분석하였다.\nTraining Dataset 이전 연구들은 주로 한정된 도메인의 텍스트를 가지고 언어 모델을 학습시켰다. 하지만 이 논문에서는 가능한 한 다양한 도메인과 맥락에서 작업을 수집하기 위해, 크고 다양한 dataset을 구축하였다.\n다양하고 방대한 텍스트의 출처로 웹 스크랩인 Common Crawl이 유망하지만, 데이터 품질 문제가 있습니다.\n이 dataset을 사용하는 대신, 문서의 품질을 중요시하는 새로운 웹 스크랩을 만들었다. 전체 웹 스크랩을 수동으로 필터링하는 비용을 줄이기 위해, 사람들이 선별한 웹 페이지를 대상으로 했다. 특히, 적어도 3 카르마를 받은 Reddit의 모든 외부 링크를 스크랩했는데, 이는 사용자들이 해당 링크를 유익하거나 재미있게 여겼는지의 지표로 볼 수 있다.\n결과적으로 나온 dataset인 WebText는 45백만 링크의 텍스트 부분집합을 포함하고 있으며, 텍스트 추출을 위해 Dragnet과 Newspaper 내용추출기를 사용하였다. 이 논문의 모든 결과는 2017년 12월 이후 링크를 제외한 초기 버전의 WebText를 사용하며, 이는 de-duplication과 cleaning 과정을 거친 후 40GB, 약 800만 개의 문서를 포함하고 있다. Wikipedia 문서는 분석을 복잡하게 할 수 있어 WebText에서 제외하였다.\nInput Representation 일반적인 언어 모델은 어떤 문자열의 확률도 계산하고 생성할 수 있어야 한다. 하지만 현재의 대규모 언어 모델은 전처리 과정으로 인해 모델링 가능한 문자열 범위가 제한된다. 유니코드 문자열을 UTF-8 바이트로 처리하는 것은 이를 해결하나, 현재 바이트 수준의 언어 모델은 대규모 데이터셋에서 단어 수준의 언어 모델만큼 효과적이지 않다. WebText에서 바이트 수준 언어 모델을 훈련시키려 했으나, 이와 비슷한 성능 격차를 경험하였다.\nByte Pair Encoding(BPE)는 문자와 단어 수준 언어 모델링 사이의 중간 지점이다. 자주 나오는 symbol sequence의 단어수준 입력과 자주 나오지 않는 symbol sequence의 글자수준 입력을 적절히 보간(interpolate)한다. BPE 구현은 byte sequence가 아닌 unicode code points에서 동작한다. 이러한 구현은 모든 unicode 문자열을 모델링하기 위해 전체 unicode symbol의 전체 공간을 포함해야 한다. multi-symbol token을 추가하기 전 130,000개가 넘는 token을 포함하는 기본사전을 필요로 하게 된다. 이는 보통의 32,000개에서 64,000개의 token의 사전에 비해 지나치게 크다. 반면, byte수준의 BPE의 사전은 256개의 token만을 필요로 한다. 그러나 BPE를 byte sequence에 직접 적용하면, 토큰 어휘를 구축하기 위한 BPE의 greedy frequency 기반 heuristic 때문에 최적이 아닌 병합이 발생한다. 예를 들어, \u0026lsquo;dog\u0026rsquo;와 같은 일반적인 단어가 다양한 형태로 나타나면서 제한된 어휘 슬롯과 모델 용량이 최적화되지 않을 수 있다. 이를 해결하기 위해 byte sequence에 대해 문자 범주를 넘어서 병합하는 것을 방지하고, 공백에 대한 예외를 추가하여 압축 효율을 향상시키고 단어의 분열을 최소화하였다.\n이 입력 표현법은 단어 수준 언어 모델의 이점과 byte 수준 접근법의 범용성을 결합시킨다. 이러한 접근법은 어떤 unicode 문자열에도 확률을 부여할 수 있으므로, 사전 처리, 토큰화, 어휘 크기와 관계없이 모든 데이터셋에서 언어 모델을 평가할 수 있다.\nModel Transformer 기반 아키텍처를 사용하며, 이는 주로 OpenAI GPT 모델을 따른다. 몇 가지 수정사항은 레이어 정규화의 위치 변경, 추가적인 레이어 정규화의 삽입, 모델 깊이를 고려한 초기화 방식의 수정, 그리고 잔여 레이어 가중치의 스케일링이다. 또한, 어휘는 50,257개로 확장되었고, 맥락 크기와 배치 크기도 각각 1024 토큰과 512로 증가시켰다.\nExperiments 크기별로 네 개의 언어 모델을 훈련시키고 벤치마킹하였다. 가장 작은 모델의 크기는 원래의 GPT와 같으며, 두 번째로 작은 모델은 BERT의 가장 큰 모델과 같다. 가장 큰 모델인 GPT-2는 GPT보다 10배 이상 많은 parameter를 가지고 있다. 각 모델의 learning rate는 WebText의 5%인 held-out 샘플을 사용하여 수동 조정하으며, 모든 모델은 여전히 WebText에 underfitted 되었으며 더 오래 학습시키면 더 높은 성능을 얻을 수 있을 것이다.\nLanguage Modeling GPT-2 모델은 문자 단위(byte level)에서 작동하고, 손실이 큰 전처리나 토큰화가 필요 없으므로 모든 언어 모델 benchmark에서 평가할 수 있다. WebText 언어 모델에 따른 dataset의 로그-확률을 계산하는 방식으로 평가 하였다. WebText 언어 모델은 표준화된 텍스트, 토큰화 유물, 섞인 문장, 문자열(40 billion 바이트 중 26번만 발생) 등을 예측해야 하기 때문에 많은 데이터셋에서 일반 분포 밖에서 테스트되어야 한다.\nWebText 언어 모델은 도메인과 데이터셋 간에 잘 transfer되며, zero-shot setting에서 8개의 dataset 중 7개에서 state-of-the-art를 달성하였다.\nChildren’s Book Test Children’s Book Test(CBT)는 다양한 카테고리의 단어에 대한 언어 모델의 성능을 평가하기 위한 테스트로, 생략된 단어에 대한 10개의 가능한 선택 중 올바른 것을 예측한다.\n모델 크기가 증가함에 따라 성능이 지속적으로 개선되며, GPT-2는 일반 명사에서 93.3%, 개체명에서 89.1%의 성능을 달성하였다.\nLAMBADA LAMBADA dataset은 텍스트의 장거리 의존성(long-range dependencies)을 평가한다. GPT-2는 이 테스트에서의 perplexity를 99.8에서 8.6으로, 정확도를 19%에서 52.66%로 향상시켰다. 추가적으로, stop-word ﬁlter를 추가함으로써 정확도를 63.24%로 더욱 향상시켰다.\nWinograd Schema Challenge Winograd Schema Challenge는 텍스트의 모호성을 해결하는 능력을 통해 시스템의 상식적 추론 능력을 측정하고자 한다. GPT-2는 정확도를 7% 향상시켜 70.70%를 달성하였다.\nReading Comprehension CoQA(The Conversation Question Answering dataset)는 7가지 다른 분야의 문서와 문서에 대한 질문자-답변자 사이의 자연어 대화가 쌍을 이루고 있다. CoQA 테스트는 독해능력과 대화에 기반한 모델의 답변능력을 평가한다. GPT-2는 미세조정 없이 55 F1 score를 달성해 4개 중 3개의 다른 모델을 능가하였다.\nSummarization GPT-2의 요약 능력은 CNN과 Daily Mail dataset을 사용해서 테스트했다. 문서 이후에 \u0026ldquo;TL;DR:\u0026rdquo; 토큰을 추가하고 Top-k 랜덤 샘플링을 통해 요약을 유도했다.\n처음 생성된 3개의 문장을 요약 결과로 하여 실험한 결과, 기사의 최근 내용에 초점을 맞추거나 특정 세부사항을 혼동하는 경향이 있다. \u0026ldquo;TL;DR:\u0026rdquo; 토큰 없이 실험한 경우, 성능이 더 하락한 것을 보면 힌트를 통한 Task 유도가 유의한 결과를 냄을 확인할 수 있었다.\nTranslation 번역 능력은 WMT-14 English-French dataset을 사용해서, 영어-프랑스어, 프랑스어-영어 두가지 경우에서 비교가 진행되었다. 번역 성능은 다른 Task에 비해 좋지 상대적으로 좋지 않다. 영어-프랑스어 테스트에서 5 BLEU를, 프랑스어-영어 테스트에서는 11.5 BLEU를 달성했다.\nQuestion Answering 언어 모델에 얼마나 많은 정보가 들어있는지 테스트하기 위해 factoid-style의 질문에 얼마나 정확한 답을 생성하는지 평가한다. Natural Questions dataset을 이용해 GPT-2의 성능을 평가하였고 \u0026lsquo;정확히 일치 하는지\u0026rsquo; 여부(exact match metric)를 지표로 비교한다. 질문의 4.1%에 대해 올바르게 답을 하였고, 이는 기존의 모델들보다 5.3배 높은 정확도이다. 매우 작은 모델들은 대체로 1%를 넘지 못하는 성능을 보였는데, 아직까지는 모델의 크기가 QA에 있어서 매우 중요한 요인이라는 것을 확인할 수 있었다. 또한, 가장 확신하는 1%의 질문에 대해 63.1%의 정확도를 보였지만, 이는 여전히 정보 검색과 문서 질문 답변 추출을 결합한 시스템의 30%에서 50% 범위의 성능보다 훨씬 떨어진다.\nGeneralization vs Memorization 최근 연구에 따르면, 일반적인 이미지 데이터셋에는 상당한 양의 중복된 이미지가 포함되어 있어, 기계 학습 시스템의 일반화 성능을 과대평가하게 만든다. 데이터셋의 크기가 커질수록 이 문제는 더욱 심화될 가능성이 있으며, 이는 테스트 데이터가 얼마나 훈련 데이터에도 포함되어 있는지 분석하는 것이 중요함을 의미한다.\n이를 연구하기 위해, WebText 훈련 데이터의 8-gram을 포함하는 Bloom 필터를 생성하였고, 주어진 데이터셋에 대해 그 데이터셋의 8-gram 중 얼마나 많은 비율이 WebText 훈련 세트에도 포함되어 있는지를 계산하였다.\n일반적인 언어 모델 데이터셋의 테스트 세트는 WebText 훈련 세트와 1-6%의 중복을 가지며, 평균 중복률은 3.2%이다. 많은 데이터셋은 자신의 훈련 분할과 더 큰 중복을 가지며, 평균 중복률은 5.9%이다.\n데이터 중복을 최소화하는 방향으로 접근하였으며, 이러한 중복이 성능에 작은, 하지만 일관적인 향상을 가져다 준다는 분석 결과를 얻었다. 중복 제거 기법을 개선함으로써 이러한 문제에 대해 더욱 효과적으로 대응할 수 있다. 그리고 이러한 중복 제거 과정에서는 n-gram 중첩 기반의 방법을 활용하는 것이 중요하다.\n또한, WebText 언어 모델의 성능이 기억력에 의존하는지를 확인하기 위해 해당 모델이 자체 테스트 세트에서 어떤 성능을 보이는지 검사하였다. 이 결과, 모델 크기가 커짐에 따라 훈련 세트와 테스트 세트에서의 성능이 함께 향상되는 경향을 보였으며, 이로부터 GPT-2가 WebText에 대해 완벽하게 적합하지 않음을 추측할 수 있다.\n마지막으로, GPT-2가 말하는 유니콘의 발견에 대한 뉴스 기사를 작성하는 능력을 보여주었다. 이는 GPT-2의 창의성을 보여주는 한 예로 볼 수 있다.\nRelated Work 이 연구는 더 큰 dataset에서 학습된 큰 언어 모델의 성능을 측정하는 데 중점을 두었다. 이 연구는 이전 연구와 비슷한 방향성을 가지고 있으며, 우리의 실험 결과는 주어진 목표의 세부 작업에 대한 추세가 큰 파라미터 범위로도 지속되는 것을 확인하였다.\n생성 모델에서는 RNN 언어 모델이 줄 너비를 추적하고 인용문이나 댓글을 감지하는 등의 흥미로운 기능을 배우는 것이 확인되었다. 또한, 위키백과 기사를 생성하도록 훈련된 모델이 언어 간 이름 번역을 배울 수 있음이 관찰되었다.\niWeb Corpus같이 웹 페이지의 대형 텍스트 말뭉치를 필터링하고 구성하는 다양한 방법, 모든 단어 벡터 표현 학습을 확대하거나, 기계 번역 모델에서 파생된 표현의 사용을 탐색하는 사전학습 방법, seq2seq 모델 등이 연구 되었고, 언어모델의 사전학습이 잡담이나 대화 같은 어려운 생성문제에 맞춰 미세조정할 때 도움이 된다는 것을 밝혀내었다.\nDiscussion 비지도 사전 학습 방법의 표현에 대한 많은 연구가 있었으며, 이는 비지도 학습이 유망한 연구 영역임을 시사한다. GPT-2는 독해에 대해 경쟁력 있는 성능을 보였지만, 요약 등의 작업에 대해서는 아직 기본적인 수준에 불과하다. 많은 NLP 작업에서 GPT-2의 제로샷 성능을 연구했지만, 아직 많은 실용적인 작업에서는 성능이 무작위 수준에 불과한 경우가 많다. 제로샷 성능은 GPT-2의 잠재적 성능의 기준을 설정하지만, 미세 조정을 통한 성능의 상한선은 아직 불분명하다. 더욱이, GPT-2의 추가 훈련 데이터와 용량이 단방향 표현의 비효율성을 극복하기에 충분한지는 아직 불확실하다.\ndecaNLP나 GLUE와 갈은 benchmark에서 미세조정 할 것을 계획하고 있으며, GPT-2의 학습데이터와 그 크기가 BERT에서 말한 단방향 표현의 비효율성을 극복할 수 있을 만큼 충분한지도 확실치 않다고 한다.\nConclusion 충분히 크고 다양한 dataset으로 학습된 큰 언어 모델인 GPT-2는 여러 도메인과 데이터셋에서 잘 수행하며, 테스트된 8개 언어 모델링 dataset 중 7개에서 state-of-the-art를 달성하였다. 이는 고용량 모델이 다양한 텍스트에 대한 가능성을 극대화하는 훈련을 통해, 명확한 지도 없이도 많은 작업을 수행하는 법을 배우기 시작한다는 것을 시사한다.\nReference Paper Github ","date":"2023-12-08T00:00:00Z","permalink":"https://kurtkim.github.io/p/gpt-2/","title":"GPT-2"},{"content":"Abstract transformer는 long-term dependency를 학습할 수 있지만, 언어 모델링에서는 고정된 길이의 컨텍스트에 제한된다. 이를 해결하기 위해, 시간적 일관성을 해치지 않고 고정 길이를 넘어선 의존성을 학습할 수 있는 새로운 아키텍처인 Transformer-XL을 제안한다. 이는 세그먼트 수준의 재발 메커니즘과 새로운 위치 인코딩 체계를 포함하며, long-term dependency를 포착하고 컨텍스트 조각화 문제를 해결한다. 결과적으로 Transformer-XL은 RNN보다 80% 더 긴, 기본 transformer보다 450% 더 긴 의존성을 학습하고, 평가 시간에서 기본 transformer보다 최대 1,800+ 배 더 빠르며, 여러 텍스트 dataset에서 state-of-the-art를 달성하였다.\nIntroduction 언어 모델링은 long-term dependency을 처리해야하는 중요한 과제로, 이는 RNN과 LSTM을 통해 다루어지곤 했다. 그러나 RNN 기반 모델들은 gradient vanishing과 explosion 등의 문제로 인해 최적화에 어려움이 있었다. LSTM은 평균적으로 200개의 문맥 단어를 사용하는데, 이는 아직 개선의 여지가 있다는 것을 보여준다.\nattention mechanism은 장거리 단어 쌍 간의 직접적인 연결을 통해 최적화를 쉽게하고 long-term dependency를 학습하는 데 도움이 된다. 그러나 이 기법은 ﬁxed-length context를 가진 세그먼트에서만 작동하며, 이로 인해 long-term dependency를 충분히 포착하지 못하고, 문맥 파편화(context fragmentation)라는 문제를 야기한다. 이는 모델이 처음 몇 개의 기호를 잘 예측하는 데 필요한 문맥 정보가 부족하게 되어 최적화가 비효율적이고 성능이 떨어지게 된다.\nﬁxed-length context의 제한을 해결하기 위해, Transformer-XL이라는 새로운 아키텍처가 제안되었다. 이는 이전 세그먼트의 은닉 상태를 재사용하고, 이를 통해 세그먼트 간에 순환 연결을 만들어 long-term dependency를 모델링하게 된다. 또한, relative positional encodings을 사용하여 temporal confusion를 일으키지 않고 상태 재사용을 가능하게 한다. 이러한 접근법은 문맥 파편화 문제를 해결하고, 더 긴 주의 길이로 일반화할 수 있는 새로운 relative positional encodings 공식을 도입하였다.\nTransformer-XL은 단어와 문자 수준 언어 모델링에서 뛰어난 결과를 보였다. 이 모델은 오직 100M 토큰만을 학습하여 상대적으로 일관된 긴 텍스트를 생성할 수 있다. 이 모델의 주요 기여는 reuse의 개념을 도입하고 새로운 positional encodings 방식을 개발한 것이다. 이 두 기술은 고정 길이 문맥 문제를 해결하기 위한 완벽한 솔루션을 제공하며, 이들 중 하나만으로는 충분하지 않다. Transformer-XL은 문자와 단어 수준 언어 모델링에서 RNN보다 더 나은 성능을 보여주는 첫 번째 self-attention 모델이다.\n순수하게 self-attention 모델에서 recurrence 개념을 도입하고 새로운 positional encoding 방식을 개발하는 기술적 기여를 하였다. 이 두 가지 기법은 고정 길이 컨텍스트의 문제를 해결하는 완전한 방법을 제공하며, Transformer-XL은 문자와 단어 수준 언어 모델링에서 RNN을 뛰어넘는 첫 self-attention 모델이다.\nRelated Work 언어 모델링 분야는 최근 몇 년 동안 컨텍스트 인코딩을 위한 새로운 구조 개발, 정규화와 최적화 알고리즘의 개선, Softmax 계산의 가속화, 그리고 출력 분포 패밀리의 확장 등 다양한 중요한 발전을 이루어냈다.\n언어 모델링에서 long-range context를 캡처하기 위한 연구들은 네트워크에 추가 입력으로 넓은 컨텍스트의 표현을 제공한다. 이는 수동으로 정의된 컨텍스트 표현부터 데이터에서 학습된 문서 수준의 주제를 사용하는 방식에 이르기까지 다양하다.\n일반적인 시퀀스 모델링에서 long-term dependency를 어떻게 포착할 것인지는 오랫동안 연구되어 온 문제이다. LSTM이 널리 적용된 이후, 기울기 소실 문제를 완화하는 데 많은 노력이 집중되었다. 이런 노력에는 더 나은 초기화, 추가적인 손실 신호, 확장된 메모리 구조, RNN 내부 아키텍처의 수정 등이 포함된다. 하지만, 이 연구는 이들과 달리 Transformer 아키텍처를 기반으로 하며, 언어 모델링이 장기 의존성을 학습하는 능력에서 이익을 얻는다는 것을 보여준다.\nModel 언어 모델링의 목표는 토큰의 말뭉치 $x = (x_1, \u0026hellip;, x_T)$의 결합 확률 $P(x)$를 추정하는 것이다. 이는 auto-regressive로 인수분해되어 문제를 각 조건부 인수를 추정하는 것으로 간소화한다. 신경망은 문맥 $x$를 은닉 상태로 인코딩하고, 이를 단어 임베딩과 곱하여 logit을 얻는다. logit은 softmax 함수를 거쳐 다음 토큰의 확률 분포를 생성한다.\nVanilla Transformer Language Models transformer나 self-attention 메커니즘을 언어 모델링에 적용하는 핵심 문제는, 임의의 긴 문맥을 고정된 크기의 표현으로 효과적으로 인코딩하는 방법입니다. 이론적으로는 전체 문맥을 transformer decoder로 처리하는 것이 가능하지만, 실제로는 제한된 자원 때문에 이는 불가능할 수 있다.\n전체 말뭉치를 관리 가능한 크기의 짧은 세그먼트로 분할하고, 각 세그먼트 내에서만 모델을 학습시키는 것이 한가지 근사치 방법이다. 이는 이전 세그먼트의 문맥 정보를 무시함을 의미한다. 이 방식의 한계는, 가장 큰 의존성 길이가 세그먼트 길이에 의해 제한되고, 이는 문자 수준 언어 모델링에서 몇 백에 불과하다. 또한, 효율성을 높이기 위해 긴 텍스트를 고정 길이 세그먼트로 분할하는 것이 일반적이지만, 이는 문맥 파편화 문제를 초래할 수 있다.\n평가 시, 바닐라 모델은 각 단계에서 학습과 같은 길이의 세그먼트를 사용하지만, 마지막 위치에서만 예측을 한다. 그 다음 단계에서 세그먼트는 오른쪽으로 한 칸 이동하고, 새 세그먼트는 처음부터 다시 처리해야 합니다. 이 방식은 각 예측이 학습 중에 제공된 가장 긴 문맥을 활용하도록 하며, 문맥 파편화 문제를 완화하지만, 매우 비용이 많이 든다. 이 논문이 제안하는 아키텍처는 평가 속도를 크게 향상시킬 수 있다.\nSegment-Level Recurrence with State Reuse 고정 길이 문맥의 한계를 극복하기 위해, transformer 아키텍처에 recurrence 메커니즘을 도입하는 것을 제안한다. 학습 중에는 이전 세그먼트의 은닉 상태 시퀀스가 고정되어 캐시되고, 이는 모델이 다음 새로운 세그먼트를 처리할 때 확장된 문맥으로 재사용된다. 이 추가 입력은 네트워크가 히스토리 내의 정보를 활용하게 하여 장기 의존성을 모델링하고 문맥 파편화를 피하게 한다. 공식적으로, 두 연속 세그먼트는 각각 $s_\\gamma$와 $s_{\\gamma+1}로 표시되며, 각 세그먼트에 대한 n 번째 layer 은닉 상태는 $h_n^\\gamma$로 표시된다.\n$$ \\tilde{h}_{\\gamma+1}^{n-1} = [SG(h_{\\gamma}^{n−1}) \\circ h_{\\gamma+1}^{n-1}] $$\n$$ q_{\\gamma}^{n+1}, k_{\\gamma}^{n+1}, v_{\\gamma}^{n+1} = h_{\\gamma+1}^{n-1}W_q^\\intercal, \\tilde{h}_{\\gamma+1}^{n-1}W_k^\\intercal, \\tilde{h}_{\\gamma+1}^{n-1}W_v^\\intercal $$\n$$ h_{\\gamma}^{n+1} = \\text{Transformer-Layer}(q_{\\gamma}^{n+1}, k_{\\gamma}^{n+1}, v_{\\gamma}^{n+1}) $$\n여기서 함수 $SG(·)$는 stop-gradient를 나타내고, 표기법 $[h_u \\circ h_v]$는 길이 차원을 따라 두 은닉 시퀀스의 연결을 나타내며, $W$는 모델 parameter를 나타낸다. standard Transformer와 비교해 볼 때, 핵심 차이점은 키 $k_{\\gamma}^{n+1}$과 값 $v_{\\gamma}^{n+1}$이 확장된 문맥 $h$와 따라서 이전 세그먼트에서 캐시된 $h_{\\gamma}^{n+1}$에 의존한다는 점이다.\n이 recurrence 메커니즘을 적용하면, 은닉 상태에서 세그먼트 수준의 recurrence를 생성하게 되어, 효과적으로 활용되는 문맥이 두 세그먼트를 넘어서 확장될 수 있다. 하지만, recurrence 의존성이 세그먼트 당 한 layer씩 아래로 이동한다는 점이 전통적인 RNN-LM과는 다르다. 이로 인해, 가능한 가장 큰 의존성 길이는 layer 수와 세그먼트 길이에 대해 선형적으로 증가한다. 이는 RNN-LM을 학습하기 위해 개발된 truncated BPTT와 유사하지만, 마지막 상태 대신 은닉 상태의 시퀀스를 캐시하는 점에서 다르다. 이 방법은 relative positional encoding 기법과 함께 적용해야 한다.\nrecurrence scheme를 도입하면, 더 긴 문맥을 활용할 수 있고 파편화 문제를 해결할 뿐만 아니라, 평가 시간이 크게 단축되는 이점도 얻을 수 있다. 평가 시에 이전 세그먼트의 표현을 재사용함으로써, Transformer-XL은 바닐라 모델에 비해 최대 1,800배 이상 빠르게 평가할 수 있다.\nrecurrence scheme는 이전 세그먼트에만 제한될 필요가 없다. 이론적으로, GPU 메모리가 허용하는 한큼 많은 이전 세그먼트를 캐시하고, 그것들을 추가적인 문맥으로 재사용할 수 있다. 따라서, 사전 정의된 길이 $M$의 오래된 은닉 상태를 캐시하고, 이를 메모리라고 부른다. 실험에서는 학습 중에 $M$을 세그먼트 길이와 동일하게 설정하고, 평가 중에는 $M$을 여러 배 늘렸다.\nRelative Positional Encodings 이전 섹션에서 제시한 아이디어는 매력적이지만, 은닉 상태 재사용에 대한 중요한 기술적 도전이 있다. 특히, 상태를 재사용할 때 위치 정보의 일관성을 어떻게 유지할 것인지가 문제이다. standard transformer에서 시퀀스 순서 정보는 positional encoding을 통해 제공되며, transformer의 실제 입력은 word embedding과 positional encoding의 요소별 덧셈이다. 이 positional encoding을 recurrence 메커니즘에 적용하면, 은닉 상태 시퀀스는 특정 방식으로 계산된다.\n$$ h_{\\gamma+1} = f(h_\\gamma, E_{s_{\\gamma+1}} + U_{1:L}) $$\n$$ h_{\\gamma} = f(h_{\\gamma-1}, E_{s_{\\gamma}} + U_{1:L}) $$\n여기서 $E_{s_{\\gamma}} \\in \\mathbb{R}^{L×d}$는 $s_\\gamma$의 단어 임베딩 시퀀스이고, $f$는 변환 함수를 나타낸다. $E_{s_{\\gamma}}$와 $E_{s_{\\gamma+1}}$ 모두 같은 positional encoding $U_{1:L}$와 연관되어 있음을 알 수 있다. 결과적으로, 모델에는 어떤 $j = 1, \u0026hellip;, L$에 대해 $x_{\\gamma, j}$와 $x_{\\gamma+1,j}$ 사이의 위치 차이를 구별하는 정보가 없어, 성능 손실이 발생한다.\nstate reuse 메커니즘을 가능하게 하기 위해, 은닉 상태에 relative position 정보만을 인코딩하는 것이 필요하다. 이를 위해, 각 layer의 attention score에 relative position 정보를 주입한다. 이는 relative position을 동적으로 attention score에 주입함으로써, 쿼리 벡터가 $x_{\\gamma, j}$와 $x_{\\gamma+1, j}$의 표현을 그들의 다른 거리에 따라 쉽게 구분할 수 있게 해준다. 이를 통해, state reuse 메커니즘이 가능해지고, absolute position은 relative position에서 재귀적으로 복구될 수 있으므로, 시간 정보를 잃지 않게 된다.\nrelative positional encoding의 개념은 이전에 기계 번역과 음악 생성에서 탐구되었다. 이 논문은 이를 다르게 유도하여, absolute positional encoding과 일대일로 대응하면서도 실증적으로 더 나은 일반화를 보여주는 새로운 형태의 relative positional encoding을 제시한다. standard transformer에서는 같은 세그먼트 내의 query와 key 벡터 사이의 attention score를 분해할 수 있다.\n$$ A_{i,j}^{abs} = E_{x_i}^\\intercal W_q^\\intercal W_k E_{x_j} + E_{x_i}^\\intercal W_q^\\intercal W_k U_j + U_i^\\intercal W_q^\\intercal W_k E_{x_j}+ U_i^\\intercal W_q^\\intercal W_k U_j $$\nrelative position 정보에만 의존하는 아이디어를 따라, 다음과 같이 네 가지 항을 reparameterize하려고 제안한다.\n$$ A_{i,j}^{abs} = E_{x_i}^\\intercal W_q^\\intercal W_k E_{x_j} + E_{x_i}^\\intercal W_q^\\intercal W_k \\color{#6580DD}{R_{i-j}}+ \\color{#DD6565}{u^\\intercal} W_q^\\intercal W_k E_{x_j}+ \\color{#DD6565}{v^\\intercal} W_q^\\intercal W_k \\color{#6580DD}{R_{i-j}} $$\n가장 먼저 변경하는 것은 key 벡터를 계산하기 위해 절대 위치 임베딩 $U_j$의 모든 출현을 그 상대적 대응체 $\\color{#6580DD}{R_{i-j}}$로 대체하는 것이다. 이는 주목할 위치에 대해서는 상대 거리만이 중요하다는 사전 정보를 반영하는 것이다. $\\color{#6580DD}{R}$은 학습 가능한 parameter 없는 sinusoid encoding matrix이다. 쿼리 위치에 관계없이 다른 단어에 대한 주목 편향성이 동일하게 유지되도록 학습 가능한 parameter를 도입한다. 이를 위해, 학습 가능한 parameter $\\color{#DD6565}{u}$ 와 $\\color{#DD6565}{v}$를 각각 도입하여 쿼리와 관련된 항을 대체한다. 마지막으로, 내용 기반의 키 벡터와 위치 기반의 키 벡터를 생성하기 위해 두 가지 가중치 행렬 $W_{k, E}$ 와 $W_{k, R}$를 의도적으로 분리한다. 새로운 parameter화를 통해 각 항은 다음과 같은 의미를 갖게 된다: 첫번째 항은 내용 기반 주소 지정, 두번째 항은 내용에 따른 위치 편향, 세번째 항은 전역 내용 편향, 그리고 마지막 항은 전역 위치 편향을 나타낸다.\nShaw et al. (2018)의 접근법은 첫번째 항과 두번째 항 만을 가지고 있으며, 두 편향 항을 생략한다. 또한, original sinusoid positional encoding에 내장된 inductive bias를 포기하고 있다. 반면에, 이 논문의 방법은 sinusoid 공식을 적용한 relative positional embedding을 사용한다. 이로 인해, 특정 길이의 메모리에서 학습된 모델은 평가 시에 메모리를 몇 배 더 길게 자동으로 일반화할 수 있다.\nrelative positional embedding을 이용한 recurrence 메커니즘을 적용하여, Transformer-XL 아키텍처를 도출하였다. 이 아키텍처는 single attention head를 가진 N-layer로 구성되며, 그 계산 절차를 요약하면 다음과 같다. $n = 1, \u0026hellip;, N$에 대해:\n$$ \\tilde{h}_{\\gamma}^{n-1} = [SG(h_{\\gamma}^{n−1}) \\circ h_{\\gamma}^{n-1}] $$\n$$ q_{\\gamma}^{n}, k_{\\gamma}^{n}, v_{\\gamma}^{n} = h_{\\gamma}^{n-1}W_q^\\intercal, \\tilde{h}_{\\gamma}^{n-1}W_k^\\intercal, \\tilde{h}_{\\gamma}^{n-1}W_v^\\intercal $$\n$$ A_{\\gamma, i, j}^n = {q_{\\gamma, i}^n}^\\intercal k_{\\gamma, j}^n + {q_{\\gamma, i}^n}^\\intercal W_{k, R}^n R_{i - j} + u^\\intercal k_{\\gamma, j} + v^\\intercal W_{k, R}^n R_{i - j} $$\n$$ a_\\gamma^n = Masked-Softmax(A_\\gamma^n)v_\\gamma^n $$\n$$ o_\\gamma^n = LayerNorm(Linear(a_\\gamma^n) + h_\\gamma^{n-1}) $$\n$$ h_\\gamma^{n} = Positionwise-Feed-Forward(o_\\gamma^n) $$\n$h_\\gamma^0 := E_{s_\\gamma}$는 단어 임베딩 시퀀스로 정의된다. $A$를 계산하는 간단한 방법은 모든 쌍 $(i, j)$에 대해 계산을 수행하며, 이는 시퀀스 길이에 대해 이차적인 비용을 요구한다. 하지만, 이 연구에서는 $i − j$의 값 범위를 인지하고 시퀀스 길이에 대해 선형적인 비용으로 줄이는 계산 절차를 제시한다.\nExperiments Main Results Transformer-XL을 다양한 데이터셋에 적용하여, 단어 수준과 문자 수준의 언어 모델링에서 state-of-the-art 시스템들과 비교하였다. 이 데이터셋들은 WikiText-103, enwik8, text8, One Billion Word, 그리고 Penn Treebank를 포함한다.\nWikiText-103은 장기 의존성을 가진 가장 큰 단어 수준 언어 모델링 벤치마크이다. 이를 활용해, 우리는 Transformer-XL의 학습과 평가를 진행했고, 그 결과 Transformer-XL은 이전의 state-of-the-art를 대폭 뛰어넘는 결과를 보여주었다.\n데이터셋 enwik8에서도 Transformer-XL 아키텍처는 이전의 state-of-the-art를 뛰어넘는 새로운 결과를 달성하였다다. 12-layer의 Transformer-XL은 모델 크기 제약하에도 불구하고, 기존의 RNN 기반 모델과 큰 차이를 보였다. 더 큰 모델 크기로 18-layer와 24-layer의 Transformer-XL을 학습시켰을 때, 문자 수준 벤치마크에서 1.0을 돌파하는 첫 번째 방법으로서 state-of-the-art를 달성하였다.\ntext8은 enwik8과 비슷하지만, 텍스트를 소문자로 변환하고 특정 문자를 제거하여 처리된 100M개의 Wikipedia 문자를 포함한다. 이러한 유사성 때문에, enwik8에서의 최적의 모델과 hyperparameter를 그대로 text8에 적용하였고, Transformer-XL은 명확한 차이로 state-of-the-art를 달성하였다.\nOne Billion Word 데이터셋은 문장이 섞여 있어 long-term dependency를 보존하지 않으며, 주로 short-term dependency 모델링 능력을 테스트한다. Transformer-XL은 주로 long-term dependency를 더 잘 포착하기 위해 설계되었지만, 이 데이터셋에서도 단일 모델 state-of-the-art를 크게 향상시켰다. 이는 Transformer-XL의 장점이 짧은 시퀀스 모델링에도 적용될 수 있음을 보여준다.\n단어 수준 데이터셋인 Penn Treebank를 테스트하기 위해, variational dropout과 weight average을 적용하여 Transformer-XL을 수정하였다. 적절한 regularization을 통해, Transformer-XL은 두 단계의 미세 조정 없이 state-of-the-art를 달성하였다. 이는 Transformer-XL이 작은 데이터셋에서도 잘 일반화될 수 있음을 보여준다.\nAblation Study Transformer-XL에서 사용한 recurrence 메커니즘과 새로운 positional encoding scheme의 효과를 검증하기 위해 두 가지 ablation study를 수행하였다.\n첫 번째 연구는 long-term dependency를 요구한는 데이터셋인 WikiText-103에서 수행되었다. absolute encoding은 half loss와 함께 잘 작동하는 것으로 나타났다. 또한, recurrence 메커니즘과 인코딩 체계 모두가 최고의 성능을 달성하고, 평가 시간 동안 더 긴 attention 시퀀스로 일반화하는데 필요하다는 것이 확인되었다. 학습 중에 backpropagation 길이는 128이지만, 이 두 기술을 사용하면 테스트 시간에 attention 길이를 640까지 늘릴 수 있다.\nrecurrence 메커니즘이 추가 메모리를 요구함에도 불구하고, 같은 GPU 메모리 제약 하에서 Transformer-XL은 기준 모델들에 비해 더 우수한 성능을 보여주었다.\n두 번째 연구는 long-term dependency를 요구하지 않는 데이터셋인 One Billion Word에서 실험을 수행하였다. 결과적으로, 세그먼트 수준의 recurrence를 사용하면 long-term dependency이 필요하지 않은 경우에도 성능이 크게 향상된다는 것을 확인하였다. 또한, relative positional encoding은 짧은 시퀀스에서도 우수한 성능을 보여주었다.\nRelative Effective Context Length Khandelwal et al. 이 제안한 Effective Context Length(ECL) 대신, Relative Effective Context Length(RECL)라는 새로운 지표를 제안하였다. 이 지표는 모델 그룹에 대해 정의되며, 긴 문맥의 이득은 최고의 짧은 문맥 모델에 대한 상대적 개선으로 측정된다. Transformer-XL은 평균적으로 900단어의 의존성을 모델링할 수 있으며, RECL은 RNN과 transformer보다 각각 80%, 450% 더 길다는 결과를 보여주었다. 이는 Transformer-XL이 long-term dependency을 모델링할 수 있다는 것을 뒷받침한다.\nGenerated Text 중간 크기의 WikiText-103에서만 학습된 Transformer-XL은 소소한 결점에도 불구하고 수천 개의 토큰으로 구성된 일관성 있는 기사를 생성할 수 있다.\nEvaluation Speed Transformer-XL와 바닐라 transformer 모델의 평가 속도를 비교한 결과, state reuse scheme 덕분에 Transformer-XL은 평가 중에 최대 1,874배의 속도 향상을 보여주었다.\nConclusions Transformer-XL은 강력한 perplexity 결과를 보이고, longer-term dependency를 더 잘 모델링하며, 평가 속도를 크게 향상시키고, 일관성 있는 텍스트를 생성할 수 있다. 이것은 텍스트 생성, 비지도 학습, 이미지와 음성 모델링 등의 분야에서 흥미로운 응용을 가능하게 한다.\nReference Paper Github ","date":"2023-12-06T00:00:00Z","permalink":"https://kurtkim.github.io/p/transformer-xl/","title":"Transformer-XL"},{"content":"Abstract BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 양방향 인코더 표현을 사용하는 언어모델로, BERT는 레이블이 없는 텍스트에서 깊은 양방향 표현을 사전 학습함으로써, 하나의 출력 레이어만을 추가해서 다양한 작업에 맞게 미세조정 할 수 있다. 이 모델은 개념적으로 단순하면서도 실증적으로 강력하며, 다양한 자연어 처리 작업에서 새로운 최고 수준의 결과를 달성하였다.\nIntroduction 언어 모델의 사전 학습은 다양한 자연어 처리 작업의 향상에 크게 기여하였다. 이는 문장 전체를 분석하여 문장 간 관계를 예측하는 자연어 추론이나 재구성과 같은 문장 수준의 작업뿐만 아니라, 명명된 개체 인식이나 질문 응답과 같이 토큰 수준의 작업을 포함한다.\n사전 학습된 모델을 downstream tasks에 적용하는 데는 두 가지 전략이 있다: feature-based와 미세조정(fine-tuning)이다. feature-based 접근법은 ELMo와 같이 사전 훈련된 표현을 추가 특성으로 사용하고, 미세 조정 접근법은 GPT와 같이 작업 특정 파라미터를 최소화하고 사전 훈련된 모든 파라미터를 미세 조정한다. 이 두가지 접근 방식은 사전 학습을 하는동안 같은 목적 함수를 공유하며, 일반적인 언어 표현을 학습하기 위해 단방향 언어 학습 모델을 사용한다.\n현재의 기술들은 특히 미세 조정 접근법에 대해 사전 학습된 표현의 가능성을 제한한다고 주장한다. 표준 언어 모델이 단방향적이므로, 사전 훈련 동안 사용할 수 있는 아키텍처가 제한되기 때문이다. OpenAI GPT의 경우, 모든 토큰이 이전 토큰만을 주목하는 \u0026lsquo;왼쪽에서 오른쪽으로\u0026rsquo;의 아키텍처를 사용한다. 이러한 제한은 문장 수준 작업이나 양방향 맥락 통합이 중요한 토큰 수준 작업에 대해 불리하다.\n이 논문에서는 BERT(Bidirectional Encoder Representations from Transformers)를 제안하여 미세 조정 기반 접근법을 개선한다. BERT는 일부 토큰을 무작위로 마스크한 \u0026ldquo;masked language model\u0026quot;을 사용하여 단방향성 제약을 완화하고, 이를 통해 깊은 양방향 Transformer를 사전 학습한다. 그리고 \u0026ldquo;next sentence prediction\u0026rdquo; 작업을 통해 텍스트 쌍 표현을 함께 사전 학습한다.\n언어 표현에 대한 양방향 사전 훈련의 중요성을 강조한다. BERT는 마스크된 언어 모델을 사용해 깊은 양방향 표현을 사전 학습하는데, 이는 단방향 언어 모델을 사용하는 기존 방법과 대조적이다. 또한, 독립적으로 훈련된 언어 모델을 얕게 연결하는 방식과도 차별화된다.\n사전 학습된 표현은 작업별 아키텍처의 필요성을 줄이며, BERT는 다양한 작업에서 최고 수준의 성능을 달성하는 첫 미세 조정 기반 표현 모델이다. 이는 다수의 작업별 아키텍처들의 성능을 능가한다.\nBERT는 11가지 자연어 처리(NLP) 작업에 대해 state-of-the-art 성능을 달성했다.\nRelated Work 일반적인 언어 표현을 사전 학습하는 것은 오랜 역사를 가지고 있으며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략하게 검토한다.\nUnsupervised Feature-based Approaches 단어의 범용적인 표현 학습은 비신경망 및 신경망 방법을 포함한 여러 연구의 주제였다. 사전 학습된 단어 임베딩은 현대 NLP 시스템의 핵심 부분이며, 처음부터 학습된 임베딩보다 크게 개선된다. 단어 임베딩 벡터를 사전 훈련하기 위해, 언어 모델링 목표와 맥락에서 단어를 구별하는 목표가 사용되었다.\n이러한 접근법은 문장이나 문단 임베딩 등의 더 큰 단위로 확장되었다. 문장 표현을 훈련시키기 위해, 후보 다음 문장을 순위 매기는 연구, 이전 문장의 표현을 기반으로 다음 문장의 단어를 생성하는 연구, 또는 노이즈 제거 오토인코더에서 파생된 연구 등이 있었다.\nELMo와 그 전임자는 단어 임베딩 연구를 다른 차원으로 확장해, 맥락에 따라 변하는 특징을 추출하였다. 이는 언어 모델의 왼쪽과 오른쪽 표현을 연결해 이루어진다. ELMo는 이를 기존의 작업 특정 아키텍처와 결합해 주요 NLP 벤치마크를 향상시켰다. 또한, 다른 연구들은 LSTM을 이용해 맥락적 표현을 학습하거나, 클로즈 작업을 통해 텍스트 생성 모델의 견고성을 향상시키는 방법을 제안하였다.\nUnsupervised Fine-tuning Approaches 첫 번째 연구들은 레이블이 없는 텍스트에서 단어 임베딩 파라미터만 사전 학습하였다.\n최근에는 문장이나 문서 인코더가 레이블이 없는 텍스트에서 사전 훈련되고, 지도학습의 다음 작업을 위해 미세조정되었다. 이 방법의 장점은 적은 양의 파라미터만 처음부터 학습하면 된다는 것이며, 이 때문에 OpenAI GPT는 여러 문장 수준 작업에서 최고 성능을 달성하였다. 이러한 모델을 사전 학습하기 위해 왼쪽에서 오른쪽으로의 언어 모델링과 오토인코더가 사용되었다.\nTransfer Learning from Supervised Data 대규모 데이터셋을 가진 자연어 추론과 기계 번역 등의 감독 학습 작업에서 효과적인 전이 학습이 보여주었다. 또한, 컴퓨터 비전 연구에서는 ImageNet으로 사전 훈련된 대규모 모델을 미세조정하여 전이 학습의 중요성을 입증하였다.\nBERT BERT는 사전 학습과 미세 조정 두 단계로 이루어진다. 사전 학습에서는 레이블이 없는 데이터로 모델을 학습하며, 미세 조정에서는 사전 훈련된 파라미터로 초기화된 BERT 모델을 downstream tasks의 레이블이 붙은 데이터로 미세 조정한다. 각 downstream tasks는 동일한 사전 훈련된 파라미터로 초기화되지만 별도의 미세 조정된 모델을 가진다.\nBERT는 다양한 작업에 걸친 통일된 아키텍처이며, 사전 학습된 아키텍처와 최종 downstream tasks 아키텍처 사이에는 거의 차이가 없다.\nModel Architecture BERT의 구조는 다중 레이어 양방향 Transformer 인코더로, Transformer의 널리 쓰이는 사용과 거의 동일하기 때문에, 모델 아키텍처의 상세한 설명은 \u0026ldquo;The Annotated Transformer\u0026quot;를 참조하도록 권장한다.\n이 연구에서는 레이어의 개수(Transformer 블록)를 $L$, 은닉의 크기를 $H$, self-attention 헤드 수를 $A$로 표시한다. $BERT_{BASE}$ (L=12, H=768, A=12, Total Parameters=110M)와 $BERT_{LARGE}$ (L=24, H=1024, A=16, Total Parameters=340M)의 두 모델 사이즈에 대한 결과를 비교한다.\n비교를 위해 $BERT_{BASE}$는 같은 모델 사이즈인 OpenAI GPT와 비교하였다. 그러나 중요한 점은, BERT Transformer는 양방향 self-attention을 사용하는 반면, GPT Transformer는 각 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 self-attention을 사용한다.\nInput/Output Representations BERT의 입력 표현은 단일 문장과 문장 쌍(예: 〈질문, 답변〉)을 하나의 토큰 시퀀스에서 명확하게 표현할 수 있다. 여기서 \u0026ldquo;문장(senetence)\u0026ldquo;은 실제 문장이 아닌 텍스트의 일부를, \u0026ldquo;시퀀스(sequence)\u0026ldquo;는 BERT 입력 토큰을 의미하며, 이는 하나 또는 두 개의 문장일 수 있다.\n30,000개의 토큰 단어를 가진 WordPiece임베딩을 사용하며, 모든 문장의 첫번째 토큰은 특별한 분류 토큰([CLS])으로 시작한다. 이 토큰에 해당하는 마지막 은닉 상태(hidden state)는 분류 작업을 위해 총 시퀀스 표현으로 사용된다. 문장 쌍은 하나의 시퀀스로 묶이며, 특별한 토큰([SEP])과 학습된 임베딩을 사용해 문장을 구분한다. 입력 임베딩은 $E$로, [CLS] 토큰과 i번째 입력 토큰의 최종 은닉 벡터는 각각 $C \\in R^H$, $T_i \\in R^H$ 로 표기한다.\n주어진 토큰에 대한 입력 표현은 해당 토큰, 세그먼트, 위치 임베딩을 합하여 구성된다.\nPre-training BERT BERT를 사전 학습하기 위해 전통적인 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로의 언어 모델을 사용하지 않았으며, 두 가지 비지도 학습 작업을 사용하여 BERT를 사전 학습하였다.\nTask #1: Masked LM 깊은 양방향 모델은 왼쪽에서 오른쪽 혹은 오른쪽에서 왼쪽 모델보다 강력하다는 것이 직관적이다. 하지만, 표준 언어 모델은 양방향 조건을 적용하면 각 단어가 간접적으로 자신을 \u0026lsquo;볼\u0026rsquo; 수 있게 되어, 모델이 문맥 속의 목표 단어를 쉽게 예측할 수 있게 되므로, 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로만 훈련된다.\n깊은 양방향 표현을 훈련하기 위해, 입력 토큰의 일부를 무작위로 마스킹하고 그 마스킹된 토큰들을 예측하는 \u0026lsquo;Masked LM\u0026rsquo; 방법을 사용한다. 이 방법은 Cloze 작업으로 불리기도 한다. 마스킹된 토큰에 해당하는 최종 은닉 벡터는 어휘에 대한 출력 softmax에 공급되며, 모든 실험에서 각 시퀀스의 15% 토큰을 무작위로 마스킹한다. 이 방법은 denoising auto-encoders와 달리 마스킹된 단어만을 예측한다.\n양방향 사전 학습 모델을 얻는 방법은 [MASK] 토큰이 미세 조정 과정에서 나타나지 않는 문제로 인해 사전 훈련과 미세 조정 사이에 불일치를 유발한다. 이를 완화하기 위해, \u0026ldquo;마스킹된\u0026rdquo; 단어를 항상 [MASK] 토큰으로 교체하지 않는다. 토큰 위치의 15%를 임의로 선택하고, 선택된 토큰은 80% 확률로 [MASK] 토큰, 10% 확률로 임의의 토큰, 10% 확률로 원래 토큰으로 교체한다. 그 후, 크로스 엔트로피 손실을 사용하여 원래 토큰을 예측한다.\nTask #2: Next Sentence Prediction (NSP) 질문 응답(QA)과 자연어 추론(NLI) 같은 작업들은 두 문장 간의 관계를 이해하는 것에 기초하며, 이는 언어 모델링만으로는 직접적으로 캡처할 수 없다. 이를 해결하기 위해, 문장간 관계를 이해하는 모델을 위한 사전 훈련 과정에서, 어떤 하나의 언어를 사용하는 말뭉치로부터 생성될 수 있는 다음 문장 예측 과제를 2진화(binarized)된 다음 문장 예측 작업을 한다. 구체적으로, 각 사전 훈련 예제에서, 선택된 두 문장 A와 B는 50%의 확률로 실제 연속하는 문장이며, 나머지 50%는 말뭉치에서 임의로 선택된 문장이다. 이 간단한 방법이 QA와 NLI에 큰 도움이 된다는 것을 입증하였다.\n다음 문장 예측(NSP) 작업은 이전 연구와 밀접한 관련이 있지만, 이전 연구에서는 문장 임베딩만을 하위 작업에 전달했으나, BERT는 모든 매개변수를 최종 작업 모델 초기화에 사용한다.\nPre-training data 사전 학습 절차는 대부분 언어 모델 사전 학습에 대한 기존 연구를 따른다. 이때 사용되는 말뭉치는 BooksCorpus(800M개의 단어)와 영어 위키백과(2,500M개의 단어)이다. 위키백과에서는 텍스트 부분만 추출하며, 긴 연속적인 시퀀스를 추출하기 위해 문장 수준이 아닌 문서 수준의 말뭉치 사용이 중요하다는 점을 강조한다.\nFine-tuning BERT Transformer의 자기 self-attention mechanism을 활용한 BERT의 미세 조정은 단일 텍스트나 텍스트 쌍을 포함한 다양한 downstream tasks를 모델링하는데 효과적이다. 텍스트 쌍을 독립적으로 인코딩한 후 양방향 cross attention을 적용하는 것이 일반적이지만, BERT는 이 두 단계를 통합하여 self-attention으로 텍스트 쌍을 인코딩함으로써 두 문장 간의 양방향 cross attention을 효과적으로 포함시킨다.\n각 작업마다 BERT에 작업 특정 입력과 출력을 연결하고 모든 매개변수를 미세 조정한다. 입력에서, 사전 훈련된 문장 A와 B는 다양한 작업(표현 변경, 함축, 질문 답변, 텍스트 분류 등)의 입력 쌍에 상응한다. 출력에서, 토큰 표현은 토큰 수준 작업에, [CLS] 표현은 분류 작업에 사용된다.\n미세 조정은 사전 훈련에 비해 상대적으로 저렴하며, 이 논문의 모든 결과는 동일한 사전 훈련 모델을 기반으로 클라우드 TPU에서는 1시간, GPU에서는 몇 시간 안에 재현 가능하다.\nExperiments 11가지 자연어 처리(NLP) 작업에 대한 BERT 미세 조정 결과를 보여준다.\nGLUE General Language Understanding Evaluation(GLUE) benchmark는 다양한 자연어 이해 작업의 모음이다.\nGLUE에서 미세 조정을 하기 위해, 입력 시퀀스를 표현하고 첫 번째 입력 토큰([CLS])에 대응하는 최종 벡터를 종합 표현으로 사용한다. 미세 조정시 도입되는 유일한 새로운 매개변수는 분류 레이어의 가중치이다. 그리고 이들을 사용해 standard classiﬁcation 손실을 계산한다.\nbatch size 32로 데이터를 3 epoch 동안 미세 조정한다. 각 작업마다 최적의 미세 조정 학습률을 선택하며, $BERT_{LARGE}$는 작은 데이터셋에서 불안정할 때 랜덤 재시작을 사용하여 최적의 모델을 선택한다. 랜덤 재시작에서는 동일한 사전 학습 체크포인트를 사용하지만 데이터 셔플링과 분류기 레이어 초기화는 다르게 합니다.\n$BERT_{BASE}$와 $BERT_{LARGE}$는 모든 작업에서 월등한 성능을 보여주며, 평균 정확도를 각각 4.5%, 7.0% 향상시켰다. 가장 큰 GLUE 작업인 MNLI에서 BERT는 정확도를 4.6% 향상시켰고, 공식 GLUE 리더보드에서는 $BERT_{LARGE}$가 80.5의 점수로 OpenAI GPT의 72.8을 능가했다.\n$BERT_{LARGE}$가 모든 작업에서 $BERT_{BASE}$를 크게 능가하며, 특히 훈련 데이터가 매우 적은 작업에서 그렇다는 것을 발견하였다.\nSQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1)는 10만 개의 크라우드 소싱 질문/답변 쌍의 컬렉션이다. 주어진 질문과 답변을 포함하는 위키백과의 문단이 주어지면, 그 문단 내에서 실제 답변의 위치나 범위를 정확하게 예측하는 것이 목표이다.\n질문 응답 작업에서는 입력 질문과 문단을 하나의 연결된 시퀀스로 표현하며, 각각 다른 임베딩을 사용한다. 미세 조정 과정에서는 시작과 끝 벡터만 추가적으로 도입되며, 답변의 시작 단어 확률은 해당 단어와 시작 벡터 간의 내적 후 softmax를 적용하여 계산된다.\n답변 범위의 시작과 끝 위치를 예측하는 공식이 사용되며, 그 중 최대 점수를 가진 범위가 최종 예측값이 된다. 훈련 목표는 정확한 시작과 끝 위치의 로그 가능도 합이며, learning rate 5e-5와 batch size 32로 3 epoch 동안 미세 조정이 이루어진다.\nBERT 모델은 다른 공개 데이터를 사용하여 학습된 모델들을 뛰어넘으며, 특히 TriviaQA에 먼저 미세 조정을 함으로써 성능을 향상시켰다. 단일 BERT 모델만으로도 최고의 앙상블 시스템을 능가하며, TriviaQA 데이터 없이도 모든 기존 시스템을 크게 앞서고있다.\nSQuAD v2.0 SQuAD 2.0 작업은 제공된 문단에 짧은 답변이 존재하지 않을 수 있다는 가능성을 허용함으로써 SQuAD 1.1 문제 정의를 확장하였고, 이로 인해 문제가 더 현실적으로 변하였다.\n이 작업을 위해 SQuAD v1.1 BERT 모델을 간단하게 확장하였다. 답변이 없는 질문은 시작과 끝이 [CLS] 토큰에 있는 답변 범위로 취급하였다. 예측 시, 답변이 없는 범위의 점수와 최고의 비-null 범위의 점수를 비교하여, 특정 임계값을 넘을 경우 non-null 답변을 예측하였다. 이 모델에서는 TriviaQA 데이터를 사용하지 않았으며, learning rate 5e-5와 batch size 48로 2 epoch 동안 미세 조정을 수행하였다.\nBERT를 사용하지 않는 시스템들과의 결과를 비교하였다. 다른 모델들에 비해 F1 점수가 5.1점 향상되었다.\nSWAG The Situations With Adversarial Generations (SWAG) dataset는 실제 상식 추론을 평가하는 113k개의 문장 쌍 완성 예제를 포함하고 있다. 주어진 문장에 대해, 작업은 네 가지 선택지 중 가장 그럴듯한 답을 선택하는 것이다.\nSWAG dataset에서 미세 조정을 할 때, 각각 주어진 문장과 가능한 연속성을 포함하는 네 개의 입력 시퀀스를 만든다. [CLS] 토큰 표현과 내적을 이루는 벡터는 각 선택지에 대한 점수를 나타내며, 이 점수는 softmax 레이어를 통해 정규화한다.\nlearning rate가 2e-5이고 batch size가 16인 상태로 모델을 3 epoch 동안 미세 조정하였다. $BERT_{LARGE}$는 ESIM+ELMo 모델을 +27.1%로, OpenAI GPT를 8.3%로 능가하였다.\nAblation Studies 상대적인 중요성을 더 잘 이해하기 위해 BERT의 여러 면에 걸쳐서 ablation 실험을 수행한다.\nEffect of Pre-training Tasks $BERT_{BASE}$의 동일한 사전 학습 데이터, 미세 조정 scheme, 그리고 hyperparameter를 사용하여 두 가지 사전 학습 목표를 평가함으로써 BERT의 깊은 양방향성의 중요성을 입증한다.\nNo NSP: \u0026ldquo;다음 문장 예측(NSP)\u0026rdquo; 과제를 하지 않은, \u0026ldquo;Masked LM(MLM)\u0026ldquo;을 사용해 훈련된 양방향 모델\nLTR \u0026amp; No NSP: 왼쪽 컨텍스트만 있는 모델은 표준 LTR LM을 사용해 훈련되며, 이는 미세 조정 시에도 유지된다. 이 모델은 NSP 작업 없이 사전 훈련되었다.\nNSP 작업을 제거하면 QNLI, MNLI, SQuAD 1.1에서 성능이 크게 저하된다. 또한, 양방향 표현을 훈련하는 것은 성능에 중요한 영향을 미치며, 특히 LTR 모델은 모든 작업에서 MLM 모델보다 성능이 떨어진다.\nSQuAD의 경우, 토큰 레벨 은닉 상태가 오른쪽 컨텍스트를 가지고 있지 않기 때문에 LTR 모델이 토큰 예측에서 성능이 떨어질 것이라는 것은 직관적으로 명확하다. 이를 개선하기 위해 무작위로 초기화된 BiLSTM을 추가했지만, 결과는 사전 훈련된 양방향 모델보다 훨씬 떨어진다. 또한, BiLSTM은 GLUE 작업에서의 성능을 저하시킨다.\nLTR과 RTL 모델을 별도로 훈련하는 것은 가능하지만, 이는 단일 양방향 모델보다 비용이 두 배 많이 들고, QA와 같은 작업에 대해 직관적이지 않다. 또한, 이 방식은 모든 계층에서 양방향 컨텍스트를 사용하는 모델보다 성능이 엄격하게 떨어진다.\nEffect of Model Size 모델 크기가 미세 조정 작업 정확도에 미치는 영향을 알아본다. BERT 모델을 같은 파라미터와 훈련 절차를 사용한 반면, 레이어의 수, 은닉 상태 개수, 어텐션 헤드 개수를 다르게 학습했다.\nGLUE 작업 결과에 따르면, 더 큰 모델은 레이블이 붙은 훈련 예시가 적은 데이터셋에서도 정확도를 개선하였다. 이는 이미 상대적으로 큰 모델을 기반으로 중요한 개선을 이루어내며, 예를 들어 $BERT_{BASE}$는 110M, $BERT_{LARGE}$는 340M의 파라미터를 가진다. 이는 기존 문헌에서 제시한 Transformer 모델 보다 훨씬 크다.\n모델 크기를 늘리는 것이 대규모 작업에서 성능을 향상시키는 것은 잘 알려져 있지만, 이 연구는 모델이 충분히 사전 훈련되었다면 작은 규모의 작업에서도 큰 개선을 가져올 수 있다는 것을 보여준다. 이전의 연구들은 사전 훈련된 모델의 크기를 늘리는 것이 혼합된 결과를 가져왔지만, 이 연구는 모델이 작업에 직접 미세 조정을 받고, 매우 적은 수의 무작위로 초기화된 추가 파라미터만 사용할 때, 작은 규모의 작업도 크고 표현력 있는 사전 훈련된 표현의 이점을 볼 수 있다.\nFeature-based Approach with BERT 지금까지의 BERT 결과는 모두 미세 조정 방식을 사용했다. 이 방식은 사전 학습된 모델에 분류 계층을 추가하고 모든 파라미터를 하류 작업에 맞게 조정하는 방법이다. 그러나, 사전 학습된 모델에서 고정 특징을 추출하는 특징 기반 접근법도 장점이 있다. 일부 작업은 Transformer 인코더 아키텍처로 표현하기 어려워 특정 작업용 모델이 필요하며, 훈련 데이터의 복잡한 표현을 미리 계산하고 이를 기반으로 저렴한 모델로 실험을 진행하면 계산적으로 이점이 있다.\n이 섹션에서는 BERT를 이름 인식(NER) 작업에 적용하여 두 가지 접근법을 비교한다. BERT 입력에는 대소문자를 구분하는 WordPiece 모델을 사용하고, 데이터에서 제공하는 최대 문서 컨텍스트를 포함한다. 이 작업은 일반적인 방식에 따라 태깅 작업으로 설정되지만, 출력에서는 CRF 계층은 사용하지 않는다. NER 레이블 세트에 대한 토큰 수준 분류기의 입력으로 첫 번째 서브토큰의 표현을 사용한다.\n미세 조정 방식을 제거하기 위해, BERT의 매개변수를 조정하지 않고 특징 기반 방식을 적용하여 활성화 함수를 추출한다. 이 문맥적인 임베딩은 랜덤하게 초기화된 두 계층의 768차원 BiLSTM에 입력으로 사용되며, 이는 분류 레이어 이전에 이루어진다.\n$BERT_{LARGE}$는 state-of-the-art 방법들과 비슷한 수준의 성능을 보여준다. 가장 효과적인 방법은 사전 훈련된 Transformer의 상위 4개 계층에서 토큰 표현을 결합하는 것이며, 이는 전체 모델을 미세 조정한 것보다 F1에서 0.3만큼 뒤떨어진다. 이는 BERT가 미세 조정과 특징 기반 접근법 모두에 효과적임을 보여준다.\nConclusion 언어 모델과 전이 학습을 통한 최근 개선은 비지도 사전 학습이 언어 이해 시스템의 중요한 부분임을 보여준다. 이 결과는 low-resource tasks조차 깊은 단방향 아키텍처에서 이익을 얻을 수 있다는 것을 보여주었다. 이 논문에서는 이를 깊은 양방향 아키텍처로 일반화함으로써, 사전 훈련된 동일 모델이 다양한 NLP 작업을 성공적으로 처리할 수 있게 한다는 것을 보여준다.\nReference Paper Github ","date":"2023-12-04T00:00:00Z","permalink":"https://kurtkim.github.io/p/bert/","title":"BERT"},{"content":"Abstract 자연어 이해는 텍스트의 함축, 질문에 대한 답변, 의미의 유사성 평가, 문서 분류 등 다양한 작업으로 구성되어 있다. 레이블이 지정된 데이터가 부족한 상황에서, 이 논문은 레이블이 없는 텍스트 데이터에 대해 언어 모델을 (생성적) 사전학습(generative pre-training)하고, 이를 특정 작업에 미세조정(fine-tuning)하는 방식을 제안한다. 이 방법은 모델 아키텍처에 최소한의 변경만을 요구하면서도 효과적인 전이를 달성하였고, 다양한 자연어 이해 벤치마크에서 우수한 성능을 보여주었다. 이 모델은 각 작업에 특별히 설계된 모델을 능가하며, 12개의 작업 중 9개에서 최고 성능을 달성하였다.\nIntroduction 자연어 처리(NLP)에서 지도 학습의 의존성을 줄이는 것은 중요한데, 이는 대부분의 딥러닝 방법이 수동 레이블링된 대량의 데이터가 필요하기 때문이다. 이런 상황에서 레이블이 없는 데이터에서 언어 정보를 추출할 수 있는 모델은 유용한 대안이 될 수 있으며, 비지도 학습을 통해 학습하는 것이 더 나은 결과를 얻는 경우도 있다. 이를 입증하는 가장 강력한 예는 사전 학습된 단어 임베딩이며, 이는 다양한 NLP 작업에서 성능 향상을 위해 널리 사용되고 있다.\n레이블이 없는 텍스트에서 단어 수준을 넘어서는 정보를 활용하는 것은 어려운 도전 과제이며, 이유는 다음과 같다. 첫째, 텍스트 표현을 학습하고 다른 곳에 유용하게 전이하는 최적화 목표가 무엇인지 확실하지 않다. 둘째, 학습된 표현을 어떤 작업에 가장 효과적으로 적용할 방법이 아직 확립되지 않았다. 이런 불확실성이 효과적인 준지도 학습 방법을 개발하는 것을 어렵게 한다.\n이 연구는 언어 이해 작업에 비지도 사전 학습(unsupervised pre-training)과 지도 미세 조정(supervised fine-tuning)을 결합하는 준지도 학습을 제안한다. 목표는 적은 조정으로 다양한 작업에 적용 가능한 표현을 학습하는 것이다. 레이블이 없는 대량의 텍스트와 수동으로 레이블링된 훈련 예제를 사용하며, 학습은 두 단계로 진행된다. 먼저, 레이블이 없는 데이터로 모델의 초기 파라미터를 학습하고, 그 다음으로 지도 학습을 통해 이 파라미터를 목표 작업에 맞게 조정한다.\n이 연구에서는 다양한 작업에서 뛰어난 성능을 보인 Transformer모델을 사용한다. 이 모델은 텍스트의 장기적인 의존성을 처리하는 더 구조화된 메모리를 제공하므로 강한 전이 성능을 보여준다. 전이 단계에서는 작업 특정 입력 조정을 사용하여 텍스트 입력을 연속 토큰 시퀀스로 처리하며, 이 방식은 사전 학습된 모델의 구조를 최소한으로 변경하면서 효과적으로 미세 조정할 수 있음을 실험적으로 입증한다.\n이 연구는 자연어 추론, 질문 응답, 의미 유사성, 텍스트 분류 등 네 가지 언어 이해 작업에서 모델을 평가하였다. 제시된 모델은 각 작업에 특화된 모델들보다 더 우수한 성능을 보여주었고, 12개 작업 중 9개에서 최고 성능을 보여주었다.\nRelated Work Semi-supervised learning for NLP 이 연구는 자연어에 대한 준지도 학습 범주에 속하며, 이는 시퀀스 라벨링이나 텍스트 분류와 같은 작업에 적용된다. 초기에는 레이블 없는 데이터를 사용해 단어나 구문 수준의 통계를 계산하였지만, 최근에는 레이블이 없는 말뭉치에서 훈련된 단어 임베딩을 활용하여 작업 성능을 향상시키는 방향으로 연구가 진행되고있다. 그러나 이 논문의 목표는 단어 수준 이상의 의미를 포착하는 것이며, 이를 위해 구문이나 문장 수준의 임베딩을 활용하여 텍스트를 벡터 표현으로 인코딩하는 방식을 채택하였다.\nUnsupervised pre-training 비지도 사전 학습은 좋은 초기화 지점을 찾는 것을 목표로 하며, 이미지 분류, 음성 인식, 엔티티 구분, 기계 번역 등 다양한 작업에서 DNN의 훈련을 돕는데 사용되고있다.\n이 연구는 언어 모델링 목표를 사용하여 신경망을 사전 학습하고, 지도 학습으로 목표 작업에서 미세 조정하는 방식을 따른다. 이 방법은 LSTM을 사용하는 이전의 방법들이 제한적인 예측 능력을 가지는 반면, Transformer는 더 넓은 범위의 언어 구조를 포착할 수 있게 한다. GPT 모델은 자연어 추론, 패러프레이즈 감지, 스토리 완성 등 다양한 작업에서 효과를 보여주었으며, 다른 모델이 새로운 파라미터를 많이 필요로 하는 반면, GPT 모델은 아키텍처에 최소한의 변경만 필요로 한다.\nAuxiliary training objectives 보조적인 비지도 학습 목표 추가는 준지도 학습의 변형 형태로, 다양한 NLP 작업을 통해 의미 역할 라벨링을 개선하는데 사용되었다. 최근에는 이러한 보조 목표를 목표 작업에 추가하여 시퀀스 라벨링 작업에서 성능을 향상시켰다. 이 연구에서도 비지도 사전 훈련이 이미 목표 작업과 관련된 다양한 언어적 요소를 학습한다는 것을 보여준다.\nFramework 학습은 큰 말뭉치에서 대용량 언어 모델을 학습하는 단계와 레이블이 달린 데이터를 활용해 모델을 목표 작업에 맞게 미세 조정하는 단계로 이루어진다.\nUnsupervised pre-training 비지도 토큰 말뭉치 $U = \\lbrace u_1, \u0026hellip; , u_n \\rbrace $ 가 주어질때, 다음 Likelihood를 최대화하도록 표준언어모델링 목적함수를 사용한다:\n$$ L_1(U) = \\sum_{i} \\log{P} (u_i | u_{i-k}, \u0026hellip; , u_{i-1}, \\theta) $$\n$k$는 context window의 크기이며, 조건부 확률 $P$는 parameter $\\theta$를 가진 신경망을 사용하여 모델링된다. 이 parameter들은 stochastic gradient descent를 사용하여 학습된다.\nGPT 모델은 언어모델로 multi-layer Transformer decoder를 사용하며, 이 모델은 입력 컨텍스트 토큰에 대해 multi-headed self-attention을 적용한 후, position-wise feedforward layer를 적용하여 목표 토큰에 대한 출력 분포를 생성한다:\n$$ h_0 = UW_e + W_p $$ $$ h_l = \\text{transformer_block}(h_{l-1}) \\forall i \\in [1, n] $$ $$ P(u) = \\text{softmax}(h_n W^T_e) $$\n$U = (u_{i-k}, \u0026hellip; , u_{i-1}) $ 는 토큰의 컨텍스트 벡터이고, $n$은 layer의 수, $W_e$ 는 토큰 임베딩 행렬, $W_p$ 는 위치 임베딩 행렬이다.\nSupervised ﬁne-tuning 모델을 학습한 후, parameter를 목표 작업에 맞게 조정한다. 레이블이 지정된 데이터셋 $C$ 는 입력 토큰 $x^1, \u0026hellip; , x^m $ 과 레이블 $y$로 구성된다. 입력은 사전 훈련된 모델을 통과하여 최종 transformer block의 활성값인 $h^m_l$ 을 얻으며, 이는 parameter $W_y$ 와 함께 선형 출력층으로 전달되어 $y$ 를 예측한다:\n$$ P(y|x^1, \u0026hellip; , x^m) = \\text{softmax}(h^m_l W_y) $$\n이는 다음을 최대화 한다.\n$$ L_2(C) = \\sum_{(x,y)} \\log{P(y|x^1, \u0026hellip; , x^m)} $$\n추가로 미세 조정을 위한 보조 목표로 언어 모델링을 포함시키는 것은 지도 모델의 일반화를 향상시키고, 수렴을 가속화하는데 도움이 된다. 구체적으로, weight $\\lambda$에 대해 다음을 최적화한다:\n$$ L_3(C) = L_2(C) + \\lambda L_1(C) $$\n미세 조정 과정에서 추가 매개변수는 $W_y$ 와 구분자 토큰의 임베딩뿐이다.\nTask-speciﬁc input transformations 텍스트 분류같은 일부 작업들은 모델을 직접 미세 조정할 수 있지만, 질문 답변이나 텍스트 함의 같은 작업들은 구조화된 입력을 필요로 하는데, 이러한 입력에 대해 사전 학습된 모델은 별도의 수정 없이도 처리할 수 있다. 대신, 이런한 입력을 모델이 처리할 수 있는 순서가 있는 시퀀스로 변환한다. 이 접근법은 작업 간에 아키텍처를 크게 변경할 필요를 없애준다. 또한, 모든 변형에는 무작위로 초기화된 시작과 종료 토큰을 포함한다.\nTextual entailment 텍스트 함의에서는, 전제 $p$와 가설 $h$를 구분자 $로 연결한다.\nSimilarity 유사성 경우, 비교되는 두 문장의 순서는 정해져 있지 않으므로, 텍스트 두 개를 다른 순서로 이어붙여 각각을 독립적으로 처리하여 두 시퀀스 표현 $h^m_l$을 생성한다.\nQuestion Answering and Commonsense Reasoning 컨텍스트 문서 $z$, 질문 $q$, 가능한 답변들 $\\lbrace a_k \\rbrace$을 받는다. 각 가능한 답변을 문맥 문서와 질문에 연결하고, 구분자 토큰을 추가해 시퀀스 $[z; q;$ $; $a_k]$ 를 만든다. 이 시퀀스들은 독립적으로 처리되고, softmax 계층을 통해 정규화되어 답변들에 대한 출력 분포를 생성한다.\nExperiments Setup Unsupervised pre-training 언어 모델 학습에 BooksCorpus 데이터셋을 사용한다. 이는 다양한 장르의 7천개가 넘는 미발행 책들을 포함하며, 연속적인 긴 텍스트를 통해 모델이 long term depency를 학습할 수 있다. ELMo에서 사용된 1B Word Benchmark 데이터셋은 문장들이 서로 섞여 있어 long term depency를 학습하기 어렵다.\nModel speciﬁcations Hyperparameter Descrption layer 12-layer decoder-only transformer with masked self-attention heads state dimension decoder: 768, attention heads: 12, position-wise FFN: 3072 optimizer Adam learning rate max: 2.5e-4, schedule: cosine annealing, warm-up step: 2,000 schedule 100 epochs batch size 64 random sample $\\times$ 512 token/sample weight initialization $N(0, 0.02)$ subword segmentation BPE (40,000 merges) dropout 0.1 regularization L2($w=0.01$) activation function Gaussian Error Linear Unit(GELU) position embedding learned positoin embeddings pre-processing cleaning: ftfy, tokenizer : spaCy Fine-tuning details 명시되지 않은 것들은 사전학습에 사용된 hyperparameter를 재사용했다.\nHyperparameter Descrption dropout 0.1 Learning rate max: 6.25e-5, warm-up: 0.2% of training batch size 32 epochs 3 auxiliary objective weight($\\lambda$) 0.5 Supervised ﬁne-tuning 자연어 추론, 질문 응답, 의미론적 유사성, 텍스트 분류등의 평가를 진행하였고, 그 중 일부는 GLUE benchmark에 포함되어 있다.\nNatural Language Inference 자연어 추론(NLI) 작업, 즉 텍스트 함의를 인식하는 것은 문장 쌍을 읽고, 그들 사이의 관계를 함의, 모순 또는 중립 중 하나로 판단하는 것으로, 이미지 캡션(SNLI), 텍스트 변환된 연설, 대중 소설, 정부 보고서(MNLI), 위키백과 기사(QNLI), 과학 시험(SciTail) 또는 뉴스 기사(RTE)를 포함한 다양한 출처의 다섯 개의 데이터셋을 사용해서 평가하였다.\n다섯 가지 데이터셋 중 네 가지에서 좋은 성능을 보여주었으며, MNLI에서 1.5%, SciTail에서 5%, QNLI에서 5.8%, SNLI에서 0.6%의 성능 향상을 보였다. 이는 GPT 모델이 여러 문장을 더 잘 이해하고, 언어적 모호성의 측면을 처리할 수 있다는 것을 보여준다.\nQuestion answering and commonsense reasoning 질문 응답 작업은 한 문장이나 여러 문장을 이해하는 능력을 평가한다. 중고등학교 시험의 영어 지문과 질문이 포함된 RACE 데이터셋을 사용한 평가에서 좋은 성능을 보여주었다. 또한, 여러 문장의 이야기 중에서 올바른 결말을 고르는 Story Cloze 평가에서도 GPT 모델은 이전 최고 성능을 크게 능가하였다. 이 결과는 GPT 모델이 넓은 범위에 걸친 문맥 정보를 잘 처리할 수 있음을 보여준다.\nSemantic Similarity 의미론적 유사성(또는 패러프레이즈 감지) 작업은 두 문장이 의미적으로 동일한지 여부를 판단한다. 뉴스 출처에서 수집된 Microsoft Paraphrase(MRPC), Quora Question Pairs(QQP), 그리고 Semantic Textual Similarity benchmark(STS-B) 데이터셋을 사용한다. 이 중 STSB와 QQP에서 좋은 성늘을 보여주었다.\nClassiﬁcation 텍스트 분류로 사용한 데이터셋은 문법적으로 맞는지를 판단하는 Corpus of Linguistic Acceptability(CoLA)와 단순 이진분류 평가인 Stanford Sentiment Treebank(SST-2)을 사용하였다. CoLA에서 35.0 에서 45.4점으로, SST-2에서 68.9 에서 72.8점으로 상승하였으며, GLUE benchmark에서도 72.8점으로 이전 최고 성능을 크게 능가하였다.\nGPT모델은 평가한 12개의 데이터셋 중 9개에서 state-of-the-art를 달성하였다. 그리고 STS-B(약 5.7k)와 같은 작은 데이터셋부터 가장 큰 SNLI(약 550k)와 같은 크기의 다양한 데이터셋에서 잘 작동함을 보여준다.\nAnalysis Impact of number of layers transferred unsupervised pre-training에서 supervised target task로 transfer하는 layer 개수의 영향을 분석했다. MultiNLI와 RACE에서 성능을 관찰했고 transferring embeddings이 성능을 향상시킨다는 것과 각 transformer layer가 최대 9%까지 성능을 향상시킨다는 결과를 얻었다. 이는 pre-trained model의 각 layer가 target task를 푸는 데 유용한 기능을 포함함을 의미한다.\nZero-shot Behaviors Trasformer를 사용한 language model이 pre-training에 효과적인 이유에 대한 가설로, Generative model이 학습하는 target tasks가 language modeling의 성능을 향상에 도움을 준다고 생각했고, 이를 검증하기 위해 pre-training 업데이트 횟수에 따른 target tasks의 성능을 fine-tuning없이 측정하였다.\n실험 결과 pre-training 업데이트 횟수에 따라 안정적 \u0026amp; 지속적으로 관련 taget task의 성능이 증가하는 것을 확인할 수 있었으며 이는 generative pre-training이 관련 task의 학습에 도움을 준다는 것을 의미한다. 반면, LSTM의 경우에는 업데이트 횟수에 따라 일관되게 안정적으로 증가하지 않고 분산을 가지면서 증가하는데, 이는 LSTM 보다 더 구조화된 transformer의 attentional memory가 transfer learning에 도움을 준다는 것을 의미한다.\nAblation studies 세 가지 ablation study를 통해 다음의 결과를 얻었다. 첫째, 미세조정 시 보조 목적함수의 도움이 큰 데이터셋에서는 두드러지지만 작은 데이터셋에서는 그렇지 않다는 것을 확인하였다. 둘째, LSTM과 Transformer를 비교한 결과, LSTM은 오직 MRPC 데이터셋에서만 Transformer를 능가하는 것을 확인하였다. 마지막으로, 사전학습 없이 지도학습을 진행한 Transformer는 모든 작업에서 성능이 저하되었다.\nConclusion 생성적 사전 학습과 미세조정을 사용한 모델을 통해 강력한 자연어 이해를 구현하였다. GPT 모델은 연속된 텍스트로 이루어진 다양한 말뭉치로 사전학습된 모델은 일반 지식(world knowledge)과 long term depency 처리하는 능력을 가질 수 있었다. 이를 통해, 우리는 지도학습 없이도 특정 작업의 성능을 향상시키는 것이 가능하다는 것을 보여주었으며, 특히 Trasformer 모델과 long term depency가 있는 텍스트 데이터셋이 이 접근법에서 잘 작동함을 확인하였다.\nReference Paper Github ","date":"2023-12-02T00:00:00Z","permalink":"https://kurtkim.github.io/p/gpt-1/","title":"GPT-1"},{"content":"Abstract recurrent와 convolutional을 완전히 제거하고 attention mechanism에만 기반한 새로운 신경망 아키텍처인 transformer를 제안한다. 이 모델은 더 우수한 품질을 제공하면서 병렬화가 가능하고 학습 시간이 훨씬 적게 든다. 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 뛰어넘는 성능을 보였고, 영어 구문 분석에도 성공적으로 적용되었다.\nIntroduction RNN, 특히 LSTM과 GRU는 언어 모델링과 기계 번역 등에서 state-of-the-art로 인정받았다. 이후에도 이러한 모델과 아키텍처의 한계를 끊임없이 넓혀가고 있다.\nrecurrent 모델은 입력과 출력 시퀀스의 위치에 따라 계산을 분류하며, 이는 순차적인 특성으로 인해 학습 예제 내의 병렬화를 방해한다. 이는 메모리 제약이 있는 긴 시퀀스에서 중요한 문제가 된다. 최근의 연구는 계산 효율성을 향상시키는 방법을 제시하였지만, sequential computation의 근본적인 제약은 여전히 남아 있다.\nattention mechanism은 시퀀스 모델링에 있어 핵심 역할을 하며, 입력이나 출력 시퀀스의 거리에 관계 없이 종속성을 모델링할 수 있다. 그러나 대부분의 경우, attention mechanism은 recurrent 네트워크와 함께 사용된다.\nrecurrent을 배제하고 attention mechanism에만 의존하는 transformer를 제안한다. transformer는 더 많은 병렬화를 가능하게 하고, 8개의 P100 GPU에서 단 12시간 학습만으로 state-of-the-art를 달성할 수 있었다.\nBackground sequential computation을 줄이는 것은 Extended Neural GPU, ByteNet, ConvS2S 등의 핵심 목표인데, 이들은 모두 CNN을 사용해 모든 입력과 출력 위치에 대한 표현을 병렬로 계산한다. 그러나 이 모델들은 두 임의의 위치간의 관계를 학습하는데 필요한 연산 수가 위치 간 거리에 따라 증가하므로, 먼 위치 간의 종속성을 학습하기 어렵다. transformer는 이를 상수 수의 연산으로 줄이지만, attention-weighted를 평균화함으로써 해상도가 감소하는 비용이 따르며, 이는 Multi-Head Attention을 통해 상쇄시킨다.\nself-attention은 단일 시퀀스의 다양한 위치를 연관시켜 시퀀스의 표현을 계산하는 방법으로, 독해, 요약, 텍스트 함의 파악, 작업 독립적 문장 표현 학습 등 다양한 작업에 성공적으로 활용되었다.\nend-to-end memory network는 recurrent attention mechanism을 기반으로 하며, 간단한 언어 질문 응답 및 언어 모델링 작업에서 좋은 성능을 보여주었다.\ntransformer는 시퀀스에 정렬된 RNN이나 convolution을 사용하지 않고, 완전히 self-attention에 의존하여 입력과 출력의 표현을 계산하는 최초의 transduction 모델이다.\nModel Architecture 대부분의 neural sequence transduction 모델은 encoder-decoder 구조를 가지고 있습니다. encoder는 기호 표현의 입력 시퀀스 $(x_1, \u0026hellip;, x_n)$를 연속적인 표현의 시퀀스 $z = (z_1, \u0026hellip;, z_n)$로 변환하고, 이를 기반으로 decoder는 한 번에 하나씩 기호의 출력 시퀀스 $(y_1, \u0026hellip;, y_m)$를 생성한다. 이때 모델은 이전에 생성된 기호를 추가 입력으로 사용하는 auto-regressive 방식을 취한다.\ntransformer는 encoder와 decoder 모두에 대해 쌓인 self-attention과 point-wise, fully connected layer을 사용하여 encoder-decoder 구조를 따른다.\nEncoder and Decoder Stacks Encoder: encoder는 $N = 6$개의 동일한 계층으로 이루어져 있으며, 각 계층은 multi-head self-attention mechanism과 positionwise fully connected feed-forward network의 두 sub-layer로 구성된다. 각 하위 계층은 residual connection과 $LayerNorm(x + Sublayer(x))$를 통해 처리되며, 모든 하위 계층과 임베딩 계층은 차원 $d_{model} = 512$인 출력을 생성한다.\nDecoder: decoder는 $N = 6$개의 동일한 계층으로 구성되며, encoder의 출력에 multi-head attention을 수행하는 세 번째 sub-layer이 추가된다. 각 sub-layer 주변의 residual connection과 layer normalization를 사용하며, 후속 위치에 주의를 기울이는 것을 방지하기 위해 decoder의 self-attention sub-layer을 수정한다. 이러한 수정은 위치 $i$의 예측이 $i$보다 작은 위치에서의 알려진 출력에만 의존하도록 보장한다.\nAttention attention 함수는 query와 key-value 쌍을 벡터 형태의 출력으로 매핑하며, 출력은 값들의 가중치 합으로 계산된다. 이때 각 값의 가중치는 query와 해당 key의 호환성에 따라 결정된다.\nScaled Dot-Product Attention \u0026ldquo;Scaled Dot-Product Attention\u0026quot;은 query와 key의 차원이 $d_k$, 값의 차원이 $d_v$인 입력을 처리한다. query와 모든 key의 내적을 계산하고, 이를 $\\sqrt{d_k}$로 나눈 후, softmax 함수를 적용하여 값에 대한 가중치를 얻는다.\n여러 쿼리들을 동시에 처리하기 위해 행렬 $Q$에 패킹하고, key와 value 또한 각각 행렬 $K$와 $V$에 패킹한다. 그리고 이를 이용해 출력 행렬을 계산한다:\n$$ Attention(Q, K, V) = softmax({{QK^\\intercal}\\over{\\sqrt{d_k}}})V $$\n가장 흔히 사용되는 attention 함수는 additive attention과 dot-product (multiplicative) attention입니다. dot-product attention은 알고리즘과 ${{1}\\over{\\sqrt{d_k}}}$의 스케일링 요소를 제외하면 동일하며, additive attention은 feed-forward network를 이용해 호환성 함수를 계산한다. 두 방법은 이론적으로 유사하지만, dot-product attention은 최적화된 행렬 곱셈 코드를 통해 더 빠르고 공간 효율적으로 구현될 수 있다.\n$d_k$ 값이 작은 경우 두 메커니즘이 유사하게 작동하지만, $d_k$ 값이 크면 스케일링 없는 dot-product attention의 성능이 떨어잔다. 이는 dot-product 값의 크기 증가로 인해 softmax 함수의 기울기가 매우 작아지는 것을 방지하기 위해, dot-product을 ${{1}\\over{\\sqrt{d_k}}}$로 스케일링한다.\nMulti-Head Attention $d_{model}$-dimensional key, value, query에 single attention 함수를 사용하는 대신, 각각을 다른 선형 변환을 통해 $d_k$, $d_k$, $d_v$ 차원으로 $h$번 변환하는 것이 유익하다는 것을 발견하였다. 이 변환된 query, key, value에 대해 병렬로 attention 함수를 수행하면, $d_v$ 차원의 출력 값이 나오며, 이들은 연결되고 다시 변환되어 최종 값이 생성된다.\nmulti-head attention은 다른 표현 하위 공간에서 다른 위치의 정보에 동시에 주의를 기울일 수 있게 해주는 반면, single attention head는 이를 평균화하여 방해한다.\n$$ MultiHead(Q, K, V) = Concat(head_1, \u0026hellip;, head_h)W^O $$ $$ where \\ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$\n프로젝션은 parameter 행렬 $W_i^Q \\in \\mathbb{R}^{d_{model \\times d_k}}$, $W_i^K \\in \\mathbb{R}^{d_{model \\times d_k}}$, $W_i^V \\in \\mathbb{R}^{d_{model \\times d_v}}$ 그리고 $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$이다.\n이 작업에서는 8개의 병렬 attention layer를 사용하며, 각 계층에 대해 $d_k = d_v = d_{model} / h = 64$를 사용한다. 각 head의 차원이 줄어들었기 때문에 전체 계산 비용은 전체 차원의 single-head attention과 유사하다.\nApplications of Attention in our Model transformer는 세 가지 다른 방식으로 multi-head attention을 사용한다:\n\u0026ldquo;encoder-decoder attention\u0026quot;에서는 이전 decoder 계층에서 query가 생성되고, encoder의 출력에서 메모리 key와 value이 생성된다. 이를 통해 decoder의 모든 위치가 입력 시퀀스 전체에 주의를 기울일 수 있습니다. 이는 일반적인 sequence-to-sequence 모델의 encoder-decoder attention mechanism을 따른다. encoder에는 self-attention layer가 있으며, 이 layer에서는 모든 key, value, query가 encoder의 이전 layer의 출력에서 생성된다. 이를 통해 encoder의 각 위치가 이전 layer의 모든 위치에 주의를 기울일 수 있다. decoder의 self-attention layer는 decoder의 각 위치가 그 위치를 포함해 그 이전의 모든 위치에 주의를 기울일 수 있게 한다. auto-regressive 속성을 유지하기 위해, 불법적인 연결에 해당하는 값을 마스킹 아웃($-\\infty$로 설정)하여 decoer 내부의 정보 흐름을 제한한다. Position-wise Feed-Forward Networks encoder와 decoder의 각 layer에는 각 위치에 독립적으로 적용되는 fully connected feed-forward network가 포함되어 있으며, 이는 두 개의 linear transformation과 그 사이의 ReLU activation 함수로 구성된다.\n$$ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $$\nlinear transformation은 다른 위치에도 동일하게 적용되지만, layer마다 다른 parameter를 사용한다. 이는 kernel size 1의 두 개의 convolution으로도 설명할 수 있다. 입력과 출력의 차원은 $d_{model} = 512$이고, inner-layer의 차원은 $d_{ff} = 2048$이다.\nEmbeddings and Softmax 학습된 임베딩을 사용하여 입력 토큰과 출력 토큰을 벡터로 변환하며, 학습된 linear transformation과 softmax 함수를 사용해 decoder 출력을 다음 토큰 확률로 변환한다. 두 임베딩 layer와 pre-softmax linear transformatio에서 동일한 가중치 행렬을 공유하고, 임베딩 layer에서는 이 가중치에 ${{1}\\over{\\sqrt{d_k}}}$를 곱한다.\nPositional Encoding transformer 모델은 recurrence와 convolution이 없기 때문에, 시퀀스의 토큰 위치에 대한 정보를 주입함으로써 시퀀스의 순서를 활용한다. 이를 위해, \u0026ldquo;positional encoding\u0026quot;을 입력 임베딩에 더하며, 이는 임베딩과 동일한 차원을 가진다. positional encoding은 학습되거나 고정될 수 있다.\n다른 주파수의 sine 함수와 cosine 함수를 사용한다:\n$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$ $$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $$\npositional encoding의 각 차원이 sinusoid에 해당하도록 sine 함수와 코사인 cosine를 사용한다. 이 함수를 선택한 이유는 모델이 상대적 위치에 따라 주의를 쉽게 배울 수 있도록 하기 위해서이다. 즉, 어떤 고정된 오프셋 $k$에 대해서도, $PE_{pos+k}$는 $PE_{pos}$의 선형 함수로 표현될 수 있다.\n학습된 positional embedding을 사용해 실험해봤고, 두 방식이 거의 동일한 결과를 생성함을 확인하였다. sinusoidal 버전을 선택한 이유는 학습 중에 접한 것보다 더 긴 시퀀스 길이로 extrapolate 할 수 있을 것이라 판단했기 때문이다.\nWhy Self-Attention recurrent및 convolutional layer와 self-attention layer을 비교한다. 이들은 모두 가변 길이의 심볼 표현 시퀀스를 동일한 길이의 다른 시퀀스로 매핑하는데 사용된다. self-attention 사용의 동기를 설명하기 위해, 세 가지 조건을 고려한다.\n하나는 각 계층에서의 전체 계산 복잡성이다. 또 다른 하나는 병렬화할 수 있는 계산량으로, 이는 필요한 최소 연속 작업의 수로 측정된다.\n세 번째는 네트워크 내에서 long-range dependency 사이의 경로 길이이다. long-range dependency를 학습하는 것은 시퀀스 변환 작업의 주요 도전 과제이다. 이를 학습하는 능력은 네트워크 내에서 신호가 이동하는 경로의 길이에 크게 영향을 받는다. 입력과 출력 시퀀스의 임의의 위치 사이의 경로가 짧을수록 long-range dependency을 학습하기 쉽다. 따라서, 다른 layer 유형으로 구성된 네트워크에서 두 입력과 출력 위치 사이의 최대 경로 길이도 비교한다.\nself-attention layer는 연속적인 연산을 통해 모든 위치를 연결하며, recurrent layer에 비해 더 빠른 계산 속도를 제공한다. 특히, 시퀀스 길이가 표현 차원보다 작은 경우에 더욱 그렇다. 매우 긴 시퀀스를 처리하는 작업의 계산 성능을 높이기 위해, self-attention은 출력 위치를 중심으로 한 입력 시퀀스의 이웃만을 고려하도록 제한될 수 있다. 이는 최대 경로 길이를 $O(n/r)$로 증가시키며, 이에 대한 연구를 미래에 더 진행할 계획이다.\n커널 너비가 $k \u0026lt; n$인 단일 convolutional layer는 모든 입력과 출력 위치를 연결하지 않는다. 이를 위해선 복수의 convolutional layer이 필요하고, 이로 인해 네트워크 내 두 위치 사이의 가장 긴 경로가 늘어난다. 보통 convolutional layer는 recurrent layer보다 $k$배의 비용이 더 들지만, 분리 가능한 convolution을 사용하면 복잡성이 크게 줄어든다. 그러나 $k = n$인 경우에도, 분리 가능한 convolution의 복잡성은 self-attention layer와 point-wise feed-forward layer의 결합과 동일하다.\nself-attention은 더 해석 가능한 모델을 만들 수 있는 이점이 있다. 우리 모델에서는 각 attention head가 다른 작업을 수행하도록 학습하며, 이들 중 많은 헤드가 문장의 구문적 및 의미적 구조와 관련된 행동을 보이는 것으로 파악되었다.\nTraining Training Data and Batching 약 450만 개의 문장 쌍을 포함하는 표준 WMT 2014 영어-독일어 데이터셋으로 학습했다. 더 큰 WMT 2014 영어-불어 데이터셋도 사용하였다. 각 학습 배치는 약 25000개의 소스 토큰과 타겟 토큰을 포함하는 문장 쌍을 포함하였다.\nHardware and Schedule 8개의 NVIDIA P100 GPU에서 모델을 학습시켰다. 기본 모델들은 각 학습 단계마다 약 0.4초가 걸렸고, 총 100,000단계 또는 12시간 동안 학습되었다. 큰 모델들은 단계 시간이 1.0초였고, 300,000단계 또는 3.5일 동안 학습되었다.\nOptimizer Adam optimizer, $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ 그리고 $\\epsilon = 10^{−9}$를 사용하였다. 다음의 공식에 따라서 learning rate을 변화시켰다:\n$$ lrate = d_{model}^{−0.5} · min(\\text{step_num}^{−0.5}, \\text{step_num} · \\text{warmup_steps}^{−1.5}) $$\nwarmup_steps 동안 learning rate을 선형적으로 증가시키고, 그 이후에는 단계 수의 역제곱에 비례하여 감소시킨다. warmup_steps = 4000을 사용하였다.\nRegularization 세 가지 유형의 regularization를 사용한다:\nResidual Dropout 각각의 sub-layer의 출력과 encoder 및 decoder 스택의 임베딩과 positional encoding의 합에 드롭아웃을 적용합니다. 기본 모델에서는 dropout rate로 $P_{drop} = 0.1$을 사용한다.\nLabel Smoothing 학습 동안에는 $\\epsilon_{ls}= 0.1$의 값을 가진 라벨 스무딩을 사용하였다. 이는 모델이 더 불확실하게 학습하도록 만드므로 혼란스러움(perplexity)을 증가시키지만, 정확도와 BLEU 점수는 향상시킨다.\nResults Machine Translation WMT 2014 영어-독일어 번역 작업에서, large transformer 모델은 이전 모델들을 2.0 BLEU 이상 뛰어넘어 state-of-the-art인 28.4의 BLEU 점수를 달성하였다. 이 모델은 8개의 P100 GPU에서 3.5일 동안 학습되었다. 기본 모델조차도 이전의 모든 모델과 앙상블을 능가하며, 경쟁 모델의 학습 비용의 일부에 불과했다.\nWMT 2014 영어-불어 번역 작업에서, BLEU 점수 41.0을 달성하여 이전에 발표된 모든 단일 모델들을 능가했고, 이전 state-of-the-art 모델의 학습 비용의 1/4 미만이었다. 이 모델은 dropout rate로 $P_{drop} = 0.1$을 사용했다.\n기본 모델에 대해 마지막 5개의 체크포인트를 평균한 단일 모델을 사용했고, 큰 모델에 대해선 마지막 20개의 체크포인트를 평균냈다. beam search를 통해 beam size 4와 length penalty $\\alpha = 0.6$을 사용했다. 이 값들은 개발 세트에서 실험 후 결정되었다. 추론 시 최대 출력 길이는 입력 길이 + 50으로 설정되었으나, 가능하다면 일찍 종료한다.\n모델 학습에 사용된 부동 소수점 연산의 수는 학습 시간, 사용된 GPU의 수, 각 GPU의 단정밀도 부동 소수점 용량의 추정치를 곱하여 추정하였다.\nModel Variations transformer의 다양한 요소의 중요성을 평가하기 위해, 기본 모델을 다양하게 변형하며 개발 세트인 newstest2013에서의 영어-독일어 번역 성능 변화를 측정하였다. beam search을 사용했지만 체크포인트 평균화는 사용하지 않았다.\nsingle-head attention은 최적 설정보다 0.9 BLEU가 떨어지며, head 수가 너무 많아져도 품질이 떨어진다.\nattention key 크기를 줄이면 모델 품질이 떨어진다는 것을 확인하였다. 이는 복잡한 호환성 함수가 필요할 수 있음을 시사한다. 더 큰 모델이 더 좋고, dropout이 over-fitting을 피하는 데 매우 유용하다는 것을 확인했다. sinusoidal positional encoding을 learned positional embedding으로 대체했을 때 기본 모델과 거의 동일한 결과를 얻었다.\nEnglish Constituency Parsing transformer가 다른 작업에 일반화할 수 있는지 확인하기 위해, 구조적 제약이 강하고 입력보다 긴 출력을 가진 영어 구성성 파싱 작업에 대한 실험을 수행하였다. RNN sequence-to-sequence 모델은 이 작업에서 state-of-the-art를 달성하지 못하였다.\nPenn Treebank의 Wall Street Journal (WSJ) 부분에 대해 약 4K 개의 학습 문장을 사용하여 $d_{model} = 1024$의 4-layer transformer를 학습시켰다. 또한, 약 17M 문장을 포함하는 대형 말뭉치를 사용하여 반지도학습 환경에서도 학습시켰다. WSJ만을 대상으로 하는 경우 16K 토큰의 어휘를, 반지도학습 설정에서는 32K 토큰의 어휘를 사용하였다.\ndropout, learning rate, beam size를 결정하기 위해 Section 22 개발 세트에서 몇 가지 실험을 수행했고, 모든 다른 parameter는 기본 번역 모델에서 변경되지 않았다. 추론 시에는 최대 출력 길이를 입력 길이 + 300으로 늘렸다. beam size 21과 $\\alpha = 0.3$을 모든 설정에 사용하였다.\n작업 특화 튜닝이 없음에도 이전 모델들보다 더 좋은 성능을 보였고, 이는 Recurrent Neural Network Grammar을 제외한 모든 이전에 보고된 모델들보다 더 좋은 결과를 가져왔다.\nRNN sequence-to-sequence 모델과는 달리, transformer는 오직 WSJ 학습 세트의 40K 문장만을 이용하여 학습했음에도 BerkeleyParser를 능가하는 성능을 보여주었다.\nConclusion transformer는 attention 기반으로 만들어진 첫 시퀀스 transduction 모델로, encoder-decoder 구조의 recurrent layer를 multi-headed self-attention로 대체하였다.\ntransformer는 recurrent나 convolution 기반 아키텍처보다 빠르게 학습되며, WMT 2014 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 달성하였다. 이 중 영어-독일어 작업에서는 이전의 모든 앙상블보다 더 뛰어난 성능을 보여주었다.\nattention 기반 모델을 다른 작업에 적용하고, 텍스트 이외의 다양한 입력과 출력 문제에 transformer를 확장하려 한다. 또한, 큰 이미지, 오디오, 비디오 등을 효율적으로 처리하기 위해 restricted attention mechanism을 연구하고, 생성 과정을 덜 순차적으로 만드는 것을 목표로 하고 있다.\nReference Paper Github ","date":"2023-12-01T00:00:00Z","permalink":"https://kurtkim.github.io/p/transformers/","title":"Transformers"}]