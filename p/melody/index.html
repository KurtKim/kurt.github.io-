<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Efficient Neural Music Generation"><title>MeLoDy</title>
<link rel=canonical href=https://kurtkim.github.io/p/melody/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="MeLoDy"><meta property='og:description' content="Efficient Neural Music Generation"><meta property='og:url' content='https://kurtkim.github.io/p/melody/'><meta property='og:site_name' content="K2H'log"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Audio'><meta property='article:tag' content='Music Generation'><meta property='article:published_time' content='2024-05-14T00:00:00+00:00'><meta property='article:modified_time' content='2024-05-14T00:00:00+00:00'><meta name=twitter:title content="MeLoDy"><meta name=twitter:description content="Efficient Neural Music Generation"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/k2h_hu16962933080322361302.jpg width=300 height=306 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>K2H'log</a></h1><h2 class=site-description>넓고 얕은 지식을 위한</h2></div></header><ol class=menu-social><li><a href=https://github.com/kurtkim/ target=_blank title=1 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/kyeong-hun-kim-430ba075/ target=_blank title=2 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></li><li><a href=https://www.instagram.com/kurt_k2h/ target=_blank title=3 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></li><li><a href=https://velog.io/@kurt_kim target=_blank title=4 rel=me><svg class="icon icon-tabler icon-tabler-pencil-minus" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a2.828 2.828.0 10-4-4L4 16v4"/><path d="M13.5 6.5l4 4"/><path d="M16 19h6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#background-on-audio-language-modeling>Background on Audio Language Modeling</a><ol><li><a href=#audio-language-modeling-with-musiclm>Audio Language Modeling with MusicLM</a><ol><li><a href=#joint-tokenization-of-music-and-text-with-mulan-and-rvq>Joint Tokenization of Music and Text with MuLan and RVQ</a></li></ol></li></ol></li><li><a href=#model-description>Model Description</a><ol><li><a href=#dual-dath-diffusion-angle-parameterized-continuous-time-latent-diffusion-models>Dual-Dath Diffusion: Angle-Parameterized Continuous-Time Latent Diffusion Models</a><ol><li><a href=#multi-chunk-velocity-prediction-for-long-context-generation>Multi-Chunk Velocity Prediction for Long-Context Generation</a></li><li><a href=#dual-path-modeling-for-efficient-and-effective-velocity-prediction>Dual-Path Modeling for Efficient and Effective Velocity Prediction</a></li></ol></li><li><a href=#audio-vae-gans-for-latent-representation-learning>Audio VAE-GANs for Latent Representation Learning</a></li><li><a href=#music-inpainting-music-continuation-and-music-prompting-with-melody>Music Inpainting, Music Continuation and Music Prompting with MeLoDy</a></li></ol></li><li><a href=#experiments>Experiments</a><ol><li><a href=#experimental-setup>Experimental Setup</a></li><li><a href=#performance-analysis>Performance Analysis</a></li></ol></li><li><a href=#discussion>Discussion</a></li><li><a href=#reference>Reference</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/paper-review/>Paper Review</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/melody/>MeLoDy</a></h2><h3 class=article-subtitle>Efficient Neural Music Generation</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>5월 14, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>11 분 정도</time></div></footer></div></header><section class=article-content><h2 id=abstract>Abstract</h2><p>본 논문은 최신 음악 생성 기술인 MusicLM의 작업량을 대폭 줄인 새로운 모델 MeLoDy를 제시한다. MusicLM의 의미론적 모델링을 기반으로, MeLoDy는 새로운 dual-path diffusion(DPD) 모델과 오디오 VAE-GAN을 활용하여 음악 오디오를 효율적으로 생성한다. 이를 통해 10초나 30초 길이의 음악 샘플링 시 전달 횟수를 각각 95.7% 또는 99.6%까지 줄이면서도 MusicLM과 동등한 품질의 음악을 생성할 수 있다. MeLoDy는 샘플링 속도의 향상과 무한 연속 생성 가능성을 제공하며, 음악성, 오디오 품질, 텍스트 연관성 면에서도 최고 수준을 달성한다.</p><hr><h2 id=introduction>Introduction</h2><p>최근 몇 년간 심층 생성 모델의 발전으로 음악 생성이 큰 관심을 받아왔다. 언어 모델(LMs)은 장기적 맥락에서의 복잡한 관계를 모델링하는 데 탁월하며, AudioLM과 같은 작업들이 이를 오디오 합성에 성공적으로 적용하였다. 또한, diffusion probabilistic 모델(DPMs)도 음성, 사운드, 음악 합성에서 뛰어난 능력을 보여주며 생성 모델 분야에서 주목받고 있다.</p><p>자유 형식 텍스트를 이용한 음악 생성은 다양한 음악 설명의 범위로 인해 도전적이다. MusicLM과 Noise2Music 같은 기존 모델들은 대규모 데이터셋 훈련을 통해 높은 충실도와 텍스트 프롬프트 준수로 최신 생성 성능을 보였지만, 큰 계산 비용이 발생한다. 반면, DPMs 기반 Moûsai는 고품질 음악 샘플링을 효율적으로 가능하게 했으나, 시연한 사례가 적고 다이내믹스가 제한적이었다. 인간 피드백을 고려한 인터랙티브한 음악 생성을 위해서는 높은 효율성을 가진 생성 모델이 필수적이다.</p><p>LMs와 DPMs의 장점을 결합하는 것에 초점을 맞춘 이 연구에서는, MusicLM의 최고 수준 LM인 의미적 LM을 활용하여 음악의 의미 구조를 모델링하고, DPMs의 비자동 회귀적 특성을 이용해 효율적으로 음향을 모델링한다. 이를 통해, 이 논문은 음악 생성 분야에 여러 새로운 기여를 제시한다.</p><ol><li>MeLoDy는 LM-guided diffusion 모델로, MusicLM보다 훨씬 적은 반복으로 고품질 음악을 실시간보다 빠르게 생성한다.</li><li>dual-path diffusion(DPD) 모델은 의미 조건부 전략을 사용해 세밀한 음향 정보를 효율적으로 동시 모델링한다.</li><li>DPD의 새로운 샘플링 체계는 이전 방법보다 생성 품질을 개선한다.</li><li>오디오 VAE-GAN은 연속적인 잠재 표현을 학습하고, DPD와 함께 고품질 오디오를 합성한다.</li></ol><hr><h2 id=related-work>Related Work</h2><p><img src=/p/melody/images/table1.png width=936 height=204 srcset="/p/melody/images/table1_hu10708441883876045942.png 480w, /p/melody/images/table1_hu6447273082457656423.png 1024w" loading=lazy class=gallery-image data-flex-grow=458 data-flex-basis=1101px></p><p><strong>Audio Generation</strong> 음악 생성 모델들은 빠르게 고품질 음악을 생성할 수 있지만, 자유 형식 텍스트를 입력으로 사용할 수 없고 단일 장르에만 특화된다. Mubert와 Riffusion같은 산업계 음악 생성기도 있지만, MusicLM과 비교해 자유 형식 텍스트 처리에는 한계가 있다. 또한, AudioSet를 이용한 텍스트-투-오디오 합성기들은 자유 형식 텍스트로 음악을 생성할 수 있으나 음악성은 제한적이다. AudioLM은 조건 없이 피아노 오디오를 이어가며, SoundStorm은 non-autoregressive decoding을 통해 AudioLM을 가속화하여 빠른 속도로 음향을 생성한다. MeLoDy는 고품질 음향을 생성하기 위해 5에서 20회의 전방 패스가 필요하다.</p><p><strong>Network Architecture</strong> 제안한 DPD 구조는 오디오 분리에 사용된 이중 경로 네트워크에서 영감을 받았고, Luo 등이 제시한 세분화 기반 처리 아이디어로 인해 여러 최신 연구가 진행되었다. 확산 모델의 목표가 소스 분리의 특별한 경우로 볼 수 있기에, 이 이중 경로 구조는 대략적이고 세밀한 음향 모델링을 동시에 가능하게 한다.</p><hr><h2 id=background-on-audio-language-modeling>Background on Audio Language Modeling</h2><h3 id=audio-language-modeling-with-musiclm>Audio Language Modeling with MusicLM</h3><p>MusicLM은 AudioLM의 오디오 언어 모델링 접근 방식을 따라, 오디오 합성을 다양한 정밀도의 오디오 토큰을 사용한 언어 모델링으로 본다. 이 모델에서는 오디오를 대표하기 위해 두 가지 토큰화 방식을 사용한다.</p><ul><li><strong>Semantic Tokenization:</strong> SSL에서 나온 표현들에 대한 K-means, e.g., w2v-BERT</li><li><strong>Acoustic Tokenization:</strong> Neural audio codec, e.g., SoundStream</li></ul><p>AudioLM은 음향 토큰의 계층 구조를 효과적으로 다루기 위해 모델링을 거친 단계와 미세 단계로 나누며, 이를 통해 세 가지 언어 모델링 작업을 정의한다: semantic 모델링, coarse acoustic 모델링, fine acoustic 모델링.</p><p>조건 토큰 시퀀스 $c_1:T_{cnd}$ 와 대상 토큰 시퀀스 $u_1:T_{tgt}$ 를 정의한 후, 각 모델링 작업에서 θ로 매개변수화된 Transformer-decoder는 autoregressive 모델링 문제를 해결하려고 한다.</p><p>$$ p_θ(u_1:T_{tgt} | c_1:T_{cnd}) = \Pi_{j=1}^{T_{tgt}} p_θ (u_j | [c_1, &mldr;, c_{T_{cnd}}, u_1, &mldr;, u_{j−1}]) $$</p><p>AudioLM은 조건 토큰을 대상 토큰에 접두사로 붙이며, semantic 모델링에는 조건이 없고, coarse acoustic 모델링은 semantic 토큰, fine acoustic 모델링은 coarse acoustic 토큰을 조건으로 사용한다. 이 언어 모델들은 실제 토큰으로 병렬 학습이 가능하지만, 추론 시에는 순차적 샘플링이 필요하다.</p><h4 id=joint-tokenization-of-music-and-text-with-mulan-and-rvq>Joint Tokenization of Music and Text with MuLan and RVQ</h4><p>MusicLM은 오디오 전용 학습의 장점을 보존하기 위해, 대규모 음악 데이터와 약하게 연관된 자유 형식 텍스트에 개별적으로 학습 가능한 두 탑 구조의 오디오-텍스트 임베딩 모델인 MuLan에 의존한다. MuLan은 음악 오디오와 텍스트 설명을 같은 임베딩 공간으로 사전 학습시키며, MusicLM에서는 이 임베딩을 residual vector quantization(RVQ)로 토큰화한다.</p><p>MusicLM은 AudioLM과 달리 MuLan 토큰을 접두사로 사용하여 semantic 및 coarse acoustic 모델링을 수행한다. 학습 시, 오디오는 MuLan 음악 타워를 통해 임베딩되고, 이후 RVQ를 통해 MuLan 토큰으로 변환된다. 이 토큰들은 semantic과 coarse acoustic 모델을 조건화한다. 텍스트 프롬프트로 음악을 생성할 때는 MuLan 텍스트 타워에서 얻은 임베딩이 RVQ를 통해 토큰화되며, 이를 바탕으로 고해상도 음악 오디오가 생성된다.</p><hr><h2 id=model-description>Model Description</h2><p><img src=/p/melody/images/figure1.png width=1096 height=378 srcset="/p/melody/images/figure1_hu1469072531606485132.png 480w, /p/melody/images/figure1_hu4478918030948780666.png 1024w" loading=lazy class=gallery-image data-flex-grow=289 data-flex-basis=695px></p><p>MeLoDy는 representation 학습을 위해 MuLan, Wav2Vec2-Conformer, 오디오 VAE 세 모듈과, semantic 및 음향 모델링을 위해 language model(LM)과 dual-path diffusion(DPD) 모델을 사용한다. 음악의 semantic 구조를 모델링하기 위해 LM을 활용하며, MuLan 모델을 사전 학습하여 조건 토큰을 얻고, semantic 토큰화에는 컨포머 블록을 사용하는 Wav2Vec2-Conformer를 사용한다.</p><h3 id=dual-dath-diffusion-angle-parameterized-continuous-time-latent-diffusion-models>Dual-Dath Diffusion: Angle-Parameterized Continuous-Time Latent Diffusion Models</h3><p>제안된 dual-path diffusion (DPD) 모델은 연속 시간에서의 diffusion probabilistic 모델(DPMs)의 변형으로, 저차원 잠재 표현을 사용하여 DPMs의 계산 부담을 줄인다. 이 모델은 사전 학습된 오토인코더를 활용해 원시 데이터를 잠재 공간에서 재구성한다.</p><p>DPD에서는 가우시안 diffusion 과정 $z_t$를, 연속 미분 가능한 함수 $α_t$와 $σ_t$에 의해 정의한다. 이 과정에서 $α_t = cos(πt/2)$, $σ_t = sin(πt/2)$로 설정함으로써 삼각 함수의 좋은 성질을 활용한다. 이를 통해 $σ_t$는 variational을 유지하며($√1 − α_t^2$), $z_t$는 각도 $δ$를 사용하여 re-parameterized될 수 있다.</p><p>$$ z_δ = cos(δ) z + sin(δ) ϵ \ \text{for any} \ δ ∈ [0, π/2], \ ϵ ∼ N(0, I) $$</p><p>$δ$가 $0$에서 $π/2$로 증가함에 따라 $z_δ$의 노이즈가 증가하는 것이 순방향 diffusion 과정을 정의한다.</p><p>샘플 생성을 위해, θ-parameterized된 variational 모델을 이용하여 diffusion 과정을 역방향으로 실행시킨다. $π/2$를 T 단계로 나누어 $z_{π/2}$에서 $z$를 샘플링한다.</p><p>$$ p_θ (z | z_{π/2}) = \int_{z_{δ_{1:T-1}}} \Pi_{t=1}^{T} p_θ (z_{δ_t − ω_t} | z_{δ_t}) dz_{δ_{1:T−1}}, \ \ δ_t = \begin{cases} {{π}\over{2}} − \sum_{i=t+1}^T ω_i, & 1 \leq t &lt; T \\ {{π}\over{2}}, & t = T \end{cases} $$</p><p>각도 일정은 $ω_1, &mldr;, ω_T$로 표현되며, 합이 $π/2$가 된다. Schneider et al.은 모든 $t$에 대해 $ω_t = π/2T$인 균일 일정을 제안하였다. 샘플링 초반에 큰 단계를 취하고 이후 작은 단계를 취하는 것이 샘플 품질을 개선할 수 있음이 밝혀졌다. 이에 따라, 더 안정적이고 고품질의 결과를 제공하는 새로운 선형 각도 일정을 설계하였다.</p><p>$$ ω_T = {{π}\over{6T}} + {{2πt}\over{3T(T+1 )}} $$</p><h4 id=multi-chunk-velocity-prediction-for-long-context-generation>Multi-Chunk Velocity Prediction for Long-Context Generation</h4><p>모델 학습에서 신경망은 다양한 노이즈 스케일을 가진 $M$개 청크로 이루어진 멀티-청크 타겟 $v_{tgt}$ 예측을 맡는다. 이는 오디오 잠재 변수의 길이 $L$과 잠재 차원 $D$를 포함하는 $z$, $z_δ$, $ϵ$에 기반하여, $v_{tgt}$을 $v_1 ⊕ ··· ⊕ v_M$으로 정의한다.</p><p>$$ v_m := cos(δ_m) ϵ [L_{m−1}: L_m, :] − sin(δ_m) z [L_{m−1}: L_{m}, :], L m := \big\lfloor {{mL}\over{M}} \big\rfloor $$</p><p>NumPy 슬라이싱 구문을 사용해 $m$번째 청크를 찾고, 학습마다 $δ_m ∼ Uniform[0, π/2]$으로 노이즈 스케일을 결정한다. $θ$ 학습엔 [1, 48] 범위의 MSE 손실을 사용한다.</p><p>$$ L_{diff} := \mathbb{E}_{z, ϵ, δ_1, &mldr;, δ_M} \big[ \parallel v_{tgt} − \hat{v}_θ (z_{noisy} ; c) \parallel_2^2 \big], $$</p><p>$$ z_{noisy} := cos(δ_m) z [L_{m−1} : L_m, :] + sin(δ_m) ϵ [L_{m−1}: L_m, :], $$</p><p>MeLoDy에서는 학습 중 SSL 모델로부터 얻은 의미 토큰들과 추론 시 LM에 의해 생성된 토큰들을 사용하여 DPD 모델을 조건화한다. 실험을 통해, 토큰 기반 조건을 사용하여 음악의 의미를 제어하고 diffusion 모델이 각 토큰의 임베딩을 학습하게 함으로써 생성 안정성이 크게 개선됨을 확인하였다. 추가적으로, 멀티-청크 예측 지원을 위해 $M$ 청크의 각도를 나타내는 벡터를 조건에 추가한다.</p><p>$$ c := \lbrace u_1, &mldr;, u_{TST}, δ \rbrace, δ := [δ_1]_{r=1}^{L_1} ⊕ ··· ⊕ [δ_M]_{r=1}^{L_M} \in \mathbb{R}^L $$</p><p>스칼라 $a$를 B번 반복해 B길이 벡터를 만드는 연산을 설명한다. 삼각함수의 정체성을 적용한 DDIM 샘플링을 통해 간소화된 업데이트 규칙을 도출한다.</p><p>$$ z_{δ_t − ω_t} = cos(ω_t) z_{δ_t} − sin(ω_t) \hat{v}_θ (z_{δ_t} ; c), $$</p><p>$t = T$에서 $t = 1$까지 실행하여 $z$ 샘플을 획득한다.</p><h4 id=dual-path-modeling-for-efficient-and-effective-velocity-prediction>Dual-Path Modeling for Efficient and Effective Velocity Prediction</h4><p><img src=/p/melody/images/figure2.png width=624 height=400 srcset="/p/melody/images/figure2_hu12771302946540659853.png 480w, /p/melody/images/figure2_hu3506866009551534652.png 1024w" loading=lazy class=gallery-image data-flex-grow=156 data-flex-basis=374px></p><p>잡음이 섞인 잠재 변수와 조건을 통합해 효과적인 속도 예측을 위한 의미 토큰을 사용하는 $\hat{v}_θ$의 방법을 소개하며, 오디오 분리에서 영감을 받은 수정된 이중 경로 기술과 함께 새로운 효율적인 음향 모델링 아키텍처를 제안한다.</p><p>먼저, DPD에서 조건이 어떻게 처리되는지를 설명한다.</p><p><strong>Encoding Angle Vector</strong> 잠재 변수의 프레임 노이즈 스케일 $δ ∈ \mathbb{R}^L$을 인코딩할 때, 위치 인코딩 대신 두 학습 가능한 벡터 $e_{start}, e_{end} ∈ \mathbb{R}^{256}$을 이용한 Slerp 유사 구면 보간을 사용한다.</p><p>$$ E_δ := MLP (sin(δ) ⊗ e_{start} + sin(δ) ⊗ e_{end} ) ∈ \mathbb{R}^{L×D_{hid}} , $$</p><p>MLP(x)는 RMSNorm과 GELU 활성화를 통해 입력 $x$를 은닉 차원 $\mathbb{R}^{D_{hid}}$로 변환한다. 여기서 $W_1, W_2, b_1, b_2$는 학습 가능한 parameter이다.</p><p><strong>Encoding Semantic Tokens</strong> 의미 정보를 나타내는 이산 토큰들은 벡터의 조회 테이블을 사용해 실수 벡터로 변환된다. Wav2Vec2-Conformer의 클러스터 수에 해당하는 어휘 크기를 기반으로 한다. 이 토큰들을 시간 축에 따라 쌓고 MLP를 적용하여 최종적으로 $E_{st} ∈ \mathbb{R}^{T_{ST} \times D_{hid}}$를 얻는다.</p><p>조건 임베딩이 주어졌을 때, $z_{noisy}$(학습 시) 또는 $z_{δ_t}$(추론 시) 입력은 먼저 선형 변환 및 각도 임베딩과의 합을 거쳐 처리된다. 이후 이중 경로 처리를 위한 분할이 이루어진다.</p><p><strong>Segmentation</strong> 분할 모듈은 2차원 입력을 $K$ 길이의 $S$개 반중복 세그먼트로 나누어 3차원 텐서 $H$로 표현한다. 이는 시퀀스 처리 길이를 sub-linear(O(√L))로 줄여 전체 시퀀스 처리의 학습 난이도를 감소시키고, MeLoDy가 더 높은 빈도의 잠재 요소를 활용할 수 있게 한다.</p><p><strong>Dual-Path Blocks</strong> 분할 후, $N$개의 이중 경로 블록을 위한 3차원 텐서 입력을 얻는다. 각 블록은 세그먼트 간 처리를 위한 거친 경로와 세그먼트 내 처리를 위한 세밀한 경로의 두 단계를 포함한다. 거친 경로 처리에는 Roformer 네트워크를, 세밀한 경로 처리에는 bi-directional RNN을 사용한다. 이 과정은 오디오 구조의 세밀한 세부 사항을 더 잘 재구성하기 위함이다. 각 처리 단계는 self-attention 및 cross-attention layer, 그리고 feature-wise linear modulation(FiLM)를 통해 노이즈 제거를 개선한다.</p><p><strong>Coarse-Path Processing</strong> 이중 경로 블록에서는 거친 경로를 먼저 병렬로 처리한다.</p><p>$$ \mathbb{H}_{c-out}^{(i)} := \text{RepeatSegments} \big( \big[ \text{Roformer} \big( \text{MergeSegments} \big( \mathbb{H}^{(i)} \big) [:, k, :] \big), k = 0, &mldr;, K_{MS}^{(i)} − 1 \big] \big) $$</p><p>이중 경로 블록에서 거친 경로 출력은 $\mathbb{H}^{(i)}$와 동일한 형태를 가지며, 세그먼트를 압축하고 확장하는 MergeSegments와 RepeatSegments 연산을 통해 세그먼트 간 정보를 집계한다. 병합은 열을 평균내어 수행되고, $K_{MS}^{(i)}$ 정의는 블록 인덱스에 따라 텐서의 폭을 변화시킨다. 이는 중간 블록에서 가장 짧고 양 끝에서 가장 긴 세그먼트를 생성된다. 원래 길이를 유지하기 위해 Roformer에서 도입된 반복 연산이 사용된다.</p><p><strong>Fine-Path Processing</strong> fine-path input $\mathbb{H}_{f-in}^{(i)}$은 RMSNorm $\big( \mathbb{H}^{(i)} + \mathbb{H}_{c-out}^{(i)} \big)$로 얻어지며, 행을 병렬로 처리하는 두 층의 SRU에 입력된다.</p><p>$$ \mathbb{H}_{f-out}^{(i)} := \big[ \text{FiLM} \big( \text{SRU} \big( \mathbb{H}_{f-in}^{(i)} [s, :, :] \big), E_δ \big[ {{sL}\over{S}}, : \big] + {{1}\over{T_{ST}}} \sum_{t=0}^{T_{ST} - 1} E_{ST} [t, :] \big), s=0, &mldr;, S - 1 \big] $$</p><p>FiLM(x, m)은 입력 $x$와 변조 조건 $m$에 대해 브로드캐스트 곱셈을 적용하는 함수이다. 다음 이중 경로 블록의 입력은 $\mathbb{H}^{(i+1)} := RMSNorm( \mathbb{H}_{f-in}^{(i)} + \mathbb{H}_{f-out}^{(i)})$으로 정의되며, $N$개의 이중 경로 블록을 처리한 후 3차원 텐서는 중첩-더하기 방식으로 2차원 행렬로 변환된다. 최종적으로, 예측된 속도가 얻어진다.</p><p>$$ \hat{v}_θ (z_{noisy}; c) := \text{RMSNorm} \big( \text{OverlapAdd} \big( \mathbb{H}^{(N+1)}) \big) \big) W_{out} $$</p><p>$W_{out} ∈ \mathbb{R}^{D_{hid} × D}$는 학습 가능한 parameter이다.</p><h3 id=audio-vae-gans-for-latent-representation-learning>Audio VAE-GANs for Latent Representation Learning</h3><p>Rombach et al. 은 latent diffusion 모델(LDMs)을 위한 KL-regularized 이미지 오토인코더를 통해 고품질 이미지 생성의 안정성을 입증하였다. 이 오토인코더는 VAE와 유사하게 KL 패널티를 부과하지만, GAN처럼 적대적으로 학습된다. 이를 VAE-GAN이라 부르며, 이미지 생성에 유망하지만 오디오 파형에 대한 성공적인 방법은 부족하다. 이 연구에서는 DPD 모델에 적용 시 현저한 안정성을 보인 오디오 VAE-GAN을 제안한다.</p><p>오디오 VAE-GAN은 24kHz 오디오 재구성을 위해 스트라이딩 요소 96을 사용하여 250Hz 잠재 시퀀스를 생성한다. 디코더는 HiFi-GAN 구조를 따르고, 인코더는 업샘플링 대신 컨볼루션 기반 다운샘플링을 적용한다. adversarial 학습에는 다기간 및 다해상도 스펙트로그램 discriminator를 사용한다. diffusion 모델의 정상 범위와 일치시키기 위해 인코더 출력은 [-1, 1] 범위로 매핑되며, 이는 극단적인 값을 0.1% 미만으로 걸러내는 역할을 한다.</p><h3 id=music-inpainting-music-continuation-and-music-prompting-with-melody>Music Inpainting, Music Continuation and Music Prompting with MeLoDy</h3><p>제안된 MeLoDy는 임의의 노이즈 조작을 통해 오디오 inpainting(interpolation)과 오디오 continuation(extrapolation)을 지원한다. diffusion 모델이 오디오 inpainting에는 성공적이었지만, 비자기회귀적 특성 때문에 오디오 continuation에는 어려움이 있었다. MeLoDy는 또한 MuLan을 기반으로 비슷한 스타일의 음악을 생성하는 음악 프롬프트 기능을 제공한다.</p><hr><h2 id=experiments>Experiments</h2><h3 id=experimental-setup>Experimental Setup</h3><p><strong>Data Preparation</strong> MeLoDy는 보컬 없는 음악에 초점을 맞춘 257k시간의 음악 데이터로 학습되었으며, ChatGPT를 사용하여 태그 기반의 텍스트를 음악 캡션으로 확장하였다. 이는 195.3M MuLan 학습에 활용되어, 오디오마다 캡션 또는 태그를 무작위로 연결함으로써 모델의 자유 형태 텍스트 처리 능력을 향상시켰다.</p><p><strong>Semantic LM</strong> 429.5M LLaMA 모델을 사용하여 음악LM과 유사한 parameter를 가진 의미론적 모델링을 진행하였다. MuLan RVQ를 통해 12개의 접두사 토큰을 생성하였고, 199.5M Wav2Vec2-Conformer에서 얻은 25Hz 임베딩을 1024-중심 k-means로 이산화하여 10초 길이의 의미론적 토큰을 학습 목표로 설정하였다.</p><p><strong>Dual-Path Diffusion</strong> DPD 모델은 숨겨진 차원 768, 블록 수 8로 설정해 총 296.6M parameter를 가진다. 10초 길이 입력을 L=2500으로 나누어 M=4 부분으로 청킹하고, K=64 크기의 세그먼트로 S=80 세그먼트를 생성하였다. 샘플과 조건 간 일치를 개선하기 위해 분류자 없는 지도를 적용하고, 학습 중에는 교차 주의를 자기 주의로 0.1 확률로 대체한다. 샘플링은 예측 속도를 선형 결합하며, 모든 생성에는 2.5 스케일의 분류자 없는 지도가 사용된다.</p><p><strong>Audio VAE-GAN</strong> 오디오 VAE-GAN은 96의 홉 사이즈로 24kHz 음악 오디오를 250Hz의 잠재 시퀀스로 인코딩하였다. 잠재 차원 D는 16이며, 총 압축율은 6배입니다. 인코더는 256개의 숨겨진 채널을, 디코더는 768개를 사용한다. 전체적으로, 오디오 VAE-GAN은 100.1M의 parameter를 포함한다.</p><h3 id=performance-analysis>Performance Analysis</h3><p><strong>Objective Metrics</strong> 생성된 오디오와 MusicCaps 참조 오디오 사이의 Frećhet audio distance(FAD)를 통해 생성 품질을 대략적으로 측정하고, 사전 학습된 MuLan을 사용한 MuLan cycle consistency(MCC)으로 텍스트와 오디오의 상관성을 평가한다.</p><p><img src=/p/melody/images/table2.png width=906 height=204 srcset="/p/melody/images/table2_hu13630831404589856861.png 480w, /p/melody/images/table2_hu2104328732670594439.png 1024w" loading=lazy class=gallery-image data-flex-grow=444 data-flex-basis=1065px></p><p><strong>Inference Speed</strong> 제안한 MeLoDy의 샘플링 효율성을 평가한 결과, 단 5개의 샘플링 단계로도 참조 세트보다 높은 MCC 점수를 달성하였다. 이는 생성된 샘플이 MusicCaps 캡션과 더 관련이 깊고, 제안된 DPD가 기존 LMs보다 훨씬 낮은 비용으로 효과적으로 작동함을 의미한다.</p><p><img src=/p/melody/images/table3.png width=1058 height=214 srcset="/p/melody/images/table3_hu4259047515497831261.png 480w, /p/melody/images/table3_hu11235538854473749921.png 1024w" loading=lazy class=gallery-image data-flex-grow=494 data-flex-basis=1186px></p><p><strong>Comparisons with SOTA models</strong> MeLoDy는 대규모 음악 데이터셋에서 학습된 MusicLM과 Noise2Music과 비교하여 평가되었다. 동일한 텍스트 프롬프트를 사용하여 7명의 음악 프로듀서가 음악성, 오디오 품질, 텍스트 연관성 측면에서 평가하였다. 총 777번의 비교를 통해 1,554개의 평가가 수집되었다. 결과적으로, MeLoDy는 음악성과 텍스트 연관성에서 MusicLM과 Noise2Music과 유사한 성능을 보였으며, 오디오 품질에서는 두 모델을 모두 능가하였다(p &lt; 0.05, p &lt; 0.01). 또한, MeLoDy는 MusicLM과 Noise2Music에 비해 훨씬 적은 NFEs를 사용하여 효율성을 입증하였다.</p><p><strong>Diversity Analysis</strong> diffusion 모델의 특징인 높은 다양성을 검증하기 위해, MeLoDy는 감정이나 시나리오 등의 텍스트 프롬프트에 기반한 생성 다양성과 유효성을 평가하는 추가 실험을 수행하였다. 데모 페이지에 공개된 샘플 결과에서는 다양한 악기와 질감의 조합을 확인할 수 있었다.</p><p><strong>Ablation Studies</strong> 제안된 방법의 두 측면에 대한 연구에서, 적은 샘플링 단계에서 음향 문제를 줄이는 효과적인 일정을 제안했고, dual-path 구조가 signal-to-noise ratio(SNR) 개선에서 다른 LDM 구조보다 우수함을 입증하였다.</p><hr><h2 id=discussion>Discussion</h2><p><strong>Limitation</strong> 비자연적인 보컬 방지를 위해 주로 무보컬 음악을 포함한 학습 데이터로 인해 프롬프트 범위가 제한되며, 학습 말뭉치는 팝과 클래식에 약간 편향되어 있다. 또한, 10초 세그먼트로 학습함으로써 긴 생성물의 다이내믹스가 제한된다.</p><p><strong>Broader Impact</strong> 음악 프로듀서, 콘텐츠 크리에이터, 일반 사용자 모두가 낮은 진입 장벽으로 창의력을 자유롭게 표현할 수 있는 음악 창작 도구로 큰 가능성을 지닌다고 믿는다. MeLoDy는 인간의 피드백을 반영하는 상호작용적 창작을 지원하며, LoRA 기술을 통해 음악 스타일의 정밀 조정이 가능하다.</p><hr><h2 id=reference>Reference</h2><ul><li><a class=link href=https://arxiv.org/pdf/2305.15719 target=_blank rel=noopener>Paper</a></li><li><a class=link href=https://efficient-melody.github.io/ target=_blank rel=noopener>Demo</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/audio/>Audio</a>
<a href=/tags/music-generation/>Music Generation</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/d2m-gan/><div class=article-details><h2 class=article-title>D2M-GAN</h2></div></a></article><article><a href=/p/museformer/><div class=article-details><h2 class=article-title>Museformer</h2></div></a></article><article><a href=/p/jen-1/><div class=article-details><h2 class=article-title>JEN-1</h2></div></a></article><article><a href=/p/musegan/><div class=article-details><h2 class=article-title>MuseGAN</h2></div></a></article><article><a href=/p/vampnet/><div class=article-details><h2 class=article-title>VampNet</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=KurtKim/kurtkim.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 K2H'log</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></body></html>