<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='Language Models are Unsupervised Multitask Learners'><title>GPT-2</title>
<link rel=canonical href=https://kurtkim.github.io/p/gpt-2/><link rel=stylesheet href=/scss/style.min.ff300df33b80e2ac49809c825614392ed1c7b27591d65d3c4043602cd162e25f.css><meta property='og:title' content='GPT-2'><meta property='og:description' content='Language Models are Unsupervised Multitask Learners'><meta property='og:url' content='https://kurtkim.github.io/p/gpt-2/'><meta property='og:site_name' content="K2H'log"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='NLP'><meta property='article:tag' content='LLM'><meta property='article:published_time' content='2023-12-08T00:00:00+00:00'><meta property='article:modified_time' content='2023-12-08T00:00:00+00:00'><meta name=twitter:title content="GPT-2"><meta name=twitter:description content="Language Models are Unsupervised Multitask Learners"><link rel="shortcut icon" href=favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/k2h_hud72815e7fea33e555ee8ed75e79e7624_40760_300x0_resize_q75_box.jpg width=300 height=306 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>K2H'log</a></h1><h2 class=site-description>넓고 얕은 지식을 위한</h2></div></header><ol class=social-menu><li><a href=https://github.com/kurtkim/ target=_blank title=1 rel=me><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/kyeong-hun-kim-430ba075/ target=_blank title=2 rel=me><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></li><li><a href=https://www.instagram.com/kurt_k2h/ target=_blank title=3 rel=me><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></li><li><a href=https://brunch.co.kr/@bigevlt target=_blank title=4 rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-pencil-minus" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a2.828 2.828.0 10-4-4L4 16v4"/><path d="M13.5 6.5l4 4"/><path d="M16 19h6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://kurtkim.github.io/ selected></option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#approach>Approach</a><ol><li><a href=#training-dataset>Training Dataset</a></li><li><a href=#input-representation>Input Representation</a></li><li><a href=#model>Model</a></li></ol></li><li><a href=#experiments>Experiments</a><ol><li><a href=#language-modeling>Language Modeling</a></li><li><a href=#childrens-book-test>Children’s Book Test</a></li><li><a href=#lambada>LAMBADA</a></li><li><a href=#winograd-schema-challenge>Winograd Schema Challenge</a></li><li><a href=#reading-comprehension>Reading Comprehension</a></li><li><a href=#summarization>Summarization</a></li><li><a href=#translation>Translation</a></li><li><a href=#question-answering>Question Answering</a></li></ol></li><li><a href=#generalization-vs-memorization>Generalization vs Memorization</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#discussion>Discussion</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#reference>Reference</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/paper-review/>Paper Review</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/gpt-2/>GPT-2</a></h2><h3 class=article-subtitle>Language Models are Unsupervised Multitask Learners</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Dec 08, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>10 분 정도</time></div></footer></div></header><section class=article-content><h2 id=abstract>Abstract</h2><p>언어 모델은 웹페이지로 구성된 새로운 데이터셋인 &lsquo;WebText&rsquo;으로 학습 함으로써, 질문 응답, 기계 번역 등의 작업을 명시적인 지도 없이 배우기 시작한다는 것을 발견하였다. 특히, GPT-2는 웹텍스트에 대해 underfitting이지만, zero-shot 환경에서 8개 테스트 언어 모델링 데이터셋 중 7개에서 state-of-the-art를 달성하였다. 즉, 언어 처리 시스템이 자연적 설명으로부터 과제수행능력을 배우는 언어처리모델을 개발할 수 있는 방법을 제안하였다.</p><hr><h2 id=introduction>Introduction</h2><p>기계 학습 시스템은 대규모 데이터셋, 고용량 모델, 지도 학습을 이용해 학습된 작업에서 우수한 성과를 보이지만, 데이터 분포의 작은 변화나 작업 정의에 민감하게 반응하고,매우 좁은 범위의 문제에서만 뛰어난 성능을 보여주고 있다. 그래서 보다 일반적인 문제 해결 능력을 갖춘 범용적인 모델 개발이 필요하며, 이는 결국 각각의 작업에 대해 훈련 데이터셋을 수동으로 생성하고 라벨링할 필요 없이 다양한 작업을 수행할 수 있는 모델을 의미한다.</p><p>기계 학습 시스템을 만드는 주요한 방법은 훈련 예제를 수집하여 시스템을 학습시키고, independent and identically distributed (IID)에서 성능을 테스트하는 것이다. 이 방법은 좁은 범위의 과제에서는 잘 작동하지만, 범용적인 이해를 필요로 하는 캡션 모델, 독해, 이미지 분류 등에서 높은 성능을 내지 못했으며, 이 방법의 한계를 보여주었다.</p><p>일반화하는 능력이 부족한 주요 원인으로 많은 연구가 단일 영역의 dataset과 단일 과제에만 맞춘 학습에만 치중되어 있기 때문이라고 보고 있다. 이를 개선하기 위해 다양한 도메인과 작업에서 훈련하고 성능을 측정하는 것이 필요하며, GLUE와 decaNLP 같은 benchmark dataset이 제안되었다.</p><p>다중 작업 학습(Multitask learning)는 일반 성능을 향상에 높이는 유망한 방법이지만, NLP에서는 아직 초기 연구 단계이다. 최근의 기계학습 시스템의 일반화를 위해서는 수백에서 수천 개의 학습 샘플을 필요로 하며, 다중 작업 학습을 위해서도 그만큼 많은 수의 효과적인 트레이닝 쌍이 필요하다. 현재의 기술로는 dataset을 필요한 수준까지 계속 확장하는 것이 어려우며, 따라서 다중 작업 학습을 위한 새로운 접근법이 필요하다.</p><p>현재 언어 작업에서 최고 성능을 보이는 모델은 사전 학습과 지도 학습을 결합한 방식을 사용한다. 이 접근법은 오랜 역사를 가지고 있으며, transfer 방식이 점차 유연해지고 있다. 초기에는 단어 벡터가 학습되어 특정 작업에 적용되었고, 그 다음으로는 순환 네트워크의 문맥 표현이 transfer 되었다. 최근 연구에서는 특정 작업에 특화된 아키텍처가 필요 없으며, 대신 self-attention block만으로 충분하다고 제안하고 있다.</p><p>현재의 방법들은 작업 수행을 위해 여전히 지도 학습이 필요하다. 하지만 지도 데이터가 거의 없거나 전혀 없을 때, 언어 모델이 상식적인 추론이나 감성 분석 등의 특정 작업을 수행하는 데 잠재력이 있다는 것이 다른 연구에서 보여져 왔다.</p><p>이 논문에서는 언어 모델이 parameter나 아키텍처 변경 없이 zero-shot setting에서 다양한 작업을 수행할 수 있는 능력을 보여준다. 이 접근법은 전이 학습의 일반화 추세를 이어가며, 작업에 따라 유망한 결과와 경쟁력 있는 성과, 그리고 state-of-the-art를 달성하는 잠재력을 보여준다.</p><hr><h2 id=approach>Approach</h2><p>핵심은 언어 모델링(language modeling)이며, 이는 각 원소가 일련의 symbol $(s_1, s_2, &mldr;, s_n)$ 으로 구성된 예제 $(x_1, x_2, &mldr;, x_n)$ 에서 비지도분포 추정을 하는 것으로 정의된다. 언어의 순차적 특성 때문에 기호들에 대한 결합 확률은 조건부 확률의 곱으로 분해하는 것이 일반적이다.</p><p>$$p(x) = \prod_{i=1}^n p(s_n | s_1, s_2, &mldr;, s_{n-1}) $$</p><p>이 방법은 $p(x)$ 및 $p(s_{n-k}, &mldr;, s_n | s_1, &mldr;, s_{s-k-1})$ 형태의 조건부의 샘플링과 추정을 가능하게 하며, 최근에는 Transformer와 같은 self-attention 아키텍처의 발전으로 이러한 조건부 확률을 계산하는 모델의 표현력이 크게 향상되었다.</p><p>단일 작업을 수행하는 학습은 확률론적 프레임워크에서 조건부 분포 $p(output | input)$를 추정하는 것으로 볼 수 있다. 하지만 일반적인 시스템은 동일한 입력에 대해 수행해야 하는 다양한 작업을 고려해야 한다. 이를 위해, 시스템은 $p(output | input, task)$를 모델링해야 한다. 이는 다중학습과 메타학습 환경에서 다양하게 형식을 갖는다. McCann et al. (2018)은 언어를 활용하여 작업, 입력, 출력을 기호 시퀀스로 지정하는 방법을 제시하였고, 이 방법을 사용하여 MQAN이라는 단일 모델을 훈련시켜 다양한 작업을 수행할 수 있음을 보여주었다.</p><p>언어 모델링은 출력 symbol에 대한 명시적인 지도 없이도 다양한 작업을 학습할 수 있다. 이는 감독된 학습 목표와 비감독된 학습 목표가 실질적으로 같기 때문인데, 이 두 목표의 global minimum은 동일하다. 예비 실험에서는 충분히 큰 언어 모델은 이러한 설정에서 다중 작업 학습을 수행할 수 있지만, 명시적으로 감독된 방법보다 학습 속도가 느리다는 것이 확인되었다.</p><p>대화(dialog)의 맥락에서 자연어를 직접 학습하는 방법은 매력적인 접근법이지만, 상호작용이 필요없는 인터넷 사이트에 존재하는 방대한 양의 데이터를 활용하는 방법을 선택하였다. 충분한 용량을 가진 언어 모델은 자연어 시퀀스에서 작업을 추론하고 수행하며, 이를 통해 더 잘 예측하도록 학습할 것으로 예상된다. 모델은 비지도 다중작업 학습을 수행하게 될 것이며, 다양한 작업에서 언어 모델의 제로샷 성능을 분석하였다.</p><h3 id=training-dataset>Training Dataset</h3><p>이전 연구들은 주로 한정된 도메인의 텍스트를 가지고 언어 모델을 학습시켰다. 하지만 이 논문에서는 가능한 한 다양한 도메인과 맥락에서 작업을 수집하기 위해, 크고 다양한 dataset을 구축하였다.</p><p>다양하고 방대한 텍스트의 출처로 웹 스크랩인 Common Crawl이 유망하지만, 데이터 품질 문제가 있습니다.</p><p>이 dataset을 사용하는 대신, 문서의 품질을 중요시하는 새로운 웹 스크랩을 만들었다. 전체 웹 스크랩을 수동으로 필터링하는 비용을 줄이기 위해, 사람들이 선별한 웹 페이지를 대상으로 했다. 특히, 적어도 3 카르마를 받은 Reddit의 모든 외부 링크를 스크랩했는데, 이는 사용자들이 해당 링크를 유익하거나 재미있게 여겼는지의 지표로 볼 수 있다.</p><p>결과적으로 나온 dataset인 WebText는 45백만 링크의 텍스트 부분집합을 포함하고 있으며, 텍스트 추출을 위해 Dragnet과 Newspaper 내용추출기를 사용하였다. 이 논문의 모든 결과는 2017년 12월 이후 링크를 제외한 초기 버전의 WebText를 사용하며, 이는 de-duplication과 cleaning 과정을 거친 후 40GB, 약 800만 개의 문서를 포함하고 있다. Wikipedia 문서는 분석을 복잡하게 할 수 있어 WebText에서 제외하였다.</p><h3 id=input-representation>Input Representation</h3><p>일반적인 언어 모델은 어떤 문자열의 확률도 계산하고 생성할 수 있어야 한다. 하지만 현재의 대규모 언어 모델은 전처리 과정으로 인해 모델링 가능한 문자열 범위가 제한된다. 유니코드 문자열을 UTF-8 바이트로 처리하는 것은 이를 해결하나, 현재 바이트 수준의 언어 모델은 대규모 데이터셋에서 단어 수준의 언어 모델만큼 효과적이지 않다. WebText에서 바이트 수준 언어 모델을 훈련시키려 했으나, 이와 비슷한 성능 격차를 경험하였다.</p><p>Byte Pair Encoding(BPE)는 문자와 단어 수준 언어 모델링 사이의 중간 지점이다. 자주 나오는 symbol sequence의 단어수준 입력과 자주 나오지 않는 symbol sequence의 글자수준 입력을 적절히 보간(interpolate)한다. BPE 구현은 byte sequence가 아닌 unicode code points에서 동작한다. 이러한 구현은 모든 unicode 문자열을 모델링하기 위해 전체 unicode symbol의 전체 공간을 포함해야 한다. multi-symbol token을 추가하기 전 130,000개가 넘는 token을 포함하는 기본사전을 필요로 하게 된다. 이는 보통의 32,000개에서 64,000개의 token의 사전에 비해 지나치게 크다. 반면, byte수준의 BPE의 사전은 256개의 token만을 필요로 한다. 그러나 BPE를 byte sequence에 직접 적용하면, 토큰 어휘를 구축하기 위한 BPE의 greedy frequency 기반 heuristic 때문에 최적이 아닌 병합이 발생한다. 예를 들어, &lsquo;dog&rsquo;와 같은 일반적인 단어가 다양한 형태로 나타나면서 제한된 어휘 슬롯과 모델 용량이 최적화되지 않을 수 있다. 이를 해결하기 위해 byte sequence에 대해 문자 범주를 넘어서 병합하는 것을 방지하고, 공백에 대한 예외를 추가하여 압축 효율을 향상시키고 단어의 분열을 최소화하였다.</p><p>이 입력 표현법은 단어 수준 언어 모델의 이점과 byte 수준 접근법의 범용성을 결합시킨다. 이러한 접근법은 어떤 unicode 문자열에도 확률을 부여할 수 있으므로, 사전 처리, 토큰화, 어휘 크기와 관계없이 모든 데이터셋에서 언어 모델을 평가할 수 있다.</p><h3 id=model>Model</h3><p>Transformer 기반 아키텍처를 사용하며, 이는 주로 OpenAI GPT 모델을 따른다. 몇 가지 수정사항은 레이어 정규화의 위치 변경, 추가적인 레이어 정규화의 삽입, 모델 깊이를 고려한 초기화 방식의 수정, 그리고 잔여 레이어 가중치의 스케일링이다. 또한, 어휘는 50,257개로 확장되었고, 맥락 크기와 배치 크기도 각각 1024 토큰과 512로 증가시켰다.</p><hr><h2 id=experiments>Experiments</h2><p><img src=/p/gpt-2/images/table2.png width=356 height=188 srcset="/p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_480x0_resize_box_3.png 480w, /p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=189 data-flex-basis=454px></p><p>크기별로 네 개의 언어 모델을 훈련시키고 벤치마킹하였다. 가장 작은 모델의 크기는 원래의 GPT와 같으며, 두 번째로 작은 모델은 BERT의 가장 큰 모델과 같다. 가장 큰 모델인 GPT-2는 GPT보다 10배 이상 많은 parameter를 가지고 있다. 각 모델의 learning rate는 WebText의 5%인 held-out 샘플을 사용하여 수동 조정하으며, 모든 모델은 여전히 WebText에 underfitted 되었으며 더 오래 학습시키면 더 높은 성능을 얻을 수 있을 것이다.</p><h3 id=language-modeling>Language Modeling</h3><p>GPT-2 모델은 문자 단위(byte level)에서 작동하고, 손실이 큰 전처리나 토큰화가 필요 없으므로 모든 언어 모델 benchmark에서 평가할 수 있다. WebText 언어 모델에 따른 dataset의 로그-확률을 계산하는 방식으로 평가 하였다. WebText 언어 모델은 표준화된 텍스트, 토큰화 유물, 섞인 문장, 문자열(40 billion 바이트 중 26번만 발생) 등을 예측해야 하기 때문에 많은 데이터셋에서 일반 분포 밖에서 테스트되어야 한다.</p><p><img src=/p/gpt-2/images/table3.png width=1302 height=248 srcset="/p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_480x0_resize_box_3.png 480w, /p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=525 data-flex-basis=1260px></p><p>WebText 언어 모델은 도메인과 데이터셋 간에 잘 transfer되며, zero-shot setting에서 8개의 dataset 중 7개에서 state-of-the-art를 달성하였다.</p><h3 id=childrens-book-test>Children’s Book Test</h3><p>Children’s Book Test(CBT)는 다양한 카테고리의 단어에 대한 언어 모델의 성능을 평가하기 위한 테스트로, 생략된 단어에 대한 10개의 가능한 선택 중 올바른 것을 예측한다.</p><p><img src=/p/gpt-2/images/figure2.png width=632 height=342 srcset="/p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_480x0_resize_box_3.png 480w, /p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=184 data-flex-basis=443px></p><p>모델 크기가 증가함에 따라 성능이 지속적으로 개선되며, GPT-2는 일반 명사에서 93.3%, 개체명에서 89.1%의 성능을 달성하였다.</p><h3 id=lambada>LAMBADA</h3><p>LAMBADA dataset은 텍스트의 장거리 의존성(long-range dependencies)을 평가한다. GPT-2는 이 테스트에서의 perplexity를 99.8에서 8.6으로, 정확도를 19%에서 52.66%로 향상시켰다. 추가적으로, stop-word ﬁlter를 추가함으로써 정확도를 63.24%로 더욱 향상시켰다.</p><h3 id=winograd-schema-challenge>Winograd Schema Challenge</h3><p><img src=/p/gpt-2/images/figure3.png width=620 height=466 srcset="/p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_480x0_resize_box_3.png 480w, /p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=133 data-flex-basis=319px></p><p>Winograd Schema Challenge는 텍스트의 모호성을 해결하는 능력을 통해 시스템의 상식적 추론 능력을 측정하고자 한다. GPT-2는 정확도를 7% 향상시켜 70.70%를 달성하였다.</p><h3 id=reading-comprehension>Reading Comprehension</h3><p>CoQA(The Conversation Question Answering dataset)는 7가지 다른 분야의 문서와 문서에 대한 질문자-답변자 사이의 자연어 대화가 쌍을 이루고 있다. CoQA 테스트는 독해능력과 대화에 기반한 모델의 답변능력을 평가한다. GPT-2는 미세조정 없이 55 F1 score를 달성해 4개 중 3개의 다른 모델을 능가하였다.</p><h3 id=summarization>Summarization</h3><p>GPT-2의 요약 능력은 CNN과 Daily Mail dataset을 사용해서 테스트했다. 문서 이후에 &ldquo;TL;DR:&rdquo; 토큰을 추가하고 Top-k 랜덤 샘플링을 통해 요약을 유도했다.</p><p><img src=/p/gpt-2/images/table4.png width=572 height=236 srcset="/p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_480x0_resize_box_3.png 480w, /p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=242 data-flex-basis=581px></p><p>처음 생성된 3개의 문장을 요약 결과로 하여 실험한 결과, 기사의 최근 내용에 초점을 맞추거나 특정 세부사항을 혼동하는 경향이 있다. &ldquo;TL;DR:&rdquo; 토큰 없이 실험한 경우, 성능이 더 하락한 것을 보면 힌트를 통한 Task 유도가 유의한 결과를 냄을 확인할 수 있었다.</p><h3 id=translation>Translation</h3><p>번역 능력은 WMT-14 English-French dataset을 사용해서, 영어-프랑스어, 프랑스어-영어 두가지 경우에서 비교가 진행되었다. 번역 성능은 다른 Task에 비해 좋지 상대적으로 좋지 않다. 영어-프랑스어 테스트에서 5 BLEU를, 프랑스어-영어 테스트에서는 11.5 BLEU를 달성했다.</p><h3 id=question-answering>Question Answering</h3><p>언어 모델에 얼마나 많은 정보가 들어있는지 테스트하기 위해 factoid-style의 질문에 얼마나 정확한 답을 생성하는지 평가한다. Natural Questions dataset을 이용해 GPT-2의 성능을 평가하였고 &lsquo;정확히 일치 하는지&rsquo; 여부(exact match metric)를 지표로 비교한다. 질문의 4.1%에 대해 올바르게 답을 하였고, 이는 기존의 모델들보다 5.3배 높은 정확도이다. 매우 작은 모델들은 대체로 1%를 넘지 못하는 성능을 보였는데, 아직까지는 모델의 크기가 QA에 있어서 매우 중요한 요인이라는 것을 확인할 수 있었다. 또한, 가장 확신하는 1%의 질문에 대해 63.1%의 정확도를 보였지만, 이는 여전히 정보 검색과 문서 질문 답변 추출을 결합한 시스템의 30%에서 50% 범위의 성능보다 훨씬 떨어진다.</p><hr><h2 id=generalization-vs-memorization>Generalization vs Memorization</h2><p>최근 연구에 따르면, 일반적인 이미지 데이터셋에는 상당한 양의 중복된 이미지가 포함되어 있어, 기계 학습 시스템의 일반화 성능을 과대평가하게 만든다. 데이터셋의 크기가 커질수록 이 문제는 더욱 심화될 가능성이 있으며, 이는 테스트 데이터가 얼마나 훈련 데이터에도 포함되어 있는지 분석하는 것이 중요함을 의미한다.</p><p>이를 연구하기 위해, WebText 훈련 데이터의 8-gram을 포함하는 Bloom 필터를 생성하였고, 주어진 데이터셋에 대해 그 데이터셋의 8-gram 중 얼마나 많은 비율이 WebText 훈련 세트에도 포함되어 있는지를 계산하였다.</p><p><img src=/p/gpt-2/images/table6.png width=892 height=144 srcset="/p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_480x0_resize_box_3.png 480w, /p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=619 data-flex-basis=1486px></p><p>일반적인 언어 모델 데이터셋의 테스트 세트는 WebText 훈련 세트와 1-6%의 중복을 가지며, 평균 중복률은 3.2%이다. 많은 데이터셋은 자신의 훈련 분할과 더 큰 중복을 가지며, 평균 중복률은 5.9%이다.</p><p>데이터 중복을 최소화하는 방향으로 접근하였으며, 이러한 중복이 성능에 작은, 하지만 일관적인 향상을 가져다 준다는 분석 결과를 얻었다. 중복 제거 기법을 개선함으로써 이러한 문제에 대해 더욱 효과적으로 대응할 수 있다. 그리고 이러한 중복 제거 과정에서는 n-gram 중첩 기반의 방법을 활용하는 것이 중요하다.</p><p><img src=/p/gpt-2/images/figure4.png width=626 height=572 srcset="/p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_480x0_resize_box_3.png 480w, /p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_1024x0_resize_box_3.png 1024w" loading=lazy class=gallery-image data-flex-grow=109 data-flex-basis=262px></p><p>또한, WebText 언어 모델의 성능이 기억력에 의존하는지를 확인하기 위해 해당 모델이 자체 테스트 세트에서 어떤 성능을 보이는지 검사하였다. 이 결과, 모델 크기가 커짐에 따라 훈련 세트와 테스트 세트에서의 성능이 함께 향상되는 경향을 보였으며, 이로부터 GPT-2가 WebText에 대해 완벽하게 적합하지 않음을 추측할 수 있다.</p><p>마지막으로, GPT-2가 말하는 유니콘의 발견에 대한 뉴스 기사를 작성하는 능력을 보여주었다. 이는 GPT-2의 창의성을 보여주는 한 예로 볼 수 있다.</p><hr><h2 id=related-work>Related Work</h2><p>이 연구는 더 큰 dataset에서 학습된 큰 언어 모델의 성능을 측정하는 데 중점을 두었다. 이 연구는 이전 연구와 비슷한 방향성을 가지고 있으며, 우리의 실험 결과는 주어진 목표의 세부 작업에 대한 추세가 큰 파라미터 범위로도 지속되는 것을 확인하였다.</p><p>생성 모델에서는 RNN 언어 모델이 줄 너비를 추적하고 인용문이나 댓글을 감지하는 등의 흥미로운 기능을 배우는 것이 확인되었다. 또한, 위키백과 기사를 생성하도록 훈련된 모델이 언어 간 이름 번역을 배울 수 있음이 관찰되었다.</p><p>iWeb Corpus같이 웹 페이지의 대형 텍스트 말뭉치를 필터링하고 구성하는 다양한 방법, 모든 단어 벡터 표현 학습을 확대하거나, 기계 번역 모델에서 파생된 표현의 사용을 탐색하는 사전학습 방법, seq2seq 모델 등이 연구 되었고, 언어모델의 사전학습이 잡담이나 대화 같은 어려운 생성문제에 맞춰 미세조정할 때 도움이 된다는 것을 밝혀내었다.</p><hr><h2 id=discussion>Discussion</h2><p>비지도 사전 학습 방법의 표현에 대한 많은 연구가 있었으며, 이는 비지도 학습이 유망한 연구 영역임을 시사한다. GPT-2는 독해에 대해 경쟁력 있는 성능을 보였지만, 요약 등의 작업에 대해서는 아직 기본적인 수준에 불과하다. 많은 NLP 작업에서 GPT-2의 제로샷 성능을 연구했지만, 아직 많은 실용적인 작업에서는 성능이 무작위 수준에 불과한 경우가 많다. 제로샷 성능은 GPT-2의 잠재적 성능의 기준을 설정하지만, 미세 조정을 통한 성능의 상한선은 아직 불분명하다. 더욱이, GPT-2의 추가 훈련 데이터와 용량이 단방향 표현의 비효율성을 극복하기에 충분한지는 아직 불확실하다.</p><p>decaNLP나 GLUE와 갈은 benchmark에서 미세조정 할 것을 계획하고 있으며, GPT-2의 학습데이터와 그 크기가 BERT에서 말한 단방향 표현의 비효율성을 극복할 수 있을 만큼 충분한지도 확실치 않다고 한다.</p><hr><h2 id=conclusion>Conclusion</h2><p>충분히 크고 다양한 dataset으로 학습된 큰 언어 모델인 GPT-2는 여러 도메인과 데이터셋에서 잘 수행하며, 테스트된 8개 언어 모델링 dataset 중 7개에서 state-of-the-art를 달성하였다. 이는 고용량 모델이 다양한 텍스트에 대한 가능성을 극대화하는 훈련을 통해, 명확한 지도 없이도 많은 작업을 수행하는 법을 배우기 시작한다는 것을 시사한다.</p><hr><h2 id=reference>Reference</h2><ul><li><a class=link href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf target=_blank rel=noopener>Paper</a></li><li><a class=link href=https://github.com/openai/gpt-2 target=_blank rel=noopener>Github</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/nlp/>NLP</a>
<a href=/tags/llm/>LLM</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/galactica/><div class=article-details><h2 class=article-title>Galactica</h2></div></a></article><article><a href=/p/bloom/><div class=article-details><h2 class=article-title>BLOOM</h2></div></a></article><article><a href=/p/helm/><div class=article-details><h2 class=article-title>HELM</h2></div></a></article><article><a href=/p/glm-130b/><div class=article-details><h2 class=article-title>GLM-130B</h2></div></a></article><article><a href=/p/flan-t5/palm/><div class=article-details><h2 class=article-title>Flan-T5/PaLM</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=KurtKim/kurtkim.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 K2H'log</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></body></html>