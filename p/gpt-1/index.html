<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='Improving Language Understanding by Generative Pre-Training'><title>GPT-1</title>
<link rel=canonical href=https://kurtkim.github.io/p/gpt-1/><link rel=stylesheet href=/scss/style.min.ff300df33b80e2ac49809c825614392ed1c7b27591d65d3c4043602cd162e25f.css><meta property='og:title' content='GPT-1'><meta property='og:description' content='Improving Language Understanding by Generative Pre-Training'><meta property='og:url' content='https://kurtkim.github.io/p/gpt-1/'><meta property='og:site_name' content="K2H'log"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='LLM'><meta property='article:tag' content='Milestone'><meta property='article:published_time' content='2023-12-02T00:00:00+00:00'><meta property='article:modified_time' content='2023-12-02T00:00:00+00:00'><meta name=twitter:title content="GPT-1"><meta name=twitter:description content="Improving Language Understanding by Generative Pre-Training"><link rel="shortcut icon" href=favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="메뉴 여닫기">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/k2h_hu16962933080322361302.jpg width=300 height=306 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>K2H'log</a></h1><h2 class=site-description>넓고 얕은 지식을 위한</h2></div></header><ol class=social-menu><li><a href=https://github.com/kurtkim/ target=_blank title=1 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/kyeong-hun-kim-430ba075/ target=_blank title=2 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></li><li><a href=https://www.instagram.com/kurt_k2h/ target=_blank title=3 rel=me><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></li><li><a href=https://velog.io/@kurt_kim target=_blank title=4 rel=me><svg class="icon icon-tabler icon-tabler-pencil-minus" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 20h4L18.5 9.5a2.828 2.828.0 10-4-4L4 16v4"/><path d="M13.5 6.5l4 4"/><path d="M16 19h6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://kurtkim.github.io/ selected></option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>다크 모드</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">목차</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related Work</a><ol><li><a href=#semi-supervised-learning-for-nlp>Semi-supervised learning for NLP</a></li><li><a href=#unsupervised-pre-training>Unsupervised pre-training</a></li><li><a href=#auxiliary-training-objectives>Auxiliary training objectives</a></li></ol></li><li><a href=#framework>Framework</a><ol><li><a href=#unsupervised-pre-training-1>Unsupervised pre-training</a></li><li><a href=#supervised-ﬁne-tuning>Supervised ﬁne-tuning</a></li><li><a href=#task-speciﬁc-input-transformations>Task-speciﬁc input transformations</a><ol><li><a href=#textual-entailment>Textual entailment</a></li><li><a href=#similarity>Similarity</a></li><li><a href=#question-answering-and-commonsense-reasoning>Question Answering and Commonsense Reasoning</a></li></ol></li></ol></li><li><a href=#experiments>Experiments</a><ol><li><a href=#setup>Setup</a><ol><li><a href=#unsupervised-pre-training-2>Unsupervised pre-training</a></li><li><a href=#model-speciﬁcations>Model speciﬁcations</a></li><li><a href=#fine-tuning-details>Fine-tuning details</a></li></ol></li><li><a href=#supervised-ﬁne-tuning-1>Supervised ﬁne-tuning</a><ol><li><a href=#natural-language-inference>Natural Language Inference</a></li><li><a href=#question-answering-and-commonsense-reasoning-1>Question answering and commonsense reasoning</a></li><li><a href=#semantic-similarity>Semantic Similarity</a></li><li><a href=#classiﬁcation>Classiﬁcation</a></li></ol></li></ol></li><li><a href=#analysis>Analysis</a><ol><li><a href=#impact-of-number-of-layers-transferred>Impact of number of layers transferred</a></li><li><a href=#zero-shot-behaviors>Zero-shot Behaviors</a></li><li><a href=#ablation-studies>Ablation studies</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#reference>Reference</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/paper-review/>Paper Review</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/gpt-1/>GPT-1</a></h2><h3 class=article-subtitle>Improving Language Understanding by Generative Pre-Training</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Dec 02, 2023</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 분 정도</time></div></footer></div></header><section class=article-content><h2 id=abstract>Abstract</h2><p>자연어 이해는 텍스트의 함축, 질문에 대한 답변, 의미의 유사성 평가, 문서 분류 등 다양한 작업으로 구성되어 있다. 레이블이 지정된 데이터가 부족한 상황에서, 이 논문은 레이블이 없는 텍스트 데이터에 대해 언어 모델을 (생성적) 사전학습(generative pre-training)하고, 이를 특정 작업에 미세조정(fine-tuning)하는 방식을 제안한다. 이 방법은 모델 아키텍처에 최소한의 변경만을 요구하면서도 효과적인 전이를 달성하였고, 다양한 자연어 이해 벤치마크에서 우수한 성능을 보여주었다. 이 모델은 각 작업에 특별히 설계된 모델을 능가하며, 12개의 작업 중 9개에서 최고 성능을 달성하였다.</p><hr><h2 id=introduction>Introduction</h2><p>자연어 처리(NLP)에서 지도 학습의 의존성을 줄이는 것은 중요한데, 이는 대부분의 딥러닝 방법이 수동 레이블링된 대량의 데이터가 필요하기 때문이다. 이런 상황에서 레이블이 없는 데이터에서 언어 정보를 추출할 수 있는 모델은 유용한 대안이 될 수 있으며, 비지도 학습을 통해 학습하는 것이 더 나은 결과를 얻는 경우도 있다. 이를 입증하는 가장 강력한 예는 사전 학습된 단어 임베딩이며, 이는 다양한 NLP 작업에서 성능 향상을 위해 널리 사용되고 있다.</p><p>레이블이 없는 텍스트에서 단어 수준을 넘어서는 정보를 활용하는 것은 어려운 도전 과제이며, 이유는 다음과 같다. 첫째, 텍스트 표현을 학습하고 다른 곳에 유용하게 전이하는 최적화 목표가 무엇인지 확실하지 않다. 둘째, 학습된 표현을 어떤 작업에 가장 효과적으로 적용할 방법이 아직 확립되지 않았다. 이런 불확실성이 효과적인 준지도 학습 방법을 개발하는 것을 어렵게 한다.</p><p>이 연구는 언어 이해 작업에 비지도 사전 학습(unsupervised pre-training)과 지도 미세 조정(supervised fine-tuning)을 결합하는 준지도 학습을 제안한다. 목표는 적은 조정으로 다양한 작업에 적용 가능한 표현을 학습하는 것이다. 레이블이 없는 대량의 텍스트와 수동으로 레이블링된 훈련 예제를 사용하며, 학습은 두 단계로 진행된다. 먼저, 레이블이 없는 데이터로 모델의 초기 파라미터를 학습하고, 그 다음으로 지도 학습을 통해 이 파라미터를 목표 작업에 맞게 조정한다.</p><p>이 연구에서는 다양한 작업에서 뛰어난 성능을 보인 Transformer모델을 사용한다. 이 모델은 텍스트의 장기적인 의존성을 처리하는 더 구조화된 메모리를 제공하므로 강한 전이 성능을 보여준다. 전이 단계에서는 작업 특정 입력 조정을 사용하여 텍스트 입력을 연속 토큰 시퀀스로 처리하며, 이 방식은 사전 학습된 모델의 구조를 최소한으로 변경하면서 효과적으로 미세 조정할 수 있음을 실험적으로 입증한다.</p><p>이 연구는 자연어 추론, 질문 응답, 의미 유사성, 텍스트 분류 등 네 가지 언어 이해 작업에서 모델을 평가하였다. 제시된 모델은 각 작업에 특화된 모델들보다 더 우수한 성능을 보여주었고, 12개 작업 중 9개에서 최고 성능을 보여주었다.</p><hr><h2 id=related-work>Related Work</h2><h3 id=semi-supervised-learning-for-nlp>Semi-supervised learning for NLP</h3><p>이 연구는 자연어에 대한 준지도 학습 범주에 속하며, 이는 시퀀스 라벨링이나 텍스트 분류와 같은 작업에 적용된다. 초기에는 레이블 없는 데이터를 사용해 단어나 구문 수준의 통계를 계산하였지만, 최근에는 레이블이 없는 말뭉치에서 훈련된 단어 임베딩을 활용하여 작업 성능을 향상시키는 방향으로 연구가 진행되고있다. 그러나 이 논문의 목표는 단어 수준 이상의 의미를 포착하는 것이며, 이를 위해 구문이나 문장 수준의 임베딩을 활용하여 텍스트를 벡터 표현으로 인코딩하는 방식을 채택하였다.</p><h3 id=unsupervised-pre-training>Unsupervised pre-training</h3><p>비지도 사전 학습은 좋은 초기화 지점을 찾는 것을 목표로 하며, 이미지 분류, 음성 인식, 엔티티 구분, 기계 번역 등 다양한 작업에서 DNN의 훈련을 돕는데 사용되고있다.</p><p>이 연구는 언어 모델링 목표를 사용하여 신경망을 사전 학습하고, 지도 학습으로 목표 작업에서 미세 조정하는 방식을 따른다. 이 방법은 LSTM을 사용하는 이전의 방법들이 제한적인 예측 능력을 가지는 반면, Transformer는 더 넓은 범위의 언어 구조를 포착할 수 있게 한다. GPT 모델은 자연어 추론, 패러프레이즈 감지, 스토리 완성 등 다양한 작업에서 효과를 보여주었으며, 다른 모델이 새로운 파라미터를 많이 필요로 하는 반면, GPT 모델은 아키텍처에 최소한의 변경만 필요로 한다.</p><h3 id=auxiliary-training-objectives>Auxiliary training objectives</h3><p>보조적인 비지도 학습 목표 추가는 준지도 학습의 변형 형태로, 다양한 NLP 작업을 통해 의미 역할 라벨링을 개선하는데 사용되었다. 최근에는 이러한 보조 목표를 목표 작업에 추가하여 시퀀스 라벨링 작업에서 성능을 향상시켰다. 이 연구에서도 비지도 사전 훈련이 이미 목표 작업과 관련된 다양한 언어적 요소를 학습한다는 것을 보여준다.</p><hr><h2 id=framework>Framework</h2><p>학습은 큰 말뭉치에서 대용량 언어 모델을 학습하는 단계와 레이블이 달린 데이터를 활용해 모델을 목표 작업에 맞게 미세 조정하는 단계로 이루어진다.</p><h3 id=unsupervised-pre-training-1>Unsupervised pre-training</h3><p>비지도 토큰 말뭉치 $U = \lbrace u_1, &mldr; , u_n \rbrace $ 가 주어질때, 다음 Likelihood를 최대화하도록 표준언어모델링 목적함수를 사용한다:</p><p>$$ L_1(U) = \sum_{i} \log{P} (u_i | u_{i-k}, &mldr; , u_{i-1}, \theta) $$</p><p>$k$는 context window의 크기이며, 조건부 확률 $P$는 parameter $\theta$를 가진 신경망을 사용하여 모델링된다. 이 parameter들은 stochastic gradient descent를 사용하여 학습된다.</p><p>GPT 모델은 언어모델로 multi-layer Transformer decoder를 사용하며, 이 모델은 입력 컨텍스트 토큰에 대해 multi-headed self-attention을 적용한 후, position-wise feedforward layer를 적용하여 목표 토큰에 대한 출력 분포를 생성한다:</p><p>$$ h_0 = UW_e + W_p $$
$$ h_l = \text{transformer_block}(h_{l-1}) \forall i \in [1, n] $$
$$ P(u) = \text{softmax}(h_n W^T_e) $$</p><p>$U = (u_{i-k}, &mldr; , u_{i-1}) $ 는 토큰의 컨텍스트 벡터이고, $n$은 layer의 수, $W_e$ 는 토큰 임베딩 행렬, $W_p$ 는 위치 임베딩 행렬이다.</p><h3 id=supervised-ﬁne-tuning>Supervised ﬁne-tuning</h3><p>모델을 학습한 후, parameter를 목표 작업에 맞게 조정한다. 레이블이 지정된 데이터셋 $C$ 는 입력 토큰 $x^1, &mldr; , x^m $ 과 레이블 $y$로 구성된다. 입력은 사전 훈련된 모델을 통과하여 최종 transformer block의 활성값인 $h^m_l$ 을 얻으며, 이는 parameter $W_y$ 와 함께 선형 출력층으로 전달되어 $y$ 를 예측한다:</p><p>$$ P(y|x^1, &mldr; , x^m) = \text{softmax}(h^m_l W_y) $$</p><p>이는 다음을 최대화 한다.</p><p>$$ L_2(C) = \sum_{(x,y)} \log{P(y|x^1, &mldr; , x^m)} $$</p><p>추가로 미세 조정을 위한 보조 목표로 언어 모델링을 포함시키는 것은 지도 모델의 일반화를 향상시키고, 수렴을 가속화하는데 도움이 된다. 구체적으로, weight $\lambda$에 대해 다음을 최적화한다:</p><p>$$ L_3(C) = L_2(C) + \lambda L_1(C) $$</p><p>미세 조정 과정에서 추가 매개변수는 $W_y$ 와 구분자 토큰의 임베딩뿐이다.</p><h3 id=task-speciﬁc-input-transformations>Task-speciﬁc input transformations</h3><p>텍스트 분류같은 일부 작업들은 모델을 직접 미세 조정할 수 있지만, 질문 답변이나 텍스트 함의 같은 작업들은 구조화된 입력을 필요로 하는데, 이러한 입력에 대해 사전 학습된 모델은 별도의 수정 없이도 처리할 수 있다. 대신, 이런한 입력을 모델이 처리할 수 있는 순서가 있는 시퀀스로 변환한다. 이 접근법은 작업 간에 아키텍처를 크게 변경할 필요를 없애준다. 또한, 모든 변형에는 무작위로 초기화된 시작과 종료 토큰을 포함한다.</p><p><img src=/p/gpt-1/images/figure1.png width=1058 height=450 srcset="/p/gpt-1/images/figure1_hu870624340289633744.png 480w, /p/gpt-1/images/figure1_hu4627024154680249911.png 1024w" loading=lazy class=gallery-image data-flex-grow=235 data-flex-basis=564px></p><h4 id=textual-entailment>Textual entailment</h4><p>텍스트 함의에서는, 전제 $p$와 가설 $h$를 구분자 <code>$</code>로 연결한다.</p><h4 id=similarity>Similarity</h4><p>유사성 경우, 비교되는 두 문장의 순서는 정해져 있지 않으므로, 텍스트 두 개를 다른 순서로 이어붙여 각각을 독립적으로 처리하여 두 시퀀스 표현 $h^m_l$을 생성한다.</p><h4 id=question-answering-and-commonsense-reasoning>Question Answering and Commonsense Reasoning</h4><p>컨텍스트 문서 $z$, 질문 $q$, 가능한 답변들 $\lbrace a_k \rbrace$을 받는다. 각 가능한 답변을 문맥 문서와 질문에 연결하고, 구분자 토큰을 추가해 시퀀스 $[z; q;$ <code>$</code>; $a_k]$ 를 만든다. 이 시퀀스들은 독립적으로 처리되고, softmax 계층을 통해 정규화되어 답변들에 대한 출력 분포를 생성한다.</p><hr><h2 id=experiments>Experiments</h2><h3 id=setup>Setup</h3><h4 id=unsupervised-pre-training-2>Unsupervised pre-training</h4><p>언어 모델 학습에 BooksCorpus 데이터셋을 사용한다. 이는 다양한 장르의 7천개가 넘는 미발행 책들을 포함하며, 연속적인 긴 텍스트를 통해 모델이 long term depency를 학습할 수 있다. ELMo에서 사용된 1B Word Benchmark 데이터셋은 문장들이 서로 섞여 있어 long term depency를 학습하기 어렵다.</p><p><img src=/p/gpt-1/images/table1.png width=1068 height=192 srcset="/p/gpt-1/images/table1_hu11479561832254097085.png 480w, /p/gpt-1/images/table1_hu14924670389748374193.png 1024w" loading=lazy class=gallery-image data-flex-grow=556 data-flex-basis=1335px></p><h4 id=model-speciﬁcations>Model speciﬁcations</h4><div class=table-wrapper><table><thead><tr><th style=text-align:center>Hyperparameter</th><th style=text-align:center>Descrption</th></tr></thead><tbody><tr><td style=text-align:center>layer</td><td style=text-align:center>12-layer decoder-only transformer with masked self-attention heads</td></tr><tr><td style=text-align:center>state dimension</td><td style=text-align:center>decoder: 768, attention heads: 12, position-wise FFN: 3072</td></tr><tr><td style=text-align:center>optimizer</td><td style=text-align:center>Adam</td></tr><tr><td style=text-align:center>learning rate</td><td style=text-align:center>max: 2.5e-4, schedule: cosine annealing, warm-up step: 2,000</td></tr><tr><td style=text-align:center>schedule</td><td style=text-align:center>100 epochs</td></tr><tr><td style=text-align:center>batch size</td><td style=text-align:center>64 random sample $\times$ 512 token/sample</td></tr><tr><td style=text-align:center>weight initialization</td><td style=text-align:center>$N(0, 0.02)$</td></tr><tr><td style=text-align:center>subword segmentation</td><td style=text-align:center>BPE (40,000 merges)</td></tr><tr><td style=text-align:center>dropout</td><td style=text-align:center>0.1</td></tr><tr><td style=text-align:center>regularization</td><td style=text-align:center>L2($w=0.01$)</td></tr><tr><td style=text-align:center>activation function</td><td style=text-align:center>Gaussian Error Linear Unit(GELU)</td></tr><tr><td style=text-align:center>position embedding</td><td style=text-align:center>learned positoin embeddings</td></tr><tr><td style=text-align:center>pre-processing</td><td style=text-align:center>cleaning: ftfy, tokenizer : spaCy</td></tr></tbody></table></div><h4 id=fine-tuning-details>Fine-tuning details</h4><p>명시되지 않은 것들은 사전학습에 사용된 hyperparameter를 재사용했다.</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>Hyperparameter</th><th style=text-align:center>Descrption</th></tr></thead><tbody><tr><td style=text-align:center>dropout</td><td style=text-align:center>0.1</td></tr><tr><td style=text-align:center>Learning rate</td><td style=text-align:center>max: 6.25e-5, warm-up: 0.2% of training</td></tr><tr><td style=text-align:center>batch size</td><td style=text-align:center>32</td></tr><tr><td style=text-align:center>epochs</td><td style=text-align:center>3</td></tr><tr><td style=text-align:center>auxiliary objective weight($\lambda$)</td><td style=text-align:center>0.5</td></tr></tbody></table></div><h3 id=supervised-ﬁne-tuning-1>Supervised ﬁne-tuning</h3><p>자연어 추론, 질문 응답, 의미론적 유사성, 텍스트 분류등의 평가를 진행하였고, 그 중 일부는 GLUE benchmark에 포함되어 있다.</p><h4 id=natural-language-inference>Natural Language Inference</h4><p>자연어 추론(NLI) 작업, 즉 텍스트 함의를 인식하는 것은 문장 쌍을 읽고, 그들 사이의 관계를 함의, 모순 또는 중립 중 하나로 판단하는 것으로, 이미지 캡션(SNLI), 텍스트 변환된 연설, 대중 소설, 정부 보고서(MNLI), 위키백과 기사(QNLI), 과학 시험(SciTail) 또는 뉴스 기사(RTE)를 포함한 다양한 출처의 다섯 개의 데이터셋을 사용해서 평가하였다.</p><p><img src=/p/gpt-1/images/table2.png width=1084 height=324 srcset="/p/gpt-1/images/table2_hu6959731196899032972.png 480w, /p/gpt-1/images/table2_hu8800468554965442538.png 1024w" loading=lazy class=gallery-image data-flex-grow=334 data-flex-basis=802px></p><p>다섯 가지 데이터셋 중 네 가지에서 좋은 성능을 보여주었으며, MNLI에서 1.5%, SciTail에서 5%, QNLI에서 5.8%, SNLI에서 0.6%의 성능 향상을 보였다. 이는 GPT 모델이 여러 문장을 더 잘 이해하고, 언어적 모호성의 측면을 처리할 수 있다는 것을 보여준다.</p><h4 id=question-answering-and-commonsense-reasoning-1>Question answering and commonsense reasoning</h4><p>질문 응답 작업은 한 문장이나 여러 문장을 이해하는 능력을 평가한다. 중고등학교 시험의 영어 지문과 질문이 포함된 RACE 데이터셋을 사용한 평가에서 좋은 성능을 보여주었다. 또한, 여러 문장의 이야기 중에서 올바른 결말을 고르는 Story Cloze 평가에서도 GPT 모델은 이전 최고 성능을 크게 능가하였다. 이 결과는 GPT 모델이 넓은 범위에 걸친 문맥 정보를 잘 처리할 수 있음을 보여준다.</p><p><img src=/p/gpt-1/images/table3.png width=1078 height=312 srcset="/p/gpt-1/images/table3_hu15042743739818784966.png 480w, /p/gpt-1/images/table3_hu12462431001630406677.png 1024w" loading=lazy class=gallery-image data-flex-grow=345 data-flex-basis=829px></p><h4 id=semantic-similarity>Semantic Similarity</h4><p>의미론적 유사성(또는 패러프레이즈 감지) 작업은 두 문장이 의미적으로 동일한지 여부를 판단한다. 뉴스 출처에서 수집된 Microsoft Paraphrase(MRPC), Quora Question Pairs(QQP), 그리고 Semantic Textual Similarity benchmark(STS-B) 데이터셋을 사용한다. 이 중 STSB와 QQP에서 좋은 성늘을 보여주었다.</p><p><img src=/p/gpt-1/images/table4.png width=1076 height=382 srcset="/p/gpt-1/images/table4_hu633176886377588970.png 480w, /p/gpt-1/images/table4_hu13367537901244527818.png 1024w" loading=lazy class=gallery-image data-flex-grow=281 data-flex-basis=676px></p><h4 id=classiﬁcation>Classiﬁcation</h4><p>텍스트 분류로 사용한 데이터셋은 문법적으로 맞는지를 판단하는 Corpus of Linguistic Acceptability(CoLA)와 단순 이진분류 평가인 Stanford Sentiment Treebank(SST-2)을 사용하였다. CoLA에서 35.0 에서 45.4점으로, SST-2에서 68.9 에서 72.8점으로 상승하였으며, GLUE benchmark에서도 72.8점으로 이전 최고 성능을 크게 능가하였다.</p><p>GPT모델은 평가한 12개의 데이터셋 중 9개에서 state-of-the-art를 달성하였다. 그리고 STS-B(약 5.7k)와 같은 작은 데이터셋부터 가장 큰 SNLI(약 550k)와 같은 크기의 다양한 데이터셋에서 잘 작동함을 보여준다.</p><hr><h2 id=analysis>Analysis</h2><h3 id=impact-of-number-of-layers-transferred>Impact of number of layers transferred</h3><p>unsupervised pre-training에서 supervised target task로 transfer하는 layer 개수의 영향을 분석했다. MultiNLI와 RACE에서 성능을 관찰했고 transferring embeddings이 성능을 향상시킨다는 것과 각 transformer layer가 최대 9%까지 성능을 향상시킨다는 결과를 얻었다. 이는 pre-trained model의 각 layer가 target task를 푸는 데 유용한 기능을 포함함을 의미한다.</p><p><img src=/p/gpt-1/images/figure2.png width=1070 height=478 srcset="/p/gpt-1/images/figure2_hu3610014852900176666.png 480w, /p/gpt-1/images/figure2_hu7218725431760674404.png 1024w" loading=lazy class=gallery-image data-flex-grow=223 data-flex-basis=537px></p><h3 id=zero-shot-behaviors>Zero-shot Behaviors</h3><p>Trasformer를 사용한 language model이 pre-training에 효과적인 이유에 대한 가설로, Generative model이 학습하는 target tasks가 language modeling의 성능을 향상에 도움을 준다고 생각했고, 이를 검증하기 위해 pre-training 업데이트 횟수에 따른 target tasks의 성능을 fine-tuning없이 측정하였다.</p><p>실험 결과 pre-training 업데이트 횟수에 따라 안정적 & 지속적으로 관련 taget task의 성능이 증가하는 것을 확인할 수 있었으며 이는 generative pre-training이 관련 task의 학습에 도움을 준다는 것을 의미한다. 반면, LSTM의 경우에는 업데이트 횟수에 따라 일관되게 안정적으로 증가하지 않고 분산을 가지면서 증가하는데, 이는 LSTM 보다 더 구조화된 transformer의 attentional memory가 transfer learning에 도움을 준다는 것을 의미한다.</p><h3 id=ablation-studies>Ablation studies</h3><p>세 가지 ablation study를 통해 다음의 결과를 얻었다. 첫째, 미세조정 시 보조 목적함수의 도움이 큰 데이터셋에서는 두드러지지만 작은 데이터셋에서는 그렇지 않다는 것을 확인하였다. 둘째, LSTM과 Transformer를 비교한 결과, LSTM은 오직 MRPC 데이터셋에서만 Transformer를 능가하는 것을 확인하였다. 마지막으로, 사전학습 없이 지도학습을 진행한 Transformer는 모든 작업에서 성능이 저하되었다.</p><p><img src=/p/gpt-1/images/table5.png width=1074 height=208 srcset="/p/gpt-1/images/table5_hu8764794027700437881.png 480w, /p/gpt-1/images/table5_hu8770173064735885593.png 1024w" loading=lazy class=gallery-image data-flex-grow=516 data-flex-basis=1239px></p><hr><h2 id=conclusion>Conclusion</h2><p>생성적 사전 학습과 미세조정을 사용한 모델을 통해 강력한 자연어 이해를 구현하였다. GPT 모델은 연속된 텍스트로 이루어진 다양한 말뭉치로 사전학습된 모델은 일반 지식(world knowledge)과 long term depency 처리하는 능력을 가질 수 있었다. 이를 통해, 우리는 지도학습 없이도 특정 작업의 성능을 향상시키는 것이 가능하다는 것을 보여주었으며, 특히 Trasformer 모델과 long term depency가 있는 텍스트 데이터셋이 이 접근법에서 잘 작동함을 확인하였다.</p><hr><h2 id=reference>Reference</h2><ul><li><a class=link href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf target=_blank rel=noopener>Paper</a></li><li><a class=link href=https://github.com/openai/finetune-transformer-lm target=_blank rel=noopener>Github</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/llm/>LLM</a>
<a href=/tags/milestone/>Milestone</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>관련 글</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/lru/><div class=article-details><h2 class=article-title>LRU</h2></div></a></article><article><a href=/p/jamba/><div class=article-details><h2 class=article-title>Jamba</h2></div></a></article><article><a href=/p/mamba/><div class=article-details><h2 class=article-title>Mamba</h2></div></a></article><article><a href=/p/mistral/><div class=article-details><h2 class=article-title>Mistral</h2></div></a></article><article><a href=/p/llama-2/><div class=article-details><h2 class=article-title>LLaMA 2</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=KurtKim/kurtkim.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 K2H'log</section><section class=powerby><a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>로 만듦<br><a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>의 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.17.0>Stack</a></b> 테마 사용 중</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script></body></html>