<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Audio on K2H'log</title><link>https://kurtkim.github.io/tags/audio/</link><description>Recent content in Audio on K2H'log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Mon, 15 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/tags/audio/index.xml" rel="self" type="application/rss+xml"/><item><title>MuseGAN</title><link>https://kurtkim.github.io/p/musegan/</link><pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/musegan/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>음악 생성과 이미지, 비디오 생성 사이의 주요 차이점은 다음과 같다: 음악은 시간적 예술이므로 시간 모델이 필수적이다. 일반적으로 여러 악기/트랙이 상호작용하면서 발전하며, 음악적 음표들은 다양한 형태로 그룹화되어 시간적 순서는 자연스럽게 정렬되지 않는다. 이 논문에서는 생성적 적대 신경망(GANs)을 이용하여 심볼릭 다중 트랙 음악 생성을 위한 세 가지 모델을 제안하며, 록 음악 데이터셋을 사용하여 훈련하고 평가하였다. 이 모델들은 인간의 입력 없이도 일관된 음악을 생성할 수 있으며, 인간-AI 협력 음악 생성으로도 확장 가능함을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>현실적이고 미적인 작품 생성은 인공지능 분야에서 흥미로운 과제 중 하나이다. 최근 몇 년간 생성적 적대 신경망(GANs)을 통해 이미지, 비디오, 텍스트 생성에서 큰 진전이 있었다. 그러나 상징적 음악 생성은 여전히 어려운 과제로 남아 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure1.png"
width="578"
height="416"
srcset="https://kurtkim.github.io/p/musegan/images/figure1_hu82980f9558dc2121d62cae923ceb145f_84657_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure1_hu82980f9558dc2121d62cae923ceb145f_84657_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>첫째, 음악은 시간의 예술로, 계층적 구조를 지닌다. 사람들은 음악을 들을 때 일관성, 리듬, 긴장감, 감정 흐름에 주목한다. 따라서 시간적 구조를 고려하는 메커니즘이 중요하다.&lt;/p>
&lt;p>둘째, 음악은 여러 악기/트랙으로 구성되며, 이 트랙들은 서로 밀접하게 상호작용하고 시간에 따라 함께 전개된다. 음악 이론에서는 화성이나 대위법 등 소리를 연관시키는 작곡 규율에 대한 논의가 많다.&lt;/p>
&lt;p>마지막으로, 음악의 음표는 화음, 아르페지오, 멜로디로 그룹화된다. 폴리포닉 음악에서는 시간 순서 부여가 자연스럽지 않아, 자연어 생성과 모노포닉 음악 생성의 성공이 폴리포닉 음악에 바로 적용되기 어렵다.&lt;/p>
&lt;p>대부분의 이전 연구에서는 상징적 음악 생성을 단순화하여 문제를 관리하였다. 단순화 방법에는 단일 트랙 모노포닉 음악 생성, 폴리포닉 음악에 시간 순서 도입, 여러 모노포닉 멜로디를 조합해 폴리포닉 음악 생성 등이 포함된다.&lt;/p>
&lt;p>이 연구의 목표는 가능한 한 이러한 단순화를 피하는 것이다. 본질적으로, 다음을 목표로 하고 있습니다: 1) 화성과 리듬 구조가 있는 멀티트랙 폴리포닉 음악 생성, 2) 다트랙 간 의존성, 3) 시간적 구조.&lt;/p>
&lt;p>두 가지 접근 방식을 제안한다: 하나는 인간 입력 없이 음악을 생성하는 것이며, 다른 하나는 인간이 제공한 트랙의 시간 구조를 따르도록 학습한다. 또한, 트랙 간 상호작용을 위해 세 가지 방법을 제안한다: 각 트랙에 대한 개별 생성기 사용, 하나의 생성기로 모든 트랙을 함께 생성, 각 트랙을 개별 생성기로 생성하되 트랙 간 공유 입력을 사용하여 조화롭고 조정된 결과를 얻는다. 음표 대신 막을 작곡 단위로 삼고, 트랜스포즈드 컨볼루션 신경망(CNNs)을 사용해 한 막씩 음악을 생성한다.&lt;/p>
&lt;p>트랙 내 외 및 다양한 모델의 생성 결과를 목표적으로 평가하기 위해 몇 가지 측정 방법을 제안하며, 144명의 청취자를 대상으로 한 주관적 평가 결과도 보고한다.&lt;/p>
&lt;p>&amp;ldquo;multi-track sequential generative adversarial network&amp;quot;을 MuseGAN이라고 부르며, 음악 생성에 초점을 맞추었지만 다른 영역에서도 멀티트랙 시퀀스 생성에 적용될 수 있는 설계라고 설명한다.&lt;/p>
&lt;p>이 논문의 기여는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>멀티트랙 시퀀스 생성을 위한 새로운 GAN 기반 모델을 제안한다.&lt;/li>
&lt;li>이 제안된 모델을 적용하여 상징적 음악을 생성하였으며, 이는 처음으로 멀티트랙 폴리포닉 음악을 생성할 수 있는 모델이다.&lt;/li>
&lt;li>제안된 모델을 트랙 조건부 생성으로 확장하였으며, 이는 인간-인공지능 협력 음악 생성이나 음악 반주에 적용될 수 있다.&lt;/li>
&lt;li>Lakh Midi Dataset (LMD)에서 유도된 173,997개의 고유한 멀티트랙 피아노 롤을 포함하는 Lakh Pianoroll Dataset (LPD)를 제시한다.&lt;/li>
&lt;li>인공 상징적 음악을 평가하기 위해 몇 가지 트랙 내 및 트랙 간 목표적 메트릭을 제안한다.&lt;/li>
&lt;/ul>
&lt;p>모든 코드, 데이터셋 및 렌더링된 오디오 샘플은 프로젝트 웹사이트에서 확인할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="generative-adversarial-networks">Generative Adversarial Networks&lt;/h2>
&lt;p>GANs의 핵심 개념은 생성자와 판별자라는 두 네트워크를 사용하여 적대적 학습을 이루는 것이다. 생성자는 임의의 노이즈 $z$를 데이터 공간으로 매핑하며, 판별자는 진짜 데이터와 생성된 데이터를 구별하는 데 학습된다. 이 과정은 생성자 $G$와 판별자 $D$ 간의 두 플레이어 minimax 게임으로 수학적으로 표현될 수 있다:&lt;/p>
&lt;p>$$ \min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}} [ \log D(x)] + \mathbb{E}_{z \sim p_z}[ \log(1 - D(G(z)))] $$&lt;/p>
&lt;p>여기서 $p_{\text{data}}$는 실제 데이터의 분포를, $p_z$는 $z$의 사전 분포를 나타낸다.&lt;/p>
&lt;p>Jensen-Shannon divergence 대신 Wasserstein 거리 또는 Earth Movers 거리를 사용하는 것이 GANs의 학습을 안정화하고 모드 붕괴를 방지하는 데 도움이 된다고 주장한다. 초기에는 Wasserstein GAN에서 Lipschitz 제약을 강제하기 위해 가중치 클리핑을 사용했으나, 이는 나중에 최적화에서 문제가 되었다. 이 문제를 해결하기 위해 판별자의 목적 함수에 기울기 페널티 항을 추가하는 방법을 제안하였다:&lt;/p>
&lt;p>$$\mathbb{E}_{x \sim \hat{p}_d} [D(x)] - \mathbb{E}_{z \sim p_z} [D(G(z))] + \lambda \mathbb{E}_{x \sim p_x} [( | \nabla_{\hat{x}} D(\hat{x})\ |_2 - 1)^2 ]$$&lt;/p>
&lt;p>여기서 $\hat{p}_x$는 실제 데이터 분포 $p_d$와 모델 분포 $p_g$에서 샘플링된 점 쌍 사이의 직선을 따라 균일하게 샘플링된 정의된 모델 분포를 나타낸다. 이 수정된 WGAN-GP 모델은 더 빠르게 수렴하고 매개변수 조정이 적게 필요하여 생성 모델링 작업에 적합하다.&lt;/p>
&lt;hr>
&lt;h2 id="proposed-model">Proposed Model&lt;/h2>
&lt;p>화음 변화는 일반적으로 막의 경계에서 발생하며, 이는 노래 작곡 시 막이 기본 구성 요소로 사용된다는 것을 의미한다.&lt;/p>
&lt;h3 id="data-representation">Data Representation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure2.png"
width="672"
height="362"
srcset="https://kurtkim.github.io/p/musegan/images/figure2_hu48fe9dd04296f8f02dff7e10a663bdea_43440_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure2_hu48fe9dd04296f8f02dff7e10a663bdea_43440_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>다중 트랙 다성악 음악을 모델링하기 위해 다중 트랙 피아노 롤 표현을 제안한다. 이는 시간 단계별로 음표의 존재를 나타내는 이진 값의 악보 형식 행렬로서, 서로 다른 트랙의 피아노 롤로 구성된다.&lt;/p>
&lt;p>한 막의 M-트랙 피아노 롤은 $x \in \lbrace 0, 1 \rbrace^{R \times S \times M}$으로 표현된다. 여기서 $R$은 막의 시간 단계 수, $S$는 음표 후보의 수, M은 트랙 수를 나타낸다. $T$ 막의 M-트랙 피아노 롤은 $\rightarrow{x} = \lbrace \rightarrow{x}(t) \rbrace_{t=1}^T$로 구성된다. $\rightarrow{x}(t) \in \lbrace 0, 1 \rbrace^{R \times S \times M}$은 막 $t$의 다중 트랙 피아노 롤을 나타낸다.&lt;/p>
&lt;p>각 막의 각 트랙에 대한 피아노 롤은 실제 데이터와 생성된 데이터 모두에 대해 고정 크기의 행렬로 표현된다. 이는 CNN을 사용할 수 있게 만든다.&lt;/p>
&lt;h3 id="modeling-the-multi-track-interdependency">Modeling the Multi-track Interdependency&lt;/h3>
&lt;p>음악 창작에는 두 가지 일반적인 방법이 있다. 첫째, 다양한 악기를 연주하는 음악가들이 미리 정해진 편곡 없이 즉흥적으로 연주하는 &amp;lsquo;잼 세션&amp;rsquo; 방식이다. 둘째, 조화 구조와 편곡에 대한 지식을 가진 작곡가가 악기들을 편성하고 음악가들이 이를 따라 연주하는 방식이다. 이 연구는 이러한 두 가지 접근 방식에 해당하는 세 가지 모델을 설계하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure3.png"
width="528"
height="540"
srcset="https://kurtkim.github.io/p/musegan/images/figure3_hu308268d6b58cddd5073713e0541eb5ab_87293_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure3_hu308268d6b58cddd5073713e0541eb5ab_87293_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="234px"
>&lt;/p>
&lt;p>&lt;strong>Jamming Model&lt;/strong> 여러 생성기가 각각 랜덤 벡터 $z_i$로부터 독립적으로 음악 트랙을 생성한다. 각 생성기는 서로 다른 판별자로부터 비판을 받는다. $M$개의 트랙을 생성하기 위해 $M$개의 생성기와 판별자가 필요하다.&lt;/p>
&lt;p>&lt;strong>Composer Model&lt;/strong> 하나의 생성기가 여러 트랙을 포함한 피아노 롤을 생성하며, 하나의 랜덤 벡터 $z$와 판별자만 사용된다. 판별자는 $M$개의 트랙을 종합적으로 검토해 음악의 진위를 판단한다. 따라서 $M$의 값과 관계없이 생성기와 판별자는 각각 하나만 필요하다.&lt;/p>
&lt;p>&lt;strong>Hybrid Model&lt;/strong> 잼 세션과 작곡의 아이디어를 결합한 하이브리드 모델을 제안한다. 각 생성기는 트랙 간 랜덤 벡터 $z$와 트랙 내 랜덤 벡터 $z_i$를 입력으로 받는다. 트랙 간 랜덤 벡터가 여러 음악가의 생성 과정을 조율하며, 하나의 판별자가 $M$개의 트랙을 평가한다. 따라서 $M$개의 생성기와 하나의 판별자만 필요하다.&lt;/p>
&lt;p>작곡가 모델과 하이브리드 모델의 주요 차이점은 유연성이다. 하이브리드 모델에서는 각 생성기에 대해 다른 네트워크 아키텍처와 입력을 사용할 수 있어, 특정 트랙을 다양하게 생성하면서도 트랙 간 상호 의존성을 유지할 수 있다.&lt;/p>
&lt;h3 id="modeling-the-temporal-structure">Modeling the Temporal Structure&lt;/h3>
&lt;p>기존 모델들은 다중 트랙 음악을 바 단위로 생성할 수 있지만, 각 바 사이에 일관성이 부족할 수 있다. 음악 구절과 같이 몇 개의 바로 이루어진 음악을 생성하기 위해 시간적 모델이 필요하다. 이를 위해 아래에서 두 가지 방법을 설계하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure4.png"
width="664"
height="406"
srcset="https://kurtkim.github.io/p/musegan/images/figure4_hud0d29f1b6e0e2fe88f25bb1539e21a54_73912_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure4_hud0d29f1b6e0e2fe88f25bb1539e21a54_73912_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="392px"
>&lt;/p>
&lt;p>&lt;strong>Generation from Scratch&lt;/strong> 첫 번째 방법은 바 진행을 추가 차원으로 삼아 고정 길이의 음악 구절을 생성하는 것이다. 생성기는 시간 구조 생성기 $G_{temp}$와 바 생성기 $G_{bar}$로 구성되어, 잡음 벡터 $z$를 잠재 벡터 시퀀스 $\vec{z} = \lbrace \vec{z}(t) \rbrace_{t=1}^T$로 매핑한다. 이 시간 정보를 이용해 $G_{bar}$는 순차적으로 피아노 롤을 생성한다.&lt;/p>
&lt;p>$$ G_{\text{piano-roll}}(\vec{z}) = G_{bar}(G_{temp}(z)(t)). $$&lt;/p>
&lt;p>&lt;strong>Track-conditional Generation&lt;/strong> 두 번째 방법은 특정 트랙의 바 시퀀스 $\vec{y}$가 주어지고, 이 트랙의 시간 구조를 학습하여 나머지 트랙을 생성한다. 트랙 조건 생성기 $G^\circ$는 조건부 바 생성기 $G_{bar}^\circ$를 사용해 각 바를 순차적으로 생성하며, 바 $t$의 나머지 트랙의 피아노 롤은 시간에 따라 달라지는 랜덤 잡음 $\vec{z}(t)$과 조건 $\vec{y}(t)$를 입력으로 받아 생성된다.&lt;/p>
&lt;p>고차원 조건으로 조건부 생성을 위해 추가적으로 인코더 $E$가 학습되어 각 시간 단계의 $\vec{y}(t)$를 해당하는 $\vec{z}(t)$ 공간으로 매핑한다.&lt;/p>
&lt;p>$$ G^\circ(\vec{z}, \vec{y}) = { G_{bar}^\circ(\vec{z}(t), E[\vec{y}(t)]) }_{t=1}^T. $$&lt;/p>
&lt;p>인코더는 주어진 트랙에서 내부 트랙 특징이 아닌 트랙 간 특징을 추출할 것으로 기대되며, 내부 트랙 특징은 다른 트랙을 생성하는 데 유용하지 않을 것으로 예상된다.&lt;/p>
&lt;hr>
&lt;h2 id="musegan">MuseGAN&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure5.png"
width="1366"
height="412"
srcset="https://kurtkim.github.io/p/musegan/images/figure5_hu005a74abacf5918238de5be00840d5a9_176262_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure5_hu005a74abacf5918238de5be00840d5a9_176262_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="331"
data-flex-basis="795px"
>&lt;/p>
&lt;p>다중 트랙 및 시간 모델을 통합한 MuseGAN의 입력 $\bar{z}$는 네 부분으로 구성된다: 트랙 간 시간 독립 랜덤 벡터 $z$, 트랙 내 시간 독립 랜덤 벡터 $z_i$, 트랙 간 시간 종속 랜덤 벡터 $z_t$, 트랙 내 시간 종속 랜덤 벡터 $z_i$, $t$.&lt;/p>
&lt;p>트랙 $i (i = 1, 2, &amp;hellip; , M)$에서, $G_{temp}$와 $G_{temp}$, $i$는 시간 종속 랜덤 벡터 $z_t$와 $z_i$, $t$를 입력으로 받아 잠재 벡터를 출력한다. 이 벡터들과 시간 독립 랜덤 벡터 $z$ 및 $z_i$를 $G_{bar}$에 입력하여 피아노 롤을 생성한다. 생성 절차는 다음과 같다:&lt;/p>
&lt;p>$$ G(\bar{z}) = { G_{bar, i} (z, G_{temp} (z_t)^{(t)}, z_i, G_{temp},i (z_i,t)^{(t)}) }_{i,t = 1}^{M, T} $$&lt;/p>
&lt;p>트랙 조건부 시나리오에서는 사용자 제공 트랙에서 유용한 트랙 간 특징을 추출하는 추가 인코더 $E$가 있다. 이 과정은 유사하게 진행될 수 있으므로, 공간 제약으로 인해 세부 사항은 생략한다.&lt;/p>
&lt;hr>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>이 연구에서 사용된 피아노 롤 데이터셋은 Lakh MIDI 데이터셋(LMD)에서 파생된 176,581개의 고유한 MIDI 파일로 구성된다. 이를 다중 트랙 피아노 롤로 변환하고, 각 마디의 높이를 128, 시간 해상도를 96으로 설정한다. pretty midi 라이브러리를 사용해 MIDI 파일을 처리하여 Lakh Pianoroll Dataset(LPD)을 생성하였다. 또한, Million Song Dataset(MSD)과 일치하는 45,129개의 MIDI로 구성된 LPD-matched 하위 집합도 있다.&lt;/p>
&lt;h3 id="data-preprocessing">Data Preprocessing&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure6.png"
width="1394"
height="300"
srcset="https://kurtkim.github.io/p/musegan/images/figure6_hufae3c956bceabb409f6439f36d872b03_170287_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure6_hufae3c956bceabb409f6439f36d872b03_170287_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="464"
data-flex-basis="1115px"
>&lt;/p>
&lt;p>이 MIDI 파일들은 웹에서 수집된 사용자 생성 파일들로 노이즈가 많다. 따라서, LPD-matched를 사용하고 추가 정제를 위해 세 가지 단계를 수행한다.&lt;/p>
&lt;p>일부 트랙은 적은 노트만 연주하여 데이터가 희소해지고 학습을 방해할 수 있다. 이를 해결하기 위해 유사한 악기의 트랙을 병합하여 다섯 개 트랙(베이스, 드럼, 기타, 피아노, 스트링)으로 압축한다. 이 과정에서 노이즈가 추가될 수 있지만, 비어 있는 부분을 채우는 것이 경험적으로 더 나은 결과를 보여주었다. 이 단계를 거쳐 LPD-5-matched가 생성되며, 다중 트랙 피아노 롤 30,887개를 포함한다.&lt;/p>
&lt;p>Raffel and Ellis (2016)에 따르면, 트랙이 멜로디인지 반주인지 명확히 구분할 수 없어 Chu, Urtasun, and Fidler (2017)나 Yang, Chou, and Yang (2017)과 같이 트랙을 멜로디, 리듬, 드럼으로 분류할 수 없다.&lt;/p>
&lt;p>둘째, 우리는 LMD와 MSD에서 제공하는 메타데이터를 활용하고, 일치하는 데 있어 더 높은 신뢰도 점수를 가진, 록 장르이면서 4/4 박자인 피아노 롤만 선택한다. 이 단계를 거쳐 LPD-5-cleansed가 생성된다.&lt;/p>
&lt;p>마지막으로, 구조적 특징 알고리즘(Serrà et al. 2012)을 사용하여 피아노 롤을 세분화하여 음악적으로 의미 있는 구절을 획득한다. 구절을 네 마디로 고려하며, 총 50,266개의 구절을 적절한 크기로 적대시하여 학습 데이터로 사용한다. 특히, 고정 길이 세그먼트만 생성하는 것으로 보이지만, 트랙 조건부 모델은 입력에 따라 어떤 길이의 음악도 생성할 수 있다.&lt;/p>
&lt;p>매우 낮거나 매우 높은 음표는 드물기 때문에, C1 아래 또는 C8 위의 음표를 제거한다. 따라서 목표 출력 텐서의 크기는 4 (bar) × 96 (time step) × 84 (note) × 5 (track) 이다.&lt;/p>
&lt;h3 id="model-settings">Model Settings&lt;/h3>
&lt;p>$G$와 $D$는 깊은 CNN으로 구현되었다. $G$는 시간 축을 먼저 확장하고, 그 다음 음 높이 축을 확장한다. $D$는 반대로 시간 축을 압축한다. Gulrajani et al. (2017)의 제안에 따라, $D$의 5번의 업데이트마다 $G$를 한 번 업데이트하며, 배치 정규화는 $G$에만 적용된다. 각 생성기의 입력 랜덤 벡터는 길이 128로 고정되며, 모델의 훈련 시간은 Tesla K40m GPU를 사용하여 24시간 미만이다. 테스트 단계에서는 $G$의 출력을 tanh 활성화 함수를 이용하여 이진화하며, 임계값은 0으로 설정한다.&lt;/p>
&lt;hr>
&lt;h2 id="objective-metrics-for-evaluation">Objective Metrics for Evaluation&lt;/h2>
&lt;p>모델을 평가하기 위해 실제 데이터와 생성된 데이터 모두에 적용할 수 있는 여러 메트릭을 설계하였다. 이에는 네 가지의 트랙 내 메트릭과 한 가지의 트랙 간 메트릭(마지막 메트릭)이 포함된다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>EB&lt;/strong>: 빈 마디 비율 (%).&lt;/li>
&lt;li>&lt;strong>UPC&lt;/strong>: 매 마디 당 사용된 음계 클래스 수 (0에서 12까지).&lt;/li>
&lt;li>&lt;strong>QN&lt;/strong>: &amp;ldquo;유효&amp;rdquo; 음표 비율 (%). 32분음표와 같이 최소 세 개의 타임 스텝 이상 지속되는 음표를 유효한 음표로 고려한다. QN은 음악이 지나치게 조각난지를 나타낸다.&lt;/li>
&lt;li>&lt;strong>DP&lt;/strong> , or drum pattern: 8비트 또는 16비트 패턴에서의 음표 비율(%), 이는 4/4 시간의 록 곡에서 일반적으로 사용된다.&lt;/li>
&lt;li>&lt;strong>TD&lt;/strong> or tonal distance: 두 트랙 사이의 조화적 거리를 측정한ㄴ다. 큰 TD는 트랙 간 조화 관계가 약한 것을 의미한다.&lt;/li>
&lt;/ul>
&lt;p>실제 데이터와 가짜 데이터의 값 비교를 통해 생성기 성능을 평가할 수 있다. 이는 GAN에서의 개념과 유사하며, 학습 과정이 진행됨에 따라 데이터 분포가 접근해야 할 이상적인 상태로 수렴하는 것을 목표로 한다.&lt;/p>
&lt;h3 id="analysis-of-training-data">Analysis of Training Data&lt;/h3>
&lt;p>학습 데이터에 적용한 이 메트릭은 다음과 같은 결과를 보여준다: EB 값은 다섯 가지 트랙 패밀리 분류가 적절함을 시사한다. UPC에서는 베이스가 멜로디를 연주하는 경향이 있어 UPC가 2.0 이하이고, 기타, 피아노 및 스트링은 주로 코드를 연주하여 UPC가 3.0 이상이다. QN의 높은 값은 피아노 롤이 지나치게 조각난 것이 아님을 나타낸다. DP에서는 드럼 음표의 88%가 8비트 또는 16비트 패턴에 속한다는 것을 보여준다. TD는 멜로디와 코드와 같은 트랙 간의 거리를 측정할 때 약 1.50 정도이며, 두 코드와 같은 트랙 간의 경우 약 1.00이다.&lt;/p>
&lt;hr>
&lt;h2 id="experiment-and-results">Experiment and Results&lt;/h2>
&lt;h3 id="example-results">Example Results&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure7.png"
width="654"
height="328"
srcset="https://kurtkim.github.io/p/musegan/images/figure7_hu81ca43af41d85b4ce7b14fa8ac369693_129446_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure7_hu81ca43af41d85b4ce7b14fa8ac369693_129446_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="478px"
>&lt;/p>
&lt;p>일부 관찰 결과는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>대부분의 트랙들은 동일한 음악 음계에서 연주된다.&lt;/li>
&lt;li>일부 샘플에서 코드 같은 간격들이 관찰된다.&lt;/li>
&lt;li>베이스는 대부분 가장 낮은 음을 연주하며 대부분 단음성이다. (즉, 멜로디를 연주한다)&lt;/li>
&lt;li>드럼은 일반적으로 8비트 또는 16비트 리듬 패턴을 가지고 있다.&lt;/li>
&lt;li>기타, 피아노 및 스트링은 주로 코드를 연주하며, 음높이가 때때로 겹쳐지기도 한다 (검은 선을 생성), 이는 좋은 조화 관계를 나타낸다.&lt;/li>
&lt;/ul>
&lt;h3 id="objective-evaluation">Objective Evaluation&lt;/h3>
&lt;p>각 모델에서 20,000개의 마디를 생성하고 제안된 목표 메트릭을 기준으로 평가한다. 조건부 생성 시나리오에서는 피아노 트랙을 조건으로 하고 나머지 네 트랙을 생성한다. 배치 정규화 레이어가 없는 감소된 작곡가 모델의 결과도 비교 대상으로 포함된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/table1.png"
width="1340"
height="302"
srcset="https://kurtkim.github.io/p/musegan/images/table1_hu60943b7076333c78422bd63f568b72a0_118124_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/table1_hu60943b7076333c78422bd63f568b72a0_118124_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="443"
data-flex-basis="1064px"
>&lt;/p>
&lt;p>인트라 트랙 메트릭에서는 잼밍 모델이 일반적으로 우수한 성능을 보인다. 모든 모델이 DP에서 잘 수행되지만, 작곡가 모델과 혼합 모델에서는 드럼의 EB가 높아 노이즈가 발생할 수 있음을 시사한다. UPC와 QN에서는 모든 모델이 학습 데이터보다 더 많은 음높이 클래스를 사용하고, 상대적으로 적은 &amp;ldquo;유효한&amp;rdquo; 음을 생성하는 경향을 보인다. 이는 이진값 피아노 롤로의 변환 과정에서 발생할 수 있는 노이즈로 해석될 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/table2.png"
width="652"
height="308"
srcset="https://kurtkim.github.io/p/musegan/images/table2_huaed3e5d5dd94581b188ce0cc61bcc41c_65543_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/table2_huaed3e5d5dd94581b188ce0cc61bcc41c_65543_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>TD 메트릭에서, 작곡가 모델과 혼합 모델이 잼밍 모델보다 TD 값이 낮다. 이는 작곡가 모델과 혼합 모델이 교차 트랙 하모닉 관계에서 더 좋은 성능을 보일 수 있음을 시사한다. 이 모델들은 다양한 트랙 조합에서 유사한 결과를 보이며, 혼합 모델은 유연성과 성능을 동시에 잘 유지하고 있음을 보여준다.&lt;/p>
&lt;h3 id="training-process">Training Process&lt;/h3>
&lt;p>훈련 과정에서 작곡가 모델의 손실을 살펴보면, 초기에는 빠르게 감소하다가 포화되는 경향을 보이며, 그 이후에는 약간의 상승 추세가 나타낸다. 이는 G가 일정 시점 이후로 무엇인가를 배우기 시작한다는 신호를 준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure8.png"
width="1392"
height="356"
srcset="https://kurtkim.github.io/p/musegan/images/figure8_hu9bc5a2a0ddabdc03878fd263d66a63d0_206149_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure8_hu9bc5a2a0ddabdc03878fd263d66a63d0_206149_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="391"
data-flex-basis="938px"
>&lt;/p>
&lt;p>학습 과정이 진행됨에 따라 각 트랙의 음 높이 범위를 빨리 파악하고, 포인트 B에서는 적절한 음 높이 범위 내에서 조각난 형태의 음을 생성하기 시작하는 것을 관찰할 수 있다. 베이스의 낮은 부분에서 점들이 모이기 시작하는 것을 확인할 수 있고, 포인트 C 이후에는 기타, 피아노 및 스트링이 긴 음표를 생성하도록 학습하는 과정을 볼 수 있다. 이 결과들은 G가 학습 과정 중에 실제로 개선되고 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/figure9.png"
width="684"
height="574"
srcset="https://kurtkim.github.io/p/musegan/images/figure9_hu27b07703150117cde075d836f0c90e50_114572_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/figure9_hu27b07703150117cde075d836f0c90e50_114572_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="285px"
>&lt;/p>
&lt;p>(b)에서는 G가 적절한 음 높이 클래스 수를 최종적으로 학습하는 것을 확인할 수 있고, (c)에서는 QN이 훈련 데이터보다 낮은 수준을 유지하고 있어 G를 더 개선할 여지가 있다는 것을 시사한다. 이는 연구자가 주관적 테스트를 시작하기 전에 생성된 결과를 평가하는 데 이러한 메트릭을 활용할 수 있다는 점을 보여준다.&lt;/p>
&lt;h3 id="user-study">User Study&lt;/h3>
&lt;p>마지막으로, 소셜 네트워크를 통해 인터넷에서 모집한 144명의 참가자를 대상으로 음악 청취 테스트를 진행하였다. 이 중 44명은 음악적 배경을 탐구하는 간단한 설문을 통해 &amp;lsquo;전문 사용자&amp;rsquo;로 분류되었다. 각 참가자는 무작위로 섞인 아홉 개의 음악 클립을 듣게 되며, 각 클립은 제안된 모델 중 하나로 생성된 세 개의 네 마디 구절로 구성되어 있다. 참가자는 5점 Likert 척도를 사용하여 각 클립을 쾌적한 조화 여부, 통일된 리듬 여부, 명확한 음악적 구조 여부, 일관성 여부, 전반적인 평가에 대해 평가한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musegan/images/table3.png"
width="634"
height="380"
srcset="https://kurtkim.github.io/p/musegan/images/table3_hub107c384eb68c94032e058ff924e40dd_86470_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musegan/images/table3_hub107c384eb68c94032e058ff924e40dd_86470_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="400px"
>&lt;/p>
&lt;p>하이브리드 모델은 전문가와 비전문가 모두가 처음부터 생성하는 경우와 전문가가 조건부 생성에서 선호하는 반면, 잼밍 모델은 비전문가가 조건부 생성에서 선호된다. 또한, 작곡가 모델과 혼합 모델은 처음부터 생성하는 경우 Harmonious 기준에서 잼밍 모델보다 높은 점수를 받았다. 이는 작곡가 모델과 혼합 모델이 트랙 간 상호 의존성을 처리하는 데 더 뛰어나다는 것을 시사한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="video-generation-using-gans">Video Generation using GANs&lt;/h3>
&lt;p>비디오 생성을 위해서는 시계열 모델이 필요하다. 이 연구의 모델 디자인은 GAN을 사용한 비디오 생성과 관련된 선행 연구들을 참고하여 개발되었다. VGAN은 비디오를 동적 전경과 정적 배경으로 분해하며, 3D와 2D CNN을 사용하여 이들을 생성하고 마스크를 통해 결합하였다. TGAN은 시간 생성기를 사용하여 고정 길이의 잠재 변수 시퀀스를 생성하고, 이를 이미지 생성기에 전달하여 비디오 프레임을 단계적으로 생성하였다. MoCoGAN은 비디오를 객체의 콘텐츠와 움직임으로 분해하고, 객체의 움직임을 캡처하기 위해 RNN을 사용하였다.&lt;/p>
&lt;h3 id="symbolic-music-generation">Symbolic Music Generation&lt;/h3>
&lt;p>최근 연구에서는 RNN을 사용하여 다양한 형식의 음악을 생성하는 여러 심볼릭 음악 생성 모델들이 제안되었다. 예를 들어, Sturm et al. (2016)은 단음성 멜로디를 생성하고, Hadjeres, Pachet, and Nielsen(2017)은 4음성 합창곡을 생성하는 데 이 기술을 사용하였다. RNN-RBM(Boulanger-Lewandowski, Bengio, and Vincent 2012)은 단일 트랙의 다중 음악롤을 생성하는 데 성공했고, Chu, Urtasun, and Fidler(2017)은 리드 시트와 단음성 드럼 트랙을 생성하기 위해 계층적 RNN을 활용하였다.&lt;/p>
&lt;p>최근 연구에서는 GAN을 이용한 음악 생성 방법들이 다양하게 탐구되고 있다. C-RNN-GAN은 음표 이벤트 시리즈를 통해 다중 음악을 생성하며, SeqGAN은 강화 학습을 활용하여 이산 토큰 시퀀스를 생성한다. MidiNet은 조건부 합성곱 GAN을 이용하여 코드 시퀀스를 따르는 멜로디를 생성하는 방법을 제안하였다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>이 연구에서는 GANs 프레임워크를 활용하여 멀티트랙 시퀀스 생성을 위한 새로운 모델을 제안하였고, 이를 딥 CNN을 이용하여 멀티트랙 피아노 롤 생성에 구현하였다. 설계한 목표 메트릭과 주관적 사용자 연구 결과는 모델이 음악에 대해 어느 정도 학습할 수 있음을 보여준다. 아직은 인간 음악가 수준에는 미치지 못하지만, 제안된 모델은 몇 가지 우수한 특성을 가지고 있으며, 추가 연구를 통해 더 발전될 수 있기를 기대한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1709.06298" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/salu133445/musegan" target="_blank" rel="noopener"
>GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://hermandong.com/musegan/" target="_blank" rel="noopener"
>Demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>VampNet</title><link>https://kurtkim.github.io/p/vampnet/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/vampnet/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>VampNet은 음악 합성, 압축, 인페인팅, 변주를 위한 masked acoustic token 모델링 기법이다. 다양한 마스킹 방법을 사용하여 일관된 고해상도 음악을 생성하며, bidirectional transformer 아키텍처를 통해 non-autoregressive 방식으로 작동한다. VampNet은 음악의 스타일, 장르, 악기 사용 등을 유지하면서 음악 생성, 압축, 인페인팅 등 다양한 작업에 적용 가능하다. 이는 VampNet을 음악 공동 창작의 강력한 도구로 만든다. 관련 코드와 오디오 샘플은 온라인에서 제공된다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">INTRODUCTION&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/vampnet/images/figure1.png"
width="678"
height="398"
srcset="https://kurtkim.github.io/p/vampnet/images/figure1_hu00210469f0340c734219db706d0a4f97_118011_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/vampnet/images/figure1_hu00210469f0340c734219db706d0a4f97_118011_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="170"
data-flex-basis="408px"
>&lt;/p>
&lt;p>discrete acoustic token 모델링의 최근 발전은 음성 및 음악 생성에 중요한 진전을 가져왔다. 또한, non-autoregressive parallel iterative decoding 방법이 이미지 합성을 위해 개발되어, 과거와 미래 정보를 모두 고려하는 작업에 더 효율적이고 빠른 추론을 제공한다.&lt;/p>
&lt;p>이 작업에서는 parallel iterative decoding과 acoustic token 모델링을 결합하여 음악 오디오 합성에 적용한 VampNet을 소개한다. 이는 parallel iterative decoding을 신경망 기반 음악 생성에 처음으로 확장한 사례이다. VampNet은 선택적으로 마스킹된 음악 토큰 시퀀스를 사용해 공백을 채우도록 유도할 수 있으며, 고품질 오디오 압축부터 원본 음악의 스타일과 장르를 유지하면서도 음색과 리듬이 변형된 변주까지 다양한 출력을 제공한다.&lt;/p>
&lt;p>auto-regressive 음악 모델은 접두사 기반 음악 생성만 가능하지만, 이 연구의 접근 방식은 프롬프트를 어디에나 둘 수 있다. 주기적, 압축, 비트 기반 마스킹 등 다양한 프롬프트 디자인을 탐구하였다. 모델이 루프와 변주 생성에 잘 반응하여 VampNet이라 명명하였다. 코드를 오픈 소스로 공개하며 오디오 샘플 청취를 권장한다.&lt;/p>
&lt;hr>
&lt;h2 id="background">BACKGROUND&lt;/h2>
&lt;p>생성 모델링의 두 단계 접근 방식은 이미지와 오디오 합성에서 계산 효율성 덕분에 주목받고 있다. 첫 번째 단계에서는 인코더를 통해 입력을 토큰으로 변환하고, 디코더를 통해 다시 변환한다. 두 번째 단계에서는 조건을 제공받아 토큰을 생성하는 모델을 학습한다.&lt;/p>
&lt;h3 id="stage-1-tokenization">Stage 1: Tokenization&lt;/h3>
&lt;p>이미지에서 시각적 토큰화는 분류와 합성에 사용되며, 주로 latent space에서 벡터 양자화를 활용한다. 오디오에서도 유사한 접근 방식이 탐구되었지만, 낮은 샘플링 속도나 음성 오디오에 제한되었다. latent space의 샘플링 속도가 낮을수록 생성이 쉬워지며, 최근에는 높은 압축률과 고샘플링 속도 오디오의 우수한 재구성 품질을 제공하는 잔여 벡터 양자화 기반 방법이 제안되고있다.&lt;/p>
&lt;p>Descript Audio Codec (DAC)을 사용하면 오디오를 효율적으로 토큰화할 수 있다. DAC는 fully convolutional 인코더를 통해 오디오를 토큰 시퀀스로 변환하고, hierarchical sequence of vector-quantizer를 사용해 압축한다. 이 방법은 residual vector 양자화를 통해 뛰어난 오디오 재구성 품질과 높은 압축 비율을 제공한다. DAC와 같은 기술은 다양한 오디오 언어 모델 구현에 중요한 역할을 한다.&lt;/p>
&lt;h3 id="stage-2-generation">Stage 2: Generation&lt;/h3>
&lt;p>오디오를 토큰으로 인코딩한 후 일반적인 접근 방식은 autoregressive 모델을 사용해 시퀀스에서 각 음향 토큰을 생성하는 것이다. 최신 오디오 생성 방법은 transformer 기반의 decoder-only 모델을 활용하여 이 과정을 수행한다. 하지만 autoregressive 샘플링은 추론 속도가 느리고 각 생성 토큰이 이전 토큰에만 의존하기 때문에 하향식 응용 프로그램에 제한이 있다.&lt;/p>
&lt;p>언어 처리에서는 마스크 모델링이 사전 학습 절차로 널리 사용된다. 이 접근 방식은 이미지와 오디오에도 확장되어 표현 학습에 적용된다. 토큰 마스킹 확률을 변화시키면 마스크 생성 모델링을 확장할 수 있으며, 이는 이미지와 언어 생성에 모두 적용된다.&lt;/p>
&lt;p>MaskGIT의 효율성은 parallel iterative decoding 절차에 있다. 모델이 한 번에 모든 토큰을 예측하지만 초기 출력의 품질이 낮을 수 있다. 이후 출력은 더 낮은 마스킹 확률로 다시 마스킹되어 모델에 입력되고, 이 과정을 통해 출력 품질이 개선된다.&lt;/p>
&lt;p>무조건적 생성 작업에서는 모델이 지침 없이 목표 데이터 분포에서 샘플을 생성해야 하므로, 고차원적 분포 때문에 어려움이 따른다. 또한 이러한 모델은 모드 붕괴, 흐릿한 샘플 등의 문제에 취약하다. 따라서 조건을 제공하면 모델이 다중 모드 문제를 해결하고 원하는 콘텐츠를 생성하는 데 도움이 된다.&lt;/p>
&lt;p>조건부 생성은 클래스 레이블, 장르 태그, 가사 또는 텍스트 설명의 형태로 이루어질 수 있다. 또한, AudioLM의 의미적 토큰이나 텍스트-음성 변환을 위한 정렬된 텍스트 또는 음소처럼 매 시간 단계마다 적용될 수 있다.&lt;/p>
&lt;p>이 연구에서는 MaskGIT와 Paella와 같은 비전 분야의 연구에서 영감을 받아 parallel iterative decoding 절차를 활용한 마스크 생성 모델링 접근 방식을 채택하였다. 인코딩된 오디오에서는 마스크되지 않은 토큰 이외의 추가 조건을 적용하지 않았으며, 추론 과정에서 다양한 마스킹 접근 방식이 유용하고 예술적인 생성을 유도하는 데 활용될 수 있음을 설명한다.&lt;/p>
&lt;p>학습 과정에서는 전체 시퀀스에서 무작위로 토큰들이 마스킹된다. 모델은 한 번의 전방향 패스에서 각 마스킹된 토큰의 값을 예측하며, 이는 과거와 미래의 모든 마스킹되지 않은 토큰에 의존한다. 학습 중에는 마스킹되는 토큰의 수를 다양하게 조절하여 추론 시 샘플링 절차를 통해 오디오를 생성할 수 있다.&lt;/p>
&lt;h2 id="method">METHOD&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/vampnet/images/figure2.png"
width="1364"
height="544"
srcset="https://kurtkim.github.io/p/vampnet/images/figure2_hue38e7d6dc17ffc3eccb0d2f034b1e645_322753_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/vampnet/images/figure2_hue38e7d6dc17ffc3eccb0d2f034b1e645_322753_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="601px"
>&lt;/p>
&lt;p>MaskGIT의 Masked Visual Token Modeling 절차를 오디오에 맞게 적용하여, 시각과 오디오 도메인의 차이를 고려한 Masked Acoustic Token Modeling 접근 방식을 개발하였다.&lt;/p>
&lt;h3 id="masked-acoustic-token-modeling">Masked Acoustic Token Modeling&lt;/h3>
&lt;p>DAC의 기술을 바탕으로 오디오 토크나이저를 학습한다. MaskGIT의 비주얼 토큰과 달리, 음향 토큰은 residual vector 양자화를 통해 계층적 구조를 가진다. 오디오 신호 $x$는 각 시간 단계 $t$에서 $D$차원의 latent vector $Z$로 인코딩되고, $N$개의 벡터 양자화기를 통해 양자화된다. 첫 번째 양자화기는 $Z$의 근사치 $\hat{Z}_1$과 잔여 오류 $R_1 = Z − \hat{Z}_1$을 생성한다. 이후 각 양자화기는 이전 잔여 오류를 처리하여 $R_i ≈ \hat{Z}&lt;em>i + 1$을 생성한다. 마지막으로 벡터 $Z$는 $N$개의 양자화기 출력을 합산하여 재구성된다: $Z = ∑&lt;/em>{i=1}^N \hat{Z}_i$.&lt;/p>
&lt;p>인코딩된 신호는 각 시간 단계에서 $N$개의 이산 토큰으로 표현된다. 모든 토큰을 한 번에 생성하는 대신, AudioLM처럼 $N$개의 토큰을 $N_c$개의 &amp;ldquo;coarse&amp;rdquo; 토큰과 $N_f$개의 &amp;ldquo;fine&amp;rdquo; 토큰으로 나눈다. 그런 다음, coarse 토큰을 기반으로 fine 토큰을 생성하는 모델과 coarse 토큰 시퀀스를 생성하는 모델을 학습한다. 샘플을 생성할 때는 먼저 coarse 모델로 coarse 토큰 시퀀스를 생성한 후, coarse-to-fine 모델로 fine 토큰을 생성한다. 마지막으로, 오디오 토크나이저의 디코더를 사용하여 토큰을 44.1kHz 파형으로 디코딩한다.&lt;/p>
&lt;h3 id="training-procedure">Training procedure&lt;/h3>
&lt;p>$Y ∈ R^T × N$이 오디오 구간에 대한 인코더 출력을 나타내는 행렬일때, $Y$의 각 요소 $y_t$,$n$은 시간 단계 $t$에서 $n$번째 레벨 코드북의 토큰이다. $Y_M$은 마스킹된 토큰들의 집합이고, $Y_U$는 마스킹되지 않은 토큰들의 집합이다. 모델은 $Y_U$와 parameter $θ$를 바탕으로 $Y_M$의 각 토큰에 대한 코드북 값들의 확률 분포를 생성한다. 학습 목표는 실제 토큰의 확률을 최대화하는 것이며, 이는 음의 로그 우도를 최소화하는 것과 같다.&lt;/p>
&lt;p>$$ [ L = - \sum_{y \in Y_M} \log p(y \mid Y_U, \theta) ] $$&lt;/p>
&lt;p>마스킹된 토큰을 예측하기 위해 multi-layer bidirectional transformer를 사용한다. 각 양자화기가 $C$개의 값을 가지는 코드북을 가지고 있고, $N$개의 양자화기가 있다면, 네트워크의 마지막 층은 $(E, CN)$ 형태의 fully connected layer가 된다. 이 출력을 $(EN, C)$로 재구조화한 후, 실제 원핫(one-hot) 토큰과 예측된 토큰 간의 교차 엔트로피 손실을 계산한다. bidirectional transformer는 입력 시퀀스의 모든 토큰을 참고하여 각 토큰의 손실을 최적화할 수 있다.&lt;/p>
&lt;p>coarse-to-fine 생성 모델에서 입력 시퀀스는 항상 $N_c$개의 coarse 토큰을 포함하고, 마스킹은 $N_f$개의 fine 토큰에만 적용된다. 마지막 층은 마스킹된 fine 토큰만 예측하며, 학습 절차는 두 모델 모두 동일하다.&lt;/p>
&lt;h3 id="sampling">Sampling&lt;/h3>
&lt;p>MaskGIT에서 사용된 것과 같은 반복적인 신뢰 기반 샘플링 접근 방식을 따른다. 마스크된 토큰의 집합을 $Y_M$, 마스크되지 않은 토큰의 집합을 $Y_U$라고 할 때, 다음을 수행한다:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Estimate.&lt;/strong> 마스크된 각 토큰 $y$에 대해, 코드북 값 $V$의 어휘에 대한 조건부 확률 분포를 추정한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sample.&lt;/strong> 각 마스크된 토큰에 대해, 분포로부터 샘플링하여 관련된 토큰 추정값 $\hat{y} ∈ V$를 생성한다. 이 단계에서는 샘플링 트릭을 사용하지 않으며, 각 토큰에 대해 범주형 확률 분포에서 그대로 샘플링한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Rank by Confidence.&lt;/strong> 샘플링된 각 토큰의 예측 로그 확률에 온도 조절된 Gumbel 노이즈를 더하여 신뢰도를 계산한다:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>$$ \text{confidence}(\hat{y}) = \log(p(\hat{y})) + \text{temp} \cdot g_t $$&lt;/p>
&lt;p>여기서 $\hat{y}$는 시간 $t$에서의 토큰 추정값이고, $g_t$는 Gumbel(0,1)에서 독립적으로 추출된 샘플이며, temp는 샘플링 반복 횟수에 따라 선형적으로 0으로 감소하는 hyperparameter이다. 그런 다음, 계산된 신뢰도를 기준으로 샘플링된 토큰 추정값 집합을 정렬한다. 높은 온도 값(e.g., &amp;gt; 6.0)은 더 높은 품질의 샘플을 생성하는 것을 발견하였다.&lt;/p>
&lt;ol start="4">
&lt;li>
&lt;p>&lt;strong>Select.&lt;/strong> 다음 샘플링 반복에서 마스크할 토큰 수 $k$를 마스킹 스케줄에 따라 선택한다. 신뢰도가 가장 낮은 $k$개의 추정값을 선택하여 제거하고, 해당 토큰을 다시 마스킹한다. 나머지 높은 신뢰도의 토큰 추정값은 $Y_U$에 넣고, 해당 토큰을 $Y_M$에서 제거한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Repeat.&lt;/strong> 반복 횟수가 도달할 때까지 1단계로 돌아간다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="prompting">Prompting&lt;/h3>
&lt;p>인터랙티브 음악 편집은 마스크되지 않은 토큰의 조건부 프롬프트를 통해 인간의 지도를 포함하여 가능하다. 이 연구의 접근 방식은 입력 오디오 자체에만 의존하여, 다양한 프롬프트가 일관성 있는 샘플을 얻는 데 유용하다. AudioLM과 달리, 우리는 프리픽스 오디오뿐만 아니라 접미사 오디오도 모델에 제공할 수 있으며, 이를 통해 지정된 프리픽스와 접미사에 맞는 오디오를 생성할 수 있다.&lt;/p>
&lt;p>모든 $P$번째 시점을 제외한 나머지 시점들을 마스킹하는 &amp;ldquo;주기적&amp;rdquo; 프롬프트를 적용할 수 있습니다. $P$ 값이 낮을수록 생성된 오디오가 원본처럼 들리며, $P$가 증가하면 모델은 원본 스타일과 일치하는 변형을 생성한다. 예를 들어, $P = 2$인 경우 모델은 업샘플러처럼 작동한다.&lt;/p>
&lt;p>&amp;ldquo;compression&amp;rdquo; 프롬프트는 가장 거친 단위를 제외한 모든 코드북을 마스킹하여, 모델이 원본과 유사한 오디오를 생성하도록 한다. 이를 낮은 $P$ 값을 가진 주기적 프롬프트와 결합하면 더 높은 압축 비율을 얻을 수 있다. 코덱의 비트레이트 $B$, 코드북 수 $N$, 다운샘플링 비율 $P$, 유지된 코드북 수 $N_k$를 고려할 때, 비트레이트는 $B/P(N − N_k)$로 달성된다.&lt;/p>
&lt;p>마지막으로, 음악의 구조를 고려한 비트 주도적 프롬프팅을 통해 모델은 비트에 해당하는 시간대를 중심으로 음악을 생성한다. 이는 원본 음악의 흥미로운 변형을 만들어내며, 이러한 프롬프트들을 결합하면 매우 유용한 음악 창작 도구를 제작할 수 있다. VampNet은 잘 설계된 사용자 인터페이스와 함께 다음 세대 음악 편집 및 창작 스위트의 기반으로 사용될 수 있다.&lt;/p>
&lt;h2 id="experiments">EXPERIMENTS&lt;/h2>
&lt;p>이 연구는 VampNet이 음악을 압축하고 생성하는 능력을 평가하는 것이 목표이다. 오디오 품질을 평가하기 위해 다중 스케일 멜 재구성 오차와 Fréchet Audio Distance (FAD)를 사용한다.&lt;/p>
&lt;p>$$ D_{F,M} = | S_{F,M} - \hat{S}_{F,M} |_1 $$&lt;/p>
&lt;p>FFT 크기 $F$와 멜 주파수 바인의 수 $M$을 사용한다. FFT 크기 $F$는 2048에서 512 사이이고, $M$은 150에서 80 사이이다. 멜 재구성은 압축 품질을 평가하는 데 유용하지만, 생성 품질 평가에는 적합하지 않는다. 대신 생성 품질을 평가할 때는 FAD를 사용한다. FAD는 실제 오디오와 생성된 오디오의 분포 중첩을 측정하여 생성된 샘플이 실제 데이터 분포 내에 있는지를 평가한다.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>JukeBox와 비슷하게, 32 kHz 샘플링 속도로 797,000개의 트랙으로 구성된 대규모 음악 데이터셋을 수집하였다. 이 트랙들은 토크나이저와 호환되도록 44.1 kHz로 재샘플링되었고, Echo Nest의 Every Noise at Once에 설명된 수천 명의 아티스트들의 음악을 포함하고 있다.&lt;/p>
&lt;p>2000개의 트랙으로 구성된 검증 및 테스트 하위 집합을 사용한다. 이들은 학습 데이터와 아티스트가 중복되지 않도록 구성된다. DAC 데이터셋을 활용하여 음악 및 비음악 데이터를 수집하고, 이를 이용해 토크나이저를 학습시켰다. 모든 오디오는 -24dBFS로 정규화되며, 학습 과정에서는 이 파일들의 메타데이터를 사용하지 않는다.&lt;/p>
&lt;h3 id="network-architecture-and-hyperparameters">Network Architecture and Hyperparameters&lt;/h3>
&lt;p>오디오 토크나이저는 44.1kHz 오디오를 8kbps 비트레이트로 압축하며, 14개 코드북을 사용한다. 다운샘플링 비율은 768배이며, 잠재 공간 주파수는 57Hz이다. 각 타임스텝마다 14개의 토큰을 예측하며, 이 중 4개는 coarse 토큰이고 나머지 10개는 fine 토큰으로 구성된다. 토크나이저는 25만 스텝 동안 학습된다.&lt;/p>
&lt;p>VampNet 아키텍처는 bidirectional transformer로, coarse 모델은 20개 attention layer와 1280 임베딩 차원, 20개 attention head를 가지며, coarse-to-fine 모델은 16개 layer를 사용한다. coarse 모델은 100만 스텝, coarse-to-fine 모델은 50만 스텝 동안 학습되며, AdamW 옵티마이저를 사용하고, learning rate 스케줄러는 Vaswani 등이 도입한 방식을 따른다. 드롭아웃은 0.1, 배치 크기는 25이며, GPU 메모리 예산은 72GB이다.&lt;/p>
&lt;h3 id="efficiency-of-vampnet">Efficiency of VampNet&lt;/h3>
&lt;p>VampNet을 사용하여 테스트 세트의 10초 예시에서 샘플링 스텝을 [1, 4, 8, 12, 36, 64, 72]로 변화시켜 현실적인 음악을 생성할 수 있는지 검증한다. 각 스텝별로 메트릭을 보고한다.&lt;/p>
&lt;h3 id="effect-of-prompts">Effect of prompts&lt;/h3>
&lt;p>다양한 프롬프트에 따른 VampNet의 반응을 이해하려 한다. 압축 프롬프트에서부터 창의적인 생성 프롬프트까지 다양한 스펙트럼을 형성하는지, 낮은 비트레이트에서의 복원이 생성적 행동을 유발하는지를 조사한다.&lt;/p>
&lt;p>평가 데이터셋에서 2000개의 10초 예시를 추출하여 이를 오디오 토크나이저를 사용해 토큰 스트림으로 인코딩한다. 그리고 이 토큰 스트림을 네 가지 방법으로 조작한다:&lt;/p>
&lt;ol>
&lt;li>Compression prompt: $C$ 코드북은 가장 coarse한 코드북부터 unmasked 상태로 남겨둔다. 다른 모든 토큰은 마스킹 처리된다. $N_k$는 1로 설정한다.&lt;/li>
&lt;li>Periodic prompt: 매 P번째 타임스텝은 unmasked 상태로 남긴다. unmasked된 타임스텝에서는 모든 코드북의 토큰이 unmasked 된다. 다른 모든 토큰들 (예: 주기 $P$에 해당하지 않는 타임스텝의 토큰들)은 마스킹 처리된다. $P$를 [8, 16, 32]에서 설정한다.&lt;/li>
&lt;li>Prefix and suffix (inpaint) prompts: 시퀀스의 시작과 끝에서 일부 세그먼트는 unmasked 상태로 남겨진다. 다른 모든 토큰들은 마스킹 처리된다. 이 프롬프트는 초 단위의 컨텍스트 길이에 의해 매개변수화된다. 컨텍스트를 1초 또는 2초로 설정하며, 이는 각각 57 또는 114 타임스텝에 해당한다.&lt;/li>
&lt;li>Beat-driven prompt: 먼저 오디오 웨이브폼을 비트 트래커로 처리한다. 그런 다음 각 감지된 비트 주변에서 비트 오른쪽의 타임스텝을 unmask한다. 각 비트 주변에 약 75ms의 unmasked 섹션을 검토하며, 이는 비트 당 약 4개의 타임스텝에 해당한다.&lt;/li>
&lt;/ol>
&lt;p>프롬프트를 사용하여 입력 토큰 스트림을 조작한 후, VampNet을 사용하여 이러한 마스킹된 토큰 스트림에서 새로운 음악을 생성하고, 생성된 신호와 원본 신호 간의 FAD와 멜 재구성 오차를 계산한다. 추가적으로 입력 토큰 스트림의 일부 토큰을 무작위로 대체하여 노이즈가 추가된 베이스라인도 평가하며, 코덱 자체와 코어스 투 파인 모델을 기준으로 설정한다.&lt;/p>
&lt;p>마지막으로, 압축과 주기적 프롬프트의 하이퍼파라미터 ($C$와 $P$)를 조정하여 모델이 압축에서 생성으로 전환되는 방법을 살펴본다. 더 많은 타임스텝이 마스크될수록, 모델은 입력 음악과 일치하지 않을 수 있는 신뢰할 수 있는 음악 단편을 생성해야 한다.&lt;/p>
&lt;h2 id="results-and-discussion">RESULTS AND DISCUSSION&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/vampnet/images/figure3.png"
width="654"
height="514"
srcset="https://kurtkim.github.io/p/vampnet/images/figure3_huabe5adb13b5b34b018f46db591add7d5_66911_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/vampnet/images/figure3_huabe5adb13b5b34b018f46db591add7d5_66911_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>VampNet은 36 스텝에서 가장 낮은 FAD를 달성했지만, 12 스텝도 비슷한 성능을 보여주었다. 실제로, 24 스텝 샘플링이 생성 품질과 계산 속도 사이에서 적절한 균형을 제공하는 것으로 나타났다. NVIDIA RTX3090에서 10초 샘플을 생성하는 데는 약 6초가 걸리며, autoregressive 모델로 동일한 시간의 오디오를 생성하는 데는 약 574단계가 필요하여 약 1분이 소요된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/vampnet/images/figure4.png"
width="666"
height="496"
srcset="https://kurtkim.github.io/p/vampnet/images/figure4_hu410b8a07ebb29258d553fcbd3bf257a7_78323_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/vampnet/images/figure4_hu410b8a07ebb29258d553fcbd3bf257a7_78323_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>노이즈가 추가된 토큰 베이스라인은 모든 프롬프트와 멜 재구성 면에서 유사하지만, FAD에서는 성능이 매우 부진하다. 이는 프롬프팅 전략이 입력 오디오와 완벽하게 일치하지 않을 수 있지만, 여전히 신뢰할 수 있는 음악 분포 내에 있다는 것을 시사한다.&lt;/p>
&lt;p>비트 주도 프롬프트가 다른 모든 프롬프트보다 낮은 FAD를 달성하여 최고의 성능을 보여주었다. $P = 16$인 주기적 프롬프트는 1초 컨텍스트를 가진 인페인팅과 유사한 성능을 보였습니다 (57 조건화 타임스텝). 따라서 조건화 토큰을 균등하게 분포시키는 주기적 프롬프트 기술은 샘플링 기술과 비교하여 적은 조건화 타임스텝으로도 품질이 높은 샘플을 생성할 수 있다는 점에서 주목받았다.&lt;/p>
&lt;p>비트 주도 프롬프트는 다른 프롬프트보다 더 안정된 템포를 유지하며, 주기적 프롬프트보다 원본 음악에 더 가까운 결과를 낸다고 질적으로 발견하였다. 비트 주도, 주기적, 그리고 인페인팅 프롬프트를 혼합하여 VampNet을 창의적으로 조정할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/vampnet/images/figure5.png"
width="674"
height="512"
srcset="https://kurtkim.github.io/p/vampnet/images/figure5_huef853c9bd7b0eff22d555204116e75ec_80875_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/vampnet/images/figure5_huef853c9bd7b0eff22d555204116e75ec_80875_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="315px"
>&lt;/p>
&lt;p>주기적과 압축 프롬프트를 결합하여 VampNet이 토큰을 더 많이 마스킹할 때 재구성과 생성 작업 사이의 변화를 보여주었다. 높은 비트레이트에서 VampNet은 원본 음악 신호를 정확하게 재구성하여 낮은 멜 스펙트로그램 오차와 FAD 값을 달성했으며, 낮은 비트레이트에서도 일관된 음악적 구조를 갖춘 오디오 신호를 생성할 수 있었다.&lt;/p>
&lt;h2 id="conclusion">CONCLUSION&lt;/h2>
&lt;p>VampNet는 음악 생성을 위한 마스크된 음향 토큰 모델링 접근법이며, 양방향으로 작동하며 입력 오디오 파일을 다양한 방식으로 프롬프팅할 수 있다. 다양한 프롬프팅 기법을 통해 VampNet은 음악 압축과 생성 사이에서 작동하며, 음악 조각에 변화를 주는 데 유용한 도구이다. 추후 연구로는 VampNet과 그의 프롬프팅 기술이 음악 공동창작에 어떻게 기여할 수 있는지, 그리고 마스크된 음향 토큰 모델링의 표현 학습 능력을 탐구할 계획이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2302.03917" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://google-research.github.io/noise2music/" target="_blank" rel="noopener"
>Demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Noise2Music</title><link>https://kurtkim.github.io/p/noise2music/</link><pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/noise2music/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Noise2Music은 텍스트 프롬프트로부터 30초짜리 고품질 음악 클립을 생성하는 diffusion 모델 시리즈이다. 텍스트에 기반한 중간 표현을 생성하는 생성 모델과 고해상도 오디오를 생성하는 캐스케이더 모델을 이용해, 장르, 템포, 악기 등의 텍스트 프롬프트 요소를 반영하는 음악을 만든다. 중간 표현으로는 spectrogram과 낮은 해상도 오디오가 사용된다. 이 과정에서 사전 학습된 대규모 언어 모델이 학습 세트 오디오의 텍스트를 생성하고 텍스트 프롬프트의 임베딩을 추출하는 데 중요한 역할을 한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Noise2Music은 텍스트 프롬프트로부터 30초 길이, 24kHz 음악 클립을 생성하는 diffusion 기반 방법이다.&lt;/p>
&lt;p>&lt;strong>Modeling:&lt;/strong> 텍스트 프롬프트로부터 30초 웨이브폼의 압축 표현을 생성하는 연쇄 diffusion 모델을 학습시키고, 이를 기반으로 16kHz 웨이브폼을 생성 후, 최종적으로 24kHz 오디오로 슈퍼 해상도 증가시킨다. 중간 표현으로는 log-mel spectrogram 또는 3.2kHz 웨이브폼을 사용하며, 1D U-Nets와 사전 학습된 언어 모델(LM)을 통해 학습한다.&lt;/p>
&lt;p>&lt;strong>Data mining:&lt;/strong> deep generative 모델을 위한 고품질 샘플 생성에 필수적인 대량의 학습 데이터를 구축하기 위해, 다양한 음악 오디오 클립과 설명적 텍스트 라벨을 짝지어진 대규모 데이터셋을 만들기 위해 데이터 마이닝 파이프라인을 사용하였다. 이 과정에서 대형 언어 모델과 사전 학습된 음악-텍스트 공동 임베딩 모델을 활용하여 오디오 클립에 대한 텍스트 라벨을 생성하고, 약 150K시간의 오디오 소스에 가짜 라벨을 붙여 학습 데이터를 구성하였다.&lt;/p>
&lt;p>&lt;strong>MuLaMCap:&lt;/strong> 이 작업을 통해 생성된 MuLan-LaMDA Music Caption 데이터셋(MuLaMCap)은 AudioSet의 음악 콘텐츠에 주석을 달아 얻은 약 400K의 음악-텍스트 쌍으로 구성되어 있다. 원본 AudioSet의 632개 라벨 중 141개만 음악 관련이었던 것에 비해, MuLaMCap은 400만 개의 다양하고 세밀한 음악 설명 어휘를 바탕으로 한다. 이 데이터셋은 사운드 분류를 넘어 음악 캡셔닝, 검색, 생성 등의 다양한 응용 분야에서 활용될 것으로 기대된다.&lt;/p>
&lt;p>&lt;strong>Evaluation:&lt;/strong> 텍스트 조건부 음악 생성 모델의 품질을 Fréchet Audio Distance (FAD)와 MuLan 유사도 점수 두 가지 지표를 사용하여 측정한다. FAD는 생성된 오디오 클립의 품질을 AudioSet과 MagnaTagATune 같은 벤치마크 데이터셋과 비교하고, MuLan 유사도 점수는 텍스트 프롬프트와 생성된 오디오 클립 간의 의미적 일치를 평가한다.&lt;/p>
&lt;p>&lt;strong>Generative ability:&lt;/strong> 이 연구의 모델은 장르, 악기, 시대 등의 기본 음악 속성을 넘어 분위기나 느낌 같은 세밀한 속성을 반영하는 복잡한 의미론을 처리할 수 있음을 입증한다. 이는 메타데이터 태그뿐만 아니라 사전 학습된 음악-텍스트 임베딩 모델을 활용하여 오디오 특성에 의미론을 연결하는 학습 데이터셋 구축을 통해 달성되었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Generative models:&lt;/strong> deep generative 모델은 다양한 도메인에서 성공을 거두어왔으며, 최근에는 고품질 샘플 생성을 위해 데이터셋 크기를 확장하는 데 집중하고 있다. 여기에는 텍스트, 음성, 이미지, 오디오에서의 최근 발전이 포함된다.&lt;/p>
&lt;p>&lt;strong>Diffusion models:&lt;/strong> diffusion 모델은 고품질의 이미지, 오디오, 비디오 생성에 효과적임을 입증하였다. 특히, 저품질 이미지에서 시작해 연속적으로 정제하여 고품질 이미지를 만드는 캐스케이드 확산 모델은 오디오 생성에도 적용되었다.&lt;/p>
&lt;p>&lt;strong>Audio generation:&lt;/strong> 외부 입력에 조건을 둔 오디오 생성 방법으로, 텍스트 조건의 spectrogram 및 오디오 생성이 연구되었다. 설명적 텍스트 기반 오디오 생성에서는 AudioGen이 auto-regressive 방법을, DiffSound가 diffusion 기반 방법을 사용하였다. 음악 생성에서는 Jukebox, Mubert, MusicLM이 auto-regressive 방식을, Riffusion이 diffusion 방식을 사용하였다.&lt;/p>
&lt;p>&lt;strong>Conditional signals in audio generation:&lt;/strong> 모델이 특정 스타일의 음악을 생성하도록 유도하는 조건 신호를 parameterized 하고 전달하는 두 가지 접근 방식이 있다. 하나는 신호를 사전 정의된 임베딩 공간에 투영하는 것으로, Jukebox는 아티스트와 장르의 고정된 어휘를, Mubert는 태그 집합을 사용한다. 다른 하나는 AudioGen과 MusicLM처럼 사전 학습된 텍스트 인코더로 사용자 프롬프트를 인코딩하는 방식이다.&lt;/p>
&lt;hr>
&lt;h2 id="methods">Methods&lt;/h2>
&lt;h3 id="diffusion-models">Diffusion models&lt;/h3>
&lt;p>diffusion 모델은 random noise에서 반복적으로 denoising 하여 샘플을 생성하는 생성 모델이다. 이 논문에서는 작업 이해에 필수적인 기본 정보만을 다룬다.&lt;/p>
&lt;p>diffusion 모델의 입력은 조건 신호 $c$, 무작위로 샘플링된 시간 단계 $t$, 그리고 시간 $t$에서의 잡음의 표준 편차 $σ_t$로 parameterized 된 noise 일정을 통해 손상된 원본 샘플 $x$로부터 얻은 샘플 $x_t$이다. 시간의 범위는 [0, 1]이며, diffusion은 시간이 증가함에 따라 진행된다. gaussian diffusion process은 표준 정규 분포에 속하는 단일 noise 벡터로 완전히 설명될 수 있으며, $x_t$는 원본 샘플, noise 일정, noise 벡터의 함수로 표현된다. 모델은 이 입력을 바탕으로 잡음 벡터를 식별하도록 학습되며, diffusion loss는 이러한 과정을 기반으로 계산된다.&lt;/p>
&lt;p>$$ \mathbb{E}&lt;em>{x,c, \epsilon,t} = \big[ w_t \parallel \epsilon&lt;/em>{\theta} ( x_t, c, t ) - \epsilon \parallel^2 \big] $$&lt;/p>
&lt;p>여기서 $w_t$는 선택한 고정 가중치 함수이다.&lt;/p>
&lt;p>추론은 시간 $t = 1$에서 무작위 잡음을 취하고 모델의 잡음 예측을 이용해 제거함으로써 이루어진다. 이 과정에서 ancestral (또는 DDPM) 샘플링을 사용하여, 생성된 샘플의 품질에 영향을 줄 수 있는 여러 parameter를 조절할 수 있는 유연한 추론 방법을 제공한다. 샘플러의 확률성 수준을 조절할 수 있고, 임의의 제거 일정을 설정하여 제거 단계를 조절할 수 있다.&lt;/p>
&lt;p>diffusion 모델 학습 시 다양한 선택지가 있으며, 이와 관련된 여러 옵션들의 자세한 내용은 부가 자료에서 확인할 수 있다.&lt;/p>
&lt;ul>
&lt;li>Loss weight($w_t$): simpliﬁed weight $w_t = 1$ 및 sigma weight $w_t = σ_t^2$&lt;/li>
&lt;li>Variance schedule: linear and cosine schedules&lt;/li>
&lt;li>Stochasticity parameter: $ γ = 0 or 1$&lt;/li>
&lt;li>Denoising step schedule&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Classiﬁer-free guidance (CFG):&lt;/strong> CFG(Ho &amp;amp; Salimans, 2022)는 생성된 샘플과 조건부 입력의 일치성을 높이기 위해 학습 중 일부 조건부 입력을 숨김으로써 네트워크가 노이즈 벡터를 조건부 및 비조건부로 예측하도록 한다. 추론 시, 조건부 입력 유무에 따라 계산된 노이즈 벡터를 사용하며, CFG로 인한 과포화를 막기 위해 dynamic clipping이 적용된다.&lt;/p>
&lt;h3 id="architecture">Architecture&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/figure1.png"
width="1234"
height="388"
srcset="https://kurtkim.github.io/p/noise2music/images/figure1_hu2c7cb634e9d61496ab9b2f5b8e53a406_187990_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/figure1_hu2c7cb634e9d61496ab9b2f5b8e53a406_187990_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="763px"
>&lt;/p>
&lt;p>diffusion 모델을 위한 1D Efficient U-Net을 배포한다. 이 모델은 다운샘플링 및 업샘플링 블록, self/cross-attention layer, combine layer으로 구성되며, two-dimensional convolution을 일차원으로 대체한 구조를 가진다.&lt;/p>
&lt;p>모델로의 진입 가능한 네 가지 경로는 다음과 같다: 1) 일정 길이 $T$의 시퀀스로 구성된 쌓인 입력과 출력, 2) 잡음이 섞인 샘플 $x_t$를 포함한 입력과 잡음 예측으로 해석되는 출력, 3) 업샘플링된 저품질 오디오, 4) cross-attention을 통한 임의 길이 벡터 시퀀스의 상호작용. 또한, U-Net의 &amp;ldquo;U&amp;rdquo; 바닥에 추가함으로써 시퀀스의 압축된 표현에 대해 모델을 조건화할 수 있다.&lt;/p>
&lt;h3 id="cascaded-diffusion">Cascaded diffusion&lt;/h3>
&lt;p>텍스트 프롬프트로부터 고품질의 30초 오디오를 생성하기 위해 두 가지 diffusion 모델을 학습한다. generator 모델은 텍스트 프롬프트에 기반한 intermediate representation을, cascader 모델은 이 intermediate representation을 바탕으로 최종 오디오를 생성한다. intermediate representation으로는 저품질 오디오와 spectrogram을 사용한다.&lt;/p>
&lt;h4 id="waveform-model">WAVEFORM MODEL&lt;/h4>
&lt;p>&lt;strong>Generator Model:&lt;/strong> generator 모델은 텍스트 입력에 따라 3.2kHz 오디오를 생성하고, 텍스트에서 파생된 벡터 시퀀스를 cross-attention 시퀀스로 네트워크에 입력한다.&lt;/p>
&lt;p>&lt;strong>Cascader Model:&lt;/strong> cascader 모델은 텍스트 프롬프트와 generator 모델이 생성한 저품질 오디오를 기반으로 16kHz 오디오를 생성한다. 텍스트 conditioning은 cross-attention를 통해 이루어지며, 저품질 오디오는 FFT와 inverse FFT 변환을 통해 업샘플링되어 모델에 입력된다.&lt;/p>
&lt;h4 id="spectrogram-model">SPECTROGRAM MODEL&lt;/h4>
&lt;p>&lt;strong>Generator Model:&lt;/strong> 이 모델은 텍스트에 따라 log-mel spectrogram을 생성하며, spectrogram은 80채널과 초당 100특징을 가진다. 입력과 출력에 채널 차원이 추가되고, spectrogram 픽셀 값은 [-1, 1]로 정규화된다. 텍스트 conditioning은 cross-attention로 달성된다.&lt;/p>
&lt;p>&lt;strong>Vocoder Model:&lt;/strong> vocoder 모델은 spectrogram에 conditioning된 16kHz 오디오를 생성하며, spectrogram은 정렬된 입력으로 사용된다. U-Net 모델의 샘플링 비율은 spectrogram의 압축 비율을 맞추기 위해 조정된다.&lt;/p>
&lt;h4 id="super-resolution-cascader">SUPER-RESOLUTION CASCADER&lt;/h4>
&lt;p>경량 cascader는 16kHz 파형을 업샘플링하여 24kHz 오디오를 생성하며, 텍스트 conditioning은 사용되지 않는다.&lt;/p>
&lt;h3 id="text-understanding">Text understanding&lt;/h3>
&lt;p>강력한 텍스트 인코더가 음악 기술 텍스트의 복잡성을 잘 포착할 수 있음을 입증한 연구를 바탕으로, T5 인코더를 사용하여 비풀링 토큰 임베딩 시퀀스로 diffusion 모델을 conditioning 한다. 다른 대규모 언어 모델이나 음악-텍스트 쌍에 대해 훈련된 CLIP 기반 임베딩과의 비교는 이 작업의 범위를 벗어난다.&lt;/p>
&lt;h3 id="pseudo-labeling-for-music-audio">Pseudo labeling for music audio&lt;/h3>
&lt;p>대규모 학습 데이터는 generative deep neural network의 품질을 보장하는 데 필수적이다. 예를 들어, Imagen은 O(1B) 이미지-텍스트 쌍으로 학습되었다. 음악 콘텐츠는 널리 존재하지만, 고품질의 음악-텍스트 쌍 데이터는 제목, 아티스트 이름 등 기본 메타데이터를 넘어서는 경우 드물다.&lt;/p>
&lt;p>MuLan과 LaMDA 모델을 활용하여 미분류 음악 오디오 클립에 세밀한 의미의 가짜 라벨을 할당하는 방식으로 음악-텍스트 쌍을 생성한다.&lt;/p>
&lt;p>다양한 음악 설명 텍스트로 구성된 여러 음악 캡션 어휘 집합을 만들었다. 이 텍스트들은 MagnaTagATune, FMA, AudioSet과 같은 표준 음악 분류 벤치마크의 캡션과는 규모와 세밀한 의미에서 차이가 있다.&lt;/p>
&lt;p>&lt;strong>LaMDA-LF:&lt;/strong> LaMDA 언어 모델을 사용해 노래 제목과 아티스트 이름을 바탕으로 15만 개의 인기 곡을 설명한다. 이를 통해 음악 설명 가능성이 높은 400만 개의 문장을 생성하였다. 대화형 애플리케이션 학습을 받은 LaMDA는 음악 생성용 사용자 프롬프트와 더 잘 어울리는 텍스트 생성을 목표로 한다.&lt;/p>
&lt;p>&lt;strong>Rater-LF:&lt;/strong> MusicCaps에서 얻은 10,028개의 캡션을 분리하여, 35,333개의 음악 설명 문장을 생성하였다.&lt;/p>
&lt;p>&lt;strong>Rater-SF:&lt;/strong> 위 평가 세트에서 평가자가 작성한 모든 단문 형식의 음악 측면 태그를 수집했으며, 이는 23,906개의 어휘로 구성된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table1.png"
width="602"
height="232"
srcset="https://kurtkim.github.io/p/noise2music/images/table1_hud2324064ea418866c6f70fc21a67e6fd_57161_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table1_hud2324064ea418866c6f70fc21a67e6fd_57161_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="622px"
>&lt;/p>
&lt;p>MuLan 모델은 레이블 없는 오디오 클립에 캡션을 할당하기 위한 zero-shot 음악 분류기로, 대규모 노이즈가 있는 텍스트-음악 쌍 데이터에 대해 대조적 학습을 통해 학습된 텍스트와 오디오 인코더를 사용한다. 이 모델은 10초 길이 음악 클립과 해당 설명 문장을 의미 임베딩 공간에서 가깝게 배치한다. 각 클립에 대해서는 오디오 임베딩을 계산하고, 임베딩 공간에서 오디오에 가장 가까운 상위 K개 캡션을 선정한다. 라벨 분포의 균형과 캡션 다양성 증가를 위해 빈도수에 반비례하는 확률로 추가 캡션을 샘플링한다. 여기서 $K=10$, $K&amp;rsquo;=3$이다.&lt;/p>
&lt;p>대규모 학습 세트의 사전 라벨링 준비로, AudioSet에서 유래한 MuLaMCap이라는 음악 캡셔닝 데이터셋을 생성하였다. 이 데이터셋은 AudioSet의 음악 분야 라벨을 가진 388,262개 학습 세트 및 4,497개 테스트 세트 예시에 사전 라벨링 방법을 적용해 만들어졌으며, 각 10초 음악 오디오는 LaMDA-LF와 평가자-LF에서 각각 3개, 평가자-SF에서 6개의 캡션과 연결된다.&lt;/p>
&lt;h3 id="training-data-mining">Training data mining&lt;/h3>
&lt;p>약 680만 개의 음악 오디오 파일을 수집하고 각각에서 30초 길이의 6개 클립을 추출해 대규모 오디오-텍스트 쌍 컬렉션을 만들었다. 총 34만 시간의 음악이며, super-resoluton 모델은 24kHz, 기타 모델은 16kHz로 샘플링하여 학습된다.&lt;/p>
&lt;p>각 사운드트랙에 대해 노래 제목, 관련 엔티티 태그, 그리고 LaMDA-LF와 Rater-SF 어휘에서 유래한 사전 레이블을 포함한 세 가지 유형의 텍스트 레이블을 사용한다. 사전 레이블은 활동과 기분, 구성 요소의 미세한 의미를 포함한 주관적 설명을 제공하여 엔티티 태그를 보완한다. 그러나 Rater-LF 어휘의 사전 레이블은 MusicCaps 평가에서 유래된 문장 때문에 학습 데이터에서 제외된다.&lt;/p>
&lt;p>대규모 사전 라벨링된 학습 세트에 저작권이 필요 없는 음악 라이브러리에서 가져온 고품질 오디오를 추가하였다. 음악 트랙은 30초 클립으로 나누고, 메타데이터는 텍스트 프롬프트로 사용합니다. 이는 약 300시간의 주석이 달린 오디오를 훈련 데이터에 추가하는 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments-and-results">Experiments and Results&lt;/h2>
&lt;h3 id="model-training-details">Model training details&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table2.png"
width="598"
height="184"
srcset="https://kurtkim.github.io/p/noise2music/images/table2_hu8edd464417fd9a5932e070ec560a851e_43994_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table2_hu8edd464417fd9a5932e070ec560a851e_43994_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="325"
data-flex-basis="780px"
>&lt;/p>
&lt;p>이 작업을 위해 1D U-Net 모델 4개, waveform generator, cascader, spectrogram generator 및 vocoder를 학습시켰다. spectrogram generator의 수렴을 위해 sigma-weighted loss가 중요했음을 발견하였다. 이는 노이즈 제거 일정의 후반부에서 손실을 더 많이 가중하는 방식이다.&lt;/p>
&lt;p>vocoder를 제외한 모든 모델은 오디오-텍스트 쌍으로 학습되며, vocoder는 오디오만으로 학습된다. 각 오디오 샘플에 대해 텍스트 배치가 형성된다. 세 개의 긴 프롬프트는 텍스트 배치의 세 개의 독립적인 요소를 구성하고, 짧은 프롬프트는 연결된 후 설정된 토큰 길이로 분할되어 텍스트 배치에 추가된다. 각 오디오 클립에 대해 해당 텍스트 배치의 랜덤 요소가 학습 시 선택되어 오디오에 대응하는 텍스트로 모델에 제공된다.&lt;/p>
&lt;p>모델들은 $β_1 = 0.9$와 $β_2 = 0.999$로 설정된 Adam 최적화 알고리즘과 cosine learning rate 스케줄(peak 1e-4, 10k warm-up step, 2.5M step)을 사용하여 학습된다. EMA는 decay rate 0.9999로 계산되어 추론 시 사용된다. superresolution cascader는 배치 크기 4096, 다른 모델들은 배치 크기 2048으로 학습된다. 추론 시 CFG를 적용하기 위해 각 배치의 10% 샘플에 대해 텍스트 프롬프트를 차단하고 cross attention layer 출력을 0으로 설정한다.&lt;/p>
&lt;p>generator 모델은 self-attention을 사용하여 30초 전체 오디오로 학습되지만, cascader와 vocoder는 self-attention을 사용하지 않고 3~4초의 짧은 구간으로 학습된다.&lt;/p>
&lt;p>cascader/vocoder 모델 학습 시, 저품질 오디오나 spectrogram에 diffusion 노이즈를 적용하는 증강과 블러 증강이 사용된다. diffusion 노이즈는 $[0, t_{max}]$ 내 무작위 시간에 적용되며, cascader는 $t_{max} = 0.5$, vocoder와 super-resolution cascader는 $t_{max} = 1.0$이다. 블러 증강은 cascader에는 1D 크기 10, vocoder에는 2D 5x5 커널이 적용된다.&lt;/p>
&lt;h3 id="model-inference-and-serving">Model inference and serving&lt;/h3>
&lt;h4 id="model-inference">MODEL INFERENCE&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table3.png"
width="568"
height="204"
srcset="https://kurtkim.github.io/p/noise2music/images/table3_hu5e3de9659947f40f9e0739edbf022ffe_41180_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table3_hu5e3de9659947f40f9e0739edbf022ffe_41180_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="278"
data-flex-basis="668px"
>&lt;/p>
&lt;p>세 가지 추론 hyperparameter인 denoising 스케줄, stochasticity parameter, 그리고 CFG 스케일을 조정한다.&lt;/p>
&lt;p>denoising step 스케줄은 시간 단계 크기 $[δ_1, &amp;hellip;, δ_n]$로 매개변수화되며, 이는 누적을 통해 denoising step으로 변환된다. 추론 비용은 시간 단계 수에 비례하므로, 고정된 추론 비용으로 시간 단계를 총 시간 1에 맞게 분배한다. 세 가지 스케줄인 &amp;ldquo;front-heavy&amp;rdquo; &amp;ldquo;uniform&amp;rdquo; &amp;ldquo;back-heavy&amp;quot;을 실험한다. front-heavy는 $t = 0$ 근처에, back-heavy는 $t = 1$ 근처에 많은 step을 할당한다. uniform은 균등하게 간격을 둔 시간 단계를 사용한다. 정확한 스케줄은 보충 자료에 제공된다.&lt;/p>
&lt;h4 id="model-serving">MODEL SERVING&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table4.png"
width="506"
height="188"
srcset="https://kurtkim.github.io/p/noise2music/images/table4_hu7ac978b9a212e66a940e4bad5fcfc2d1_34798_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table4_hu7ac978b9a212e66a940e4bad5fcfc2d1_34798_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="645px"
>&lt;/p>
&lt;p>Google Cloud TPU V4에서 모델을 제공하고, 각 요청으로 30초 길이 음악 클립 4개를 생성한다. GSPMD를 사용해 모델을 네 TPU V4 장치에 분할하여 응답 시간을 50% 이상 단축하였다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;h4 id="parameter-selection-for-the-models">PARAMETER SELECTION FOR THE MODELS&lt;/h4>
&lt;p>모델 parameter는 저자들의 판단과 컴퓨팅 자원 및 시간의 가용성을 고려하여 경험적으로 선택된다. 저자들이 만든 독립적인 개발 프롬프트를 통해 학습된 모델로 오디오를 생성하며, 평가는 16kHz 파형으로 진행되나, super-resolution cascader는 사용되지 않는다.&lt;/p>
&lt;h4 id="evaluation-metrics">EVALUATION METRICS&lt;/h4>
&lt;p>텍스트 조건 음악 생성 모델의 품질을 FAD와 MuLan 유사도 점수로 측정한다.&lt;/p>
&lt;p>FAD는 생성된 오디오 예제의 품질을 참조 오디오 클립과 비교하여 측정한다. 오디오 인코더를 사용해 두 세트의 오디오 임베딩을 계산하고, 이 임베딩 분포가 가우시안 분포라고 가정하여 평균 벡터와 상관 행렬을 통해 Frechet 거리를 계산한다.&lt;/p>
&lt;p>FAD 지표 계산을 위해 세 가지 오디오 인코더가 사용된다: VGG, Trill, MuLan. VGG와 Trill은 프레임별 임베딩을, MuLan은 클립별 임베딩을 생성한다. 이 인코더들은 서로 다른 데이터셋과 작업에서 학습되어, 각기 다른 오디오 측면에 초점을 맞춘다. FAD VGG가 일반 오디오 품질, FAD Trill이 음성 품질, FAD MuLan이 음악적 의미를 평가한다고 가정한다.&lt;/p>
&lt;p>contrastive 모델 MuLan은 오디오-텍스트 및 오디오-오디오 쌍의 유사성을 코사인 유사도로 정량화한다. 음악-텍스트 쌍 평가 세트에서, 텍스트 프롬프트로 생성된 오디오와 해당 텍스트 또는 실제 오디오 간의 평균 유사성을 계산한다. 또한, 평가 세트의 평균 MuLan 유사도를 실제 오디오와 비교하고, 셔플된 랜덤 오디오 쌍과도 비교한다.&lt;/p>
&lt;h4 id="evaluation-datasets">EVALUATION DATASETS&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table5.png"
width="610"
height="368"
srcset="https://kurtkim.github.io/p/noise2music/images/table5_hu5cbf3c68a2edd3c41b086b37010e2a4f_82095_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table5_hu5cbf3c68a2edd3c41b086b37010e2a4f_82095_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="397px"
>&lt;/p>
&lt;p>평가 데이터셋에 대한 모델의 FAD를 Riffusion 3과 Mubert 4의 기준 모델과 비교하였고, 생성된 오디오와 평가 데이터셋 간의 오디오-텍스트 및 오디오-오디오 MuLan 유사도 점수를 보고하며, 실제 오디오와 섞인 오디오에 대한 메트릭도 포함하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table6.png"
width="610"
height="492"
srcset="https://kurtkim.github.io/p/noise2music/images/table6_hub0c01257fd1a6f1d9de305a9b2c2cf39_102917_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table6_hub0c01257fd1a6f1d9de305a9b2c2cf39_102917_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="123"
data-flex-basis="297px"
>&lt;/p>
&lt;p>평가 지표는 결과가 기준 모델들보다 유리할 수 있어 신중하게 해석해야 한다. 이는 학습 데이터 분포가 평가 데이터셋과 더 유사할 가능성과 MuLan 기반 지표가 이 연구의 모델에 편향될 수 있기 때문이다. 이러한 지표는 AudioSet 도메인에서 모델 성능을 나타내며, MuLan 모델이 편향되지 않은 표현을 학습했다면 지표는 유효하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/table7.png"
width="610"
height="82"
srcset="https://kurtkim.github.io/p/noise2music/images/table7_hucb052a831a0f5d0bb258877e30b193b9_15379_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/table7_hucb052a831a0f5d0bb258877e30b193b9_15379_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="743"
data-flex-basis="1785px"
>&lt;/p>
&lt;p>mantic alignment을 평가하기 위해 인간 청취 테스트를 진행하였다. 테스트는 (Agostinelli et al., 2023)과 동일한 설정으로, 다섯 가지 출처를 사용하였다. 참가자들은 두 개의 10초 클립을 듣고 텍스트 캡션에 더 잘 맞는 클립을 5점 척도로 평가하였다. 총 3k 평가가 수집되었으며, 각 출처는 1.2k 쌍별 비교에 참여하였다. 각 모델이 1.2k 비교 중에서 얻은 &amp;ldquo;승리&amp;rdquo; 수를 보고한다. 파형 모델은 MusicLM과 유사한 성능을 보였지만, 실제 오디오에는 뒤처졌다.&lt;/p>
&lt;h3 id="inference-parameter-ablations">Inference parameter ablations&lt;/h3>
&lt;p>모델의 추론 parameter를 조정하며 그 효과를 관찰하였다. 평가 수치를 생성한 체크포인트보다 덜 학습된 체크포인트로 소거 실험을 진행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/figure2.png"
width="510"
height="338"
srcset="https://kurtkim.github.io/p/noise2music/images/figure2_hu86ee75141e55af5056e016721a929bd4_66453_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/figure2_hu86ee75141e55af5056e016721a929bd4_66453_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="362px"
>&lt;/p>
&lt;p>추론 중 노이즈 제거 일정과 CFG 스케일 변화에 따른 VGG 측정 FAD와 MuLan 유사도 점수의 변화를 보여준다. parameter는 한 번에 하나씩만 변경되고, 나머지는 기준값에서 고정된다.&lt;/p>
&lt;p>FAD 지표와 유사도 점수는 대체로 상관관계가 있으나, cascader에서는 FAD가 나빠질 때 유사도 점수가 올라갈 수 있다. 최적의 CFG 스케일이 존재하며, 너무 큰 CFG 스케일은 생성 품질을 해친다. 생성기의 CFG 스케일이 노이즈 제거 일정보다 중요하며, cascader의 노이즈 제거 일정의 영향은 크다.&lt;/p>
&lt;h3 id="inference-cost-and-performance">Inference cost and performance&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/noise2music/images/figure3.png"
width="492"
height="334"
srcset="https://kurtkim.github.io/p/noise2music/images/figure3_huf94b510bb5d4976e5baa75a34f42a821_59498_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/noise2music/images/figure3_huf94b510bb5d4976e5baa75a34f42a821_59498_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="353px"
>&lt;/p>
&lt;p>추론 시간으로 측정한 추론 비용과 품질 지표의 관계를 보여준다. 생성기 혹은 cascader/vocoder의 추론 단계 수를 조절하고 단계 크기를 반비례로 조정한다. 생성기의 추론 비용 증가는 복합적인 효과를 보이나, cascader/vocoder의 추론 단계가 많을수록 생성 품질이 보통 향상된다.&lt;/p>
&lt;hr>
&lt;h2 id="qualitative-analysis">Qualitative analysis&lt;/h2>
&lt;p>&lt;strong>Content representation:&lt;/strong> google-research.github.io/noise2music#table-2에서는 모델이 텍스트 프롬프트의 음악적 요소를 반영하여 음악을 생성하는 예시를 제시한다. 텍스트에 나타난 장르, 악기, 분위기, 보컬 특성, 음악 시대 등이 생성 음악에 구현된다.&lt;/p>
&lt;p>&lt;strong>Creative prompts:&lt;/strong> 모델은 분포 밖 프롬프트로 고품질 오디오 생성에 어려움을 겪지만, 흥미로운 예시를 생성할 수 있다. Google-research.github.io/noise2music#table-3에서는 모델이 품질 높은 음악을 생성한 창의적인 프롬프트 예시를 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>&lt;strong>Spectrogram vs. waveform approach:&lt;/strong> 스펙트로그램 모델은 웨이브폼 모델보다 학습 및 서비스 비용이 저렴하고 시간 길이 측면에서 더 확장 가능하다. 스펙트로그램은 고주파 정보를 포함하는 반면, 웨이브폼 모델은 생성 과정에서 해석 가능한 표현을 제공하여 디버깅과 튜닝이 용이하게 하다. 이러한 차이는 웨이브폼 모델을 더 쉽게 학습할 수 있는 이유 중 하나이다.&lt;/p>
&lt;p>&lt;strong>Future directions:&lt;/strong> 텍스트 프롬프트 기반 음악 생성의 가능성을 보여주었지만, 모델 해석 가능성 증가, 텍스트-오디오 정렬 개선, 비용 감소, 오디오 생성 길이 확장 등 여러 방면에서 개선이 필요하다. 또한, 이미지에 적용된 것과 같이 다양한 오디오 작업에 모델을 미세 조정하는 것도 흥미로운 방향이 될 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="broader-impact">Broader Impact&lt;/h2>
&lt;p>이 연구는 예술가와 콘텐츠 제작자들에게 유용한 도구로 성장할 잠재력이 있다. 이를 실현하기 위해 뮤지션들과 협력하여 모델을 공동 창작 도구로 개발하는 추가 작업이 필요하다.&lt;/p>
&lt;p>제안된 모델은 학습 데이터에 내재된 패턴과 편향을 모방하며, 이로 인해 텍스트와 음악 코퍼스의 잠재적 편향이 모델 출력에 반영될 수 있다. 이러한 편향은 감지하기 어렵고 평가 벤치마크로 완전히 포착되지 않으며, 결과적으로 모델은 비하적이거나 해로운 언어를 생성할 수 있다.&lt;/p>
&lt;p>음악 장르의 복잡성과 시간 및 맥락에 따른 변화를 인식하며, 학습 데이터는 전 세계 음악 문화의 불균등한 대표성을 반영한다. 음악 분류와 라벨링은 커뮤니티 참여 없이 이루어질 수 있다. 또한, 음악 샘플이 특정 지역이나 문화의 전체를 대표한다고 보거나, 특정 장르의 다양성을 하나의 라벨로 간주하지 않도록 주의가 필요하다. 보컬 생성 시, 문화적 또는 종교적 장르 요청에서 비하적 언어나 과장된 표현이 나타날 수 있으며, 이는 특히 정치적 투쟁과 관련된 음악 장르에서 중요하다.&lt;/p>
&lt;p>다른 기술과 마찬가지로, 이 연구 결과도 오용되거나 악용될 수 있다. 생성된 콘텐츠가 학습 데이터의 예시와 정확히 일치할 경우 발생할 수 있는 잠재적 오용의 위험을 인정한다. 책임 있는 모델 개발 관행에 따라, 중복 검사는 현재 예시를 생성하고 공개하는 파이프라인에 내장되어 있으며, 향후 작업에서도 계속 유지될 것이다.&lt;/p>
&lt;p>생성 모델 개선을 위해 안전 문제 식별 및 해결이 중요하며, 한계와 위험을 명확히 이해할 때까지 모델 공개를 보류한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2307.04686" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/hugofloresgarcia/vampnet" target="_blank" rel="noopener"
>Demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MeLoDy</title><link>https://kurtkim.github.io/p/melody/</link><pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/melody/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>본 논문은 최신 음악 생성 기술인 MusicLM의 작업량을 대폭 줄인 새로운 모델 MeLoDy를 제시한다. MusicLM의 의미론적 모델링을 기반으로, MeLoDy는 새로운 dual-path diffusion(DPD) 모델과 오디오 VAE-GAN을 활용하여 음악 오디오를 효율적으로 생성한다. 이를 통해 10초나 30초 길이의 음악 샘플링 시 전달 횟수를 각각 95.7% 또는 99.6%까지 줄이면서도 MusicLM과 동등한 품질의 음악을 생성할 수 있다. MeLoDy는 샘플링 속도의 향상과 무한 연속 생성 가능성을 제공하며, 음악성, 오디오 품질, 텍스트 연관성 면에서도 최고 수준을 달성한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>최근 몇 년간 심층 생성 모델의 발전으로 음악 생성이 큰 관심을 받아왔다. 언어 모델(LMs)은 장기적 맥락에서의 복잡한 관계를 모델링하는 데 탁월하며, AudioLM과 같은 작업들이 이를 오디오 합성에 성공적으로 적용하였다. 또한, diffusion probabilistic 모델(DPMs)도 음성, 사운드, 음악 합성에서 뛰어난 능력을 보여주며 생성 모델 분야에서 주목받고 있다.&lt;/p>
&lt;p>자유 형식 텍스트를 이용한 음악 생성은 다양한 음악 설명의 범위로 인해 도전적이다. MusicLM과 Noise2Music 같은 기존 모델들은 대규모 데이터셋 훈련을 통해 높은 충실도와 텍스트 프롬프트 준수로 최신 생성 성능을 보였지만, 큰 계산 비용이 발생한다. 반면, DPMs 기반 Moûsai는 고품질 음악 샘플링을 효율적으로 가능하게 했으나, 시연한 사례가 적고 다이내믹스가 제한적이었다. 인간 피드백을 고려한 인터랙티브한 음악 생성을 위해서는 높은 효율성을 가진 생성 모델이 필수적이다.&lt;/p>
&lt;p>LMs와 DPMs의 장점을 결합하는 것에 초점을 맞춘 이 연구에서는, MusicLM의 최고 수준 LM인 의미적 LM을 활용하여 음악의 의미 구조를 모델링하고, DPMs의 비자동 회귀적 특성을 이용해 효율적으로 음향을 모델링한다. 이를 통해, 이 논문은 음악 생성 분야에 여러 새로운 기여를 제시한다.&lt;/p>
&lt;ol>
&lt;li>MeLoDy는 LM-guided diffusion 모델로, MusicLM보다 훨씬 적은 반복으로 고품질 음악을 실시간보다 빠르게 생성한다.&lt;/li>
&lt;li>dual-path diffusion(DPD) 모델은 의미 조건부 전략을 사용해 세밀한 음향 정보를 효율적으로 동시 모델링한다.&lt;/li>
&lt;li>DPD의 새로운 샘플링 체계는 이전 방법보다 생성 품질을 개선한다.&lt;/li>
&lt;li>오디오 VAE-GAN은 연속적인 잠재 표현을 학습하고, DPD와 함께 고품질 오디오를 합성한다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/melody/images/table1.png"
width="936"
height="204"
srcset="https://kurtkim.github.io/p/melody/images/table1_hud0e58292cc9c0dda8d666e3af97b24df_56915_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/melody/images/table1_hud0e58292cc9c0dda8d666e3af97b24df_56915_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="458"
data-flex-basis="1101px"
>&lt;/p>
&lt;p>&lt;strong>Audio Generation&lt;/strong> 음악 생성 모델들은 빠르게 고품질 음악을 생성할 수 있지만, 자유 형식 텍스트를 입력으로 사용할 수 없고 단일 장르에만 특화된다. Mubert와 Riffusion같은 산업계 음악 생성기도 있지만, MusicLM과 비교해 자유 형식 텍스트 처리에는 한계가 있다. 또한, AudioSet를 이용한 텍스트-투-오디오 합성기들은 자유 형식 텍스트로 음악을 생성할 수 있으나 음악성은 제한적이다. AudioLM은 조건 없이 피아노 오디오를 이어가며, SoundStorm은 non-autoregressive decoding을 통해 AudioLM을 가속화하여 빠른 속도로 음향을 생성한다. MeLoDy는 고품질 음향을 생성하기 위해 5에서 20회의 전방 패스가 필요하다.&lt;/p>
&lt;p>&lt;strong>Network Architecture&lt;/strong> 제안한 DPD 구조는 오디오 분리에 사용된 이중 경로 네트워크에서 영감을 받았고, Luo 등이 제시한 세분화 기반 처리 아이디어로 인해 여러 최신 연구가 진행되었다. 확산 모델의 목표가 소스 분리의 특별한 경우로 볼 수 있기에, 이 이중 경로 구조는 대략적이고 세밀한 음향 모델링을 동시에 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="background-on-audio-language-modeling">Background on Audio Language Modeling&lt;/h2>
&lt;h3 id="audio-language-modeling-with-musiclm">Audio Language Modeling with MusicLM&lt;/h3>
&lt;p>MusicLM은 AudioLM의 오디오 언어 모델링 접근 방식을 따라, 오디오 합성을 다양한 정밀도의 오디오 토큰을 사용한 언어 모델링으로 본다. 이 모델에서는 오디오를 대표하기 위해 두 가지 토큰화 방식을 사용한다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Semantic Tokenization:&lt;/strong> SSL에서 나온 표현들에 대한 K-means, e.g., w2v-BERT&lt;/li>
&lt;li>&lt;strong>Acoustic Tokenization:&lt;/strong> Neural audio codec, e.g., SoundStream&lt;/li>
&lt;/ul>
&lt;p>AudioLM은 음향 토큰의 계층 구조를 효과적으로 다루기 위해 모델링을 거친 단계와 미세 단계로 나누며, 이를 통해 세 가지 언어 모델링 작업을 정의한다: semantic 모델링, coarse acoustic 모델링, fine acoustic 모델링.&lt;/p>
&lt;p>조건 토큰 시퀀스 $c_1:T_{cnd}$ 와 대상 토큰 시퀀스 $u_1:T_{tgt}$ 를 정의한 후, 각 모델링 작업에서 θ로 매개변수화된 Transformer-decoder는 autoregressive 모델링 문제를 해결하려고 한다.&lt;/p>
&lt;p>$$ p_θ(u_1:T_{tgt} | c_1:T_{cnd}) = \Pi_{j=1}^{T_{tgt}} p_θ (u_j | [c_1, &amp;hellip;, c_{T_{cnd}}, u_1, &amp;hellip;, u_{j−1}]) $$&lt;/p>
&lt;p>AudioLM은 조건 토큰을 대상 토큰에 접두사로 붙이며, semantic 모델링에는 조건이 없고, coarse acoustic 모델링은 semantic 토큰, fine acoustic 모델링은 coarse acoustic 토큰을 조건으로 사용한다. 이 언어 모델들은 실제 토큰으로 병렬 학습이 가능하지만, 추론 시에는 순차적 샘플링이 필요하다.&lt;/p>
&lt;h4 id="joint-tokenization-of-music-and-text-with-mulan-and-rvq">Joint Tokenization of Music and Text with MuLan and RVQ&lt;/h4>
&lt;p>MusicLM은 오디오 전용 학습의 장점을 보존하기 위해, 대규모 음악 데이터와 약하게 연관된 자유 형식 텍스트에 개별적으로 학습 가능한 두 탑 구조의 오디오-텍스트 임베딩 모델인 MuLan에 의존한다. MuLan은 음악 오디오와 텍스트 설명을 같은 임베딩 공간으로 사전 학습시키며, MusicLM에서는 이 임베딩을 residual vector quantization(RVQ)로 토큰화한다.&lt;/p>
&lt;p>MusicLM은 AudioLM과 달리 MuLan 토큰을 접두사로 사용하여 semantic 및 coarse acoustic 모델링을 수행한다. 학습 시, 오디오는 MuLan 음악 타워를 통해 임베딩되고, 이후 RVQ를 통해 MuLan 토큰으로 변환된다. 이 토큰들은 semantic과 coarse acoustic 모델을 조건화한다. 텍스트 프롬프트로 음악을 생성할 때는 MuLan 텍스트 타워에서 얻은 임베딩이 RVQ를 통해 토큰화되며, 이를 바탕으로 고해상도 음악 오디오가 생성된다.&lt;/p>
&lt;hr>
&lt;h2 id="model-description">Model Description&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/melody/images/figure1.png"
width="1096"
height="378"
srcset="https://kurtkim.github.io/p/melody/images/figure1_hu21ed2117a5c252e4733d888f1f008fd4_122756_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/melody/images/figure1_hu21ed2117a5c252e4733d888f1f008fd4_122756_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="695px"
>&lt;/p>
&lt;p>MeLoDy는 representation 학습을 위해 MuLan, Wav2Vec2-Conformer, 오디오 VAE 세 모듈과, semantic 및 음향 모델링을 위해 language model(LM)과 dual-path diffusion(DPD) 모델을 사용한다. 음악의 semantic 구조를 모델링하기 위해 LM을 활용하며, MuLan 모델을 사전 학습하여 조건 토큰을 얻고, semantic 토큰화에는 컨포머 블록을 사용하는 Wav2Vec2-Conformer를 사용한다.&lt;/p>
&lt;h3 id="dual-dath-diffusion-angle-parameterized-continuous-time-latent-diffusion-models">Dual-Dath Diffusion: Angle-Parameterized Continuous-Time Latent Diffusion Models&lt;/h3>
&lt;p>제안된 dual-path diffusion (DPD) 모델은 연속 시간에서의 diffusion probabilistic 모델(DPMs)의 변형으로, 저차원 잠재 표현을 사용하여 DPMs의 계산 부담을 줄인다. 이 모델은 사전 학습된 오토인코더를 활용해 원시 데이터를 잠재 공간에서 재구성한다.&lt;/p>
&lt;p>DPD에서는 가우시안 diffusion 과정 $z_t$를, 연속 미분 가능한 함수 $α_t$와 $σ_t$에 의해 정의한다. 이 과정에서 $α_t = cos(πt/2)$, $σ_t = sin(πt/2)$로 설정함으로써 삼각 함수의 좋은 성질을 활용한다. 이를 통해 $σ_t$는 variational을 유지하며($√1 − α_t^2$), $z_t$는 각도 $δ$를 사용하여 re-parameterized될 수 있다.&lt;/p>
&lt;p>$$ z_δ = cos(δ) z + sin(δ) ϵ \ \text{for any} \ δ ∈ [0, π/2], \ ϵ ∼ N(0, I) $$&lt;/p>
&lt;p>$δ$가 $0$에서 $π/2$로 증가함에 따라 $z_δ$의 노이즈가 증가하는 것이 순방향 diffusion 과정을 정의한다.&lt;/p>
&lt;p>샘플 생성을 위해, θ-parameterized된 variational 모델을 이용하여 diffusion 과정을 역방향으로 실행시킨다. $π/2$를 T 단계로 나누어 $z_{π/2}$에서 $z$를 샘플링한다.&lt;/p>
&lt;p>$$ p_θ (z | z_{π/2}) = \int_{z_{δ_{1:T-1}}} \Pi_{t=1}^{T} p_θ (z_{δ_t − ω_t} | z_{δ_t}) dz_{δ_{1:T−1}}, \ \ δ_t = \begin{cases} {{π}\over{2}} − \sum_{i=t+1}^T ω_i, &amp;amp; 1 \leq t &amp;lt; T \\ {{π}\over{2}}, &amp;amp; t = T \end{cases} $$&lt;/p>
&lt;p>각도 일정은 $ω_1, &amp;hellip;, ω_T$로 표현되며, 합이 $π/2$가 된다. Schneider et al.은 모든 $t$에 대해 $ω_t = π/2T$인 균일 일정을 제안하였다. 샘플링 초반에 큰 단계를 취하고 이후 작은 단계를 취하는 것이 샘플 품질을 개선할 수 있음이 밝혀졌다. 이에 따라, 더 안정적이고 고품질의 결과를 제공하는 새로운 선형 각도 일정을 설계하였다.&lt;/p>
&lt;p>$$ ω_T = {{π}\over{6T}} + {{2πt}\over{3T(T+1 )}} $$&lt;/p>
&lt;h4 id="multi-chunk-velocity-prediction-for-long-context-generation">Multi-Chunk Velocity Prediction for Long-Context Generation&lt;/h4>
&lt;p>모델 학습에서 신경망은 다양한 노이즈 스케일을 가진 $M$개 청크로 이루어진 멀티-청크 타겟 $v_{tgt}$ 예측을 맡는다. 이는 오디오 잠재 변수의 길이 $L$과 잠재 차원 $D$를 포함하는 $z$, $z_δ$, $ϵ$에 기반하여, $v_{tgt}$을 $v_1 ⊕ ··· ⊕ v_M$으로 정의한다.&lt;/p>
&lt;p>$$ v_m := cos(δ_m) ϵ [L_{m−1}: L_m, :] − sin(δ_m) z [L_{m−1}: L_{m}, :], L m := \big\lfloor {{mL}\over{M}} \big\rfloor $$&lt;/p>
&lt;p>NumPy 슬라이싱 구문을 사용해 $m$번째 청크를 찾고, 학습마다 $δ_m ∼ Uniform[0, π/2]$으로 노이즈 스케일을 결정한다. $θ$ 학습엔 [1, 48] 범위의 MSE 손실을 사용한다.&lt;/p>
&lt;p>$$ L_{diff} := \mathbb{E}_{z, ϵ, δ_1, &amp;hellip;, δ_M} \big[ \parallel v_{tgt} − \hat{v}_θ (z_{noisy} ; c) \parallel_2^2 \big], $$&lt;/p>
&lt;p>$$ z_{noisy} := cos(δ_m) z [L_{m−1} : L_m, :] + sin(δ_m) ϵ [L_{m−1}: L_m, :], $$&lt;/p>
&lt;p>MeLoDy에서는 학습 중 SSL 모델로부터 얻은 의미 토큰들과 추론 시 LM에 의해 생성된 토큰들을 사용하여 DPD 모델을 조건화한다. 실험을 통해, 토큰 기반 조건을 사용하여 음악의 의미를 제어하고 diffusion 모델이 각 토큰의 임베딩을 학습하게 함으로써 생성 안정성이 크게 개선됨을 확인하였다. 추가적으로, 멀티-청크 예측 지원을 위해 $M$ 청크의 각도를 나타내는 벡터를 조건에 추가한다.&lt;/p>
&lt;p>$$ c := \lbrace u_1, &amp;hellip;, u_{TST}, δ \rbrace, δ := [δ_1]_{r=1}^{L_1} ⊕ ··· ⊕ [δ_M]_{r=1}^{L_M} \in \mathbb{R}^L $$&lt;/p>
&lt;p>스칼라 $a$를 B번 반복해 B길이 벡터를 만드는 연산을 설명한다. 삼각함수의 정체성을 적용한 DDIM 샘플링을 통해 간소화된 업데이트 규칙을 도출한다.&lt;/p>
&lt;p>$$ z_{δ_t − ω_t} = cos(ω_t) z_{δ_t} − sin(ω_t) \hat{v}_θ (z_{δ_t} ; c), $$&lt;/p>
&lt;p>$t = T$에서 $t = 1$까지 실행하여 $z$ 샘플을 획득한다.&lt;/p>
&lt;h4 id="dual-path-modeling-for-efficient-and-effective-velocity-prediction">Dual-Path Modeling for Efficient and Effective Velocity Prediction&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/melody/images/figure2.png"
width="624"
height="400"
srcset="https://kurtkim.github.io/p/melody/images/figure2_hua658bc6fae800dea1031a80a0fa0e06b_84021_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/melody/images/figure2_hua658bc6fae800dea1031a80a0fa0e06b_84021_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="374px"
>&lt;/p>
&lt;p>잡음이 섞인 잠재 변수와 조건을 통합해 효과적인 속도 예측을 위한 의미 토큰을 사용하는 $\hat{v}_θ$의 방법을 소개하며, 오디오 분리에서 영감을 받은 수정된 이중 경로 기술과 함께 새로운 효율적인 음향 모델링 아키텍처를 제안한다.&lt;/p>
&lt;p>먼저, DPD에서 조건이 어떻게 처리되는지를 설명한다.&lt;/p>
&lt;p>&lt;strong>Encoding Angle Vector&lt;/strong> 잠재 변수의 프레임 노이즈 스케일 $δ ∈ \mathbb{R}^L$을 인코딩할 때, 위치 인코딩 대신 두 학습 가능한 벡터 $e_{start}, e_{end} ∈ \mathbb{R}^{256}$을 이용한 Slerp 유사 구면 보간을 사용한다.&lt;/p>
&lt;p>$$ E_δ := MLP (sin(δ) ⊗ e_{start} + sin(δ) ⊗ e_{end} ) ∈ \mathbb{R}^{L×D_{hid}} , $$&lt;/p>
&lt;p>MLP(x)는 RMSNorm과 GELU 활성화를 통해 입력 $x$를 은닉 차원 $\mathbb{R}^{D_{hid}}$로 변환한다. 여기서 $W_1, W_2, b_1, b_2$는 학습 가능한 parameter이다.&lt;/p>
&lt;p>&lt;strong>Encoding Semantic Tokens&lt;/strong> 의미 정보를 나타내는 이산 토큰들은 벡터의 조회 테이블을 사용해 실수 벡터로 변환된다. Wav2Vec2-Conformer의 클러스터 수에 해당하는 어휘 크기를 기반으로 한다. 이 토큰들을 시간 축에 따라 쌓고 MLP를 적용하여 최종적으로 $E_{st} ∈ \mathbb{R}^{T_{ST} \times D_{hid}}$를 얻는다.&lt;/p>
&lt;p>조건 임베딩이 주어졌을 때, $z_{noisy}$(학습 시) 또는 $z_{δ_t}$(추론 시) 입력은 먼저 선형 변환 및 각도 임베딩과의 합을 거쳐 처리된다. 이후 이중 경로 처리를 위한 분할이 이루어진다.&lt;/p>
&lt;p>&lt;strong>Segmentation&lt;/strong> 분할 모듈은 2차원 입력을 $K$ 길이의 $S$개 반중복 세그먼트로 나누어 3차원 텐서 $H$로 표현한다. 이는 시퀀스 처리 길이를 sub-linear(O(√L))로 줄여 전체 시퀀스 처리의 학습 난이도를 감소시키고, MeLoDy가 더 높은 빈도의 잠재 요소를 활용할 수 있게 한다.&lt;/p>
&lt;p>&lt;strong>Dual-Path Blocks&lt;/strong> 분할 후, $N$개의 이중 경로 블록을 위한 3차원 텐서 입력을 얻는다. 각 블록은 세그먼트 간 처리를 위한 거친 경로와 세그먼트 내 처리를 위한 세밀한 경로의 두 단계를 포함한다. 거친 경로 처리에는 Roformer 네트워크를, 세밀한 경로 처리에는 bi-directional RNN을 사용한다. 이 과정은 오디오 구조의 세밀한 세부 사항을 더 잘 재구성하기 위함이다. 각 처리 단계는 self-attention 및 cross-attention layer, 그리고 feature-wise linear modulation(FiLM)를 통해 노이즈 제거를 개선한다.&lt;/p>
&lt;p>&lt;strong>Coarse-Path Processing&lt;/strong> 이중 경로 블록에서는 거친 경로를 먼저 병렬로 처리한다.&lt;/p>
&lt;p>$$ \mathbb{H}_{c-out}^{(i)} := \text{RepeatSegments} \big( \big[ \text{Roformer} \big( \text{MergeSegments} \big( \mathbb{H}^{(i)} \big) [:, k, :] \big), k = 0, &amp;hellip;, K_{MS}^{(i)} − 1 \big] \big) $$&lt;/p>
&lt;p>이중 경로 블록에서 거친 경로 출력은 $\mathbb{H}^{(i)}$와 동일한 형태를 가지며, 세그먼트를 압축하고 확장하는 MergeSegments와 RepeatSegments 연산을 통해 세그먼트 간 정보를 집계한다. 병합은 열을 평균내어 수행되고, $K_{MS}^{(i)}$ 정의는 블록 인덱스에 따라 텐서의 폭을 변화시킨다. 이는 중간 블록에서 가장 짧고 양 끝에서 가장 긴 세그먼트를 생성된다. 원래 길이를 유지하기 위해 Roformer에서 도입된 반복 연산이 사용된다.&lt;/p>
&lt;p>&lt;strong>Fine-Path Processing&lt;/strong> fine-path input $\mathbb{H}_{f-in}^{(i)}$은 RMSNorm $\big( \mathbb{H}^{(i)} + \mathbb{H}_{c-out}^{(i)} \big)$로 얻어지며, 행을 병렬로 처리하는 두 층의 SRU에 입력된다.&lt;/p>
&lt;p>$$ \mathbb{H}_{f-out}^{(i)} := \big[ \text{FiLM} \big( \text{SRU} \big( \mathbb{H}_{f-in}^{(i)} [s, :, :] \big), E_δ \big[ {{sL}\over{S}}, : \big] + {{1}\over{T_{ST}}} \sum_{t=0}^{T_{ST} - 1} E_{ST} [t, :] \big), s=0, &amp;hellip;, S - 1 \big] $$&lt;/p>
&lt;p>FiLM(x, m)은 입력 $x$와 변조 조건 $m$에 대해 브로드캐스트 곱셈을 적용하는 함수이다. 다음 이중 경로 블록의 입력은 $\mathbb{H}^{(i+1)} := RMSNorm( \mathbb{H}_{f-in}^{(i)} + \mathbb{H}_{f-out}^{(i)})$으로 정의되며, $N$개의 이중 경로 블록을 처리한 후 3차원 텐서는 중첩-더하기 방식으로 2차원 행렬로 변환된다. 최종적으로, 예측된 속도가 얻어진다.&lt;/p>
&lt;p>$$ \hat{v}_θ (z_{noisy}; c) := \text{RMSNorm} \big( \text{OverlapAdd} \big( \mathbb{H}^{(N+1)}) \big) \big) W_{out} $$&lt;/p>
&lt;p>$W_{out} ∈ \mathbb{R}^{D_{hid} × D}$는 학습 가능한 parameter이다.&lt;/p>
&lt;h3 id="audio-vae-gans-for-latent-representation-learning">Audio VAE-GANs for Latent Representation Learning&lt;/h3>
&lt;p>Rombach et al. 은 latent diffusion 모델(LDMs)을 위한 KL-regularized 이미지 오토인코더를 통해 고품질 이미지 생성의 안정성을 입증하였다. 이 오토인코더는 VAE와 유사하게 KL 패널티를 부과하지만, GAN처럼 적대적으로 학습된다. 이를 VAE-GAN이라 부르며, 이미지 생성에 유망하지만 오디오 파형에 대한 성공적인 방법은 부족하다. 이 연구에서는 DPD 모델에 적용 시 현저한 안정성을 보인 오디오 VAE-GAN을 제안한다.&lt;/p>
&lt;p>오디오 VAE-GAN은 24kHz 오디오 재구성을 위해 스트라이딩 요소 96을 사용하여 250Hz 잠재 시퀀스를 생성한다. 디코더는 HiFi-GAN 구조를 따르고, 인코더는 업샘플링 대신 컨볼루션 기반 다운샘플링을 적용한다. adversarial 학습에는 다기간 및 다해상도 스펙트로그램 discriminator를 사용한다. diffusion 모델의 정상 범위와 일치시키기 위해 인코더 출력은 [-1, 1] 범위로 매핑되며, 이는 극단적인 값을 0.1% 미만으로 걸러내는 역할을 한다.&lt;/p>
&lt;h3 id="music-inpainting-music-continuation-and-music-prompting-with-melody">Music Inpainting, Music Continuation and Music Prompting with MeLoDy&lt;/h3>
&lt;p>제안된 MeLoDy는 임의의 노이즈 조작을 통해 오디오 inpainting(interpolation)과 오디오 continuation(extrapolation)을 지원한다. diffusion 모델이 오디오 inpainting에는 성공적이었지만, 비자기회귀적 특성 때문에 오디오 continuation에는 어려움이 있었다. MeLoDy는 또한 MuLan을 기반으로 비슷한 스타일의 음악을 생성하는 음악 프롬프트 기능을 제공한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="experimental-setup">Experimental Setup&lt;/h3>
&lt;p>&lt;strong>Data Preparation&lt;/strong> MeLoDy는 보컬 없는 음악에 초점을 맞춘 257k시간의 음악 데이터로 학습되었으며, ChatGPT를 사용하여 태그 기반의 텍스트를 음악 캡션으로 확장하였다. 이는 195.3M MuLan 학습에 활용되어, 오디오마다 캡션 또는 태그를 무작위로 연결함으로써 모델의 자유 형태 텍스트 처리 능력을 향상시켰다.&lt;/p>
&lt;p>&lt;strong>Semantic LM&lt;/strong> 429.5M LLaMA 모델을 사용하여 음악LM과 유사한 parameter를 가진 의미론적 모델링을 진행하였다. MuLan RVQ를 통해 12개의 접두사 토큰을 생성하였고, 199.5M Wav2Vec2-Conformer에서 얻은 25Hz 임베딩을 1024-중심 k-means로 이산화하여 10초 길이의 의미론적 토큰을 학습 목표로 설정하였다.&lt;/p>
&lt;p>&lt;strong>Dual-Path Diffusion&lt;/strong> DPD 모델은 숨겨진 차원 768, 블록 수 8로 설정해 총 296.6M parameter를 가진다. 10초 길이 입력을 L=2500으로 나누어 M=4 부분으로 청킹하고, K=64 크기의 세그먼트로 S=80 세그먼트를 생성하였다. 샘플과 조건 간 일치를 개선하기 위해 분류자 없는 지도를 적용하고, 학습 중에는 교차 주의를 자기 주의로 0.1 확률로 대체한다. 샘플링은 예측 속도를 선형 결합하며, 모든 생성에는 2.5 스케일의 분류자 없는 지도가 사용된다.&lt;/p>
&lt;p>&lt;strong>Audio VAE-GAN&lt;/strong> 오디오 VAE-GAN은 96의 홉 사이즈로 24kHz 음악 오디오를 250Hz의 잠재 시퀀스로 인코딩하였다. 잠재 차원 D는 16이며, 총 압축율은 6배입니다. 인코더는 256개의 숨겨진 채널을, 디코더는 768개를 사용한다. 전체적으로, 오디오 VAE-GAN은 100.1M의 parameter를 포함한다.&lt;/p>
&lt;h3 id="performance-analysis">Performance Analysis&lt;/h3>
&lt;p>&lt;strong>Objective Metrics&lt;/strong> 생성된 오디오와 MusicCaps 참조 오디오 사이의 Frećhet audio distance(FAD)를 통해 생성 품질을 대략적으로 측정하고, 사전 학습된 MuLan을 사용한 MuLan cycle consistency(MCC)으로 텍스트와 오디오의 상관성을 평가한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/melody/images/table2.png"
width="906"
height="204"
srcset="https://kurtkim.github.io/p/melody/images/table2_hu80114c53cec779aae89ce48bcc5dc851_49895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/melody/images/table2_hu80114c53cec779aae89ce48bcc5dc851_49895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="444"
data-flex-basis="1065px"
>&lt;/p>
&lt;p>&lt;strong>Inference Speed&lt;/strong> 제안한 MeLoDy의 샘플링 효율성을 평가한 결과, 단 5개의 샘플링 단계로도 참조 세트보다 높은 MCC 점수를 달성하였다. 이는 생성된 샘플이 MusicCaps 캡션과 더 관련이 깊고, 제안된 DPD가 기존 LMs보다 훨씬 낮은 비용으로 효과적으로 작동함을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/melody/images/table3.png"
width="1058"
height="214"
srcset="https://kurtkim.github.io/p/melody/images/table3_hud4d43342172c7419f7a00edbceb6082d_56892_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/melody/images/table3_hud4d43342172c7419f7a00edbceb6082d_56892_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="494"
data-flex-basis="1186px"
>&lt;/p>
&lt;p>&lt;strong>Comparisons with SOTA models&lt;/strong> MeLoDy는 대규모 음악 데이터셋에서 학습된 MusicLM과 Noise2Music과 비교하여 평가되었다. 동일한 텍스트 프롬프트를 사용하여 7명의 음악 프로듀서가 음악성, 오디오 품질, 텍스트 연관성 측면에서 평가하였다. 총 777번의 비교를 통해 1,554개의 평가가 수집되었다. 결과적으로, MeLoDy는 음악성과 텍스트 연관성에서 MusicLM과 Noise2Music과 유사한 성능을 보였으며, 오디오 품질에서는 두 모델을 모두 능가하였다(p &amp;lt; 0.05, p &amp;lt; 0.01). 또한, MeLoDy는 MusicLM과 Noise2Music에 비해 훨씬 적은 NFEs를 사용하여 효율성을 입증하였다.&lt;/p>
&lt;p>&lt;strong>Diversity Analysis&lt;/strong> diffusion 모델의 특징인 높은 다양성을 검증하기 위해, MeLoDy는 감정이나 시나리오 등의 텍스트 프롬프트에 기반한 생성 다양성과 유효성을 평가하는 추가 실험을 수행하였다. 데모 페이지에 공개된 샘플 결과에서는 다양한 악기와 질감의 조합을 확인할 수 있었다.&lt;/p>
&lt;p>&lt;strong>Ablation Studies&lt;/strong> 제안된 방법의 두 측면에 대한 연구에서, 적은 샘플링 단계에서 음향 문제를 줄이는 효과적인 일정을 제안했고, dual-path 구조가 signal-to-noise ratio(SNR) 개선에서 다른 LDM 구조보다 우수함을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>&lt;strong>Limitation&lt;/strong> 비자연적인 보컬 방지를 위해 주로 무보컬 음악을 포함한 학습 데이터로 인해 프롬프트 범위가 제한되며, 학습 말뭉치는 팝과 클래식에 약간 편향되어 있다. 또한, 10초 세그먼트로 학습함으로써 긴 생성물의 다이내믹스가 제한된다.&lt;/p>
&lt;p>&lt;strong>Broader Impact&lt;/strong> 음악 프로듀서, 콘텐츠 크리에이터, 일반 사용자 모두가 낮은 진입 장벽으로 창의력을 자유롭게 표현할 수 있는 음악 창작 도구로 큰 가능성을 지닌다고 믿는다. MeLoDy는 인간의 피드백을 반영하는 상호작용적 창작을 지원하며, LoRA 기술을 통해 음악 스타일의 정밀 조정이 가능하다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2305.15719" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://efficient-melody.github.io/" target="_blank" rel="noopener"
>Demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MBD</title><link>https://kurtkim.github.io/p/mbd/</link><pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/mbd/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 연구에서는 저비트율의 이산 표현에서 다양한 오디오 모달리티(예: 음성, 음악, 환경 소리)를 생성하는 고해상도 Multi-Band Diffusion 기반 프레임워크를 제안한다. 이 방법은 기존의 생성 모델이 완벽하지 않은 조건에서 audible artifact를 생성하는 문제를 해결하려고 한다. 제안된 접근법은 동일한 비트율에서 지각 품질 면에서 state-of-the-art의 생성 기술을 능가한다고 주장한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>neural-based vocoder는 최신 neural network 아키텍처를 활용해 자연스러운 음색과 억양의 고품질 음성을 생성하는 데 뛰어난 성과를 보여주었다.&lt;/p>
&lt;p>Self-Supervised Learning(SSL)은 음성 데이터에 적용되어 감정과 억양 등의 맥락적 정보를 담은 표현을 생성하며, 이로부터 파형 오디오를 만드는 것이 새로운 연구 주제가 되었다. 이 과정은 SSL을 사용해 오디오 표현을 학습하고, 그 후 GAN 접근법으로 음성을 디코딩하는 두 단계로 이루어진다. 이 방법들은 뛰어난 성능을 보이지만, 불안정하고 학습하기 어렵다는 단점이 있다.&lt;/p>
&lt;p>압축 모델은 복원 손실을 활용하여 데이터의 의미 있는 표현을 학습하는 SSL 모델로 볼 수 있다. 이 모델들은 오디오 표현과 합성을 동시에 학습하는 과정에서 다양한 오디오 도메인을 모델링할 수 있다. 모델은 스펙트로그램 매칭, 특성 매칭, 그리고 다양한 적대적 손실 등 복잡하게 조합된 목표를 통해 최적화된다. 하지만 매우 낮은 비트율에서는 메탈릭한 목소리나 왜곡 같은 눈에 띄는 아티팩트가 추가될 수 있다.&lt;/p>
&lt;p>모델 최적화 이후 학습된 표현은 다양한 오디오 작업에 활용될 수 있다. Kreuk et al. 은 텍스트를 통한 오디오 생성을, Wang et al. 은 zero-shot 텍스트-음성 변환을 제안하였다. 또한, Agostinelli et al. 은 텍스트-음악 생성에, Hsu et al. 은 조용한 비디오에서 음성 생성에 이 표현을 적용하였다.&lt;/p>
&lt;p>이 연구에서는 MULTI-BAND DIFFUSION(MBD)이라는 새로운 diffusion 기반 방법을 제시하였다. 이 방법은 이산 압축 표현에서 음성, 음악, 환경 소리 등의 고품질 오디오 샘플을 생성할 수 있다. 이 방법은 다양한 작업과 오디오 도메인에 적용 가능하며, 전통적인 GAN 기반 decoder를 대체할 수 있다. 결과적으로, 이 방법은 평가된 기준선을 크게 웃돌아 성능을 보였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/figure1.png"
loading="lazy"
>&lt;/p>
&lt;p>&lt;strong>Our Contributions:&lt;/strong> 오디오 합성을 위한 새로운 diffusion 기반 모델을 제안한다. 이 모델은 각각의 주파수 대역을 독립적으로 처리하는 diffusion 모델, 주파수 이퀄라이저, 그리고 풍부한 고조파를 가진 오디오 데이터를 위한 파워 노이즈 스케줄러를 포함한다. 이 방법은 객관적 지표와 인간 연구를 통해 최첨단 GAN과 diffusion 기반 접근법에 비해 더 효율적임을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>neural audio synthesis은 웨이브넷과 같은 autoregressive 모델로 시작되었으나, 이는 학습이 느리고 어려운 단점이 있다. 음성 합성 분야에서는, mel-spectrogram에 기반한 다양한 방식, 특히 GAN 기반의 HiFi-GAN 같은 모델이 탐색되었다. 최근에는 HiFi-GAN을 사용해, HuBERT, VQ-VAE, CPC 같은 self-supervise 방법으로 학습한 저 비트율 표현과 함께 기본 주파수와 스피커 정보를 조합하여, 스피커와 기본 주파수로부터 독립적인 제어 가능한 음성을 생성하는 연구가 이루어졌다.&lt;/p>
&lt;p>diffusion-based vocoder는 이미지 생성에서의 diffusion 성공에 영감을 받아 개발되었다. Diffwave는 기본 diffusion 방정식을 오디오에 적용하며, PriorGrad는 조건부 mel-spectrogram의 에너지를 사용해 사전 노이즈 분포를 조정하는 Diffwave의 확장이다. Wavegrad는 연속적인 노이즈 수준에 조건을 사용한다. Takahashi et al. 은 노래하는 목소리의 복잡한 분포를 다루며, 계층적 모델로 고품질 오디오를 생성한다. 최근 연구는 diffusion을 사용해 고해상도 오디오를 생성하지만, 아직 오디오 모달리티 범위가 좁다.&lt;/p>
&lt;p>대부분의 diffusion 모델은 복잡한 데이터를 샘플링하기 위해 업샘플링을 사용하지만, 이 과정은 병렬 처리가 불가능하다. 최근, SimpleDiffusion이라는 연구에서는 단일 모델을 이용해 복잡한 diffusion 과정을 단순화하면서도, 낮은 주파수에 집중하여 고품질의 결과를 얻는 방법을 제안하였다. 그러나 이 아이디어는 아직 오디오 처리 분야에는 적용되지 않았다.&lt;/p>
&lt;p>이 연구는 SoundStream과 EnCodec 같은 adversarial neural audio codec에 대한 대체 방안을 제시한다. 이는 다양한 손실 조합으로 학습된 encoder, quantizer, decoder 구조를 갖추고 있지만, diffusion 기반 decoder는 더 높은 품질의 오디오 생성을 주관적 평가를 통해 달성한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;h3 id="background">Background&lt;/h3>
&lt;p>Ho et al. (2020)의 연구에 따르면, Markov chain을 사용한 diffusion 과정에서 깨끗한 데이터 $x_0$에 점진적으로 Gaussian noise를 추가해, 결국 standard Gaussian noise에 가까운 noise가 섞인 데이터 $x_T$를 생성한다. 이 과정의 확률이 다음과 같이 정의된다:&lt;/p>
&lt;p>$$ q(x_{0:\gamma} | x_0) = \Pi_{t=1}^T q (x_t | x_{t-1}) $$&lt;/p>
&lt;p>$q(x_t | x_{t-1})$는 가우시안 분포를 따르며, $\beta_t$는 noise schedule을 나타낸다. 이를 통해 Markov chain의 어떤 단계도 효율적으로 샘플링할 수 있다.&lt;/p>
&lt;p>$$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $$&lt;/p>
&lt;p>$\bar{\alpha}_t$는 잡음 수준을 나타내고, DDPM은 잡음이 섞인 데이터 $x_T$에서 깨끗한 데이터 $x_0$로 복원하는 것을 목표로 한다.&lt;/p>
&lt;p>$$ p(x_{\gamma : 0}) = p(x_{\gamma}) \Pi_{t=1}^T p_{\theta} (x_{t-1} | x_t) $$&lt;/p>
&lt;p>$p_\theta(x_t | x_{t+1})$는 diffusion chain을 역으로 하는 학습된 분포이고, $p(x_T)$는 학습되지 않은 사전 분포이다. 이상적인 잡음 조건에서 사전 분포는 $N(0, I)$로 근사할 수 있다.&lt;/p>
&lt;p>Ho et al. (2020)에 따르면, $p_\theta(x_{t-1} | x_t)$ 분포는 $N(\mu_\theta(x_t, t), \sigma_t I)$로 나타낼 수 있으며, $\mu_\theta$는 reparameterize가 가능하다.&lt;/p>
&lt;p>$$ \mu_{\theta} (x_t, t) = {{1}\over{\sqrt{1 - \beta_t}}} \big( x_t - {{\beta}\over{\sqrt{1 - \bar{\alpha}&lt;em>t}}} \epsilon&lt;/em>{\theta} (x_t, t) \big) $$&lt;/p>
&lt;p>이 reparametrization를 통해 신경망 $\epsilon_\theta$는 오염된 데이터 $x_t$에서 잡음을 예측하도록 학습된다. Ho et al. (2020)의 방법에 따라, $x_t$ 샘플링 후 L2 손실을 최적화하여 신경망을 학습할 수 있다.&lt;/p>
&lt;p>$$ L = \mathbb{E}_{x_0 \sim d(x_0), \epsilon \sim \mathcal{N}(0,I), t \sim \mathcal{U}\lbrace 1, \ldots, T \rbrace} ( \Vert \epsilon - \epsilon\theta\left(\sqrt{x_0} + \sqrt{1-t}\right) \Vert^2 ) $$&lt;/p>
&lt;p>이러한 모델을 사용하면, 다음 방정식을 사용하여 diffusion 과정을 반복적으로 역전할 수 있다:&lt;/p>
&lt;p>$$ x_{t-1} = {{1}\over{\sqrt{1 - \beta_t}}} \big( x_i - {{\beta_t}\over{\sqrt{1 - \bar{\alpha}_t}}} \epsilon_{\theta} (x_t, t) \big) + \sqrt{\sigma_t} \epsilon $$&lt;/p>
&lt;p>여기서 $\sigma$는 $\tilde{\beta}t = (1 - \bar{\alpha}{t-1})/(1 - \bar{\alpha}_t) \beta_t$와 $\beta_t$ 사이에서 결정해야 하는 parameter이며, 이 실험에서는 $\sigma_t = \beta_t$로 설정한다.&lt;/p>
&lt;h3 id="multi-band-diffusion">Multi-Band Diffusion&lt;/h3>
&lt;p>Multi-Band Diffusion 방법은 Frequency Eq. Processor, Scheduler Tuning, Band-Specific Training의 세 가지 핵심 요소로 구성된다.&lt;/p>
&lt;p>&lt;strong>Frequency Eq. Processor&lt;/strong> diffusion 과정 이론은 모든 종류의 분포에서 샘플링을 가능하게 하지만, waveform 도메인의 다양한 오디오 모달리티를 위한 diffusion 네트워크 학습은 아직 해결되지 않은 문제이다. 다른 주파수 밴드에서 에너지 레벨의 균형이 효율적인 샘플링에 중요하다고 가정한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/figure2.png"
loading="lazy"
>&lt;/p>
&lt;p>white Gaussian noise는 모든 주파수에서 동등한 에너지를 가지지만, 자연 소리는(예: 음악, 연설) 다른 분포를 보이며, 특히 높은 주파수에서 더 많은 에너지를 가진다. 이로 인해 diffusion 과정에서 고주파수 내용이 저주파수보다 먼저 사라지고, 역 과정에서 고주파수에 더 큰 영향을 받게 된다.&lt;/p>
&lt;p>이 문제를 해결하기 위해 멜 스케일을 기반으로 한 밴드 패스 필터를 사용하여 깨끗한 신호 $x_0$를 여러 주파수 밴드로 나누고, 각 밴드 $b_i$의 에너지를 정규화합니다.&lt;/p>
&lt;p>$$ \hat{b}_i = b_i \cdot \big( {{\sigma_i^{\epsilon}\over{\sigma_i^d}}} \big)^p $$&lt;/p>
&lt;p>$\sigma_i^{\epsilon}$과 $\sigma_i^d$는 standard Gaussian noise와 데이터셋 신호의 밴드 $i$ 에너지를 나타내며, 매개변수 $ρ$로 에너지 수준 조정을 제어한다($ρ=0$은 조정 없음, $ρ=1$은 완전 일치). 고주파수 밴드의 instability를 피하기 위해, 음악 도메인에서 $\sigma_i^d$를 계산한다.&lt;/p>
&lt;p>&lt;strong>Scheduler Tuning.&lt;/strong> 노이즈 스케줄은 diffusion 모델의 품질을 결정하는 핵심 hyperparameter이다.&lt;/p>
&lt;p>raw waveform 생성에는 주로 linear 또는 cosine 스케줄이 사용되지만, 고샘플링 레이트에서는 성능이 떨어진다는 것을 발견하였다. 따라서, 이 연구에서는 더 급진적인 p-power 스케줄 사용을 제안한다.&lt;/p>
&lt;p>$$ \beta_t = \big( \sqrt[p]{\beta_0} + {{t}\over{T}} ( \sqrt[p]{\beta_T} - \sqrt[p]{\beta_0} ) \big) $$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/figure3.png"
loading="lazy"
>&lt;/p>
&lt;p>학습 중 주입되는 노이즈의 분산($\beta_0$과 $\beta_T$)은 중요한 hyperparameter이다. 생성 시 노이즈 스케줄을 학습 후에 결정할 수 있음에도 불구하고, 실제로 학습 노이즈 스케줄은 diffusion 모델에 있어 중요한 역할을 한다. 이 스케줄은 학습 예제에 주입되는 노이즈 레벨을 결정하며, 제안된 파워 스케줄을 사용하면 대부분의 예제에 매우 적은 양의 노이즈를 주입하게 된다.&lt;/p>
&lt;p>diffusion 과정의 마지막 단계에서, 모델이 추정하는 노이즈가 실제 데이터보다 못한 경우가 종종 발생한다. 이는 학습의 제한된 정밀도 때문이라고 추정된다. 이 문제를 해결하기 위해, 해당 시간 단계를 건너뛰는 것과 같은 효과를 내기 위해 모델을 정체 함수로 대체하고, 이 현상을 방지하기 위해 $\beta_t$ 값을 조정하여 $\sqrt{1-\alpha_t}$ 값을 충분히 크게 한다.&lt;/p>
&lt;p>&lt;strong>Band-Specific Training.&lt;/strong> audio diffusion 모델은 낮은 주파수를 먼저 생성하고, 역 과정의 마지막에서 고주파수를 처리한다. 오디오 데이터는 시간과 주파수에 걸쳐 복잡하게 얽혀 있어, 전대역 오디오 데이터를 사용한 학습은 고주파수 생성 시 항상 정확한 낮은 주파수를 제공한다. 하지만, 이 방식은 생성 초기의 오류를 역 과정에서 증폭시키는 문제를 가지고 있다.&lt;/p>
&lt;p>각 주파수 대역을 독립적으로 학습시키는 멀티밴드 확산 방식을 제안하였다. 이 접근법은 샘플의 지각 품질을 크게 향상시켰으며, 모델 채널에 따른 주파수 대역 분할은 같은 결과를 내지 못했다. 이는 학습 시 이전에 생성된 내용을 모델에 제공하지 않음으로써 샘플링 오류 누적을 방지할 수 있다는 우리의 가설을 확인시켜 준다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental Setup&lt;/h2>
&lt;h3 id="model--hyperparameters">Model &amp;amp; Hyperparameters&lt;/h3>
&lt;p>&lt;strong>Overview.&lt;/strong> 이 접근법은 EnCodec decoder를 대체하며, 필요에 따라 품질과 속도 사이에서 원본과 diffusion decoder를 자유롭게 전환할 수 있는 유연성을 제공한다.&lt;/p>
&lt;p>&lt;strong>Architecture.&lt;/strong> Chen et al., Kong et al., Lee et al.의 연구에 이어 Ronneberger et al.이 제안한 대칭형 U-net 네트워크를 사용하고, Défossez et al.의 두 residual block과 stride 4의 downsampling/upsampling block을 적용하였다. input audio conditioning과 timestep $t$는 네트워크 병목에 통합되고, 고차원 데이터 확산시 병목 부근에 계산 자원을 집중하는 것이 좋다고 Hoogeboom et al.이 권장한다. 이에 따라 growth rate를 4로 설정했으며, 모델의 크기는 1GB이다.&lt;/p>
&lt;p>&lt;strong>Input Conditioning.&lt;/strong> 공개된 24kHz EnCodec 모델의 latent representation을 사용하며, 이는 학습 동안 고정된다. 임베딩 시퀀스는 UNet 병목 차원에 맞게 linear interpolation으로 upsample 된다. 실험에는 1.5kbps, 3kbps, 6kbps 비트레이트에 해당하는 EnCodec 코드북 1, 2, 4를 사용한 재구성이 포함되며, 여러 코드북 사용 시 임베딩은 코드북들의 평균으로 계산된다.&lt;/p>
&lt;p>&lt;strong>Schedule.&lt;/strong> 제안된 power schedule로 diffusion 모델을 학습시켰다. 이때 파워 $p=7.5$, 초기 $\beta_0=1.0e−5$, 최종 $\beta_T=2.9e−2$를 사용하였다. 생성 시에는 20단계, 학습 시에는 1000단계를 사용하는 것이 모델의 다양성 증가와 다양한 노이즈 수준에서의 학습 가능성 때문에 유익하다는 것을 발견했다. 실험에서는 가장 간단한 시간 단계 하위 샘플링 방식 $S = \lbrace i * {{1000}\over{N}}, i \in \lbrace 0, 1, &amp;hellip;, N \rbrace \rbrace$ 을 사용, 여기서 $N$은 샘플링 단계 수(기본값 20)이다.&lt;/p>
&lt;p>&lt;strong>Frequency EQ processor.&lt;/strong> 실험에서 $ρ = 0.4$ 값을 가진 8개 멜 스케일 주파수 밴드를 활용하며, 내부 음악 데이터셋을 통해 해당 밴드 값들을 계산한다.&lt;/p>
&lt;p>&lt;strong>Band Splitting.&lt;/strong> 별개의 diffusion 과정을 사용하며, julius로 멜 스케일 기반 4개 주파수 밴드를 균등 분할한다. 이 밴드들은 프로세서와 무관하며, 모든 모델은 같은 hyperparameter, schedule, conditioning input EnCodec 토큰을 공유한다.&lt;/p>
&lt;p>&lt;strong>Training.&lt;/strong> Adam optimizer, batch size 128, learning rate 1e-4로 모델 학습. 16GB Nvidia V100 4개로 한 모델 학습에 2일 소요된다.&lt;/p>
&lt;p>&lt;strong>Computational cost and model size.&lt;/strong> diffusion 모델 샘플링의 비용은 생성을 위한 모델 패스 수에 의해 발생한다.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>다양한 도메인에서 학습을 진행한다. Common Voice 7.0(9096시간)과 DNS 챌린지 4(2425시간)로 음성 데이터를, MTG-Jamendo(919시간)로 음악 데이터를, FSD50K(108시간)와 AudioSet(4989시간)으로 환경 소리 데이터를 사용한다. AudioSet은 연구 재현을 위해서만 사용되며, 평가에는 내부 음악 데이터셋 샘플을 활용한다.&lt;/p>
&lt;h3 id="evaluation-metrics">Evaluation Metrics&lt;/h3>
&lt;p>&lt;strong>Human evaluation.&lt;/strong> 인간 연구에 MUSHRA 프로토콜을 적용해, 숨겨진 참조와 낮은 앵커를 사용한다. 크라우드 소싱을 통해 모집된 평가자들은 제공된 샘플의 품질을 1에서 100 사이로 평가하였다. 테스트 세트의 각 카테고리에서 무작위로 선정된 5초 길이의 50개 샘플에 대해 샘플 당 최소 10개의 평가를 받았다. 잡음이 많은 평가와 이상치를 제거하기 위해, 참조 녹음을 20% 이상의 경우에 90 미만으로, 낮은 앵커 녹음을 50% 이상의 경우에 80 이상으로 평가한 평가자들을 제외하였다.&lt;/p>
&lt;p>&lt;strong>Objective metrics.&lt;/strong> 두 가지 자동 평가 방법을 사용한다. 첫 번째는 ViSQOL 메트릭이고, 두 번째는 복원된 신호의 멜-스펙트로그램 충실도를 새로운 메트릭으로 측정하는 방법이다. 이를 위해, 참조 파형 신호와 복원된 신호를 정규화하고, $M$ 멜과 $H$ 홉 길이를 사용해 멜-스펙트로그램을 계산한다.&lt;/p>
&lt;p>$$ z = mel \big[ {{x}\over{\epsilon + \sqrt{ \langle x^2 \rangle}}} \big], \ \text{and} \ \hat{z} = mel \big[ {{\hat{x}}\over{\epsilon + \sqrt{ \langle x^2 \rangle}}} \big] $$&lt;/p>
&lt;p>멜-스펙트로그램의 왜곡을 분석하기 위해, 우리는 신호 대 잡음비(SNR)를 각 시간 단계와 주파수 빈에서 계산한다. 계산 시 -25dB와 +25dB 사이로 SNR 값을 제한하여 수치적 불안정성과 기준 멜-스펙트로그램의 거의 0에 가까운 값들로 인한 과도한 영향을 방지한다. 이는 신경망의 계산과 학습의 제한된 정밀도로 인해 완전히 0의 에너지 수준을 출력하는 것이 어렵다는 점을 고려한 것이다.&lt;/p>
&lt;p>$$ s = clamp [10 \cdot (log 10 (z) − log 10 (δ))., \ −25dB, +25dB] $$&lt;/p>
&lt;p>시간 단계별로 평균을 내고 멜 스케일 밴드를 3등분하여 저, 중, 고주파수의 멜-SNR(L, M, H)을 산출한다. 모든 밴드의 평균은 Mel-SNR-A로 보고된다. 24kHz에서는 512샘플 프레임에 대해 STFT를 사용, 홉 길이는 128, 멜 밴드는 80개이다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;h3 id="multi-modalities-model">Multi modalities model&lt;/h3>
&lt;p>압축 작업에서 EnCodec과 비교해 diffusion 방식의 성능을 검토합니다. EnCodec encoder로 오디오 샘플에서 토큰을 추출하고, Multi-Band Diffusion과 원본 decoder로 디코딩한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/table1.png"
loading="lazy"
>&lt;/p>
&lt;p>DNS에서 깨끗한 음성, 손상된 음성, Jamendo와 내부 음악 데이터셋에서 각각 음악 샘플 50개씩, 총 4가지 부분집합에 대해 주관적 평가를 실시하였다. 모든 음성 샘플은 DNS 챌린지의 방 임펄스 응답을 이용해 확률 0.2로 울림효과를 부여받는다. 6kbps, 3kbps, 1.5kbps 비트율에서 세 가지 주관적 연구 결과를 제공하며, 평가는 상대적으로 이루어져 연구 간 비교는 불가하다. 6kbps에서 저품질 앵커와 지상 진실 샘플로 Opus를 포함시켰고, EnCodec과의 비교는 모델 크기가 결과에 제한적이지 않음을 명시한다.&lt;/p>
&lt;p>Multi-Band Diffusion 방법은 음성 압축에서 EnCodec보다 최대 30% 더 우수한 성능을 보이고, 음악 데이터에서는 EnCodec과 비슷한 수준이다. 전체적으로, 모든 비트율에서 EnCodec보다 우수하다. GAN 기반 방법이 금속성 소리를 만들 수 있지만, diffusion 방법은 더 자연스러운 고주파 내용을 제공한다.&lt;/p>
&lt;p>같은 데이터로 훈련된 HifiGAN과 PriorGrad와 이 연구의 모델을 비교한다. 이때 각 모델의 원본 논문에 제시된 hyperparameter를 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/table2.png"
loading="lazy"
>&lt;/p>
&lt;p>후반부에서는 EnCodec을 사용하지 않는 다른 종단간 오디오 코덱들과 비교를 추가한다. 이 중에는 24kHz에서 6kbps로 운영되는 DAC의 사전 학습된 모델이 포함된다. EnCodec + Multi-Band Diffusion이 다른 양자화 방식을 사용하는 DAC와 비슷한 수준이라고 보여준다. Multi-Band Diffusion을 DAC의 오디오 토큰으로 학습시키면 오디오 품질이 더 향상될 것으로 예상된다.&lt;/p>
&lt;h3 id="ablations">Ablations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/images/table3.png"
loading="lazy"
>&lt;/p>
&lt;p>이 연구에서는 ViQOL 점수와 Mel-SNR을 사용하여 다양한 모달리티에서 150개 샘플의 복원 성능을 객관적으로 비교하였다. 연구 결과, EnCodec 방법이 객관적 지표에서는 우수한 성능을 보였지만, 주관적 평가에서는 다소 낮은 성능을 보여주었다. 반면, diffusion 기반 방법은 더 자연스러운 오디오 생성을 가능하게 하며, 생성적 작업에 있어서 선호되는 방법으로 주장된다. 이러한 차이는 각각의 방법이 콘텐츠 복원을 위해 특화되어 있기 때문에 발생한다.&lt;/p>
&lt;p>본 논문에서 제시한 한 요소를 제외하고 모델을 평가하는 소거 연구를 통해 기여도의 영향을 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/table4.png"
loading="lazy"
>&lt;/p>
&lt;p>연구 결과, 단계 수를 20까지 늘리면 출력 품질이 향상되지만 그 이상은 효과가 줄어든다. 단일 모델 대비 네 모델 사용이 오디오 품질과 모든 측정 지표에서 우수함을 보여주었다. 또한, 주파수 밴드 재조정으로 ViSQOL 점수가 0.2 향상되었으며, 스케줄 방식은 기존 방식보다 성능이 0.2~0.4 개선되었다. 제안한 데이터 처리 기술로도 ViSQOL 점수가 0.2 증가했으며, 이는 주로 고주파수에 영향을 미쳤다.&lt;/p>
&lt;h3 id="text-to-audio">Text to audio&lt;/h3>
&lt;p>이 모델은 조건 없이 오디오를 생성할 수 없으나, 생성 언어 모델과 결합 시 품질이 크게 향상된다.&lt;/p>
&lt;p>&lt;strong>Text to Speech.&lt;/strong> 최근 텍스트에서 음성으로 변환하는 TTS 분야에서 오디오 코덱에 언어 모델을 적용하는 연구가 주목받고 있다. 이 분야에서 VALL-E와 SPEAR-TSS와 같은 모델이 좋은 성과를 보여주었다. Multi-Band Diffusion 토큰 decoder를 사용하여 오디오 품질을 더욱 향상시킬 수 있다고 주장한다. 이를 검증하기 위해 공개적으로 이용 가능한 Bark 3 모델을 사용하였고, 이 모델은 텍스트를 오디오 토큰으로 변환한 뒤, 이를 다시 처리하여 최종 오디오를 생성한다. 실험 결과, 사전 학습된 Bark 모델을 사용했을 때 음성 프롬프트의 5% 미만, 노래 목소리 프롬프트의 약 30%에서 언어 모델이 목소리를 생성하지 못하는 경우가 있었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/images/table5.png"
loading="lazy"
>&lt;/p>
&lt;p>&lt;strong>Text to Music.&lt;/strong> 최근 오디오 토큰의 언어 모델링을 통한 음악 생성 분야에서 MusicLM과 MusicGen 같은 프로젝트를 통해 큰 진전이 이루어졌다. MusicGen의 압축 모델로 생성된 토큰을 기반으로 한 확산 모델을 학습시켜 디코딩 방식의 유연성을 입증했으며, 이 모델은 32kHz 샘플링 레이트와 16개 멜 스케일 밴드의 표준 편차를 가진 데이터셋에서 학습되었다.&lt;/p>
&lt;p>표준 MusicGen 대비 MUSHRA 점수를 +4 향상시켰으며, diffusion decoder로 생성된 아티팩트가 적다. 특히, 복잡한 음악 요소가 있는 경우, Multi-Band Diffusion 출력이 원본보다 훨씬 명확함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>diffusion 기반 디코딩 방법은 기존 decoder 대비 오디오 품질을 크게 향상시키지만, 더 많은 계산력을 요구하고 처리 속도가 느리다. 이 방법으로 더 자연스러운 오디오와 적은 아티팩트를 생성하지만, 실시간 성능이 중요한 경우에는 적합하지 않을 수 있다.&lt;/p>
&lt;p>&lt;strong>Ethical concerns.&lt;/strong> 생성 AI가 아님에도 Wang et al. (2023) 같은 기술과 결합하여 목소리의 진정성을 높일 수 있으나, 진짜 같은 딥페이크와 보이스 피싱 같은 오용의 위험이 있다. 학습 데이터의 질과 양에 의존하는 이 방법은 광범위한 시나리오에서 최적화되기 위해 큰 데이터셋으로 세심히 학습되었지만, 데이터셋의 불균형으로 인한 소수 집단에 대한 편향 가능성을 인정한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2308.02560.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MusicLM</title><link>https://kurtkim.github.io/p/musiclm/</link><pubDate>Wed, 06 Mar 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/musiclm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>MusicLM은 &amp;ldquo;a calming violin melody backed by a distorted guitar riff&amp;quot;과 같은 텍스트 설명을 바탕으로 고품질의 음악을 생성하는 모델이다. 이 모델은 sequenceto-sequence 모델링을 통해 음악을 생성하며, 음질과 텍스트 설명의 정확성에서 이전 모델들을 능가한다. 또한, 텍스트와 멜로디 모두에 조건을 두어 휘파람과 허밍된 멜로디를 텍스트 캡션에 따라 변형할 수 있다. 이를 지원하기 위해, 5.5k의 음악-텍스트 쌍 데이터셋인 MusicCaps를 공개하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>conditional neural 오디오 생성은 text-to-speech, lyrics-conditioned 음악 생성, MIDI 시퀀스로부터의 오디오 합성 등 다양한 분야에서 활용된다. 최근에는 text-to-image 생성 연구의 발전에 따라, 고수준의 캡션으로부터 오디오를 생성하는 연구가 진행되었다. 그러나 이러한 모델은 아직 단순한 음향 장면에 한정되어 있어, 풍부한 오디오 시퀀스를 생성하는 것은 여전히 도전 과제로 남아 있다.&lt;/p>
&lt;p>최근에 소개된 AudioLM은 오디오 합성을 discrete representation 공간에서의 언어 모델링 작업으로 취급하여 고해상도와 장기 일관성을 동시에 달성한다. 오디오 신호의 내용에 대한 가정 없이 오디오 전용 말뭉치에서 현실적인 오디오를 생성하는 방법을 학습한다. 이러한 시스템이 적절한 데이터에 학습된다면 더 풍부한 출력을 생성할 수 있을 것으로 보인다.&lt;/p>
&lt;p>고품질 오디오 합성의 본질적인 어려움 외에도, 오디오-텍스트 쌍 데이터의 부족이 주요 장애 요인이다. 이는 이미지 도메인과 대조적으로, 이미지 도메인에서는 대규모 데이터셋의 사용이 뛰어난 이미지 생성 품질에 크게 기여하였다. 또한, 오디오의 주요 특성을 단어로 명확하게 포착하거나 시퀀스 전체에 대한 캡션을 제공하는 것은 이미지를 설명하는 것보다 더욱 어렵다.&lt;/p>
&lt;p>이 연구에서는 텍스트 설명으로 고해상도 음악을 생성하는 MusicLM 모델을 소개한다. MusicLM은 AudioLM의 multi-stage autoregressive 모델링을 활용하고, 텍스트 조건부를 추가한다. 쌍 데이터의 부족함을 해결하기 위해, 공동 music-text 모델인 MuLan을 활용하여 음악과 텍스트 설명을 임베딩 공간에서 가까운 표현으로 투영한다. 이렇게 하면 학습 시간에 캡션의 필요성을 제거하고, 대규모 오디오 말뭉치에서 학습할 수 있다. 학습 시에는 오디오에서 계산된 MuLan 임베딩을, 추론 시에는 텍스트 입력에서 계산된 MuLan 임베딩을 사용한다.&lt;/p>
&lt;p>MusicLM은 라벨이 없는 대규모 음악 데이터셋에 학습되어 복잡한 텍스트 설명에 따른 일관성 있는 음악을 생성한다. 이를 평가하기 위해, 전문 음악가들이 준비한 5.5k의 예시를 포함하는 고품질 음악 캡션 데이터셋인 MusicCaps를 소개하고, 이를 공개하여 미래의 연구를 지원한다.&lt;/p>
&lt;p>이 연구의 실험은 MusicLM이 품질과 캡션 준수 면에서 이전 시스템을 능가한다는 것을 보여준다. 또한, 음악의 일부 측면을 단어로 설명하는 것이 어렵거나 불가능한 경우에도, 이 방법이 텍스트를 넘어서는 조건부 신호를 지원한다. 특히, 오디오 형태의 추가적인 멜로디를 받아들여 원하는 멜로디를 따르는 음악 클립을 생성하는 확장된 MusicLM을 소개한다.&lt;/p>
&lt;p>음악 생성과 관련된 위험성, 특히 창의적 콘텐츠의 부당한 사용을 인지하고 있다. 이에 따라, 대형 언어 모델에 대한 기억력 연구를 철저히 수행하였고, MuLan 임베딩을 MusicLM에 입력하면 생성된 토큰의 시퀀스가 학습 세트의 해당 시퀀스와 크게 다르다는 결과를 얻었다.&lt;/p>
&lt;p>이 작업의 주요 기여는 다음과 같다:&lt;/p>
&lt;ol>
&lt;li>텍스트 조건부 신호에 충실하게 몇 분 동안 일관된 고품질 음악을 생성하는 MusicLM 모델을 소개한다.&lt;/li>
&lt;li>멜로디와 같은 다른 조건부 신호로 방법을 확장하고, 텍스트 프롬프트에 따라 합성한다. 또한, 최대 5분 길이의 음악 클립을 일관되게 생성하는 것을 보여준다.&lt;/li>
&lt;li>text-to-music 생성 작업을 위한 첫 번째 평가 데이터셋인 MusicCaps를 공개한다. 이는 음악가들이 준비한 5.5k의 음악-텍스트 쌍으로 구성된 고품질 데이터셋이다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="background-and-related-work">Background and Related Work&lt;/h2>
&lt;p>다양한 도메인의 생성 모델링에서 Transformer 기반의 autoregressive 모델과 U-Net 기반의 diffusion 모델이 주도하고 있다. 이 섹션에서는 discrete 토큰을 다루는 autoregressive 생성 모델에 초점을 맞춰 MusicLM과 관련된 작업을 검토한다.&lt;/p>
&lt;h3 id="quantization">Quantization&lt;/h3>
&lt;p>자연어 처리, 이미지, 비디오 생성 등에서 autoregressively 하게 discrete 토큰의 시퀀스를 모델링하는 것이 효과적임이 입증되었다. 연속 신호에 대한 autoregressive 모델의 성공에는 양자화가 중요하며, 이는 컴팩트한 discrete 표현을 제공하면서도 높은 품질의 재구성을 가능하게 한다. VQ-VAEs는 다양한 도메인에서 낮은 비트레이트에서 뛰어난 재구성 품질을 보여주며, 많은 접근법의 기본 양자화 도구로 사용되었다.&lt;/p>
&lt;p>SoundStream은 일반 오디오를 낮은 비트레이트로 압축하면서도 높은 재구성 품질을 유지하는 neural audio codec이다. 이를 위해 residual vector quantization(RVQ)를 사용하여 비트레이트와 품질을 높이는 데 큰 계산 비용 없이 확장성을 제공한다. RVQ는 대상 비트레이트가 증가함에 따른 코드북 크기의 급증을 방지하며, 각 양자화기가 계층적 구조를 가지게 된다. 이는 고품질 재구성에 유리하며, 생성에도 바람직한 속성이다. 이 작업에서는 24kHz 음악을 6kbps로 고품질로 재구성할 수 있는 SoundStream을 오디오 토크나이저로 사용한다.&lt;/p>
&lt;h3 id="generative-models-for-audio">Generative Models for Audio&lt;/h3>
&lt;p>장기적인 일관성을 유지하면서 고품질 오디오를 생성하는 것은 어려운 문제지만, 최근에는 Jukebox와 PerceiverAR 같은 일련의 방법론이 이 문제를 해결하려 노력하였다. Jukebox는 높은 시간적 일관성을 달성하기 위해 VQVAEs의 계층을 제안하지만, 생성된 음악에서 아티팩트가 나타났다. 반면 PerceiverAR은 고품질 오디오를 달성하지만, 장기적인 시간적 일관성을 저해하였다.&lt;/p>
&lt;p>AudioLM은 계층적 토크나이징과 생성 체계를 사용하여 일관성과 고품질 합성 사이의 균형을 맞춘다. 이 방법은 의미 토큰과 음향 토큰 두 가지 유형을 구분하여 장기 구조를 모델링하고 미세한 음향 세부 사항을 포착한다. 이를 통해 AudioLM은 대본이나 기호적 음악 표현에 의존하지 않고도 일관되고 고품질의 음성과 피아노 음악을 생성할 수 있다.&lt;/p>
&lt;p>MusicLM은 AudioLM을 기반으로 하되, 추가적으로 (1) 생성 과정을 설명적인 텍스트에 조절하는, (2) 이 조절을 멜로디와 같은 다른 신호로 확장하는, 그리고 (3) 피아노 음악을 넘어 다양한 음악 장르의 긴 시퀀스를 모델링하는 세 가지 기여를 한다.&lt;/p>
&lt;h3 id="conditioned-audio-generation">Conditioned Audio Generation&lt;/h3>
&lt;p>텍스트 설명에서 오디오를 생성하는 것은 최근 연구 주제로 다루어졌다. DiffSound는 텍스트 인코더로 CLIP을 사용하고, 확산 모델을 적용하여 텍스트 임베딩에 따른 양자화된 멜 스펙트로그램 특성을 예측한다. 반면 AudioGen은 T5 인코더를 텍스트 임베딩에 사용하고, autoregressive transformer decoder로 EnCodec에 의해 생성된 오디오 코드를 예측한다. 두 연구 모두 AudioSet과 AudioCaps 같은 적절한 양의 쌍을 이룬 학습 데이터에 의존한다.&lt;/p>
&lt;p>텍스트에 기반한 음악 생성에 초점을 맞춘 연구 중, Mubert는 텍스트 프롬프트를 transformer로 임베딩하고, 이를 바탕으로 음악 태그를 선택하여 노래 생성 API에 쿼리한다. 반면 Riffusion은 안정적인 diffusion 모델을 음악-텍스트 데이터셋의 멜 스펙트로그램 음악 조각에 미세조정한다. 이 두 연구를 기준선으로 사용하여, 이 연구가 오디오 생성 품질과 텍스트 설명의 준수를 개선한다는 것을 보여준다.&lt;/p>
&lt;p>음악의 기호적 표현(예: MIDI)이 강력한 조절 형태로서 생성 과정을 주도하는 것이 보여졌다. 그러나 MusicLM은 허밍된 멜로디와 같은 방법을 통해 더 자연스럽고 직관적인 조절 신호를 제공하며, 이는 텍스트 설명과 결합될 수 있다.&lt;/p>
&lt;h3 id="text-conditioned-image-generation">Text-Conditioned Image Generation&lt;/h3>
&lt;p>텍스트 조건의 이미지 생성 모델은 구조적 개선과 대량의 고품질 쌍 학습 데이터 덕분에 큰 진전을 이루었다. 이 방법은 transformer 기반의 autoregressive 접근법과 확산 기반 모델을 포함하며, 이는 텍스트 프롬프트에서 비디오를 생성하는 것으로 확장되어 왔습니다.&lt;/p>
&lt;p>이 연구의 접근법은 DALL·E 2와 가장 유사하며, 둘 다 텍스트 인코딩을 위해 CLIP에 의존한다. 하지만, DALL·E 2가 diffusion 모델을 decoder로 사용하는 반면, 이 연구는 AudioLM을 기반으로 한 deocer를 사용한다. 또한, 오디오 전용 데이터셋에서 학습이 가능하도록 텍스트 임베딩을 음악 임베딩으로 매핑하는 사전 모델을 생략하고, 추론 시에 음악 임베딩을 텍스트 임베딩으로 단순히 대체한다.&lt;/p>
&lt;h3 id="joint-embedding-models-for-music-and-text">Joint Embedding Models for Music and Text&lt;/h3>
&lt;p>MuLan은 음악과 텍스트의 공동 임베딩 모델로, 각 모달리티에 대한 임베딩 탑을 포함한다. 이 탑들은 대조 학습을 통해 두 모달리티를 128차원의 공유 임베딩 공간으로 매핑한다. 텍스트 임베딩 네트워크는 대규모 텍스트 데이터에서 사전 학습된 BERT를 사용하며, 오디오 탑은 ResNet-50 변형을 사용한다.&lt;/p>
&lt;p>MuLan은 음악 클립과 그에 대응하는 텍스트 주석의 쌍에 학습된다. 데이터 품질에 대한 요구가 낮아, 약한 연관성만 가진 음악-텍스트 쌍에서도 상호 연관성을 학습할 수 있다. 이로 인해 음악을 자연 언어 설명에 연결하여 검색이나 zero-shot 음악 태깅에 활용할 수 있다. 이 작업에서는 Huang et al. (2022)의 사전 학습된 모델을 사용한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>이 섹션에서는 MusicLM과 그 구성 요소에 대해 설명한다. 섹션 3.1은 오디오 표현을 제공하는 모델에 대해, 그리고 섹션 3.2는 이를 텍스트 조건의 음악 생성에 어떻게 활용하는지에 대해 다룬다.&lt;/p>
&lt;h3 id="representation-and-tokenization-of-audio-and-text">Representation and Tokenization of Audio and Text&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/figure1.png"
width="644"
height="334"
srcset="https://kurtkim.github.io/p/musiclm/images/figure1_hu6e041e7e12f30cda1cfeed8c59f97029_66923_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/figure1_hu6e041e7e12f30cda1cfeed8c59f97029_66923_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="462px"
>&lt;/p>
&lt;p>conditional autoregressive 음악 생성을 위해 세 가지 모델을 사용해 오디오 표현을 추출한다. AudioLM의 접근법을 따라 SoundStream의 self-supervise 오디오 표현과 w2vBERT를 각각 음향 토큰과 의미 토큰으로 사용하며, 조절을 위해 MuLan의 음악 임베딩과 텍스트 임베딩을 활용한다. 이 모델들은 독립적으로 사전 학습되고 고정되어, sequence-to-sequence 모델링에 필요한 이산적인 오디오와 텍스트 표현을 제공한다.&lt;/p>
&lt;p>&lt;strong>SoundStream.&lt;/strong> 24 kHz monophonic 오디오를 위해 스트라이딩 요소 480의 SoundStream 모델을 사용하며, 이로 인해 50 Hz의 임베딩이 생성된다. 이 임베딩들은 RVQ에 의해 학습된 양자화로 인해 6 kbps의 비트레이트를 가지게 되며, 1초의 오디오는 600개의 토큰으로 표현된다. 이 토큰들을 &amp;ldquo;acoustic token&amp;quot;이라 부르며, 이는 $A$로 표기된다.&lt;/p>
&lt;p>&lt;strong>w2v-BERT.&lt;/strong> 600M parameter를 가진 w2v-BERT masked-language-modeling (MLM) 모듈의 중간 계층을 사용한다. 이 모델을 사전 학습하고 고정한 후, 7번째 계층에서 임베딩을 추출하고, 이를 k-means의 중심을 사용해 양자화한다. 결과적으로, 오디오의 모든 초당 25개의 의미 토큰을 생성하며, 이는 $S$로 표기된다.&lt;/p>
&lt;p>&lt;strong>MuLan.&lt;/strong> MusicLM을 학습시키기 위해, MuLan의 오디오 임베딩 네트워크에서 오디오 시퀀스의 표현을 추출한다. 이 표현은 연속적이지만, 오디오와 조절 신호가 동질적인 이산 토큰 기반 표현을 가지도록 MuLan 임베딩을 양자화한다. 이는 조절 신호를 autoregressively 하게 모델링하는 연구를 지원한다.&lt;/p>
&lt;p>MuLan은 10초 오디오 입력에 작동하며, 긴 오디오 시퀀스를 처리하기 위해 1초 간격으로 10초 윈도우에서 오디오 임베딩을 계산하고 평균화한다. 이후 1024 크기의 어휘를 가진 12개의 벡터 양자화기를 사용하는 RVQ로 이산화하여, 오디오 시퀀스에 대해 12개의 MuLan 오디오 토큰 $M_A$를 생성한다. 추론 시에는 텍스트 프롬프트에서 MuLan 텍스트 임베딩을 추출하고 같은 RVQ로 양자화하여 12개의 토큰 $M_T$를 얻는다.&lt;/p>
&lt;p>학습 중에 $M_A$에 조건을 부여하는 것은 학습 데이터를 쉽게 확장할 수 있도록 하며, 대조적인 손실로 학습된 MuLan 같은 모델을 활용해 잡음이 많은 텍스트 설명에 대한 강건성을 높이는 두 가지 이점이 있다.&lt;/p>
&lt;h3 id="hierarchical-modeling-of-audio-representations">Hierarchical Modeling of Audio Representations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/figure2.png"
width="1326"
height="394"
srcset="https://kurtkim.github.io/p/musiclm/images/figure2_hu5f857058def17e8cc3de10acbfba69e5_117180_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/figure2_hu5f857058def17e8cc3de10acbfba69e5_117180_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="336"
data-flex-basis="807px"
>&lt;/p>
&lt;p>discrete 오디오 표현을 AudioLM과 결합해 텍스트 조건의 음악 생성을 달성한다. 이를 위해, 각 단계가 decoder-only Transformer에 의해 autoregressively 하게 모델링되는 계층적인 sequenceto-sequence 모델링 작업을 제안한다.&lt;/p>
&lt;p>첫 번째 단계는 의미 모델링 단계로, MuLan 오디오 토큰에서 의미 토큰 $S$로의 매핑을 학습한다. 이는 분포 $p(S_t | S_{&amp;lt;t}, M_A)$를 모델링함으로써 이루어지며, 여기서 $t$는 시간 단계에 해당하는 시퀀스 내의 위치이다. 두 번째 단계는 음향 모델링 단계로, 음향 토큰 $A_q$는 MuLan 오디오 토큰과 의미 토큰 모두에 의해 조건이 부여되어 예측되며, 이는 분포 $p(A_t | $A_{&amp;lt;t}, S, M_A)$를 모델링한다.&lt;/p>
&lt;p>긴 토큰 시퀀스를 피하고자, AudioLM은 음향 모델링 단계를 대략적인 단계와 세부적인 단계로 분할하는 방법을 제안하였으며, 이 연구에서도 이를 따랐다. 대략적인 단계에서는 SoundStream RVQ의 첫 네 단계를, 세부적인 단계에서는 나머지 여덟 단계를 모델링한다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental Setup&lt;/h2>
&lt;h3 id="models">Models&lt;/h3>
&lt;p>AudioLM의 의미 단계와 음향 단계 모델링에 decoder-only Transformer를 사용한다. 이 모델은 24개 layer, 16개 attention head, 1024의 embedding dimension, 4096의 차원의 feed-forward layer, 0.1의 dropout, 그리고 relative positional embedding으로 구성되어 있으며, 각 단계는 430M의 parameter를 가진다.&lt;/p>
&lt;h3 id="training-and-inference">Training and Inference&lt;/h3>
&lt;p>MusicLM의 다른 구성 요소를 학습시키기 위해 사전 학습된 MuLan에 의존한다. SoundStream과 w2v-BERT는 FMA 데이터셋에서, 토크나이저와 의미 및 음향 모델링 단계의 모델은 24 kHz에서 280k시간의 음악을 포함하는 5M 개의 오디오 클립 데이터셋에서 학습된다. 각 단계는 학습 데이터를 여러 번 반복하여 학습하며, 의미와 음향 단계에서는 각각 30초와 10초의 임의 오디오를 사용한다. 세부적인 음향 모델링 단계는 3초 자르기에서 학습된다.&lt;/p>
&lt;p>추론 시에는 MuLan이 학습한 오디오와 텍스트 사이의 임베딩 공간을 사용하며, $M_A$를 $M_T$로 대체한다. 그 후, $M_T$가 주어졌을 때 $A$를 얻기 위해 위에서 설명한 단계를 따른다. 모든 단계에서 autoregressive 샘플링을 위해 온도 샘플링을 사용하며, 의미 모델링 단계는 1.0, 대략적인 음향 모델링 단계는 0.95, 세밀한 음향 모델링 단계는 0.4의 온도를 사용한다. 이 값들은 생성된 음악의 다양성과 시간적 일관성 사이의 좋은 균형을 위해 선택되었다.&lt;/p>
&lt;h3 id="evaluation-dataset">Evaluation Dataset&lt;/h3>
&lt;p>MusicLM 평가를 위해 고품질 음악 캡션 데이터셋인 MusicCaps를 준비하였다. 이 데이터셋은 AudioSet의 5.5k 음악 클립과 전문 음악가들의 영어 설명이 포함되어 있다. 각 음악 클립에는 자유 형식의 캡션과 장르, 기분, 템포 등을 설명하는 음악 측면이 포함되어 있다.&lt;/p>
&lt;p>MusicCaps는 AudioSet의 오디오 클립과 텍스트 설명을 포함한 AudioCaps를 보완한다. AudioCaps는 음악이 아닌 내용이 포함되어 있지만, MusicCaps는 전문가의 상세 주석이 포함된 음악에만 초점을 맞추고 있다. AudioSet의 학습 및 평가 분할로부터 다양한 장르의 예시를 추출하였으며, 1k 예시로 구성된 장르별 균형 분할도 제공한다.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>MusicLM을 평가하기 위해, 오디오 품질과 텍스트 설명 충실도라는 음악 생성의 두 가지 중요한 측면을 측정하는 다양한 메트릭을 사용한다.&lt;/p>
&lt;p>&lt;strong>Fréchet Audio Distance (FAD).&lt;/strong>&lt;/p>
&lt;p>Fréchet Audio Distance는 사람의 인식과 잘 맞는 오디오 품질 지표이다. 이 점수가 낮은 모델은 신뢰할 수 있는 오디오를 생성할 것으로 예상되지만, 생성된 샘플이 제공된 텍스트 설명을 반드시 따르는 것은 아니다.&lt;/p>
&lt;p>공개적으로 사용 가능한 두 가지 오디오 임베딩 모델, 즉 음성 데이터에 학습된 Trill 2와 YouTube-8M 오디오 이벤트 데이터셋에 학습된 VGGish 3에 기반한 FAD를 보고한다. 학습 데이터의 차이로 인해, 이 두 모델은 오디오 품질의 다른 측면을 측정하게 될 것으로 예상한다.&lt;/p>
&lt;p>&lt;strong>KL Divergence (KLD).&lt;/strong> 텍스트 설명과 음악 클립 사이에는 다대다 관계가 있어, 오디오 파형 수준에서 직접 비교는 불가능하다. 입력 텍스트 충실도를 평가하기 위해, AudioSet에서 다중 레이블 분류를 위해 학습된 LEAF 분류기를 사용하여 생성된 음악과 참조 음악의 클래스 예측을 계산하고, 이들 사이의 KL-Divergence를 측정한다. KL-Divergence가 낮으면, 생성된 음악은 분류기에 따라 참조 음악과 유사한 음향 특성을 가질 것으로 예상된다.&lt;/p>
&lt;p>&lt;strong>MuLan Cycle Consistency (MCC).&lt;/strong> 음악-텍스트 임베딩 모델인 MuLan을 사용해 음악-텍스트 쌍의 유사성을 측정한다. MusicCaps의 텍스트 설명과 그에 기반한 생성된 음악에서 임베딩을 계산하고, 이들 간의 평균 코사인 유사성을 MCC 메트릭으로 정의한다.&lt;/p>
&lt;p>&lt;strong>Qualitative evaluation.&lt;/strong> 생성된 샘플이 텍스트 설명을 얼마나 잘 따르는지 평가하기 위해 A대 B 인간 평가 작업을 설정하였다. 평가자들은 텍스트 설명과 두 가지 다른 모델에 의해 생성된 음악 샘플을 비교하며, 샘플에 대한 강한, 약한 또는 무관한 선호도를 선택한다. 음악 품질은 이미 FAD 메트릭으로 평가되었으므로, 이를 고려하지 않도록 지시받았다.&lt;/p>
&lt;p>참조 음악과 n개의 다른 모델의 출력을 고려해 총 n + 1개의 조건을 설정하고, 이들 간의 쌍을 비교한다. 각 조건이 얼마나 선호되는지를 승리의 수로 계산하여 결과를 집계하고 순위를 매긴다. 샘플은 평가 데이터의 장르 균형 1k 부분 집합에서 선택된다.&lt;/p>
&lt;p>&lt;strong>Training data memorization.&lt;/strong> 큰 언어 모델은 학습 데이터의 패턴을 기억할 수 있다. 이를 바탕으로 MusicLM이 음악 세그먼트를 얼마나 기억하는지 연구한다. 학습 세트에서 무작위로 선택한 예시에 대해, MuLan 오디오 토큰과 첫 번째 의미 토큰 시퀀스를 포함한 프롬프트를 모델에 제공한다. greedy decoding을 통해 의미 토큰의 연속을 생성하고, 이를 데이터셋의 대상 토큰과 비교한다. 생성된 토큰과 대상 토큰이 전체 샘플링 세그먼트에서 완전히 일치하는 예시의 비율을 측정한다.&lt;/p>
&lt;p>음향적으로 유사한 오디오 세그먼트가 다른 토큰 시퀀스로 이어질 수 있다는 관찰을 기반으로, 근사치 일치를 감지하는 방법을 제안한다. 생성된 토큰과 대상 토큰의 의미 토큰 수 히스토그램을 계산하고, 이들 사이의 일치 비용을 측정한다. 이를 위해 먼저 의미 토큰 쌍 간의 거리 행렬을 계산하고, 그 후 Sinkhorn 알고리즘을 사용해 최적의 전송 문제를 해결한다. 부정 쌍을 구성하고 이들의 일치 비용 분포를 측정해 일치 임계값을 보정한다. 이 임계값은 0.01% 미만의 거짓 긍정 근사치 일치를 초래하는 0.85로 설정된다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>MusicLM을 평가하기 위해, 최근의 기준선인 Mubert와 Riffusion과 비교한다. Mubert API를 쿼리하고 Riffusion 모델에서 추론을 실행하여 오디오를 생성한다. 이 논문과 함께 공개적으로 발표하는 평가 데이터셋인 MusicCaps에서 평가를 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/table1.png"
width="600"
height="170"
srcset="https://kurtkim.github.io/p/musiclm/images/table1_hu932e6e5791924bf2b016ff6504760e37_25595_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/table1_hu932e6e5791924bf2b016ff6504760e37_25595_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="352"
data-flex-basis="847px"
>&lt;/p>
&lt;p>&lt;strong>Comparison to baselines.&lt;/strong> 오디오 품질을 측정하는 FAD 메트릭에 따르면, MusicLM은 Mubert와 Riffusion보다 더 나은 점수를 얻었다. 또한, 입력 텍스트 설명에 대한 충실도를 측정하는 KLD와 MCC에 따라, MusicLM은 기준선에 비해 텍스트 설명에서 더 많은 정보를 캡처하는 능력을 보여준다.&lt;/p>
&lt;p>텍스트 충실도 평가를 보완하기 위해 인간 청취 테스트를 진행하였다. 참가자들은 두 개의 10초 클립과 텍스트 캡션을 보고, 어떤 클립이 캡션의 텍스트를 가장 잘 설명하는지 5점 척도로 평가한다. 총 1200개의 평가를 수집하였고, 각 소스는 600개의 쌍별 비교에 참여하였다. MusicLM은 두 가지 기준선에 비해 확실히 선호되지만, 실제 참조 음악과의 차이는 여전히 존재한다. 청취 연구의 자세한 내용은 부록 B에서 확인할 수 있다.&lt;/p>
&lt;p>실제 참조가 MusicLM보다 선호되는 경우, 이는 주로 아래의 패턴 때문이다: (1) 캡션은 매우 상세하고, 여러 악기나 비음악적인 측면을 설명한다; (2) 캡션은 오디오의 재생 순서를 설명한다; (3) 부정적인 표현이 사용되는데, 이는 MuLan에 잘 반영되지 않는다.&lt;/p>
&lt;p>결론적으로, 이 방법론은 MusicCaps의 자유형 텍스트 캡션에서 세부적인 정보를 잘 포착하며, KLD와 MCC 메트릭은 텍스트 설명에 대한 충실도를 정량적으로 측정하고, 이는 인간의 평가와 일치한다는 것을 확인하였다.&lt;/p>
&lt;p>&lt;strong>Importance of semantic tokens.&lt;/strong> 의미론적 모델링과 음향 모델링을 분리하는 것의 유용성을 검증하기 위해, Transformer 모델을 학습시켜 MuLan 토큰에서 음향 토큰을 직접 예측하였다. FAD 메트릭은 비슷하지만, 의미론적 모델링 단계를 제거하면 KLD와 MCC 점수가 악화됨을 발견하였다. 이는 의미 토큰이 텍스트 설명에 따른 순응을 돕는다는 것을 나타낸다. 또한, 샘플을 들어보니 장기 구조에서의 저하가 관찰되었다.&lt;/p>
&lt;p>&lt;strong>Information represented by audio tokens.&lt;/strong> 추가 실험을 통해 의미론적 토큰과 음향 토큰이 어떤 정보를 포착하는지 연구하였다. 첫 번째 실험에서는 텍스트 토큰과 의미론적 토큰을 고정하고, 음향 모델링 단계를 반복하여 샘플을 생성하였다. 결과적으로 샘플들은 다양했지만, 장르나 리듬, 멜로디 등에서 공통점을 보였습니다. 두 번째 실험에서는 텍스트 토큰만 고정하고 의미론적 토큰과 음향 토큰을 생성했을 때, 멜로디와 리듬에서 더 큰 다양성을 보였다. 이 연구의 샘플들은 동반 자료에서 확인할 수 있다.&lt;/p>
&lt;p>&lt;strong>Memorization analysis.&lt;/strong> 의미론적 토큰 프롬프트의 길이를 변화시킬 때 일치도를 보고합니다. 정확한 일치의 비율은 매우 작은 것을 확인했으며, 근사치 일치(τ = 0.85 사용)의 경우, 프롬프트 길이가 증가함에 따라 일치하는 예시의 비율이 증가한다. 또한, 일치 점수가 낮은 시퀀스는 토큰 다양성이 낮은 것으로 확인되었다. 이러한 패턴은 의미론적 토큰이 정확하게 일치하더라도, 음향 모델링이 생성된 샘플에 추가적인 다양성을 도입한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="extensions">Extensions&lt;/h2>
&lt;p>&lt;strong>Melody conditioning.&lt;/strong> MusicLM은 텍스트 설명과 함께 허밍, 노래, 휘파람, 악기 연주 등의 형태로 제공되는 멜로디에 기반한 음악을 생성할 수 있도록 확장되었다. 이를 위해, 멜로디는 같지만 음향이 다른 오디오 쌍으로 구성된 데이터셋을 만들고, 공통 임베딩 모델을 학습시켜 같은 멜로디를 가진 오디오 클립의 임베딩이 서로 가까워지도록 하였다.&lt;/p>
&lt;p>MusicLM의 멜로디를 추출하기 위해, 멜로디 임베딩을 양자화하고 결과 토큰 시퀀스를 MuLan 오디오 토큰과 연결한다. 추론 과정에서는, 입력 오디오 클립에서 멜로디 토큰을 계산하고 이를 MuLan 텍스트 토큰과 연결한다. 이러한 방식으로, MusicLM은 입력 오디오 클립의 멜로디를 따르면서도 텍스트 설명을 준수하는 음악을 성공적으로 생성할 수 있다.&lt;/p>
&lt;p>&lt;strong>Long generation and story mode.&lt;/strong> MusicLM은 시간 차원에서 자기회귀적 생성을 사용하여 학습 시 사용한 것보다 더 긴 시퀀스를 만들 수 있다. 의미론적 모델링은 30초 시퀀스에서 학습되며, 더 긴 시퀀스를 위해 15초 간격으로 진행하고, 15초를 접두사로 사용하여 추가적인 15초를 생성한다. 이 방법으로 몇 분 동안 일관성을 유지하는 긴 오디오 시퀀스를 생성할 수 있다.&lt;/p>
&lt;p>소소한 수정을 통해 MusicLM은 시간이 지나면서 텍스트 설명이 변화하는 동안 긴 오디오 시퀀스를 생성할 수 있게 되었다. 이를 스토리 모드라고 부르며, 여러 텍스트 설명에서 $M_T$를 계산하고 15초마다 조건 신호를 변경한다. 이 방법으로, 모델은 텍스트 설명에 따라 음악 컨텍스트를 변경하면서도 템포가 일관되고 의미론적으로 타당한 부드러운 전환을 생성한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>MusicLM은 텍스트 조건에 충실하면서 몇 분 동안 일관된 24 kHz의 고품질 음악을 생성하는 모델이다. 이 모델은 뮤지션들이 준비한 5.5k 음악-텍스트 쌍의 고품질 데이터셋인 MusicCaps에서 기존 모델을 능가하는 성능을 보여준다.&lt;/p>
&lt;p>이 방법의 한계 중 일부는 MuLan으로부터 비롯되며, 이는 모델이 부정을 잘못 이해하고 텍스트에 기술된 시간 순서를 정확하게 따르지 않는다는 점을 포함한다. 또한, 정량적 평가의 추가적인 개선이 필요하며, MCC 점수는 MuLan에 의존하기 때문에 이 방법에 유리하다.&lt;/p>
&lt;p>미래의 연구는 가사 생성, 텍스트 조건화 및 보컬 품질 개선, 고수준 노래 구조(서론, 구절, 후렴구 등)의 모델링, 그리고 더 높은 샘플 속도에서 음악 모델링에 초점을 맞출 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="broader-impact">Broader Impact&lt;/h2>
&lt;p>MusicLM은 텍스트를 기반으로 고품질 음악을 생성해 인간의 창의적 음악 작업을 돕지만, 여러 위험 요소가 있다. 학습 데이터의 편향이 생성된 샘플에 반영될 수 있으며, 이것은 대표성이 부족한 문화에 대한 음악 생성의 적절성과 문화적 강탈 문제를 불러일으킬 수 있다.&lt;/p>
&lt;p>창의적 콘텐츠의 잘못된 사용 가능성을 인지하며, 책임있는 모델 개발 원칙에 따라 의미 모델링에 초점을 맞춘 연구를 수행하였다. 기억된 예시는 극히 적었고, 예시의 1%만 대략적으로 일치하였다. 음악 생성과 관련된 위험을 해결하기 위한 미래 연구의 필요성을 강조하며, 현재 모델 공개 계획은 없다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2301.11325.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/musiclm-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MuLan</title><link>https://kurtkim.github.io/p/mulan/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/mulan/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문은 &amp;ldquo;MuLan&amp;quot;이라는 새로운 음향 모델을 소개한다. MuLan은 44M 개의 음악 녹음과 자유 형식의 텍스트 주석을 학습하여 음악 오디오를 제한 없는 자연어 음악 설명과 직접 연결한다. 이 모델은 다양한 음악 장르와 텍스트 스타일과 호환되며, 오디오-텍스트 표현은 기존의 ontologie를 포함하면서 zero-shot 기능으로 발전하였다. 이 모델의 유연성은 전송 학습, zero-shot 음악 태깅, 음악 분야의 언어 이해 및 크로스-모달 검색 애플리케이션 등의 실험을 통해 입증되었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>분류기는 보통 사전에 정의된 클래스 목록으로 예제들을 레이블링하도록 학습되며, 이는 클래스 간 관계를 나타내는 구조화된 체계로 지정된다. 최근 신경 언어 모델링의 발전으로, 연구자들은 원시 콘텐츠 정보에 접근하기 위한 자연어 인터페이스를 탐색하고 있다. 이는 주로 시각과 오디오 이벤트 분야에서 이루어지며, 미디어 콘텐츠와 자연어 캡션을 함께 임베딩하는 것이 전이 학습, 크로스-모달 검색, 자동 캡셔닝, zero-shot 분류 등에서 효과적임을 보여주었다.&lt;/p>
&lt;p>이러한 연구의 성공은 대규모 학습 자료와 언어와 다른 모드 사이의 복잡한 관계를 모델링 할 수 있는 유연한 신경망 구조에 크게 의존한다. 특히, 시각 분야는 웹에서 대량의 캡션 이미지를 사용할 수 있어 이점을 얻었다. 그러나 환경 오디오 분야에서는 대규모 오디오-캡션 쌍이 덜 접근 가능하며, 이에 따라 작은 캡션 데이터셋에 의존하였다. 이러한 데이터셋들은 사운드를 묘사하는 언어의 다양성을 충분히 담지 못하며, zero-shot 설정에서의 성공은 여전히 부족하다.&lt;/p>
&lt;p>이 논문은 오디오와 자연어를 함께 임베딩하는 작업에 대해 다루되, 특히 음악 분야에 초점을 맞춘다. 목표는 모든 음악 개념을 관련 음악 오디오와 연결할 수 있는 유연한 언어 인터페이스를 제작하는 것이다. 이를 위해, 메타데이터, 댓글, 플레이리스트 데이터에서 추출한 텍스트 주석을 44M 개 이상의 인터넷 음악 비디오 학습 세트에 매핑하는 전략을 사용한다. 하지만 텍스트 데이터가 음악 콘텐츠를 정확히 참조하는 경우는 일부에 불과하므로, 음악 설명을 식별하기 위해 별도로 학습된 텍스트 분류기를 사용한 텍스트 사전 필터링을 탐색한다.&lt;/p>
&lt;p>이 논문에서는 대규모 데이터셋을 사용하여 자연어 인터페이스가 탑재된 새로운 세대의 음악 오디오 임베딩 모델인 &amp;ldquo;MuLan&amp;quot;을 학습시킨다. MuLan은 음악 오디오와 텍스트 간의 공유 임베딩 공간을 만들어내는 two-tower parallel encoder 구조를 사용한다. 이 모델은 다양한 음악 정보 검색 작업에서 최고 수준의 성능을 보이며, 텍스트에서 음악으로의 크로스-모달 검색, zero-shot 음악 태깅, 음악 도메인의 언어 이해 등의 기능을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Audio representation learning.&lt;/strong> 대규모로 일반적인 콘텐츠 표현을 사전 학습하여 전이 학습하는 방법이 여러 분야에서 주요 접근법이 되었다. 이는 오디오 표현 학습에도 적용되었으며, 일반 환경 오디오와 음악 오디오 모두에서 다양한 사전 훈련 메커니즘이 탐색되었다. ImageNet과 AudioSet에서 사전 학습된 오디오 스펙트로그램 변환기는 다양한 태깅 작업에서 최고의 성과를 보였으며, Million Song Database를 사용한 연구에서는 음악 오디오 표현 학습의 초기 기준선을 제시하였다.&lt;/p>
&lt;p>비지도 및 자기 지도 사전 학습에서는 구별적 모델과 생성적 모델 모두 성공적이었다. 구별적 학습은 같은 녹음에서 추출된 오디오 세그먼트에 더 높은 유사성을 부여하는 표현을 학습하는 방식을 사용하였다. 이와 비슷하게, SSAST는 생성적인 마스크된 스펙트로그램 패치 모델링을 탐색하였다. 생성 모델의 중간 임베딩이 downstream 분류를 위한 강력한 오디오 표현을 제공한다는 것이 확인되었으며, 사용자 상호작용 통계와 시각적 단서 같은 약한 감독도 검토되었다.&lt;/p>
&lt;p>이 연구는 음악 오디오와 약하게 연결된 풍부한 텍스트 주석을 활용한 크로스-모달 감독 방법을 개발하는 것에 집중하고 있다. 이를 통해 학습된 표현의 전이 학습 능력을 평가하고, 다양한 오디오 encoder 구조를 비교 분석하고 있다.&lt;/p>
&lt;p>&lt;strong>Cross-modal contrastive learning.&lt;/strong> 대규모 데이터를 활용한 대조 학습의 성공에 기반하여, 이미지-텍스트 모델에 오디오 타워를 추가하고 크로스-모달 정렬을 강화하는 삼중 모드 구조를 제안하였다. 오디오 분야에서는 오디오의 잠재적 표현과 관련 태그를 정렬하기 위해 대조 학습이 사용되었다. 후속 연구에서는 사전 학습된 비문맥적 단어 임베딩 모델을 사용하여 태그를 넘어 새로운 용어로 일반화하는 능력을 지원하였다. 이 방법은 음악 도메인을 위한 오디오-텍스트 쌍의 대규모 컬렉션을 채굴하는데 초점을 맞추고 있으며, 이를 통해 처음으로 임의적인 zero-shot 음악 태깅과 검색이 가능해졌다.&lt;/p>
&lt;p>&lt;strong>Music text joint embedding models.&lt;/strong> 콘텐츠 기반 음악 정보 검색은 풍부한 의미를 가진 텍스트와 음악의 광범위하고 세밀한 특성을 연결하는 것을 필요로 한다. 일반적인 접근법은 다중 레이블 분류 작업을 통해 텍스트 레이블 클래스의 의미를 음악에 적용하는 것이다. 이를 위해, 음악 비디오와 연관된 자연어 텍스트에서 대량의 어휘를 추출하고, 이를 통해 오디오 인코더를 학습시키며, 텍스트 레이블을 오디오 특성과 정렬시키는 방법이 사용되었다. 또한, 분류, 회귀, 메트릭 학습 등의 다양한 학습 작업을 통해 자유 형식의 텍스트와 음악 오디오를 정렬하는 방법이 탐색되었다.&lt;/p>
&lt;p>이 논문의 작업은 MuLaP와 가장 유사하며, 이 연구에서는 제작 음악 라이브러리에서 250K의 오디오-캡션 쌍을 채굴하여 multimodal Transformer를 학습시켰다. 그러나 그들의 초기 융합 방식은 전이 학습 응용에 임베딩의 활용성을 제한한다. 반면, 이 접근법은 임의의 음악 오디오에 대한 자연어 인터페이스를 제공하는 공동 임베딩 공간을 만들어, 크로스-모달 검색, zero-shot 태깅, 언어 이해 등의 가능성을 열어냈다.&lt;/p>
&lt;hr>
&lt;h2 id="proposed-approach">Proposed Approach&lt;/h2>
&lt;p>목표는 음악 오디오와 자유 형식의 자연어 텍스트에 대한 공유 임베딩 공간을 만드는 것으로, 이 공간에서는 근접성이 의미를 공유하는 지표로 작용한다. 이를 위해 크로스-모달 대조 학습과 두 타워 구조를 사용하며, 대규모의 (오디오, 텍스트) 쌍 훈련 데이터를 채굴하여 이 작업을 지원한다.&lt;/p>
&lt;h3 id="learning-framework">Learning Framework&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/figure1.png"
width="682"
height="410"
srcset="https://kurtkim.github.io/p/mulan/images/figure1_hu9b99b9c0e8c2595b9653bf7a0e387718_79893_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/figure1_hu9b99b9c0e8c2595b9653bf7a0e387718_79893_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="399px"
>&lt;/p>
&lt;p>각 MuLan 모델은 오디오와 텍스트 입력 모달 각각에 대한 별도의 임베딩 네트워크로 구성된다. 이 두 네트워크는 가중치를 공유하지 않지만, 같은 차원의 $l_2$-normalized embedding space에서 종료한다. 오디오 임베딩 네트워크는 log mel spectrogram 컨텍스트 윈도우를, 텍스트 임베딩 네트워크는 널 패딩 텍스트 토큰 시퀀스를 입력으로 받는다.&lt;/p>
&lt;p>음악 녹음과 관련 텍스트 요소를 가지고, (오디오, 텍스트) 쌍의 크로스-모달 학습 데이터셋을 구축한다. 각 녹음에서 log mel spectrogram을 계산하고, 텍스트 요소를 고정 길이로 조정한다. 그런 다음, 미니 배치는 무작위로 선택된 녹음과 그에 대응하는 텍스트 요소의 쌍으로 구성된다. 이 샘플링 방식은 학습 오디오와 모든 관련 텍스트를 전부 커버하기 위해 여러 에포크가 필요하다는 것을 의미한다. 각 예시에 대해 여러 텍스트 주석을 연결하는 실험도 진행했지만, 일반적으로는 잘 작동하지 않았다.&lt;/p>
&lt;p>$$ \sum_{i=1}^{B} - log \big[ {{h[f(x^{(i)}), g(t^{(i)})]}\over{\sum_{j \neq i} h[f(x^{(i)}), g(t^{(j)})] + h[f(x^{(j)}), g(t^{(i)})] }} \big] $$&lt;/p>
&lt;p>$h[a, b] = exp(a^T b/τ)$ 형태의 비평 함수 $h$는 $a, b ∈ \mathbb{R}^d$와 학습 가능한 온도 hyperparameter $τ ∈ (0, 1]$에 의존한다. 이 함수는 내적을 통해 코사인 유사도를 계산하며, 대상 오디오-텍스트 쌍에 대해 큰 값을, 비대상 쌍에 대해서는 0에 가까운 값을 생성하는 것이 목표이다. 또한, 1보다 작은 $τ$ 값은 $h$의 출력 범위를 확장한다. 이전 연구에 따르면, 대량의 배치 크기는 대조적 손실 최적화에 도움이 된다.&lt;/p>
&lt;h3 id="audio-embedding-network">Audio Embedding Network&lt;/h3>
&lt;p>오디오 임베딩에 검증된 Resnet-50 아키텍처를 적용하였다. 이를 위해 log mel spectrogram을 흑백 이미지로 처리하고, 학습 클립에서 무작위로 선택된 10초 윈도우를 입력으로 사용한다. 학습 중에는 SpecAugment를 적용하고, 마지막에는 시간과 mel 채널에 대한 평균 풀링과 L2 정규화된 linear fully connected layer를 적용한다. 이 모델은 AudioSet의 로지스틱 회귀를 사용하여 사전 학습되며, 최종 분류 계층은 미세 조정 전에 제거된다.&lt;/p>
&lt;p>Audio Spectrogram Transformer (AST)는 log mel spectrogram에서 추출된 패치의 토큰 시퀀스에 12개의 Transformer 블록을 적용하는 아키텍처이다. 학습 중에는 SpecAugment를 적용하고, positional encoding과 [CLS] 토큰을 추가한다. [CLS] 토큰 위치에서의 최종 인코딩에 linear fully-connected layer과 L2 정규화를 적용하여 오디오 임베딩 네트워크의 출력을 형성한다. 이 모델은 공개된 AST 체크포인트를 사용하여 사전학습을 시작한다.&lt;/p>
&lt;h3 id="text-embedding-network">Text Embedding Network&lt;/h3>
&lt;p>텍스트 임베딩 모델로는 일반적으로 사용되는 BERT 아키텍처를 사용한다. 이는 텍스트 입력을 토큰 시퀀스로 변환하고, [CLS] 토큰 임베딩을 오디오-텍스트 임베딩 공간으로 변환하는 역할을 한다. 이 모델은 공개 체크포인트를 사용하여 사전학습을 시작한다.&lt;/p>
&lt;h3 id="training-dataset-mining">Training Dataset Mining&lt;/h3>
&lt;p>5000만 개의 인터넷 음악 비디오에서 30초 클립을 추출하여 MuLan 임베딩 모델을 학습시키는 데 사용한다. 음악 오디오 감지기를 통해 음악 컨텐츠가 절반 미만인 클립을 제거한 후, 약 4400만 개의 클립, 즉 대략 370K 시간의 오디오가 남는다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table1.png"
width="648"
height="186"
srcset="https://kurtkim.github.io/p/mulan/images/table1_huc988009fa7bb561c10195b33fcb9b7fd_48533_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table1_huc988009fa7bb561c10195b33fcb9b7fd_48533_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="348"
data-flex-basis="836px"
>&lt;/p>
&lt;p>각 음악 비디오에 대해, 비디오 제목과 태그, 비디오 설명과 댓글, 그리고 데이터셋의 인터넷 음악 비디오에 연결된 재생목록 제목 등 노이즈가 많은 텍스트 데이터를 고려한다. 이러한 텍스트는 반드시 사운드트랙의 음악적 속성을 참조하고 있다는 것이 보장되지 않으며, 특히 댓글 데이터는 가장 많은 노이즈를 포함하고 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table2.png"
width="600"
height="154"
srcset="https://kurtkim.github.io/p/mulan/images/table2_hu12be41209accd48d06dcfc43168466a0_31265_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table2_hu12be41209accd48d06dcfc43168466a0_31265_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>노이즈가 많은 텍스트를 고려하여, 음악 서술적 주석으로 필터링된 SF 및 LF 텍스트 데이터를 사용하여 MuLan을 학습시켰다. 이를 위해, 사전 학습된 BERT 모델을 미세 조정하고, 이를 통해 LF 주석을 필터링하였다. 또한, SF 주석을 정리하기 위한 규칙 기반 필터링도 적용하였다. 하지만, 재생목록 제목과 필터링된 장형 주석은 데이터셋의 총 녹음 중 일부분만 사용할 수 있다.&lt;/p>
&lt;p>AudioSet을 오디오-텍스트 쌍의 세트로 변환하여, 이를 ASET으로 표기한다. 모든 예시를 포함시키고, 각 예시에 첨부된 레이블 문자열을 텍스트 주석으로 사용하여 약 2M 개의 10초 클립 학습 세트를 생성한다. 이 네 가지 다른 데이터 소스의 규모 불균형을 고려하여, SF:LF:PL:ASET을 2:2:1:1의 비율로 미니 배치를 구성한다. 이 방식으로, 규모가 작더라도 필터링된 LF 주석이 미니 배치의 1/3를 차지하게 된다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>Resnet-50과 AST 오디오 encoder를 사용하여 MuLan을 평가하며, 두 경우 모두 텍스트 encoder로 BERT-base-uncased를 사용한다. AudioSet, 단형 태그, 장형 문장, 재생목록 정보 등, 44M 음악 녹음에서 추출한 오디오-텍스트 쌍과 처리된 텍스트 레이블에 대해 모든 모델을 14 epoch 동안 학습시킨다. M-Resnet-50과 M-AST는 비슷한 성능을 보이므로, 더 나은 학습 효율성을 위해 M-Resnet-50을 선택적으로 사용한다.&lt;/p>
&lt;h3 id="evaluation-tasks">Evaluation Tasks&lt;/h3>
&lt;h4 id="zero-shot-music-tagging">Zero-shot Music Tagging&lt;/h4>
&lt;p>음악 클립의 오디오 임베딩과 태그 텍스트의 임베딩 간 코사인 유사도를 이용해 예측 점수를 정의한다. 컨텍스트 텍스트 encoder를 사용하여 유연한 예측 공간을 만들고, 크로스-모달 대조 학습을 통해 언어의 의미를 오디오 표현에 연결함으로써 보이지 않는 타겟 레이블에도 적용할 수 있다.&lt;/p>
&lt;p>MagnaTagATune과 AudioSet의 음악 부분을 이용한 두 가지 음악 태깅 벤치마크로 평가를 진행한다. MagnaTagATune에서는 상위 50개 태그 세트와 전체 188개 태그 세트를 고려하며, 각 오디오 클립을 10초짜리 3개 세그먼트로 분할해 클립 레벨 임베딩을 얻는다. AudioSet에서는 25개 장르 태깅 작업과 전체 음악 하위 트리를 포함한 141개 태깅 작업을 고려한다. 테스트 세트에서의 수신기 운영 특성 곡선(AUC-ROC)의 클래스 균형을 보고한다.&lt;/p>
&lt;p>AudioSet은 대조학습에 포함되고, 일부 MTAT 클래스는 AudioSet 온톨로지와 겹친다. 이로 인해 AudioSet과 MTAT 평가는 완전한 zero-shot이 아니다. 하지만, MuLan 학습 중에는 다양한 자유 형식의 언어 감독이 AudioSet 감독을 희석시킨다. 따라서 MuLan 모델과 기존 AudioSet 분류기를 비교함으로써, AudioSet 온톨로지를 넘어서는 클래스를 지원하는 유연한 자연 언어 인터페이스로 넘어가는 비용을 측정할 수 있다.&lt;/p>
&lt;h4 id="transfer-learning-with-linear-probes">Transfer Learning with Linear Probes&lt;/h4>
&lt;p>zero-shot 실험 외에도, 오디오 encoder를 downstream 태깅 작업에 적용하는 일반적인 피처 추출기로 평가한다. MagnaTagATune과 AudioSet 벤치마크를 다시 사용하고, 학습 데이터셋을 이용해 고정된 128차원 오디오 임베딩 위에 독립적인 클래스별 로지스틱 회귀 계층을 학습시킨다. 이 방법은 이전 전이 학습 연구의 평가 프로토콜을 따르므로, 성능을 직접 비교할 수 있다.&lt;/p>
&lt;h4 id="music-retrieval-from-text-queries">Music Retrieval from Text Queries&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table3.png"
width="634"
height="390"
srcset="https://kurtkim.github.io/p/mulan/images/table3_hubf8d334ad104ab5b9e994323df780b47_99837_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table3_hubf8d334ad104ab5b9e994323df780b47_99837_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>MuLan은 임베딩 공간에서 쿼리에 가장 가까운 음악 클립을 찾는 능력을 제공한다. 이는 컨텐츠 특징이 메타데이터 기반 방법보다 더 세밀하고 완전한 유사성 정보를 제공할 수 있는 음악 검색 애플리케이션에 중요하다. 전문가가 큐레이션한 7,000개의 재생 목록을 사용하여 평가하며, 각 재생 목록은 제목, 설명, 그리고 10-100개의 음악 녹음으로 구성되어 있다. 재생 목록 평가는 약 100K의 고유 녹음을 포함한다.&lt;/p>
&lt;p>전문가 큐레이션 재생 목록 데이터를 사용해 제목과 설명을 쿼리로 하는 두 개의 크로스-모달 검색 평가 세트를 만든다. 각 데이터셋에 대해 해당 재생 목록에 속하는 녹음을 검색 대상으로 사용하고, 100K 녹음 전체를 후보군으로 사용한다. AUC-ROC과 평균 정밀도(mAP)를 보고하며, zero-shot 태깅과 같은 임베딩 평균화 및 코사인 유사도 기반 점수 매기기 사용한다. 재생 목록 정보는 음악 태깅 벤치마크의 태그와는 다르게, 보다 세밀한 정보를 담고 있어 음악 검색 엔진에 제시되는 쿼리와 유사하다.&lt;/p>
&lt;h4 id="text-triplet-classiﬁcation">Text Triplet Classiﬁcation&lt;/h4>
&lt;p>텍스트 encoder는 도메인 내 음악 데이터와 크로스-모달 대조 손실을 사용해 미세 조정되었다. 텍스트 encoder가 음악 관련 텍스트를 얼마나 잘 이해하는지 평가하기 위해, 트리플렛 분류 작업을 통해 텍스트 임베딩을 직접 평가한다. 각 트리플렛은 (앵커, 긍정, 부정) 형태의 텍스트로 이루어져 있습니다. AudioSet 온톨로지를 사용한 첫 번째 테스트 세트와 전문가 큐레이션 재생 목록 데이터를 사용한 두 번째 테스트 세트를 구성하였다.&lt;/p>
&lt;h3 id="results-and-discussion">Results and Discussion&lt;/h3>
&lt;h4 id="music-tagging">Music Tagging&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table4.png"
width="646"
height="630"
srcset="https://kurtkim.github.io/p/mulan/images/table4_hu5a3de2cab49c8abeac5520f681303a61_139581_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table4_hu5a3de2cab49c8abeac5520f681303a61_139581_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="102"
data-flex-basis="246px"
>&lt;/p>
&lt;p>zero-shot 태깅 메트릭에서 MResnet-50과 M-AST는 비슷한 성능을 보였다. 하지만 학습 텍스트의 레이블 의미와 태깅 평가의 레이블 의미 사이에는 큰 차이가 있을 수 있어, 성능이 저하될 수 있다. 특히, MTAT 간격은 AudioSet보다 크게 나타났으며, 이는 비특정적인 의미나 여러 가지 의미를 가진 태그와 간단한 부정을 포함하는 태그에서 매우 나쁜 성능 때문이다. 이는 BERT의 알려진 문제로, 부정된 개념의 의미를 적절하게 모델링하지 못한다(&amp;ldquo;not rock&amp;quot;의 임베딩은 &amp;ldquo;rock&amp;quot;과 비슷하다).&lt;/p>
&lt;p>AudioSet만을 사용한 학습은 AudioSet 평가에서 가장 높은 AUC를 얻지만, 일반적으로 더 많은 데이터 소스를 포함하면 다른 모든 작업에서 성능이 향상된다. 무필터 데이터로의 학습은 필터링된 버전과 비슷한 성능을 달성하는데, 이는 원시 텍스트 데이터의 노이즈에도 불구하고 모델이 유용한 연관성을 학습하는 것을 보여준다. 텍스트 필터링이 너무 공격적이었을 수 있으며, 대비 학습의 높은 노이즈 허용성 때문에, 강하게 연결된 오디오-텍스트 쌍으로 제한하는 것의 이점이 유용한 쌍의 큰 세트를 잃는 것으로 상쇄되었을 수 있다.&lt;/p>
&lt;p>MuLan 오디오 임베딩에 선형 프로브를 적용하면 모든 태깅 작업에서 최고 수준의 전이 학습 성능을 달성한다. 이는 MuLan의 사전 학습된 오디오 encoder가 고품질 음악 오디오 임베딩을 계속 생성하면서 새로운 자연어 응용 프로그램을 지원한다는 것을 입증한다. 또한, 종단간 학습 기준선에 비해 선형 프로브 결과는 대부분이 더 우수하며, 최고 수준의 AST AudioSet 분류기를 약간만 뒤따른다.&lt;/p>
&lt;h4 id="music-retrieval-from-text-queries-1">Music Retrieval from Text Queries&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table5.png"
width="566"
height="258"
srcset="https://kurtkim.github.io/p/mulan/images/table5_hu62430557e012f8c71275209d360899b6_54993_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table5_hu62430557e012f8c71275209d360899b6_54993_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="526px"
>&lt;/p>
&lt;p>MuLan 모델은 쿼리 검색 평가 작업에서 놀랍게도 강인한 성능을 보여준다. 대규모 언어 자원으로 사전 학습된 BERT를 기반으로 하지만, AudioSet 클립과 레이블 주석만으로 학습을 하면 음악에 대한 도메인 내 자연어를 연결하는 능력이 제한적이다. 그러나 인터넷에서 추출한 대규모 단형태 태그를 포함하면 모델이 더 세분화된 음악 개념을 배울 수 있게 되고, 댓글과 재생 목록 데이터를 추가하면 더 복잡한 쿼리를 연결하는 데 도움이 된다. 또한, 필터링되지 않은 학습 텍스트를 사용하여도 유사한 성능을 달성하는 것으로 보아, 주석 노이즈에 대한 훈련의 강인함이 확인된다.&lt;/p>
&lt;h3 id="text-triplet-classiﬁcation-1">Text Triplet Classiﬁcation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table6.png"
width="476"
height="368"
srcset="https://kurtkim.github.io/p/mulan/images/table6_hufba128f1e6743bcd07342508bb89e20b_70296_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table6_hufba128f1e6743bcd07342508bb89e20b_70296_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="129"
data-flex-basis="310px"
>&lt;/p>
&lt;p>MuLan 텍스트 임베딩은 Sentence Transformer, SimCSE, Universal Sentence Embedding 등과 같은 기준선과 비교된다. 모든 기준선은 Transformer 기반 모델이며, MuLan 텍스트 encoder는 크로스-모달 손실로만 학습된다. 장형 텍스트 주석을 포함하면, 음악 도메인에 특화된 텍스트 임베딩 모델은 일반적인 문장 임베딩 모델을 능가한다. 놀랍게도, 텍스트 전용 미세 조정 손실을 사용하지 않아도 성공적인 특화가 이루어졌다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>약하게 연결된 텍스트와 오디오 데이터를 이용해 학습된 음악 오디오와 자연어 공동 임베딩 모델을 제시하였다. 이 모델은 다양한 응용에서 자연어 인터페이스의 유연성을 보여주며, 음악 태깅 벤치마크에서 최고 수준의 전이 학습 성능을 보여준다. 이는 음악 오디오에 대한 자유형 자연어 인터페이스를 구축하는 첫 시도로, 약한 신호와 절대적인 노이즈를 더 잘 구분하는 개선된 텍스트 필터링 방법을 통해 더욱 발전될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2208.12415.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/musiclm-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>EnCodec</title><link>https://kurtkim.github.io/p/encodec/</link><pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/encodec/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>neural network를 활용한 state-of-the-art real-time, high-ﬁdelity, audio codec을 소개한다. 이는 아티팩트를 효율적으로 줄이고 고품질 샘플을 생성하는 스트리밍 encoder-decoder 구조이다. 학습을 안정화하기 위해 loss balancer mechanism을 도입하였으며, lightweight Transformer 모델을 사용하여 얻은 표현을 최대 40%까지 더 압축하는 방법을 연구하였다. 이 모델은 말하기, 소음이 많은 반향성 말하기, 음악 등 다양한 오디오 도메인에서 우수한 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>2021년에 스트리밍 오디오와 비디오가 인터넷 트래픽의 82%를 차지했고, 이런 트렌드는 오디오 압축의 중요성을 강조한다. 손실 압축은 샘플의 비트레이트와 왜곡을 최소화하는 것을 목표로 한다. 오디오 codec은 중복성을 제거하고 컴팩트한 비트 스트림을 생성하기 위해 encoder와 decoder를 결합한다. neural network를 활용한 encoder-decoder 메커니즘은 오디오 신호에 중점을 둔 연구의 일환으로서 탐구되어 왔다.&lt;/p>
&lt;p>lossy neural compression 모델에서는 두 가지 문제가 발생한다. 첫 번째는 학습 세트를 과적합하지 않고, 아티팩트가 많은 오디오를 생성하지 않도록 다양한 신호를 표현해야 하는 것이다. 이를 위해 다양한 학습 세트와 perceptual 손실로 작용하는 discriminator network를 사용하였다. 두 번째 문제는 계산 시간과 크기를 모두 고려하여 효율적으로 압축하는 것이다.&lt;/p>
&lt;p>실시간으로 단일 CPU 코어에서 작동하는 모델에 제한을 두며, neural encoder의 출력에 대한 residual vector quantization를 사용하여 효율적으로 압축한다. 이에 대한 여러 방법이 이전의 연구에서 제안되었다.&lt;/p>
&lt;p>end-to-end neural compression 모델 설계가 encoder-decoder 아키텍처, quantization 방법, perceptual 손실 등을 포함한 선택의 집합이라고 주장한다. 이 모델의 평가는 객관적인 방법과 인간의 인식에 의존하는 방법 두 가지를 사용하였고, 이를 통해 이 모델이 음성과 음악 압축에서 state-of-the-art를 달성하였음을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Speech and Audio Synthesis.&lt;/strong> 최근의 neural audio generation 기술 발전은 컴퓨터가 효율적으로 자연스러운 오디오를 생성하도록 하였다. autoregressive 모델인 WaveNet이 초기 성공을 거뒀지만, 추론 속도가 느렸다. 여러 다른 방법이 탐색되었지만, 특히 Generative Adversarial Network (GAN) 기반의 방법이 주목 받았다. 이들은 다양한 adversarial network를 결합하여 더 빠른 속도로 autoregressive 모델의 품질을 달성하였다. 이 연구는 이러한 adversarial 손실을 활용하고 확장하여 오디오 생성 중의 아티팩트를 줄이는 데 초점을 맞추고 있다.&lt;/p>
&lt;p>&lt;strong>Audio Codec.&lt;/strong> 낮은 비트레이트의 음성과 오디오 codec에 대한 연구가 오랫동안 이루어졌지만, 품질은 제한적이었다. excitation signal을 모델링하는 것은 여전히 어려운 과제로 남아 있다. 현재 state-of-the-art인 전통적인 오디오 codec은 Opus와 Enhanced Voice Service (EVS)로, 다양한 비트레이트, 샘플링 레이트, 실시간 압축을 지원하며 높은 코딩 효율성을 보여준다.&lt;/p>
&lt;p>최근에 제안된 neural based audio codec은 놀라운 결과를 보여주었다. 대부분의 방법들은 latent space를 quantizing한 후 decoder에 입력하는 방식을 사용하였다. 여러 연구들에서 다양한 접근법이 시도되었으며, 가장 관련성이 높은 연구로는 SoundStream 모델이 있다. 이 모델은 Residual Vector Quantization layer를 포함하는 fully convolutional encoder-decoder 아키텍처를 제안하였고, reconstruction 손실과 adversarial perceptual 손실 모두를 사용하여 최적화하였다.&lt;/p>
&lt;p>&lt;strong>Audio Discretization.&lt;/strong> 최근에는 discrete 값으로 오디오와 음성을 표현하는 방법이 다양한 작업에 적용되었다. raw 오디오의 discrete 표현을 학습하기 위한 계층적 VQ-VAE 기반 모델은 고품질 음악 생성을 가능하게 했고, 음성에 대한 self-supervised 학습 방법이 conditional 및 unconditional 음성 생성에 사용되었다. 이러한 방법은 음성 재합성, 음성 감정 변환, 대화 시스템, 음성-음성 번역 등의 분야에도 적용되었다.&lt;/p>
&lt;hr>
&lt;h2 id="model">Model&lt;/h2>
&lt;p>오디오 신호의 기간이 $d$라면, 이 신호는 $x \in [−1, 1]^{C_a \times T}$ 시퀀스로 표현될 수 있다. 여기서 $C_a$는 오디오 채널의 수이고, $T = d \cdot f_{sr}$는 주어진 샘플 비율 $f_{sr}$에서의 오디오 샘플 수이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/figure1.png"
width="1366"
height="608"
srcset="https://kurtkim.github.io/p/encodec/images/figure1_hu93b04515f5473038596df8394bd78148_284692_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/figure1_hu93b04515f5473038596df8394bd78148_284692_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="539px"
>&lt;/p>
&lt;p>EnCodec 모델은 오디오 신호를 처리하는 세 가지 주요 요소로 구성된다. 첫째, encoder 네트워크 $E$는 오디오를 latent representation $z$로 변환한다. 둘째, quantization layer $Q$는 vector quantization 를 이용해 압축된 표현 $z_q$를 생성한다. 셋째, decoder 네트워크 $G$는 compressed latent representation $z_q$을 원래의 시간 도메인 신호 $x$로 재구성한다. 이 시스템은 시간과 주파수 도메인에서의 reconstruction 손실 최소화를 목표로 학습되며, 이 과정에는 다른 해상도에서 작동하는 판별자의 discriminator 손실이 포함된다.&lt;/p>
&lt;h3 id="encoder--decoder-architecture">Encoder &amp;amp; Decoder Architecture&lt;/h3>
&lt;p>EnCodec 모델은 streaming과 convolutional-based encoder-decoder 구조로, latent representation에 순차적 모델링을 적용한다. 이 구조는 다양한 오디오 작업에서 뛰어난 성과를 보였으며, source separation, enhancement, neural vocoder, audio codec, artiﬁcial bandwidth extension 등에 활용되었다. 이 모델은 24 kHz와 48 kHz 오디오에 동일하게 적용된다.&lt;/p>
&lt;p>&lt;strong>Encoder-Decoder.&lt;/strong> EnCodec의 encoder 모델 $E$는 1D convolution과 여러 convolution block으로 구성된다. 각 block은 residual unit과 strided convolution으로 이루어진 down-sampling layer를 포함하며, down-sampling이 있을 때마다 채널 수가 두 배씩 증가한다. 이어서 시퀀스 모델링을 위한 LSTM 계층과 1D convolution layer가 뒤따른다. 이 모델은 low-latency streamable과 high ﬁdelity non-streamable에 따라 두 가지 변형으로 사용된다. encoder는 24 kHz에서 초당 75개, 48 kHz에서는 초당 150개의 latent step을 출력하며, decoder는 이를 받아 최종 오디오를 생성한다.&lt;/p>
&lt;p>&lt;strong>Non-streamable.&lt;/strong> non-streamable 설정에서는 각 convolution에 대해 총 패딩 $K - S$를 사용하고, 입력을 1초 청크로 분할한다. 10ms의 오버랩을 통해 클릭을 방지하고, 각 청크를 모델에 공급하기 전에 normalization한다. decoder의 출력에 inverse operation을 적용하고, 스케일 전송에 대한 negligible bandwidth overhead를 최소화한다. layer normalization를 사용하여 상대적인 스케일 정보를 유지한다.&lt;/p>
&lt;p>&lt;strong>Streamable.&lt;/strong> streamable 설정에서는 모든 패딩을 첫 번째 시간 단계 전에 배치한다. 스트라이드가 있는 transposed convolution을 사용하여, 처음 $s$ 시간 단계를 출력하고, 다음 프레임이 준비되면 나머지를 완성하거나, 스트림 끝에서 버린다. 이 패딩 방식 덕분에 모델은 첫 320 샘플을 받자마자 320 샘플을 출력할 수 있다. 또한, streamable 설정에 부적합한 layer normalization 대신 weight normalization를 사용한다. 이렇게 normalization을 유지함으로써 목표 지표에서 약간의 향상을 얻었다.&lt;/p>
&lt;h3 id="residual-vector-quantization">Residual Vector Quantization&lt;/h3>
&lt;p>encoder의 출력을 quantize 하기 위해 Residual Vector Quantization (RVQ)을 사용한다. Vector quantization는 입력 벡터를 코드북의 가장 가까운 항목에 투영하는 것이며, RVQ는 이를 개선하여 quantization 후의 residual을 계산하고 추가로 quantizing 한다.&lt;/p>
&lt;p>Dhariwal et al. 과 Zeghidour et al. 이 설명한 학습 절차를 따르며, 각 입력에 대한 코드북 항목을 exponential moving average을 사용해 업데이트한다. 사용되지 않는 항목은 현재 batch에서 샘플링된 후보로 대체된다. encoder의 기울기를 계산하기 위해 straight-through-estimator를 사용하고, quantizer의 입력과 출력 사이의 MSE로 구성된 commitment 손실을 전체 학습 손실에 추가한다.&lt;/p>
&lt;p>학습 시간에 residual step의 수를 조절하여, 단일 모델이 multiple bandwidth 목표를 지원할 수 있다. 모든 모델은 최대 32개(48 kHz 모델은 16개)의 코드북을 사용하며, 각 코드북은 1024개의 항목을 가진다. variable bandwidth 학습 시, 4의 배수로 코드북의 수를 무작위로 선택한다. 이렇게 하여, encoder에서 나오는 continuous latent represention을 discrete set of index로 변환하고, 이를 decoder로 들어가기 전에 다시 벡터로 변환한다.&lt;/p>
&lt;h3 id="language-modeling-and-entropy-coding">Language Modeling and Entropy Coding&lt;/h3>
&lt;p>실시간보다 빠른 compression/decompression을 목표로, small Transformer 기반 언어 모델을 학습시킨다. 모델은 5개 layer, 8개 head, 200개 channel, feed-forward block의 800 dimension을 가진다. 학습 시, bandwidth과 해당 코드북의 수를 선택하고, 시간 단계별로 discrete representation을 continuous representation으로 변환한다. Transformer의 출력은 linear layer에 공급되어 각 코드북에 대한 estimated distribution의 logit을 제공한다. 이 방법은 코드북간의 잠재적 정보를 무시하면서도 추론을 가속화합니다. 모델은 5초 시퀀스에서 훈련됩니다.&lt;/p>
&lt;p>&lt;strong>Entropy Encoding.&lt;/strong> 언어 모델로부터 얻은 추정 확률을 활용하기 위해, range based arithmetic coder를 사용한다. 다른 아키텍처나 ﬂoating point approximation으로 인해 동일한 모델의 평가가 다르게 나올 수 있어 디코딩 오류가 발생할 수 있다. 특히, batch 평가와 real-life streaming 평가 사이에는 큰 차이가 있을 수 있다. 따라서 추정 확률을 $10^{-6}$의 정밀도로 반올림하며, 총 범위 너비를 $2^{24}$로, 최소 범위 너비를 $2$로 설정한다. 처리 시간에 미치는 영향에 대해서는 추후 논의하고자 한다.&lt;/p>
&lt;h3 id="training-objective">Training objective&lt;/h3>
&lt;p>reconstruction 손실, perceptual 손실 (via discriminators), 그리고 RVQ commitment 손실을 결합한 학습 목표를 상세히 설명한다.&lt;/p>
&lt;p>&lt;strong>Reconstruction Loss.&lt;/strong> reconstruction loss term은 시간 도메인과 주파수 도메인의 손실 항으로 이루어진다. 시간 도메인에서는 목표 오디오와 압축 오디오 사이의 L1 거리를 최소화하고, 주파수 도메인에서는 mel-spectrogram에서의 L1과 L2 손실의 선형 조합을 사용한다.&lt;/p>
&lt;p>$$ l_f(x, \hat{x}) = {{1}\over{|\alpha| \cdot |s|}} \sum_{\alpha_i \in \alpha} \sum_{i \in e} \parallel S_i(X) - S_i(\hat{x}) \parallel_1 + \alpha \parallel S_i(X) - S_i(\hat{x}) \parallel_2 $$&lt;/p>
&lt;p>$S_i$는 window size가 $2^i$ 이고 hop length가 $2^i/4$인 normalized STFT를 사용한 64-bins mel-spectrogram이다. $e = 5, &amp;hellip;, 11$은 스케일의 집합을 나타내고, $\alpha$$는 L1과 L2 항 사이의 균형을 맞추는 스칼라 계수의 집합이다. 단, 이 논문에서는 $\alpha_i = 1$을 선택하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/figure2.png"
width="1228"
height="364"
srcset="https://kurtkim.github.io/p/encodec/images/figure2_hu25e8a1a1e6db1cb982324c308cf16f9a_97901_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/figure2_hu25e8a1a1e6db1cb982324c308cf16f9a_97901_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="337"
data-flex-basis="809px"
>&lt;/p>
&lt;p>&lt;strong>Discriminative Loss.&lt;/strong> 생성된 샘플의 품질을 향상시키기 위해, multi-scale STFT-based (MS-STFT) discriminator에 기반한 perceptual 손실 항을 도입하였다. 이 discriminator는 audio signal의 다양한 구조를 포착하도록 설계되었으며, 복소수 값을 가진 multi-scale STFT에서 작동하는 동일 구조의 네트워크로 구성되어 있다. 각 하위 네트워크는 2D convolutional layer, 팽창율이 증가하는 2D convolution, 그리고 주파수 축에서 스트라이드를 가지고 있다. 이 discriminator는 STFT window length가 다양한 5개의 스케일을 사용하며, 오디오의 샘플링 레이트에 따라 window size를 조정한다. LeakyReLU 활성화 함수와 weight normalization을 사용한다.&lt;/p>
&lt;p>generator에 대한 adversarial 손실은 discriminator의 수(K)에 따라 구성되며, 이는 $l_g(\hat{x}) = {{1}\over{K}} \sum_k max(0, 1 − D_k(\hat{x}))$ 공식으로 표현된다. 또한, 이전 neural vocoder 연구와 같이, generator에 대한 상대적 특징 매칭 손실을 추가적으로 포함한다.&lt;/p>
&lt;p>$$ l_{feat}(x, \hat{x}) = {{1}\over{KL}} \sum_{k = 1}^K \sum_{l = 1}^L {{\parallel D_k^l(x) - D_k^l(\hat{x}) \parallel_1}\over{mean(\parallel D_k^l(x) \parallel_1)}} $$&lt;/p>
&lt;p>평균은 모든 차원에서 계산되며, discriminator들은 hinge 손실 adversarial 손실 함수를 최소화하는 것을 목표로 한다. discriminator가 decoder를 쉽게 압도하는 경향이 있으므로, 24 kHz에서는 2/3의 확률로, 48 kHz에서는 0.5의 확률로 discriminator의 가중치를 업데이트한다.&lt;/p>
&lt;p>&lt;strong>Multi-bandwidth training.&lt;/strong> 24kHz와 48kHz에서 모델은 각각 다양한 bandwidth을 지원하도록 학습된다. 이 과정에서 RVQ step에서의 코드북 선택이 중요하며, 특정 bandwidth에 대해 전용 discriminator를 사용하면 오디오 품질이 향상된다. 이렇게 선택된 bandwidth은 entire batch에 적용되며, 해당 discriminator만 업데이트 된다.&lt;/p>
&lt;p>&lt;strong>VQ commitment loss.&lt;/strong> encoder 출력과 양자화된 값 사이에 commitment 손실 $l_w$를 추가한다. 각 residual step $c \in \lbrace 1, &amp;hellip;, C \rbrace$에서, 현재 residual과 코드북 $q_c(z_c)$ 의 가장 가까운 항목을 이용해 $l_w$를 정의한다. 이때, residual step의 수는 현재 batch의 bandwidth 목표에 따라 달라진다.&lt;/p>
&lt;p>$$ l_w = \sum_{c = 1}^{C} \parallel z_c - q_c(z_c) \parallel_2^2 $$&lt;/p>
&lt;p>전반적으로, generator는 batch를 합산한 다음 손실을 최적화하도록 학습된다.&lt;/p>
&lt;p>$$ L_G = \lambda_t \cdot l_t(x, \hat{x}) + \lambda_f \cdot l_f(x, \hat{x}) + \lambda_g \cdot l_g(\hat{x}) + \lambda_{feat} \cdot l_{feat}(x, \hat{x}) + \lambda_w \cdot l_w(w) $$&lt;/p>
&lt;p>여기서 $\lambda_t, \lambda_f, \lambda_g, \lambda_{feat}, \lambda_w$는 각 항목들 사이의 균형을 맞추기 위한 스칼라 계수들이다.&lt;/p>
&lt;p>&lt;strong>Balancer.&lt;/strong> discriminator로부터 나오는 gradient의 varying scale을 안정화시키기 위해 손실 balancer가 도입되었다. 이는 다른 손실 가중치를 더 쉽게 이해할 수 있게 돕는다. 모델의 출력에만 의존하는 손실들을 고려하고, 이들의 exponential moving average $g_i = {{\delta l_i}\over{\delta \hat{x}}},\langle \parallel g_i \parallel_2 \rangle_{\beta}$를 정의한다. 주어진 weight 집합 $(\lambda_i)$와 reference norm $R$에 따라, 이를 정의한다.&lt;/p>
&lt;p>$$ \tilde{g}_i = R {{\lambda_i}\over{\sum_j \lambda_j}} \cdot {{g_i}\over{\langle \parallel g_i \parallel_2 \rangle_{\beta}}} $$&lt;/p>
&lt;p>원래의 gradient 대신 수정된 gradient를 네트워크로 backpropaga한다. 이로 인해 최적화 문제는 변화하지만, 각 손실 스케일에 상관없이 가중치를 해석할 수 있게 한다. 만약 가중치의 합이 1이라면, 각 가중치는 해당 손실로부터 모델 gradient의 비율로 해석될 수 있다. 모든 discriminator 손실은 balancer를 통해 적용되지만, commitment 손실은 모델 출력에 대해 정의되지 않아 제외된다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments-and-results">Experiments and Results&lt;/h2>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>EnCodec은 다양한 도메인의 24kHz 모노포닉 오디오에 대해 학습되며, fullband 스테레오 EnCodec는 48kHz 음악에 대해 학습된다. 음성에 대해서는 DNS 챌린지 4와 Common Voice 데이터셋을, 일반 오디오에 대해서는 AudioSet과 FSD50K를, 음악에 대해서는 Jamendo 데이터셋을 사용한다. 추가로 소유한 음악 데이터셋을 사용하여 모델을 평가한다.&lt;/p>
&lt;p>학습과 검증을 위해, 단일 소스 샘플링 또는 여러 소스 혼합을 포함하는 혼합 전략을 사용한다. 이는 네 가지 전략으로 나뉘는데, Jamendo에서 단일 소스를 샘플링하거나, 다른 데이터셋에서 단일 소스를 샘플링하거나, 모든 데이터셋에서 두 소스를 혼합하거나, 음악을 제외한 모든 데이터셋에서 세 소스를 혼합한다. 각 전략은 특정 확률로 실행된다.&lt;/p>
&lt;p>오디오는 파일별로 정규화되며, 무작위 게인을 적용한다. 클리핑된 샘플은 제외하고, 일정 확률로 잔향을 추가한다. 테스트는 DNS에서의 깨끗한 음성, FSDK50K 샘플과 혼합된 음성, Jamendo 샘플, 소유한 음악 샘플 등 네 가지 카테고리를 사용한다.&lt;/p>
&lt;h3 id="baselines">Baselines&lt;/h3>
&lt;p>Opus와 EVS라는 두 종류의 오디오 코덱을 기본 베이스라인으로 사용하며, 추가적으로 MP3 압축도 베이스라인으로 활용한다. 마지막으로, 업샘플링된 오디오에서 EnCodec와 비교하기 위해 SoundStream 모델을 사용한다. 또한 SoundStream 버전을 약간 개선하여 사용하였다. 이는 relative feature 손실과 layer normalization을 적용함으로써 오디오 품질을 향상시키는데 도움이 되었다.&lt;/p>
&lt;h3 id="evaluation-methods">Evaluation Methods&lt;/h3>
&lt;p>주관적 평가를 위해 MUSHRA 프로토콜을 따르며, 샘플의 지각 품질을 1부터 100까지 평가하는 주석 처리자를 모집하였다. 테스트 세트의 각 카테고리에서 무작위로 선택한 샘플에 대해 주석을 추가하였으며, 노이즈가 많은 주석과 이상치는 제거했다. 객관적 평가를 위해서는 ViSQOL과 SI-SNR를 사용하였다.&lt;/p>
&lt;h3 id="training">Training&lt;/h3>
&lt;p>모든 모델은 Adam optimizer를 사용하여 300 epoch 동안 학습되고, 각 epoch는 1초당 64개의 예제로 구성된 batch로 2,000번의 업데이트를 포함한다. 모델은 8개의 A100 GPU를 사용하여 학습되며, 특정 가중치가 적용된 balancer를 사용한다. 가중치는 24kHz 모델의 경우 λt = 0.1, λf = 1, λg = 3, λfeat = 3 이고, 48kHz 모델은 서로 다른 가중치를 사용한다.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/figure3.png"
width="1318"
height="472"
srcset="https://kurtkim.github.io/p/encodec/images/figure3_hu6acc880db562e567b4095ddc4a8ce71e_84409_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/figure3_hu6acc880db562e567b4095ddc4a8ce71e_84409_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="279"
data-flex-basis="670px"
>&lt;/p>
&lt;p>다양한 bandwidth을 가진 EnCodec의 결과를 베이스라인과 비교하였다. Gumbel-Softmax와 DiffQ와 같은 다른 양자화기를 탐색했지만, 이들이 비슷하거나 더 나쁜 결과를 보여주어 보고하지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/table1.png"
width="1272"
height="490"
srcset="https://kurtkim.github.io/p/encodec/images/table1_hu29e81f7405dfee94c4dc5b1c29f38ce2_144576_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/table1_hu29e81f7405dfee94c4dc5b1c29f38ce2_144576_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="623px"
>&lt;/p>
&lt;p>동일한 bandwidth을 고려하면, EnCodec는 모든 베이스라인보다 우수한 성능을 보여준다. 추가적인 언어 모델을 적용하면 bandwidth을 약 25-40% 줄일 수 있다. 하지만 높은 bandwidth에서는 압축 비율이 낮아지는 것을 관찰할 수 있는데, 이는 사용된 Transformer 모델의 크기가 작아서 모든 코드북을 함께 모델링하기 어렵기 때문일 수 있다.&lt;/p>
&lt;h4 id="ablation-study">Ablation study&lt;/h4>
&lt;p>다음으로, discriminator 설정, streaming, multitarget bandwidth, balancer의 효과를 더 잘 평가하기 위해 ablation 연구를 수행한다.&lt;/p>
&lt;p>&lt;strong>The eﬀect of discriminators setup.&lt;/strong> 이전 연구에서는 생성된 오디오의 perceptual 품질을 향상시키기 위해 여러 discriminator를 제안하였다. Multi-Scale Discriminator(MSD) 모델은 다양한 해상도에서 raw waveform에 작용한다. Kong et al. 은 Multi-Period Discriminator(MPD) 모델을 추가로 제안했는데, 이는 waveform을 여러 주기를 가진 2D 입력으로 변환한다. 그리고 STFT Discriminator(Mono-STFTD) 모델은 complex-valued STFT에 작용하는 단일 네트워크이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/table2.png"
width="722"
height="236"
srcset="https://kurtkim.github.io/p/encodec/images/table2_hu9223290a02f4c74eded16762d914f148_47075_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/table2_hu9223290a02f4c74eded16762d914f148_47075_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="305"
data-flex-basis="734px"
>&lt;/p>
&lt;p>MS-STFTD discriminator는 MSD+MonoSTFTD, MPD only, MS-STFTD only, MS-STFTD+MPD와 같은 세 가지 다른 설정과 비교되었다. 결과는 MS-STFTD와 같은 multi-scale STFT-based discriminator만을 사용하는 것이 오디오의 고품질을 생성하는 데 충분하며, 모델 학습을 단순화하고 학습 시간을 줄인다는 것을 보여준다. MPD 판별자를 추가하면 MUSHRA 점수가 약간 향상되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/table3.png"
width="622"
height="222"
srcset="https://kurtkim.github.io/p/encodec/images/table3_hu902f2b842537c031dd581455f48490f8_37562_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/table3_hu902f2b842537c031dd581455f48490f8_37562_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="672px"
>&lt;/p>
&lt;p>&lt;strong>The eﬀect of the streamable modeling.&lt;/strong> streamable 설정과 non-streamable 설정을 비교한 결과, 예상대로 non-streamable 설정에서 streamable 설정으로 전환하면 성능이 약간 저하되지만, 스트리밍 추론이 가능해지면서도 성능이 강하게 유지된다.&lt;/p>
&lt;p>&lt;strong>The eﬀect of the balancer.&lt;/strong> balancer의 영향을 평가하는 결과를 제시했다. balancer의 유무와 상관없이 다양한 값들을 고려하여 EnCodec 모델을 학습시켰고, 예상대로 balancer가 학습 과정을 크게 안정화시키는 것을 확인하였다.&lt;/p>
&lt;h4 id="stereo-evaluation">Stereo Evaluation&lt;/h4>
&lt;p>이전의 모든 결과는 monophonic 설정만을 고려하였다. 그러나 음악 데이터의 경우 스테레오 compression이 중요하므로, discriminator 설정을 수정하여 현재 설정을 스테레오로 변경하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/table4.png"
width="926"
height="336"
srcset="https://kurtkim.github.io/p/encodec/images/table4_hu6b695076b4e9fd83468cd13cfa20d3b4_80376_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/table4_hu6b695076b4e9fd83468cd13cfa20d3b4_80376_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="275"
data-flex-basis="661px"
>&lt;/p>
&lt;p>6 kbps에서 작동하는 EnCodec은 Opus를 크게 능가하고, 64 kbps MP3와 비슷한 성능을 보여준다. 12 kbps에서의 EnCodec은 24 kbps에서의 EnCodec와 비교 가능한 성능을 보여준다. 언어 모델과 entropy coding을 사용하면 20%에서 30%의 가변 이득을 얻을 수 있다.&lt;/p>
&lt;h3 id="latency-and-computation-time">Latency and computation time&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/encodec/images/table5.png"
width="944"
height="250"
srcset="https://kurtkim.github.io/p/encodec/images/table5_hu2beba8ea81e367bfe5ad26acea09aeb6_48294_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/encodec/images/table5_hu2beba8ea81e367bfe5ad26acea09aeb6_48294_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="377"
data-flex-basis="906px"
>&lt;/p>
&lt;p>실시간 요소는 오디오의 지속 시간과 처리 시간의 비율로, 방법이 실시간보다 빠를 때 1보다 크다. 이는 6 kbps에서 MacBook Pro 2019 CPU의 단일 스레드에서 모든 모델을 분석한 결과이다.&lt;/p>
&lt;p>&lt;strong>Initial latency.&lt;/strong> 24 kHz streaming EnCodec 모델은 initial latency 시간이 13.3ms이다. 반면, 48 kHz non-streaming 버전은 사용된 normalization으로 인해 initial latency 시간이 1초이다. entropy coding 사용시 오버헤드를 줄이기 위해 각 프레임마다 스트림을 플러시할 수 없어 initial latency 시간이 증가하며, 이로 인해 지연 시간이 13ms 증가한다.&lt;/p>
&lt;p>&lt;strong>Real time factor.&lt;/strong> EnCodec 모델은 처리 속도가 Lyra v2보다 느리지만, 실시간보다 10배 빠르게 오디오를 처리하여 실제 응용에 적합하다. entropy coding의 이점은 비용이 따르지만, 실시간보다 빠른 처리로 latency가 크게 중요하지 않은 응용에 사용 가능하다. 48 kHz에서는 처리 속도가 실시간보다 느리지만, 효율적인 구현이나 accelerated hardware를 사용하면 성능을 개선할 수 있다. 실시간 처리가 필요하지 않은 경우에도 활용 가능하다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>다양한 sample rate와 bandwidth에서 고품질 오디오 샘플을 생성하는 실시간 신경 오디오 압축 모델인 EnCodec을 소개하였다. 간단하지만 효과적인 spectrogram-only adversarial 손실을 통해 샘플 품질을 향상시켰고, 새로운 gradient balancer를 통해 학습을 안정화하고 손실에 대한 가중치의 해석 가능성을 개선하였다. 또한, small Transformer 모델을 사용하여 품질 저하 없이 bandwidth을 최대 40%까지 줄일 수 있음을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2210.13438.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/encodec" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>SoundStream</title><link>https://kurtkim.github.io/p/soundstream/</link><pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/soundstream/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>SoundStream이라는 새로운 neural audio codec은 음성, 음악, 일반 오디오를 효율적으로 압축할 수 있다. 이 codec은 fully convolutional encoder/decoder network와 residual vector quantizer로 구성되어 있으며, 학습 과정은 최근의 text-to-speech와 speech enhancement 기술을 활용한다. 이 모델은 3 kbps에서 18 kbps까지 다양한 비트레이트에서 작동할 수 있으며, 실시간 스마트폰 CPU에서 스트림 가능한 추론을 지원한다. 3 kbps의 SoundStream은 12 kbps의 Opus를 뛰어넘고, 9.6 kbps의 EVS에 근접한다. 추가적으로, 이 codec은 추가적인 지연 없이 압축과 향상을 동시에 수행할 수 있어, 배경 소음 억제 등의 기능도 가능하다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>audio codec은 waveform codec과 parametric codec 두 가지로 나눌 수 있다. waveform codec은 입력 오디오 샘플을 충실히 재구성하는 것을 목표로 한다. 이는 transform coding technique을 사용하며, 오디오 콘텐츠의 유형에 대한 가정을 거의 하지 않는다. 따라서 일반 오디오에 대해 작동할 수 있지만, 비트레이트가 낮아질수록 코딩 아티팩트가 발생하는 경향이 있다. 반면 parametric codec은 특정 오디오에 대한 가정을 통해 이 문제를 해결하려고 한다. 이는 오디오 합성 과정을 설명하는 parametric 모델을 사용하며, 샘플마다 완벽하게 재구성하는 것이 아니라 원본과 지각적으로(perceptually) 유사한 오디오를 생성하는 것을 목표로 한다.&lt;/p>
&lt;p>전통적인 waveform과 parametric codec은 신호 처리 기법과 심리음향학, 음성 합성 등의 도메인 지식을 활용해 설계된다. 최근에는 머신러닝 모델이 오디오 압축에 성공적으로 적용되어, 데이터 기반 솔루션의 가치를 입증하였다. 이러한 모델은 기존 코덱의 품질을 향상시키는 후처리 단계로 사용될 수 있으며, 이는 주파수 대역폭 확장, 오디오 denoising, 패킷 손실 은폐 등을 통해 이루어진다.&lt;/p>
&lt;p>머신러닝 기반 모델은 audio codec 구조의 핵심 부분으로 사용되며, 최근의 text-to-speech(TTS) 기술 발전의 중요한 역할을 한다. 예를 들어, 텍스트에서 음성을 생성하는 WaveNet이라는 모델은 neural codec의 decoder로 사용되었다. 다른 neural audio codec들은 WaveRNN을 사용한 LPCNet이나 WaveGRU를 사용한 Lyra와 같은 다양한 모델 구조를 채택하였으며, 이들은 모두 낮은 비트레이트에서의 음성을 목표로 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure1.png"
width="598"
height="580"
srcset="https://kurtkim.github.io/p/soundstream/images/figure1_hu8deee926add611e6b91d2738216b716a_57970_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure1_hu8deee926add611e6b91d2738216b716a_57970_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="103"
data-flex-basis="247px"
>&lt;/p>
&lt;p>이 논문에서는 SoundStream이라는 새로운 audio codec을 제안한다. 이 코덱은 음성, 음악, 일반 오디오를 이전 codec보다 효율적으로 압축하며, state-of-the-art neural audio 합성 기술과 새로운 학습 가능한 양자화 모듈을 활용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure2.png"
width="1462"
height="466"
srcset="https://kurtkim.github.io/p/soundstream/images/figure2_hu31e553135f5e9d42aeb59c457eb96681_155594_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure2_hu31e553135f5e9d42aeb59c457eb96681_155594_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="313"
data-flex-basis="752px"
>&lt;/p>
&lt;p>SoundStream의 구조는 fully convolutional encoder와 decoder로 구성되어 있다. encoder는 시간 영역 waveform을 입력으로 받아 낮은 샘플링 비율의 임베딩 시퀀스를 생성하고, 이를 residual vector quantizer로 양자화한다. decoder는 양자화된 임베딩을 받아 원본 waveform의 근사치를 재구성한다.&lt;/p>
&lt;p>모델은 reconstruction과 adversarial 손실을 모두 사용하여 end-to-end로 학습되며, discriminator가 decoding된 오디오와 원본 오디오를 구별하는 역할을 한다. encoder와 decoder 모두 causal convolution만 사용하므로, 전체적인 아키텍처의 대기 시간은 원래 waveform과 임베딩 사이의 시간 resampling ratio에 의해 결정된다.&lt;/p>
&lt;p>요약하자면, 이 논문은 다음과 같은 주요 기여를 한다:&lt;/p>
&lt;ul>
&lt;li>모든 구성 요소(encoder, decoder, quantizer)가 reconstruction과 adversarial 손실의 혼합으로 end-to-end로 학습되어 뛰어난 오디오 품질을 달성하는 neural audio codec인 SoundStream을 제안한다.&lt;/li>
&lt;li>residual vector quantizer 를 도입하고, 그 설계로 인해 암시되는 rate-distortion-complexity 트레이드오프를 조사한다. 또한, &amp;ldquo;quantizer dropout&amp;quot;이라는 새로운 기법을 제안하여 단일 모델이 다양한 비트레이트를 처리할 수 있게 한다.&lt;/li>
&lt;li>encoder를 학습함으로써 mel-spectrogram 특성을 사용하는 방법보다 코딩 효율성이 크게 향상된다는 것을 입증한다.&lt;/li>
&lt;li>주관적 품질 지표를 통해 SoundStream이 다양한 비트레이트에서 Opus와 EVS를 모두 능가한다는 것을 보여준다.&lt;/li>
&lt;li>낮은 대기 시간에서 작동하는 스트리밍 추론을 지원하도록 설계되었으며, 스마트폰에서 실시간으로 단일 CPU 스레드에서 실행된다.&lt;/li>
&lt;li>추가적인 대기 시간 없이 오디오 압축과 향상을 동시에 수행하는 SoundStream 코덱 변형을 제안한다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Traditional audio codecs&lt;/strong> Opus와 EVS는 다양한 콘텐츠 유형, 비트레이트, 샘플링 레이트에 대해 높은 코딩 효율성을 제공하며 실시간 오디오 통신에 필요한 낮은 대기 시간을 보장하는 최첨단 오디오 코덱이다. 이 논문에서는 이들과 SoundStream을 주관적 평가를 통해 비교한다.&lt;/p>
&lt;p>&lt;strong>Audio generative models&lt;/strong> 텍스트나 코딩된 특성을 오디오 waveform으로 변환하는 여러 생성 모델이 개발되었다. WaveNet과 SampleRNN은 고품질의 오디오를 생성하지만 계산 복잡성이 높다. 그러나 Parallel WaveNet은 병렬 계산을 가능하게 하여 속도를 향상시킨다. 또한, 최근에는 계산 복잡성이 낮으면서 고품질의 오디오를 생성하는 adversarial 모델, MelGAN과 HiFiGAN이 등장하였다. 이들 모델의 설계 방식은 SoundStream의 decoder 설계와 손실 계산에 영향을 미쳤다.&lt;/p>
&lt;p>&lt;strong>Audio enhancement&lt;/strong> 딥 뉴럴 네트워크는 denoising부터 주파수 대역폭 확장 등 다양한 오디오 향상 작업에 활용되었다. 이 논문에서는 추가 대기 시간 없이 단일 모델로 오디오 향상과 압축을 동시에 수행할 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;strong>Vector quantization&lt;/strong> optimal quantizer를 학습하는 것은 높은 코딩 효율성을 달성하는 핵심이다. 벡터 양자화는 전통적인 오디오 코덱의 구성 요소였으며, 최근에는 신경망 모델에서 입력 특성의 압축에 사용되었다. 하지만, 비율이 증가하면서 코드북의 크기가 급격히 커지는 문제가 있다. 이를 해결하기 위해, SoundStream에서는 나머지 모델과 함께 end-to-end로 학습되는 residual vector quantizer를 도입하였다. 이는 신경망에서 이런 형태의 벡터 양자화가 처음으로 사용되는 경우이다.&lt;/p>
&lt;p>&lt;strong>Neural audio codecs&lt;/strong> end-to-end neural audio codec은 데이터 기반 방법을 사용해 효율적인 오디오 표현을 학습한다. 이는 초기에 음성 코딩에 적용된 autoencoder 네트워크에 기반하며, 최근에는 더 복잡한 deep convolutional 네트워크로 발전하였다. VQVAE 음성 codec과 Lyra는 낮은 비트레이트에서 효율적인 오디오 압축을 보여주었으며, 일반 오디오를 대상으로 한 end-to-end audio codec은 높은 비트레이트에서 효과적이다. 이러한 모델은 여러 autoencodering 모듈과 psychoacoustic 모델을 사용하여 학습 중인 손실 함수를 주도한다.&lt;/p>
&lt;p>SoundStream은 인코딩하는 신호의 성질에 대한 가정 없이 다양한 오디오 컨텐츠 유형에 적용할 수 있다. end-to-end 방식으로 학습되며, encoder를 학습하면 오디오 품질이 크게 향상된다. 추가 비용 없이 단일 모델이 다른 비트레이트에서 작동하는 능력을 가지며, 이는 residual vector quantizer와 quantizer dropout 학습 체계 덕분이다. SoundStream은 스마트폰 CPU에서 실시간으로 음성, 음악, 일반 오디오를 압축할 수 있으며, 이는 neural audio codec이 넓은 범위의 비트레이트에서 state-of-the-art codec을 능가하는 첫 번째 사례이다.&lt;/p>
&lt;p>&lt;strong>Joint compression and enhancement&lt;/strong> 최근 연구는 압축과 강화를 동시에 진행하는 방법을 탐구하였다. 하지만 SoundStream은 실시간으로 노이즈를 제어할 수 있는 시간 의존적 조절 계층을 사용해, 일반적으로 제거될 수 있는 자연소리와 음향 장면을 인코딩할 수 있도록 설계되었다. 이는 일반적인 목적의 오디오 코덱으로서의 SoundStream의 역할을 강화한다.&lt;/p>
&lt;hr>
&lt;h2 id="model">Model&lt;/h2>
&lt;p>$f_s$ 에서 샘플링된 단일 채널 녹음 $x \in \mathbb{R}^T$ 를 고려한다. SoundStream 모델은 세 개의 구성 요소로 이루어진 시퀀스로 구성된다:&lt;/p>
&lt;ul>
&lt;li>encoder는 x를 임베딩 시퀀스로 매핑한다.&lt;/li>
&lt;li>residual vector quantizer는 각 임베딩을 유한한 코드북 집합의 벡터 합으로 대체함으로써 표현을 목표 비트 수로 압축한다.&lt;/li>
&lt;li>decoder는 양자화된 임베딩에서 손실이 있는 reconstruction $\hat{x} \in \mathbb{R}^T$를 생성한다.&lt;/li>
&lt;/ul>
&lt;p>이 모델은 discriminator와 함께 adversarial 손실과 reconstruction 손실을 사용하여 end-to-end로 학습된다. denoising을 적용할 시기를 결정하는 조절 신호를 선택적으로 추가할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure3.png"
width="1490"
height="750"
srcset="https://kurtkim.github.io/p/soundstream/images/figure3_hu207783821ec8a2d6d1315741fb6c3687_256153_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure3_hu207783821ec8a2d6d1315741fb6c3687_256153_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h3 id="encoder-architecture">Encoder architecture&lt;/h3>
&lt;p>encoder 아키텍처는 스트리밍 SEANet encoder와 동일한 구조를 따르며, 1D convolution layer와 convolution block으로 구성된다. 각 block은 dilated convolution을 포함하는 residual unit으로 구성되며, 다운샘플링 시 채널 수가 두 배로 늘어난다. 마지막 1D convolution layer는 임베딩의 차원을 설정한다. 실시간 추론을 위해 모든 convolution은 causal 이며, ELU activation을 사용한다. 입력 waveform과 임베딩 사이의 temporal resampling ratio는 convolution block의 수와 스트라이딩 시퀀스에 의해 결정된다.&lt;/p>
&lt;h3 id="decoder-architecture">Decoder architecture&lt;/h3>
&lt;p>decoder 아키텍처는 업샘플링을 위한 transposed convolution과 residual unit으로 구성된 convolution block을 포함하며, encoder와 반대 순서의 스트라이드를 사용하여 입력 waveform과 동일한 해상도의 waveform을 재구성한다. 업샘플링 시 채널 수는 절반으로 줄어들며, 마지막 decoder block은 임베딩을 waveform 도메인으로 투영한다. encoder와 decoder 양쪽에서 동일한 채널 수는 동일한 parameter에 의해 제어되며, encoder와 decoder 사이에서 채널 수가 다른 경우도 조사하였다.&lt;/p>
&lt;h3 id="residual-vector-quantizer">Residual Vector Quantizer&lt;/h3>
&lt;p>quantizer의 목표는 encoder $enc(x)$의 출력을 bit/second(bps)로 표현된 목표 비트율 $R$로 압축하는 것이다. SoundStream을 end-to-end로 학습시키기 위해, quantizer는 backpropagation에 의해 encoder와 decoder와 함께 학습되어야 한다. vector quantizer (VQ)는 $enc(x)$의 $D$차원 프레임 각각을 인코드하기 위해 $N$개의 벡터로 구성된 코드북을 학습한다. 그런 다음 인코드된 오디오 $enc(x) \in \mathbb{R}^{S \times D}$는 $S \times D$ 형태의 one-hot vector 시퀀스로 매핑되며, 이는 $S log_2 N$ 비트를 사용하여 표현할 수 있다.&lt;/p>
&lt;p>&lt;strong>Limitations of Vector Quantization&lt;/strong> 비트율 $R = 6000 bps$를 목표로 하는 코덱 예시에서, 스트라이딩 계수 $M = 320$을 사용하면, 샘플링 레이트가 $24000 Hz$인 1초 오디오는 encoder의 출력에서 75 프레임으로 표현된다. 이는 각 프레임에 80 비트가 할당되는 것을 의미한다. 그러나 plain vector quantizer를 사용하면, 실행 불가능한 수준인 $N = 2^{80}$ 벡터의 코드북을 저장해야 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/algorithm1.png"
width="730"
height="382"
srcset="https://kurtkim.github.io/p/soundstream/images/algorithm1_hu8d95d3b76f128120902b700062dfe2b1_65126_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/algorithm1_hu8d95d3b76f128120902b700062dfe2b1_65126_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>&lt;strong>Residual Vector Quantizer&lt;/strong> 이 문제를 해결하기 위해, residual vector quantizer를 채택하여 $N_q$ layer의 $VQ$를 연속적으로 적용한다. 양자화되지 않은 입력 벡터는 첫 $VQ$를 거치고, quantization residual이 계산된 후 추가적인 vector quantizer에 반복적으로 양자화된다. 전체 비율 예산은 각 VQ에 균등하게 할당되며, 예를 들어 $N_q = 8$을 사용할 경우, 각 quantizer는 1024 크기의 코드북을 사용한다. $N_q$ parameter는 계산 복잡성과 코딩 효율성 사이의 균형을 제어한다.&lt;/p>
&lt;p>각 quantizer의 코드북은 exponential moving average 업데이트로 학습되며, 코드북 사용을 개선하기 위해 두 가지 방법을 사용한다. 첫째, 코드북 벡터의 초기화를 위해 첫 번째 학습 배치에서 k-means 알고리즘을 실행한다. 둘째, 코드북 벡터가 여러 배치 동안 입력 프레임을 할당받지 못하면 현재 배치에서 무작위로 샘플링된 입력 프레임으로 대체한다. 이를 위해 각 벡터에 대한 할당의 exponential moving average을 추적하고, 이 값이 2 이하로 떨어지는 벡터를 대체한다.&lt;/p>
&lt;p>&lt;strong>Enabling bitrate scalability with quantizer dropout&lt;/strong> residual vector quantization는 각 코드북의 크기를 고정하고 $VQ$ layer의 수를 조절함으로써 비트레이트를 제어한다. vector quantizer는 encoder/decoder와 함께 학습되지만, 여러 목표 비트레이트에서 작동할 수 있는 단일 비트레이트 스케일러블 모델이 더 실용적이다. 이 방식은 encoder와 decoder 양쪽에서 모델 parameter를 저장하는 데 필요한 메모리를 줄일 수 있다.&lt;/p>
&lt;p>각 입력 예제에 대해 무작위로 선택된 $n_q$ 범위 안에서 quantizer를 사용하여 모델을 학습시킨다. 이는 quantization layer에 적용된 구조화된 드롭아웃의 한 형태로 볼 수 있다. 이 방법을 통해 모델은 모든 목표 비트레이트에 대해 오디오를 인코드하고 디코드하도록 학습된다. 이전 neural compression 모델들과 달리, residual vector quantization의 주요 장점은 임베딩의 차원이 비트레이트와 함께 변경되지 않는다는 것이다. 이렇게 하면 encoder나 decoder의 아키텍처 변경이 필요 없으므로, 특정 비트레이트에 대해 학습된 모델의 성능을 일치시키는 단일 SoundStream 모델을 학습시킬 수 있다.&lt;/p>
&lt;h3 id="discriminator-architecture">Discriminator architecture&lt;/h3>
&lt;p>adversarial 손실을 계산하기 위해 단일 waveform을 입력으로 받는 wave-based discriminator와 복소수 STFT를 입력으로 받는 STFT-based discriminator, 총 두 가지 discriminator를 사용한다. 이 두 discriminator는 모두 fully convolutional 이므로, 출력 로짓의 수는 입력 오디오의 길이에 비례하게 된
다.&lt;/p>
&lt;p>wave-based discriminator는 여러 해상도(original, 2-times down-sampled, 4-times down-sampled)에서 입력 오디오에 적용되는 세 개의 동일한 구조의 모델을 사용한다. 각 모델은 initial plain convolution과 네 개의 grouped convolution, 그리고 두 개의 plain convolution layer을 거쳐 logit을 생성힌다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure4.png"
width="702"
height="814"
srcset="https://kurtkim.github.io/p/soundstream/images/figure4_hu8c9725bb6b793387a0e90f837b4f50cc_126775_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure4_hu8c9725bb6b793387a0e90f837b4f50cc_126775_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="206px"
>&lt;/p>
&lt;p>STFT-based discriminator는 단일 스케일에서 작동하며, STFT 계산에는 1024 샘플의 window length와 256 샘플의 hop length를 사용힌다. 이 판별자는 2D-convolution과 일련의 residual block을 거친다. 이 block들은 3×3 convolution을 시작으로 (1, 2) 또는 (2, 2)의 스트라이드를 가진 다른 convolution으로 이어진다. 여기서 $(s_t, s_f)$는 시간 축과 주파수 축을 따라 다운샘플링 요인을 나타낸다.총 6개의 residual block이 있으며, 네트워크의 깊이가 깊어질수록 채널 수가 증가힌다. 마지막 residual block 출력에서는 activation은 $T/(H \cdot 2^3) \times F/2^6$ 형태를 가지며, 여기서 $T$는 시간 도메인의 샘플 수이고 $F = W/2$는 주파수 통의 수이다. 마지막 layer에서는 주파수 통을 통해 logit을 집계하여 1-dimensional signal time domain을 얻는다.&lt;/p>
&lt;h3 id="training-objective">Training objective&lt;/h3>
&lt;p>SoundStream generator $G(x)$는 입력 waveform $x$를 처리하며, 이는 encoder, quantizer, decoder를 통과한다. 디코드된 파형은 $\hat{x} = G(x)$로 표시된다. SoundStream은 perception-distortion trade-off에 따라, signal reconstruction ﬁdelity와 perceptual quality을 모두 달성하기 위해 다양한 손실을 사용하여 학습된다.&lt;/p>
&lt;p>adversarial 손실은 perceptual quality을 향상시키는 데 사용되며, discriminator의 logit에 대한 hinge 손실로 정의되며, 여러 discriminator와 시간에 걸쳐 평균화된다. 보다 공식적으로, $k \in \lbrace 0, &amp;hellip;, K \rbrace$ 로 개별 discriminator를 인덱싱하게 하고, 여기서 $k = 0$은 STFT-based discriminator를 나타내고 $k \in \lbrace 1, &amp;hellip;, K \rbrace$ 는 waveform-based discriminator의 다른 해상도를 나타낸다. $T_k$는 시간 차원을 따라 $k$번째 discriminator의 출력에서의 logit 수를 나타낸다. discriminator는 원래의 오디오와 디코드된 오디오를 분류하기 위해 최소화함으로써 학습된다.&lt;/p>
&lt;p>$$ L_D = E_x \big[ {{1}\over{K}} \sum_K {{1}\over{T_K}} \sum_t max(0.1 - D_{k, t}(x)) \big] + E_x \big[ {{1}\over{K}} \sum_K {{1}\over{T_K}} \sum_t max(0.1 - D_{k, t}(g(x))) \big] $$&lt;/p>
&lt;p>generator에 대한 adversarial 손실은&lt;/p>
&lt;p>$$ L_g^{adj} = E_x \big[ {{1}\over{K}} \sum_{K, t} {{1}\over{T_K}} max(0.1 - D_{k, t}(g(x))) \big] $$&lt;/p>
&lt;p>원본 $x$에 대한 디코딩된 신호 $\hat{x}$의 ﬁdelity를 촉진하기 위해, 두 가지 추가적인 손실을 채택한다:&lt;/p>
&lt;ol>
&lt;li>discriminator가 정의하는 feature space에서 계산된 &amp;ldquo;feature&amp;rdquo; 손실 $L_G^{feat}$&lt;/li>
&lt;li>multi-scale spectral reconstruction 손실 $L_G^{rec}$&lt;/li>
&lt;/ol>
&lt;p>더 구체적으로, feature 손실은 생성된 오디오에 대한 discriminator의 내부 layer 출력과 해당 타겟 오디오에 대한 출력 사이의 average absolute difference를 계산함으로써 구해진다.&lt;/p>
&lt;p>$$ L_g^{feat} = E_x \big[ {{1}\over{KL}} \sum_{K, l} {{1}\over{T_{K, l}}} \sum_t | D_{k, t}^{(l)}(x) - D_{k, t}^l(g(x)) \big] $$&lt;/p>
&lt;p>여기서 $L$은 내부 layer의 수이고, $D_{k,t}^{(l)} (l \in \lbrace 1, &amp;hellip;, L \rbrace )$는 판별자 $k$의 계층 $l$의 $t$번째 출력이며, $T_{k,l}$은 시간 차원에서 계층의 길이를 나타낸다.&lt;/p>
&lt;p>multi-scale spectral reconstruction은 다음을 따른다:&lt;/p>
&lt;p>$$ L_g^{rec} = \sum_{s \in 2^6, &amp;hellip; ,2^{11}} \sum_t \Vert S_t^s(x) - S_t^s(G(x))\Vert_1 + \alpha_s \sum_t \Vert log S_t^s(x) − log S_t^s(G(x)) \Vert_2 $$&lt;/p>
&lt;p>여기서 $S_t^s(x)$는 window length가 $s$이고 hop length가 $s/4$인 64-bin melspectrogram의 t-th 프레임을 나타낸다. $\alpha_s = \sqrt{s/2}$로 설정한다.&lt;/p>
&lt;p>overall generator 손실은 다른 손실 component의 weighted sum이다:&lt;/p>
&lt;p>$$ L_G = \lambda_{adj} L_G^{adj} + \lambda_{feat} L_G^{feat} + \lambda_{rec} L_G^{rec} $$&lt;/p>
&lt;p>모든 실험에서 $\lambda_{adv} = 1, \lambda_{feat} = 100\lambda_{rec} = 1$로 설정하였다.&lt;/p>
&lt;h3 id="joint-compression-and-enhancement">Joint compression and enhancement&lt;/h3>
&lt;p>전통적인 오디오 처리에서는 compression과 enhancement가 별도의 모듈에서 이루어지지만, 각 처리 단계는 end-to-end latency에 영향을 미친다. 그러나 SoundStream은 compression과 enhancement을 동시에 수행하는 동일한 모델로 설계되어 전체 지연 시간을 증가시키지 않는다.&lt;/p>
&lt;p>enhancement의 종류는 학습 데이터의 선택에 따라 결정된다. 이 논문에서는 오디오 compression과 배경 소음 제거를 결합하는 것이 가능하다는 것을 보여준다. 모델은 denoising을 유연하게 활성화하거나 비활성화할 수 있게 학습되며, 이는 두 가지 모드를 나타내는 조절 신호를 통해 가능하다. 이를 위해 학습 데이터는 (inputs, targets, denoise)의 형태로 구성된다. denoising이 활성화되면, 네트워크는 노이즈가 있는 입력의 청정한 버전을 생성하도록 학습되고, 비활성화되면 노이즈가 있는 음성을 재구성하도록 학습된다. 또한, denoising이 활성화되어도 SoundStream이 청정한 오디오에 부정적인 영향을 미치지 않도록 하였다.&lt;/p>
&lt;p>conditioning signal을 처리하기 위해, residual unit 사이에 Feature-wise Linear Modulation (FiLM) layer를 사용하는데, 이것은 네트워크 특징을 입력으로 받아 다음과 같이 변형한다.&lt;/p>
&lt;p>$$ \tilde{a}_{n,c} = \gamma_{n,c} a_{n,c} + \beta_{n,c} $$&lt;/p>
&lt;p>여기서 $a_{n,c}$ 는 $c$번째 채널의 $n$번째 activation이다. 계수 $\gamma_{n,c}$ 와 $\beta_{n,c}$는 denoising 모드를 결정하는 two-dimensional one-hot encoding을 입력으로 하는 linear layer에 의해 계산된다. 이를 통해 시간에 따른 denoising 수준을 조정할 수 있다.&lt;/p>
&lt;p>원칙적으로 FiLM layer는 encoder와 decoder 아키텍처 어디에나 적용될 수 있지만, preliminary 실험에서는 encoder나 decoder의 병목 부분에서 조절을 적용하는 것이 효과적이었다. 다른 깊이에서 FiLM layer를 적용해도 추가적인 개선은 관찰되지 않았다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluation-setup">Evaluation Setup&lt;/h2>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>SoundStream은 깨끗한 음성, 잡음이 있는 음성, 그리고 음악에 대해 학습되었다. 이를 위해 다양한 데이터셋을 사용하였고, 잡음이 있는 음성은 LibriTTS의 음성과 Freesound의 잡음을 혼합하여 만들었다. 또한, 실세계 데이터셋도 수집하여 테스트에 사용하였다. 이 모든 데이터를 바탕으로 객관적, 주관적 측정치를 계산하였다.&lt;/p>
&lt;h3 id="evaluation-metrics">Evaluation metrics&lt;/h3>
&lt;p>SoundStream의 평가는 인간 평가자들에 의한 주관적 평가로 이루어졌다. MUSHRA에서 착안한 방법론을 사용하였고, 각각의 샘플은 20번씩 평가되었다. 평가자들은 영어를 모국어로 사용하며 헤드폰을 착용하였다. 또한, 데이터의 품질을 보장하기 위해 특정 기준을 충족하지 못하는 평가는 제외하였다.&lt;/p>
&lt;p>개발과 hyperparameter 선택에는 계산 가능한 객관적 지표를 사용하였다. 라이센스 제한으로 인해 일반적으로 사용되는 PESQ와 POLQA 대신, 오픈소스화된 ViSQOL 지표를 선택하였다. 이 지표는 POLQA와 비슷한 성능을 보였으며, 주관적 평가와 강한 상관관계를 보였기 때문에 모델 선택과 연구에 사용되었다.&lt;/p>
&lt;h3 id="baselines">Baselines&lt;/h3>
&lt;p>Opus는 다목적 음성 및 오디오 코덱으로, 4 kHz에서 24 kHz까지의 신호 대역폭과 6 kbps에서 510 kbps까지의 비트레이트를 지원한다. 인터넷 음성 통신, Zoom, Microsoft Teams, Google Meet 등에서 널리 사용되며, YouTube 스트리밍에도 사용된다. 또한, 최신 3GPP에 의해 표준화된 Enhanced Voice Services (EVS) 코덱도 소개되었다. 이 논문에서는 이 두 코덱과 최근 제시된 Lyra 코덱을 SoundStream 코덱과 비교한다. 이러한 비교를 위해 다양한 비트레이트에서 SoundStream과 기준선에 의해 처리된 오디오를 공개 웹페이지에서 제공한다.&lt;/p>
&lt;hr>
&lt;h2 id="result">Result&lt;/h2>
&lt;h3 id="comparison-with-other-codecs">Comparison with other codecs&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure5.png"
width="1460"
height="544"
srcset="https://kurtkim.github.io/p/soundstream/images/figure5_hud5b24b83c1ea0d9333a920c2be3303d2_144223_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure5_hud5b24b83c1ea0d9333a920c2be3303d2_144223_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="268"
data-flex-basis="644px"
>&lt;/p>
&lt;p>본 논문의 주요 결과는 SoundStream이 다른 비트레이트에서 Opus와 EVS와 비교될 때 더 우수한 성능을 보여준다는 것이다. 특히, SoundStream은 절반의 비트레이트인 3 kbps에서 작동하면서도 Opus 6 kbps와 EVS 5.9 kbps를 크게 초과하였다. SoundStream의 품질을 맞추기 위해, EVS는 최소 9.6 kbps, Opus는 최소 12 kbps를 필요로 하며, 이는 SoundStream보다 3.2배에서 4배 더 많은 비트를 사용하는 것을 의미한다. 또한, SoundStream은 3 kbps에서 작동할 때 Lyra를 능가하였고, 6 kbps와 12 kbps에서도 비슷한 결과를 보여주었다. 중간 비트레이트에서는 EVS와 Opus는 각각 2.2배에서 2.6배, 높은 비트레이트에서는 1.3배에서 1.6배 더 많은 비트를 사용해야 동일한 품질을 얻을 수 있었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure6.png"
width="1464"
height="476"
srcset="https://kurtkim.github.io/p/soundstream/images/figure6_hud274fbce4050d7128a3cb5d9563dfb8b_194173_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure6_hud274fbce4050d7128a3cb5d9563dfb8b_194173_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="307"
data-flex-basis="738px"
>&lt;/p>
&lt;p>SoundStream이 깨끗한 음성과 잡음이 있는 음성을 인코딩할 때 일관된 품질을 보인다는 것을 확인할 수 있다. 또한, SoundStream은 최소 3 kbps에서 음악을 인코딩하며, 이는 12 kbps의 Opus와 5.9 kbps의 EVS보다 상당히 높은 품질을 보여준다. 이는 이렇게 낮은 비트레이트에서 다양한 콘텐츠 유형에 적용되는 첫 codec이다.&lt;/p>
&lt;h3 id="objective-quality-metrics">Objective quality metrics&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure7.png"
width="1364"
height="448"
srcset="https://kurtkim.github.io/p/soundstream/images/figure7_hu886a31e634c7c0c2143d1d1882392712_125705_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure7_hu886a31e634c7c0c2143d1d1882392712_125705_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;p>SoundStream의 rate-quality 곡선을 보여주며, 3 kbps에서 18 kbps까지의 비트레이트에서 품질이 비트레이트가 감소함에 따라 점차 감소하지만, 최저 비트레이트에서도 3.7 이상을 유지하는 것을 보여준다. SoundStream은 일정한 비트레이트에서 작동하며, 각 인코딩된 프레임에 동일한 수의 비트가 할당된다. 또한, 통계적 중복성을 활용하지 않는 가정 하에 비트레이트 하한을 측정하였으며, 이 결과 7%에서 20% 사이의 비율 절약이 가능함을 보여준다.&lt;/p>
&lt;p>다양한 콘텐츠 유형을 인코딩할 때 달성되는 rate-quality tradeoff를 조사한 결과, 깨끗한 음성을 인코딩할 때 가장 높은 품질을 얻을 수 있었다. 반면, 내용의 다양성 때문에 음악을 인코딩하는 것은 더욱 도전적인 작업이었다.&lt;/p>
&lt;h3 id="bitrate-scalability">Bitrate scalability&lt;/h3>
&lt;p>다양한 비트레이트를 제공하는 단일 모델 학습을 통한 비트레이트 확장성을 조사하였다. 이를 평가하기 위해 세 가지 SoundStream 설정을 고려하였다. 놀랍게도, 18 kbps에서 학습된 모델은 더 낮은 비트레이트에서도 좋은 성능을 보여주었다. 비트레이트가 감소할수록 품질 감소는 증가했지만, quantizer dropout 전략을 사용하면 이 차이가 사라졌다. 또한, 비트레이트 확장 가능 모델은 일정한 비트레이트에서 비트레이트 특정 모델을 약간 능가하는 것으로 나타났다. 이러한 결과는 quantizer dropout이 비트레이트 확장성을 제공하는 것 외에도 regularizer 역할을 할 수 있음을 보여준다.&lt;/p>
&lt;p>MUSHRA 주관적 평가를 통해, 비트레이트 확장 가능한 SoundStream 변형이 3 kbps에서는 비트레이트 특정 변형보다 약간만 나쁘며, 6 kbps와 12 kbps에서는 비트레이트 특정 변형과 동일한 품질을 보여줌을 확인하였다.&lt;/p>
&lt;h3 id="ablation-studies">Ablation studies&lt;/h3>
&lt;p>SoundStream에 적용된 몇 가지 설계 선택의 영향을 평가하기 위해 여러 가지 추가 실험을 수행하였다. 특별히 명시되지 않는 한, 모든 실험은 6 kbps에서 작동한다.&lt;/p>
&lt;p>&lt;strong>Advantage of learning the encoder&lt;/strong> SoundStream의 학습 가능한 encoder를 고정된 mel-ﬁlterbank로 대체하는 것이 품질에 미치는 영향을 조사하였다. 결과적으로, ViSQOL이 3.96에서 3.33으로 크게 떨어지는 것으로 보아, 품질이 크게 감소하는 것을 확인하였다. 이는 encoder를 학습하고 비트레이트를 절반으로 줄일 때보다도 나쁘다는 것을 의미한다. 이는 학습 가능한 encoder의 복잡성이 rate-quality trade-off에서 큰 개선을 가져다준다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/table1.png"
width="694"
height="292"
srcset="https://kurtkim.github.io/p/soundstream/images/table1_hu0b524032db973917da7eb9169bddd3d4_58815_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/table1_hu0b524032db973917da7eb9169bddd3d4_58815_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="570px"
>&lt;/p>
&lt;p>&lt;strong>Encoder and decoder capacity&lt;/strong> 학습 가능한 encoder의 사용은 계산 비용이 큰 단점이지만, SoundStream은 동일한 비트레이트에서 더 나은 지각 품질을 제공하고, 제한된 자원의 하드웨어에서 실시간으로 동작해야 한다. encoder와 decoer의 채널 수를 조절하여 계산 효율성과 오디오 품질이 어떻게 변하는지 측정하였다. 모델 용량을 줄이면 복원 품질에는 거의 영향을 미치지 않으면서 실시간 요소가 크게 증가하는 것을 확인하였다. 더 작은 encoder를 사용하면 품질을 희생하지 않고 큰 속도 향상을 달성할 수 있었다. 그러나 decoer의 용량을 줄이면 품질에 더 큰 영향을 미치는 것을 확인하였다. 이는 신경 이미지 압축 분야의 최근 연구 결과와 일치한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/table2.png"
width="738"
height="114"
srcset="https://kurtkim.github.io/p/soundstream/images/table2_hu3fec695c2eaaed11f1848dfcd1291f1e_25235_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/table2_hu3fec695c2eaaed11f1848dfcd1291f1e_25235_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="647"
data-flex-basis="1553px"
>&lt;/p>
&lt;p>&lt;strong>Vector quantizer depth and codebook size&lt;/strong> 단일 프레임을 인코드하는 데 필요한 비트 수는 quantizer의 수와 코드북 크기에 따라 다르며, 이를 통해 동일한 목표 비트레이트를 달성할 수 있다. 큰 코드북을 가진 적은 수의 벡터 quantizer를 사용하면 계산 복잡성이 증가하지만, 높은 코딩 효율성을 달성할 수 있다. 반면, 80개의 1비트 quantizer를 사용하면 품질 저하가 약간밖에 되지 않는 것으로 나타났다. 하지만 코드북 크기를 늘리면 메모리 요구사항이 빠르게 증가할 수 있다. 따라서 residual vector quantizer는 높은 비트레이트에서 작동하는 신경 codec을 학습하는데 실용적이고 효과적인 해결책을 제공한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/table3.png"
width="734"
height="146"
srcset="https://kurtkim.github.io/p/soundstream/images/table3_hueb9f5edff2b65d0d3c949184724b76c2_35492_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/table3_hueb9f5edff2b65d0d3c949184724b76c2_35492_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="502"
data-flex-basis="1206px"
>&lt;/p>
&lt;p>&lt;strong>Latency&lt;/strong> 모델의 아키텍처 latency는 스트라이드의 곱으로 결정되며, 이는 오디오의 한 프레임이 몇 개의 샘플로 이루어져 있는지를 나타낸다. residual vector quantizer에 할당된 비트 예산은 이 지연에 따라 조정되며, 지연 시간을 늘리면 프레임 당 예산이 증가해야 한다. 세 가지 다른 구성에서 예산을 조정하는 방법은 코드북 크기를 고정하고 양자화기의 수를 변경하는 것이다. 이 세 가지 구성은 오디오 품질 면에서 동등하나, 모델의 지연을 늘리면 실시간 처리 능력이 크게 증가한다. 이는 단일 프레임의 인코딩/디코딩이 더 긴 오디오 샘플에 해당하기 때문이다.&lt;/p>
&lt;h3 id="joint-compression-and-enhancement-1">Joint compression and enhancement&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/figure8.png"
width="1338"
height="428"
srcset="https://kurtkim.github.io/p/soundstream/images/figure8_huec21d81d79f66815300bc1a6d74b2d42_137662_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/figure8_huec21d81d79f66815300bc1a6d74b2d42_137662_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="312"
data-flex-basis="750px"
>&lt;/p>
&lt;p>compression과 background noise suppression을 동시에 수행하는 SoundStream 변형을 평가하였다. 이 모델은 임베딩에 conditioning signal을 적용하는 두 가지 구성을 고려하며, 각각 encoder와 decoder 측에 conditioning signal을 추가한다. 다른 비트레이트에서 모델을 학습하고, 노이즈가 있는 음성 샘플을 사용하여 denoising이 활성화되거나 비활성화될 때의 오디오 품질을 평가하였다. 결과적으로 denoising이 활성화될 때 오디오 품질이 크게 향상되며, encoder나 decoder에서 denoising하는 것 사이에 큰 차이는 없었다. 또한, denoising을 유연하게 활성화하거나 비활성화할 수 있는 추론 시간 모델은 denoising이 항상 활성화된 모델과 비교하여 성능에서 추가 비용이 발생하지 않았다.&lt;/p>
&lt;p>denoising이 비트레이트 절약에 어떤 영향을 미치는지 조사하였다. 학습 데이터의 샘플에서 경험적 확률 분포를 측정하고, 테스트 샘플에서의 분포를 바탕으로 비트레이트 하한선을 추정하였다. 결과적으로, encoder 측 denoising과 고정 denoising이 decoder 측 denoising에 비해 상당한 비트레이트 절약을 제공함을 확인하였다. 이는 양자화 전에 denoising을 적용하면 더 적은 비트로 인코딩할 수 있다는 것을 의미한다.&lt;/p>
&lt;h3 id="joint-vs-disjoint-compression-and-enhancement">Joint vs. disjoint compression and enhancement&lt;/h3>
&lt;p>제안된 모델은 compression과 enhancement를 동시에 수행할 수 있다. 이를 SoundStream이 compression을 담당하고 전용 denoising 모델이 enhancement를 담당하는 구성과 비교하였다. 이때 두 가지 변형을 고려했는데, 하나는 compression 후 denoising이 이루어지는 경우(decoder 측에서 denoising 적용)이고, 다른 하나는 denoising 후 compression이 이루어지는 경우(encoder 측에서 denoising 적용)이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/soundstream/images/table4.png"
width="648"
height="240"
srcset="https://kurtkim.github.io/p/soundstream/images/table4_hu5229fc4b5957a5df73e5a62e59eea0a1_45356_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/soundstream/images/table4_hu5229fc4b5957a5df73e5a62e59eea0a1_45356_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="270"
data-flex-basis="648px"
>&lt;/p>
&lt;p>VCTK 데이터셋을 사용하여 다양한 모델을 평가하였다. 이때, 압축과 강화를 동시에 수행하는 단일 모델은 두 개의 별개 모델을 사용하는 것과 거의 동일한 품질을 달성하며, 계산 비용은 절반으로 줄이고 추가적인 아키텍처 지연을 일으키지 않았다. 또한, 입력 신호 대 잡음 비가 증가할수록 성능 간격이 줄어드는 것을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>SoundStream이라는 새로운 neural audio codec을 제안한다. 이는 다양한 비트레이트와 콘텐츠 유형에서 state-of-the-art audio codec을 능가한다. SoundStream은 encoder, residual vector quantizer, decoder로 구성되어 우수한 오디오 품질을 제공하며, 실시간으로 스마트폰 CPU에서 작동 가능하다. quantizer dropout을 통해 비트레이트 확장성을 달성하고, 추가적인 대기 시간 없이 압축과 강화를 하나의 모델에서 수행할 수 있음을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2107.03312.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/wesbz/SoundStream" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MusicGEN</title><link>https://kurtkim.github.io/p/musicgen/</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/musicgen/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>conditional music generation을 위한 &amp;ldquo;MusicGEN&amp;quot;이라는 언어 모델을 개발하였다. 이 모델은 여러 스트림의 압축된 이산 음악 표현을 다루며, 효율적인 토큰 교차 패턴과 single-stage transformer를 사용해 여러 모델을 계층적으로 구성하거나 업샘플링할 필요가 없다. 이 방법을 통해 텍스트 설명이나 멜로디 특징에 따라 높은 품질의 음악 샘플을 생성할 수 있음을 입증하였다. 실증적 평가를 통해 제안된 접근법이 기존 벤치마크보다 우수하다는 것을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>text-to-music은 텍스트 설명을 바탕으로 음악을 생성하는 작업이다. 이 과정은 long range sequence를 모델링하고 full frequency spectrum을 사용해야 하므로 어렵다. 또한, 다양한 악기의 하모니와 멜로디를 포함하는 음악은 복잡한 구조를 가지며, 이로 인해 음악 생성 과정에서는 멜로디 오류를 범할 여지가 거의 없다. 키, 악기, 멜로디, 장르 등 다양한 요소를 제어할 수 있는 능력은 음악 창작자에게 필수적이다.&lt;/p>
&lt;p>self-supervised audio representation, sequential modeling, audio synthesis 등의 최근 연구 진보가 새로운 모델 개발을 가능하게 한다. 최근 연구들은 오디오 신호를 같은 신호를 표현하는 여러 이산 토큰의 스트림으로 나타내는 것을 제안하였는데, 이를 통해 고품질의 오디오 생성과 효과적인 오디오 모델링이 가능해졌다. 그러나 이는 여러 parallel dependent stream을 동시에 모델링해야한다는 비용을 수반한다.&lt;/p>
&lt;p>Kharitonov et al. 과 Kreuk et al. 은 음성 토큰의 다중 스트림을 병렬로 모델링하는 지연 접근법을 제안하였다. Agostinelli et al. 은 음악 세그먼트를 다양한 세부성의 이산 토큰 시퀀스로 표현하고 이를 autoregressive 모델로 모델링하는 방식을 제안하였다. Donahue et al. 은 비슷한 접근법을 가요 생성 작업에 적용했고, Wang et al. 은 문제를 두 단계로 해결하는 방법을 제안하였다: 첫 번째 토큰 스트림만 모델링한 후, non-autoregressive 방식으로 나머지 스트림을 모델링한다.&lt;/p>
&lt;p>이 연구에서는 텍스트 설명에 따른 고품질 음악을 생성하는 &amp;ldquo;MusicGEN&amp;quot;이라는 단순하고 조절 가능한 모델을 소개한다. 이 모델은 음향 토큰의 병렬 스트림을 모델링하는 프레임워크를 제안하며, 스테레오 오디오 생성을 추가 비용 없이 확장할 수 있다. 또한, 비지도 멜로디 조건 설정을 통해 생성된 샘플의 제어력을 향상시키고, 주어진 조화와 멜로디 구조에 맞는 음악을 생성할 수 있다. MusicGEN은 평가에서 100점 만점에 84.8점의 높은 점수를 받았으며, 이는 최고 기준선의 80.5점보다 우수한 성능을 보여준다. 마지막으로, 인간 평가에 따르면 MusicGEN은 주어진 조화 구조에 잘 맞는 멜로디를 가진 고품질 샘플을 생성하며, 텍스트 설명을 충실히 따른다.&lt;/p>
&lt;p>&lt;strong>Our contribution:&lt;/strong> 32 kHz에서 고품질 음악을 생성하는 간단하고 효율적인 모델, MusicGEN을 제안한다. 이 모델은 효율적인 코드북 교차 전략을 통해 일관된 음악을 생성하며, 텍스트와 멜로디 조건에 모두 부합하는 단일 모델을 제공한다. 생성된 오디오는 제공된 멜로디와 일치하고 텍스트 조건 정보에 충실하다. 또한, 주요 설계 선택에 대한 광범위한 객관적 평가와 인간 평가를 제공한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>MusicGEN은 텍스트나 멜로디에 의존하는 autoregressive transformer-based decoder이다. 이 모델은 양자화된 오디오 토큰을 사용하며, 이는 고해상도 복구를 가능하게 한다. 병렬 스트림은 Residual Vector Quantization (RVQ)를 통해 생성되며, 각 스트림은 다양한 코드북에서 생성된 이산 토큰으로 구성된다. 이 연구에서는 다양한 코드북 교차 패턴에 적용 가능한 새로운 모델링 프레임워크를 소개하며, 이를 통해 양자화된 오디오 토큰의 내부 구조를 활용한다. MusicGEN은 텍스트나 멜로디를 기반으로 한 조건부 생성을 지원한다.&lt;/p>
&lt;h3 id="audio-tokenization">Audio tokenization&lt;/h3>
&lt;p>Residual Vector Quantization (RVQ)를 사용하여 양자화된 latent space와 adversarial reconstruction 손실을 가진 EnCodec을 사용한다. 이는 오디오 무작위 변수를 연속 텐서로 인코딩하고, 이를 다시 양자화하여 병렬 이산 토큰 시퀀스를 생성한다. RVQ에서는 각 양자화기가 이전 양자화기의 양자화 오류를 인코딩하므로, 다른 코드북의 양자화 값은 일반적으로 독립적이지 않다. 이 과정에서 첫 번째 코드북이 가장 중요하게 작용한다.&lt;/p>
&lt;h3 id="codebook-interleaving-patterns">Codebook interleaving patterns&lt;/h3>
&lt;p>&lt;strong>Exact flattened autoregressive decomposition.&lt;/strong> autoregressive 모델은 일정한 길이 $S$를 가진 이산 랜덤 시퀀스 $U$가 필요하며, 이 시퀀스는 {$1, &amp;hellip;, N$}$^S$에서 선택된다. 관례적으로 시퀀스의 시작은 $U_0 = 0$, 즉 특별 토큰으로 표현된다. 이를 통해 분포를 모델링한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, p_t (U_{t−1}, &amp;hellip;, U_0) \triangleq \mathbb{P} [U_t | U_{t−1}, &amp;hellip;, U_0] $$&lt;/p>
&lt;p>auto-regressive density $p$를 이용해 랜덤 변수의 두 번째 시퀀스인 $\tilde{U}$를 만든다. 이때, $\tilde{U}_0 = 0$으로 초기화하고, $t &amp;gt; 0$인 모든 경우에 대해 재귀적으로 정의한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, \mathbb{P} \big[\tilde{U}_t | \tilde{U}_{t−1}, &amp;hellip;, \tilde{U}_0 \big] = p_t (\tilde{U}_{t−1}, &amp;hellip;, \tilde{U}_0) $$&lt;/p>
&lt;p>$U$와 $\tilde{U}$가 같은 분포를 가진다는 것이 바로 확인된다. 이는 딥러닝 모델로 $p$의 완벽한 추정치 $\tilde{p}$를 맞출 수 있다면, $U$의 분포도 정확히 맞출 수 있다는 것을 의미한다.&lt;/p>
&lt;p>EnCodec 모델로부터 얻은 $Q$ 표현의 문제는 각 시간 단계마다 $K$개의 코드북이 있다는 점이다. 이를 해결하기 위해 $Q$를 펼쳐 $S = d \cdot f_r \cdot K$로 설정할 수 있다. 이 방식은 첫 번째 시간 단계의 각 코드북을 순차적으로 예측한다. 이론적으로 $Q$의 분포를 정확하게 모델링할 수 있지만, 복잡성이 증가하고 가장 낮은 샘플 속도 $f_r$에서 얻는 이익이 일부 손실된다.&lt;/p>
&lt;p>여러 가지 flattening 방법이 가능하며, 모든 $\hat{p_t}$ 함수를 한 모델로 추정할 필요는 없다. 예를 들어, MusicLM은 두 개의 모델을 사용해 첫 번째 $K/2$ 코드북과 나머지 $K/2$ 코드북을 각각 모델링한다. 이렇게 해도 autoregressive step의 수는 $df_r \cdot K$로 동일하다.&lt;/p>
&lt;p>&lt;strong>Inexact autoregressive decomposition.&lt;/strong> 일부 코드북이 병렬로 예측되는 autoregressive 분해를 고려하는 것이 가능하다. 즉, $V_0 = 0$을 정의하고, 모든 $t$와 $k$에 대해 $V_{t, k} = Q_{t, k}$로 시퀀스를 설정한다. 이때, 코드북 인덱스 $k$를 생략하면, 시간 $t$에서 모든 코드북이 연결된 것을 의미한다.&lt;/p>
&lt;p>$$ p_{t, k} (V_{t−1}, &amp;hellip;, V_0) \triangleq \mathbb{P} [V_{t, k} | V_{t−1}, \dot, &amp;hellip;, V_0] $$&lt;/p>
&lt;p>재귀적으로 $\tilde{V}_0 = 0$을 다시 정의하고, 모든 $t &amp;gt; 0$에 대해 이를 정의한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, \mathbb{P} \big[\tilde{V}_{t, k} \big] = p_{t, k} (\tilde{V}_{t−1}, &amp;hellip;, \tilde{V}_0) $$&lt;/p>
&lt;p>일반적으로 정확한 분포 $p_{t,k}$를 가정하더라도 $\tilde{V}$는 $V$와 동일한 분포를 따르지 않는다. 실제로, 모든 $t$에 대해 $(V_{t,k})$ $k$가 $V_{t−1}, &amp;hellip;, $V_0$에 조건부로 독립인 경우에만 적절한 생성 모델을 가진다. $t$가 증가함에 따라 오류가 누적되고 두 분포는 점점 멀어진다. 이 분해법은 부정확하지만 원래의 프레임 속도를 유지하므로, 학습과 추론이 특히 긴 시퀀스에 대해 크게 가속화된다.&lt;/p>
&lt;p>&lt;strong>Arbitrary codebook interleaving patterns.&lt;/strong> 다양한 분해 실험을 진행하고, 부정확한 분해의 영향을 측정하기 위해 코드북 교차 패턴을 사용한다. 모든 시간 단계와 코드북 인덱스의 쌍을 나타내는 $\Omega$ 집합을 고려하며, 코드북 패턴은 $P_0 = \emptyset$으로 시작해 $P_s$가 $\Omega$ 의 부분집합인 시퀀스이다. 이 패턴은 $\Omega$를 모델링하는 데 사용되며, 모든 위치를 병렬로 예측합니다. 실용적으로, 각 $P_s$에서 코드북 인덱스가 최대 한 번만 나타나는 패턴으로 제한한다.&lt;/p>
&lt;p>&amp;ldquo;parallel&amp;rdquo; 패턴과 같은 여러 분해를 쉽게 정의할 수 있다. 이 패턴은 다음과 같이 주어진다.&lt;/p>
&lt;p>$$ P_s = \lbrace (s, k) : k \in \lbrace 1, &amp;hellip;, K \rbrace \rbrace $$&lt;/p>
&lt;p>코드북 사이에 &amp;ldquo;delay&amp;quot;를 도입하는 것도 가능하다.&lt;/p>
&lt;p>$$ P_s = \lbrace (s − k + 1, k) : k \in \lbrace 1, &amp;hellip;, K \rbrace , s − k \geq 0 \rbrace $$&lt;/p>
&lt;p>다양한 코드북 패턴의 장단점을 실증적으로 평가하여, 병렬 코드북 시퀀스 모델링의 중요성을 강조한다.&lt;/p>
&lt;h3 id="model-conditioning">Model conditioning&lt;/h3>
&lt;p>&lt;strong>Text conditioning.&lt;/strong> 입력 오디오에 대응하는 텍스트를 표현하는 세 가지 주요 방법에 대해 실험하였다: T5 인코더를 사용하는 Kreuk et al. 의 방법, 지시기반 언어 모델을 사용하는 Chung et al. 의 방법, 그리고 공동 텍스트-오디오 표현인 CLAP을 사용하는 방법이다. 이 세 가지 방법 모두 조건부 오디오 생성 테스트에서 사용되었다.&lt;/p>
&lt;p>&lt;strong>Melody conditioning.&lt;/strong> 텍스트보다는 다른 오디오 트랙이나 휘파람, 허밍 등에서 얻은 멜로디 구조를 조건으로 삼는 것이 음악에 더 적합하다. 이를 위해 입력의 chromagram과 text description에 동시에 조건을 부여하여 멜로디 구조를 제어하는 실험을 진행하였다. 하지만 raw chromagram에 조건을 부여하면 과적합이 발생해 원본 샘플이 재구성되는 문제가 발생하였다. 이를 해결하기 위해 각 시간 단계에서 주요 time-frequency 빈도를 선택하는 정보 병목 방법을 도입하였다. 이는 supervised proprietary 데이터가 필요 없는 unsupervised 학습 방법으로, 데이터 수집 비용을 줄이는 효과가 있다.&lt;/p>
&lt;h3 id="model-architecture">Model architecture&lt;/h3>
&lt;p>&lt;strong>Codebook projection and positional embedding.&lt;/strong> 코드북 패턴에 따라 각 패턴 단계에서는 일부 코드북만 사용된다. 각 코드북은 최대 한 번만 사용되거나 아예 사용되지 않는다. 코드북이 사용되면, 해당 값은 학습된 임베딩 테이블을 통해 표현되고, 사용되지 않으면 특별 토큰으로 표시된다. 이렇게 변환된 각 코드북의 기여를 합산하며, 첫 번째 입력은 모든 특별 토큰의 합이 된다. 마지막으로, 현재 단계를 인코딩하기 위해 사인 임베딩을 합산한다.&lt;/p>
&lt;p>&lt;strong>Transformer decoder.&lt;/strong> 입력값은 여러 layer와 차원을 가진 transformer를 통해 처리된다. 각 layer는 causal self-attention block으로 구성되고, 조건부 신호 $C$에 따라 cross-attention block을 사용한다. 멜로디 조건을 사용할 경우, 조건부 텐서 $C$를 transformer 입력의 접두어로 사용한다. layer는 fully connected block으로 끝나며, 이 block은 linear layer, ReLU, 그리고 다시 linear layer로 구성된다. 각 block은 residual skip 연결로 래핑되고, 각 block에는 layer normalization가 적용된다.&lt;/p>
&lt;p>&lt;strong>Logits prediction.&lt;/strong> transformer decoder의 출력은 패턴 단계에서 $Q$의 값에 대한 logit 예측으로 변환된다. 코드북이 존재하면, 코드북 특정 linear layer를 적용하여 logit 예측을 얻는다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental setup&lt;/h2>
&lt;h3 id="models-and-hyperparameters">Models and hyperparameters&lt;/h3>
&lt;p>&lt;strong>Audio tokenization model.&lt;/strong> 32 kHz 단음 오디오를 위한 비인과적인 5층 EnCodec 모델을 사용하며, 이는 50 Hz의 frame rate와 initial hidden size 64를 가진다. 4개의 양자화기를 가진 RVQ로 임베딩을 양자화하고, 오디오 시퀀스에서 무작위로 잘린 1초 오디오 세그먼트로 모델을 학습시킨다.&lt;/p>
&lt;p>&lt;strong>Transformer model.&lt;/strong> 300M, 1.5B, 3.3B parameter의 크기를 가진 autoregressive transformer 모델을 학습시킨다. 이 모델들은 긴 시퀀스의 처리 속도와 메모리 사용량을 개선하기 위해 memory efficient Flash attention을 사용한다. 이 기술은 xFormers 패키지에서 구현되어 있다.&lt;/p>
&lt;p>이 모델들은 전체 음악 트랙에서 무작위로 샘플링된 30초 오디오 클립을 학습 데이터로 사용한다. 각 모델은 1M 단계 동안 AdamW optimizer를 사용하여 학습되며, batch size는 192, $\beta_1$은 0.9, $\beta_2$는 0.95, weight decay는 0.1, 그리고 gradient clipping은 1.0의 값을 가진다.&lt;/p>
&lt;p>300M parameter 모델의 경우, D-Adaptation 기반의 automatic step-size를 사용하여 모델의 수렴을 개선한다. 이 방법은 모델의 크기가 더 큰 경우에는 별다른 효과가 없었다.&lt;/p>
&lt;p>모델의 learning rate은 4000 step의 warmup을 가진 cosine learning rate을 따르며, 이동 평균은 0.99의 decay를 사용하여 계산된다.&lt;/p>
&lt;p>각 모델은 각각 32, 64, 96의 GPU를 사용하여 학습되며, mixed precision 방식을 사용한다. 더욱이, bfloat16이 시스템에서 불안정성을 초래하므로 float16을 사용한다.&lt;/p>
&lt;p>마지막으로, 샘플링 과정에서는 top-k 샘플링 방법을 사용하여 상위 250개의 토큰만을 유지하고, 이 토큰들의 확률 분포를 이용하여 샘플링을 진행한다. 이때의 temperature 값은 1.0이다.&lt;/p>
&lt;p>&lt;strong>Text preprocessing.&lt;/strong> Kreuk et al. 은 불용어를 제거하고 텍스트를 표제어화하는 텍스트 정규화 방법을 제안하였다. 음악 데이터셋에서는 음악 키, 템포, 악기 유형 등의 추가 정보를 텍스트 설명에 병합하는 실험을 진행하였다. 또한, 단어 dropout을 텍스트 augmentation 전략으로 사용하였다. 최종 모델에서는 0.25 확률로 정보 병합, 0.5 확률로 텍스트 dropout, 0.3 확률로 단어 dropout을 적용하였다.&lt;/p>
&lt;p>&lt;strong>Codebook patterns and conditioning.&lt;/strong> 30초의 오디오를 1500개의 autoregressive step으로 변환하는 &amp;ldquo;delay&amp;rdquo; 교차 패턴을 사용한다. 텍스트 조건 부여에는 T5 텍스트 encoder를 사용하며, 필요에 따라 멜로디 조건 부여를 추가한다. FLAN-T5와 CLAP를 실험하고, 각 텍스트 encoder의 성능을 비교하였다. 멜로디 조건 부여에는 chromagram을 계산하고 양자화하는 방법을 사용하였다. 학습 중에는 조건을 일정 확률로 드롭하고, 추론 시에는 가이드 스케일을 적용한다.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>&lt;strong>Training datasets.&lt;/strong> 20K 시간의 라이선스 음악, 내부 데이터셋의 10K 고품질 음악 트랙, 그리고 ShutterStock과 Pond5 음악 데이터 컬렉션을 사용하여 MusicGEN을 학습시킨다. 이 데이터셋들은 모두 텍스트 설명, 장르, BPM, 태그 등의 메타데이터와 함께 32 kHz로 샘플링된 전체 길이의 음악을 포함하며, 오디오는 모노로 다운믹스된다.&lt;/p>
&lt;p>&lt;strong>Evaluation datasets.&lt;/strong> MusicCaps 벤치마크에서 평가하였다. 이 벤치마크는 전문 음악가들이 준비한 5.5K의 샘플과 장르별로 균형을 이루는 1K의 샘플로 구성되어 있다. 균형되지 않은 샘플에서 객관적 지표를 보고하고, 질적 평가를 위해 장르 균형 샘플에서 예제를 추출하였다. 또한, 멜로디 평가와 소거 연구를 위해 학습 세트와 아티스트가 중복되지 않는 528개의 음악 트랙으로 구성된 평가 세트를 사용하였다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>&lt;strong>Baselines.&lt;/strong> Riffusion과 Mousai, 두 가지 text-to-music 생성 모델과 MusicGEN을 비교한다. 오픈소스 Riffusion 모델을 이용해 추론을 실행하고, Mousai의 경우 저자들이 제공한 오픈소스를 이용해 이 연구의 데이터셋으로 모델을 학습시켜 비교하였다. 가능한 경우, MusicLM과 Noise2Music과도 비교하였다.&lt;/p>
&lt;p>&lt;strong>Evaluation metrics.&lt;/strong> 객관적인 지표인 Fréchet Audio Distance(FAD), Kullback-Leiber Divergence(KL), 그리고 CLAP 점수를 이용하여 제안한 방법을 평가한다. FAD 점수는 생성된 오디오의 타당성을 나타내며, KL-Divergence는 원본 음악과 생성된 음악 사이의 레이블 확률을 비교한다. 이때, KL이 낮을수록 생성된 음악이 원본 음악과 유사한 개념을 가지고 있다고 판단한다. 마지막으로, CLAP 점수는 트랙 설명과 생성된 오디오 사이의 정렬을 정량화한다.&lt;/p>
&lt;p>인간 평가자들을 활용하여 overall quality(OVL)과 relevance to the text input(REL)을 평가하는 연구를 진행하였다. 평가자들은 제공된 오디오 샘플의 품질과 텍스트와의 일치도를 각각 1에서 100의 범위로 평가하였다. 이 평가는 Amazon Mechanical Turk 플랫폼에서 모집한 평가자들을 통해 진행되었고, 각 샘플은 최소 5명의 평가자에 의해 평가되었다. 잡음 주석과 이상치는 CrowdMOS 패키지를 통해 필터링하였으며, 모든 샘플은 공정성을 위해 -14dB LUFS에서 정규화되었다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>text-to-music 생성 작업에 대한 제안된 방법의 결과를 제시하며, 이전의 연구와 비교한다. 또한, 멜로디 특징에 기반한 음악 생성 능력을 평가하고, 스테레오 오디오 생성을 위해 코드북 패턴을 확장하는 방법을 설명한다.&lt;/p>
&lt;h3 id="comparison-with-the-baselines">Comparison with the baselines&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table1.png"
width="1132"
height="372"
srcset="https://kurtkim.github.io/p/musicgen/images/table1_hu82e130725237512cebe1a4c4f1872ed9_104459_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table1_hu82e130725237512cebe1a4c4f1872ed9_104459_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;p>제안된 방법과 Mousai, Riffusion, MusicLM, 그리고 Noise2Music와의 비교 한다. Noise2Music과 MusicLM의 공식 구현이 없으므로, 각각의 원고에서 보고된 FAD만을 보고한다. 인간 연구에서는 MusicCaps의 악기만을 사용한 40개 샘플을 사용하였으며, MusicGEN의 chromagram 학습에서 누수를 방지하기 위해 테스트 시간 동안 보류된 세트에서 무작위로 chromagram을 샘플링하였다.&lt;/p>
&lt;p>결과적으로 MusicGEN은 오디오 품질과 텍스트 설명 준수 측면에서 인간 청취자들에게 더 높은 평가를 받았다. Noise2Music은 MusicCaps에서 FAD 측면에서 가장 우수했으며, 텍스트 조건부로 학습된 MusicGEN이 뒤를 이었다. 멜로디 조건을 추가하면 객관적 지표가 저하되지만, 인간 평가에는 큰 영향을 미치지 않았다.&lt;/p>
&lt;p>낮은 평가를 받은 모델에 대해서는 FAD가 주관적 평가와 상관관계가 있지만, 높은 평가를 받은 모델에는 그러지 않았다. 또한, MusicCaps의 많은 샘플이 &amp;ldquo;noisy&amp;rdquo; 녹음이라는 설명을 포함하고 있어, 오디오 품질 향상이 일정 수준을 넘어설 경우 FAD가 악화될 수 있다는 사실을 발견하였다.&lt;/p>
&lt;h3 id="melody-evaluation">Melody evaluation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table2.png"
width="974"
height="192"
srcset="https://kurtkim.github.io/p/musicgen/images/table2_hu84589e5fca9f397abb8c20fca19cbe40_51604_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table2_hu84589e5fca9f397abb8c20fca19cbe40_51604_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="507"
data-flex-basis="1217px"
>&lt;/p>
&lt;p>텍스트와 멜로디를 동시에 고려하는 MusigGEN을 객관적, 주관적 지표로 평가하였다. 이를 위해 새로운 지표인 chroma cosine-similarity를 도입하였으며, 이는 참조 샘플과 생성된 샘플의 chroma 사이의 average cosine-similarity를 측정한다. 또한, 인간 연구를 통해 생성된 음악과 멜로디 사이의 관계를 평가하였다. 결과적으로 MusigGEN은 주어진 멜로디를 따르는 음악을 성공적으로 생성하며, chroma를 떨어뜨려도 성능이 유지되는 것으로 나타났다.&lt;/p>
&lt;h3 id="fine-tuning-for-stereophonic-generation">Fine-tuning for stereophonic generation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/figure2.png"
width="1160"
height="518"
srcset="https://kurtkim.github.io/p/musicgen/images/figure2_hu82a0dfe475666588aef20e9ef9119079_204671_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/figure2_hu82a0dfe475666588aef20e9ef9119079_204671_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;p>스테레오 데이터로의 생성을 확장하려는 실험을 진행하였다. 동일한 EnCodec 토크나이저를 사용하여 왼쪽과 오른쪽 채널에 독립적으로 적용하였고, 사전 학습된 단일 음향 MusicGEN 모델을 스테레오 오디오를 포함하는 데이터셋으로 미세 조정하였다. &amp;ldquo;delay&amp;rdquo; 패턴을 재사용하고, &amp;ldquo;stereo delay&amp;quot;과 &amp;ldquo;stereo partial delay&amp;rdquo; 두 가지 변형을 도입하였다. 이 간단한 전략을 통해 추가적인 계산 비용 없이 스테레오 오디오를 생성할 수 있었다. 스테레오 출력을 모노로 다운믹싱하면, 모노 모델과 거의 동등한 품질을 느낄 수 있었다. 전반적으로 스테레오 오디오가 모노보다 높게 평가되었으며, &amp;ldquo;stereo partial delay&amp;quot;이 &amp;ldquo;stereo delay&amp;quot;에 비해 전반적인 품질과 텍스트 관련성에서 약간의 향상을 보였습니다.&lt;/p>
&lt;h3 id="ablation">Ablation&lt;/h3>
&lt;p>다양한 코드북 패턴의 제거 연구와 모델 크기, 기억 연구 결과를 소개한다. 이 모든 연구는 보류된 평가 세트에서 임의로 샘플링된 30초 분량의 1K 샘플을 사용하여 수행되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table4.png"
width="1022"
height="318"
srcset="https://kurtkim.github.io/p/musicgen/images/table4_huf62e92f64d1bdb73d42bde58b852d6de_88931_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table4_huf62e92f64d1bdb73d42bde58b852d6de_88931_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="321"
data-flex-basis="771px"
>&lt;/p>
&lt;p>&lt;strong>The effect of the codebook interleaving patterns.&lt;/strong> 여러 코드북 패턴을 평가하였다. 이는 &amp;ldquo;delay&amp;rdquo;, &amp;ldquo;partial delay&amp;rdquo;, &amp;ldquo;parallel&amp;rdquo;, &amp;ldquo;coarse first&amp;rdquo;, &amp;ldquo;Partial flattening&amp;rdquo;, &amp;ldquo;flattening&amp;rdquo; 등의 패턴을 포함한다. 이 중 &amp;ldquo;flattening&amp;rdquo; 패턴은 생성을 개선하지만 높은 계산 비용이 들며, 간단한 &amp;ldquo;delay&amp;rdquo; 방식을 사용하면 비슷한 성능을 더 적은 비용으로 달성할 수 있음을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table5.png"
width="1076"
height="196"
srcset="https://kurtkim.github.io/p/musicgen/images/table5_hud8b26f41b0e31e8e2f3e6f87f76a2efe_54637_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table5_hud8b26f41b0e31e8e2f3e6f87f76a2efe_54637_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="548"
data-flex-basis="1317px"
>&lt;/p>
&lt;p>&lt;strong>The effect of model size.&lt;/strong> 다양한 모델 크기(300M, 1.5B, 3.3B parameter)에 대한 결과를 보고하였다. 모델 크기를 크게하면 성능이 향상되지만, 학습과 추론 시간이 더 길어진다. 주관적인 품질은 1.5B에서 최적이며, 더 큰 모델은 텍스트 프롬프트를 더 잘 이해한다.&lt;/p>
&lt;p>&lt;strong>Memorization experiment.&lt;/strong> 우리는 MusicGEN의 기억능력을 분석하였다. 학습 세트에서 20,000개의 예제를 무작위로 선택하고, 각각에 대해 원본 오디오에 해당하는 프롬프트를 모델에 입력하였다. greedy decoding을 사용하여 5초 길이의 오디오를 생성하고, 생성된 오디오와 원본 오디오가 일치하는 비율을 보고하였다. 또한, 80% 이상 일치하는 경우의 비율도 보고하였다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>&lt;strong>Audio representation.&lt;/strong> 최근 연구는 음악 신호를 압축된 표현으로 변환하고 이를 기반으로 생성 모델을 적용하는 방식을 주로 사용하고 있다. Lakhotia et al. 은 k-means를 사용한 음성 표현의 양자화를, Défossez et al. 과 Zeghidour et al. 은 residual vector quantization를 사용한 원시 파형에 대한 VQ-VAE 적용을 제안하였다. 이러한 방법들은 텍스트에서 오디오로의 생성에 활용되고 있다.&lt;/p>
&lt;p>&lt;strong>Music generation.&lt;/strong> 음악 생성은 다양한 방법으로 연구되어 왔다. Dong et al. 은 GAN을 사용한 심볼symbolic릭 음악 생성을, Bassan et al. 은 symbolic 음악의 비지도학습 분할을 제안하였다. Ycart et al. 은 RNN을 이용한 polyphonic 음악 모델링을, Ji et al. 은 음악 생성에 대한 딥러닝 방법들을 포괄적으로 조사하였다.&lt;/p>
&lt;p>Dhariwal et al. 은 hierarchical VQ-VAE를 사용하여 음악 샘플을 discrete representation으로 변환하고, 이를 통해 음악을 생성하는 방법을 제안하였다. Gan et al. 은 주어진 비디오에 대한 음악을 생성하면서 미디 노트를 예측하는 방법을, Agostinelli et al. 은 의미 토큰과 음향 토큰을 사용하여 음악을 표현하는 방법을 제안하였다. Donahue et al. 은 이러한 접근법을 노래 동반 생성 작업에 적용하였다.&lt;/p>
&lt;p>diffusion 모델을 사용하는 것은 대안적인 접근법이다. Schneider et al., Huang et al., Maina, Forsgren and Martiros는 이를 텍스트에서 음악으로 변환하는 작업에 적용하였다. Schneider et al.과 Huang et al. 은 오디오 생성과 샘플링 비율 증가에 diffusion 모델을 사용하였다. Forsgren et al. Martiros는 5초 오디오 세그먼트 생성과 장기 시퀀스 생성을 위해 spectrogram을 이용한 diffusion 모델을 미세조정하였다.&lt;/p>
&lt;p>&lt;strong>Audio generation.&lt;/strong> 텍스트에서 오디오로 변환하는 연구가 다양하게 진행되었다. Yang et al. 은 오디오 spectrogram을 VQ-VAE를 이용해 표현하고, 이를 기반으로 텍스트 CLIP 임베딩에 조건화된 diffusion 모델을 적용하였다. Kreuk et al. 과 Sheffer와 Adi는 각각 transformer 언어 모델과 이미지-오디오 생성을 위한 접근법을 제안하였다. 또한, Huang et al. 과 Liu et al. 은 텍스트-오디오 작업을 위해 latent diffusion 모델을 사용하면서 이를 다양한 작업에 확장하는 방법을 제안하였다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>텍스트와 멜로디에 따라 조절 가능한 음악 생성 모델인 MusicGEN을 소개하였다. 이 모델은 단순한 코드북 교차 전략을 통해 고품질의 음악 생성을 가능하게 하고, autoregressive 시간 단계를 줄일 수 있다. 또한 모델 크기, 조건화 방법, 텍스트 전처리 기법의 영향에 대한 포괄적인 연구를 제공하며, 생성된 오디오의 멜로디를 제어하는 chromagram 기반 조건화를 소개하였다.&lt;/p>
&lt;p>&lt;strong>Limitations&lt;/strong> 이 모델의 생성 방법은 조건에 따른 세밀한 제어를 허용하지 않아 CF guidance에 주로 의존한다. 텍스트 조건에 대한 데이터 augmentation은 상대적으로 간단하지만, 오디오 조건에 대한 데이터 augmentation와 guidance에 대해 추가 연구가 필요하다.&lt;/p>
&lt;p>&lt;strong>Broader impact.&lt;/strong> 대규모 생성 모델은 윤리적 도전을 제시한다. 모든 학습 데이터가 권리 소유자와 합의 하에 이루어지며, 데이터셋의 다양성 부족 문제를 인지하고 있다. 이 문제를 해결하기 위해 단순화된 모델을 사용하여 새로운 데이터셋에 대한 응용을 확대하고 있다. 또한, 이러한 모델이 아티스트에 대한 불공정한 경쟁을 일으킬 수 있음을 인지하며, 이 문제를 해결하기 위해 열린 연구를 통해 모든 참가자가 모델에 동등하게 접근할 수 있도록 노력하고 있다. 고급 제어 기법을 통해, 모델이 음악 애호가와 전문가 모두에게 도움이 될 수 있도록 하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2306.05284.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/audiocraft" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>CLAP</title><link>https://kurtkim.github.io/p/clap/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/clap/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 연구에서는 contrastive learning을 통해 자연어 설명과 오디오 데이터를 결합한 오디오 표현을 개발하는 방법을 제안한다. 이를 위해 633,526개의 오디오-텍스트 쌍을 모은 LAION-Audio-630K를 공개하고, 이를 활용해 오디오와 텍스트를 처리하는 모델을 구축하였다. 이 모델은 텍스트-오디오 검색에서 우수한 성능을 보였고, zero-shot 오디오 분류에서는 state-of-the-art를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>오디오는 텍스트, 이미지와 더불어 중요한 정보 유형이지만, 세부적인 주석이 필요한 오디오 작업은 데이터 수집이 노동 집약적이어서 사용 가능한 데이터가 제한적이다. 이 때문에 많은 감독 없이 다양한 오디오 작업에 적합한 효과적인 오디오 표현을 만드는 것은 어려운 과제이다.&lt;/p>
&lt;p>Contrastive learning 방식은 인터넷에서 모아진 대규모 노이즈 데이터로 모델을 학습시키는데 효과적이다. 최근 제안된 Contrastive Language-Image Pretraining (CLIP) 방식은 텍스트와 이미지를 shared latent space에 투영하여 학습한다. 이 방식은 데이터 주석에 제약받지 않으며, ImageNet 데이터셋의 변형에 대해 zero-shot 설정에서 높은 정확도를 보여준다. 또한, 오디오와 자연어도 중복 정보를 포함하며, 이를 통해 crossmodal 정보의 오디오 표현을 형성할 수 있다. 이러한 모델 학습은 쌍으로 된 오디오와 텍스트 데이터만을 필요로 하므로 수집이 상대적으로 쉽다.&lt;/p>
&lt;p>최근 연구들은 텍스트-오디오 검색 작업에 대한 대조적(contrastive) 언어-오디오 사전 학습 모델을 제안하였다. 일부 연구는 오디오 encoder로 Pretrained Audio Neural Network (PANN)을, 텍스트 encoder로 BERT를 사용하며, 다른 연구는 성능 향상을 위해 HTSAT와 RoBERTa를 추가로 앙상블하였다. 또한, AudioClip과 WaveCLIP과 같은 연구들은 이미지-오디오(또는 이미지-오디오-언어) 사전 학습 모델에 초점을 맞추었다. 이러한 모든 모델들은 오디오 도메인에서의 대조적 학습에 큰 잠재력을 보여주고 있다.&lt;/p>
&lt;p>현재의 언어-오디오 대조적 학습 연구들은 전체적인 강점을 아직 다 보여주지 못하였다. 모델들은 대부분 작은 데이터셋에서 학습되었고, 오디오/텍스트 encoder의 선택과 hyperparameter 설정에 대한 충분한 조사가 없었다. 또한, 모델들은 다양한 길이의 오디오를 처리하는 데 어려움을 겪었으며, 텍스트-오디오 검색에만 초점을 맞추고 downstream task에서의 오디오 표현을 평가하지 않았다. 이러한 문제점들을 해결하고 더 많은 하downstream task에 대한 일반화 능력을 발견하는 것이 필요하다.&lt;/p>
&lt;p>이 논문에서는 이전 연구를 바탕으로, 위의 문제점들을 개선하기 위해 데이터셋, 모델 설계, 실험 설정에 대한 기여를 한다:&lt;/p>
&lt;ul>
&lt;li>이 논문에서는 633,526개의 오디오-텍스트 쌍을 포함하는 현재 가장 큰 공개 오디오 캡션 데이터셋인 LAION-Audio-630K를 공개하고, 학습 과정을 돕기 위해 키워드-캡션 모델을 활용해 AudioSet의 레이블을 캡션으로 확장하였다. 이 데이터셋은 다른 오디오 작업에도 활용될 수 있다.&lt;/li>
&lt;li>이 논문에서는 대조적 언어-오디오 사전 학습 파이프라인을 구축하고, 이를 위해 두 개의 오디오 인코더와 세 개의 텍스트 인코더를 선택하였다. 또한, 성능 향상과 variable-length inputs 처리를 위해 feature fusion mechanism을 활용하였다.&lt;/li>
&lt;li>이 논문에서는 텍스트-오디오 검색과 zero-shot 및 지도 오디오 분류와 같은 downstream task에 대한 모델의 포괄적인 실험을 수행하였다. 데이터셋의 확장, keyword-to-caption augmentation, feature fusion이 모델 성능을 향상시키는 데 도움이 된다는 것을 보여주었다. 이를 통해 텍스트-오디오 검색과 오디오 분류 작업에서 state-of-the-art를 달성하였다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="laion-audio-630k-and-training-dataset">LAION-Audio-630K And Training Dataset&lt;/h2>
&lt;h3 id="laion-audio-630k">LAION-Audio-630K&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table1.png"
width="664"
height="206"
srcset="https://kurtkim.github.io/p/clap/images/table1_huc915c892c5fa79f7a24207ab27da5b09_35590_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table1_huc915c892c5fa79f7a24207ab27da5b09_35590_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="322"
data-flex-basis="773px"
>&lt;/p>
&lt;p>LAION-Audio-630K는 총 4,325.39시간에 걸친 633,526쌍의 오디오-텍스트 데이터셋을 수집하였다. 이 데이터셋은 사람의 활동, 자연 소리, 오디오 효과 등을 포함하며, 공개적으로 사용 가능한 여러 웹사이트에서 수집하였다. 현재로서는 LAION-Audio-630K가 공개적으로 이용 가능한 가장 큰 오디오-텍스트 데이터셋이다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>이 논문에서는 모델 성능이 데이터셋의 크기와 유형에 따라 어떻게 변화하는지 테스트하기 위해, 세 가지 학습 세트 설정을 사용하였다. 이들 설정은 AudioCaps+Clotho (약 55K 샘플), LAION-Audio-630K (약 630K 샘플), Audioset (1.9 백만 오디오 샘플)을 포함하며, 모든 중복 데이터는 제외하였다.&lt;/p>
&lt;h3 id="dataset-format-and-preprocessing">Dataset Format and Preprocessing&lt;/h3>
&lt;p>이 작업에서 사용된 모든 오디오 파일은 48kHz sample rate의 mono channel로 전처리되었다. 레이블만 있는 데이터셋의 경우, 템플릿이나 키워드-캡션 모델을 사용해 레이블을 캡션으로 확장하였다. 이를 통해 대조적 언어-오디오 사전 학습 모델의 학습에 더 많은 데이터를 활용할 수 있게 되었고, 총 오디오 샘플 수는 2.5M개로 증가하였다.&lt;/p>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;h3 id="contrastive-language-audio-pretraining">Contrastive Language-Audio Pretraining&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/figure1.png"
width="680"
height="972"
srcset="https://kurtkim.github.io/p/clap/images/figure1_huef2bf2bcce39fef884bc41272d03fdcb_166045_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/figure1_huef2bf2bcce39fef884bc41272d03fdcb_166045_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="69"
data-flex-basis="167px"
>&lt;/p>
&lt;p>오디오 데이터 $X_i^a$와 텍스트 데이터 $X_i^t$의 입력을 각각 처리하기 위해 두 개의 encoder를 사용한다. 여기서 $(X_i^a, X_i^t)$는 $i$로 색인된 오디오-텍스트 쌍 중 하나이다. 오디오 임베딩 $E_i^a$와 텍스트 임베딩 $E_i^t$는 각각 오디오 encoder $\mathbf{f}_{audio}(\cdot)$와 텍스트 encoder $\mathbf{f}_{text}(\cdot)$에 의해 얻어지며, projection layer를 사용한다:&lt;/p>
&lt;p>$$ E_i^a = \text{MLP}_{audio}(\mathbf{f}_{audio}(X_i^a)) $$&lt;/p>
&lt;p>$$ E_i^t = \text{MLP}_{text}(\mathbf{f}_{text}(X_i^t)) $$&lt;/p>
&lt;p>오디오/텍스트 projection layer는 2-layer multilayer perceptron(MLP)이다. 이는 ReLU activation function을 사용하여 encoder ouptput을 동일한 차원 $D$로 매핑한다(i.e., $E_i^a, E_i^t \in \mathbb{R}^D$). 이로 인해 오디오 데이터와 텍스트 데이터의 관계를 더 잘 파악할 수 있다.&lt;/p>
&lt;p>이 모델은 오디오와 텍스트 임베딩 간의 contrastive learning을 통해 학습되며, 이 때 다음의 loss function를 사용한다:&lt;/p>
&lt;p>$$ \mathbf{L} = {{1}\over{2N}} \sum_{i=1}^N (log {{exp(E_i^a \cdot E_i^t / \gamma)}\over{\sum_{j=1}^N exp(E_i^a \cdot E_j^t / \gamma)}} + log {{exp(E_i^t \cdot E_a^t / \gamma)}\over{\sum_{j=1}^N exp(E_i^t \cdot E_j^a / \gamma)}})$$&lt;/p>
&lt;p>$\gamma$는 손실을 조정하는 학습 가능한 parameter이며, 로그항은 오디오-텍스트 또는 텍스트-오디오 변환을 고려한다. $N$은 일반적으로 데이터 수를 나타내지만, 효율적인 학습을 위해 학습 시 배치 크기로 사용된다.&lt;/p>
&lt;p>학습된 모델의 임베딩$(E^a, E^b)$은 다양한 문맥에서 활용되어, 각 작업에 따른 성능을 향상시키는 데 도움을 준다.&lt;/p>
&lt;h3 id="downstream-tasks-in-inference-stage">Downstream Tasks in Inference Stage&lt;/h3>
&lt;p>&lt;strong>Text-to-Audio Retrieval&lt;/strong> target 오디오 임베딩 $E_p^a$는 cosine similarity 함수를 사용하여 $M$개의 텍스트 임베딩 $E^t = \lbrace E_1^t, &amp;hellip;, E_M^t \rbrace$중에서 가장 가까운 텍스트 임베딩 $E_q^t$를 찾아, 가장 잘 매치되는 텍스트를 결정할 수 있다. 이는 오디오와 텍스트 간의 가장 적합한 대응을 찾는데 사용된다.&lt;/p>
&lt;p>&lt;strong>Zero-shot Audio Classiﬁcation&lt;/strong> $M$개의 오디오 클래스 $C = \lbrace C_1, &amp;hellip;, C_M \rbrace$에 대해, $M$개의 프롬프트 텍스트 $X^t = \lbrace X_1^t, &amp;hellip;, X_M^t \rbrace$를 구성하고, 주어진 오디오 $X_p^a$에 대해 코사인 유사도를 통해 $X^t$중에서 가장 최적의 매치 $X_q^t$를 찾는다. 이 방법의 장점은 오디오 카테고리가 제한되지 않고, 분류 작업을 텍스트-오디오 검색 작업으로 변환할 수 있다는 점이다.&lt;/p>
&lt;p>&lt;strong>Supervised Audio Classiﬁcation&lt;/strong> 모델 학습 후, 주어진 오디오 $X_p^a$의 임베딩 $E_p^a$은 projection layer를 추가하고 미세조정하여 고정 카테고리 분류 작업으로 매핑될 수 있다.&lt;/p>
&lt;h3 id="audio-encoders-and-text-encoders">Audio Encoders and Text Encoders&lt;/h3>
&lt;p>PANN과 HTSAT 두 모델을 오디오 encoder로 선택하였다. PANN은 CNN 기반, HTSAT은 transformer 기반 모델이며, 둘 다 바로 앞(penultimate) layer의 output $L$을 MLP layer으로 보낸다다. 각각의 output 차원 $L_{PANN} = 2048$, $L_{HTSAT} = 768$이다.&lt;/p>
&lt;p>텍스트 encoder로 CLIP transformer, BERT, RoBERTa를 선택하였다다. output 차원은 각각 $L_{CLIP} = 512$, $L_{BERT} = 768$, $L_{RoBERTa} = 768$이며, 오디오와 텍스트 output을 모두 512 차원으로 매핑하기 위해 2layer MLP를 적용하였다.&lt;/p>
&lt;h3 id="feature-fusion-for-variable-length-audio">Feature Fusion for Variable-Length Audio&lt;/h3>
&lt;p>오디오는 길이가 가변적인 특성을 가지기 때문에, 전체 오디오를 인코더에 입력하고 임베딩의 평균을 출력하는 전통적인 방식은 계산 효율이 떨어진다. 따라서 대략적인 전역 정보와 랜덤 샘플링된 지역 정보를 결합하여 다양한 길이의 오디오에 대해 일정한 계산 시간 내에서 학습하고 추론한다.&lt;/p>
&lt;p>$T$초의 오디오와 ﬁxed chunk duration $d = 10$초에 대해:&lt;/p>
&lt;ul>
&lt;li>$T \leq d$인 경우: 먼저 입력을 반복한 다음, 그것을 0값으로 채운다. 예를 들어, 3초의 입력은 $3 \times 3 = 9$초로 반복되고, 1초의 0 값으로 패딩된다. 이 방식은 짧은 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다.&lt;/li>
&lt;li>$T &amp;gt; d$인 경우: 먼저 입력을 $T$에서 $d$초로 다운샘플링하여 전역 입력으로 사용한다. 그런 다음 입력의 앞 ${1}\over{3}$, 중간 ${1}\over{3}$, 뒤 ${1}\over{3}$에서 각각 무작위로 $d$초 클립을 슬라이스하여 지역 입력으로 사용한다. 우리는 이 $4 \times d$ 입력들을 오디오 encoder의 첫 번째 layer로 보내어 초기 특징을 얻고, 그런 다음 세 개의 지역 특징들이 시간 축에서 3-stride를 가진 다른 2D-Convolution 계층에 의해 하나의 특징으로 변환된다. 마지막으로, 지역 특징 $X_{local}^a$와 전역 특징 $X_{global}^a$는 다음과 같이 결합된다:&lt;/li>
&lt;/ul>
&lt;p>$$ X_{fusion}^a = \alpha X_{global}^a + (1 - \alpha)X_{local}^a $$&lt;/p>
&lt;p>여기서 $\alpha = \mathbf{f}_{AFF}(X_{global}^a, X_{local}^a)$는 두 입력의 결합 요소를 학습하기 위한 두 개의 분기를 가진 CNN 모델인 attention feature fusion (AFF)에 의해 얻어진 요소이다. &amp;ldquo;slice &amp;amp; vote&amp;rdquo; 방법과 비교하여, 특징 결합은 첫 몇 개의 layer에서만 오디오 슬라이스를 처리하기 때문에 학습 시간을 절약한다. 이 방식은 긴 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다.&lt;/p>
&lt;h3 id="keyword-to-caption-augmentation">Keyword-to-Caption Augmentation&lt;/h3>
&lt;p>일부 데이터셋에서는 오디오에 대응하는 키워드로 레이블이나 태그를 사용한다. 이러한 키워드를 바탕으로 사전 학습된 언어 모델 T5를 사용하여 캡션을 생성하며, output 문장에서 편향을 제거하는 후처리를 진행한다. 예를 들어, &amp;ldquo;여성&amp;quot;과 &amp;ldquo;남성&amp;quot;을 &amp;ldquo;사람&amp;quot;으로 교체하여 성별 편향을 제거한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>제안한 모델에 대해 세 가지 실험을 수행한다. 다양한 encoder를 사용해 최적의 조합을 찾고, 다양한 데이터셋 크기에서 특징 결합과 keyword-to-caption augmentation을 적용해 효과를 검증하며, 오디오-텍스트 검색과 텍스트-오디오 검색에서의 성능을 평가한다. 마지막으로 최적의 모델로 zero-shot과 지도 오디오 분류 실험을 수행한다.&lt;/p>
&lt;h3 id="hyperparameters-and-training-details">Hyperparameters and Training Details&lt;/h3>
&lt;p>AudioCaps, Clotho, LAIONAudio-630K, AudioSet 데이터셋을 사용해 모델을 학습시킨다. 오디오 데이터는 10-second input length, 480 hop size, 1024 window size, 64 mel-bins으로 처리하며, 입력은 $(T = 1024, F = 64)$의 형태를 가진다. 텍스트 데이터는 최대 토큰 길이를 77로 토큰화한다.&lt;/p>
&lt;p>10초보다 긴 오디오는 무작위로 10초 세그먼트로 분할한다. 학습 중에는 $\beta_1 = 0.99, \beta_2 = 0.9$의 Adam optimizer를 사용하고, warm-up과 learning rate $10^{−4}$의 cosine learning rate decay를 사용한다. AudioCaps+Clotho 데이터셋에서는 batch size를 768로, LAION-Audio-630K를 포함하는 학습 데이터셋에서는 2304로, AudioSet을 포함하는 학습 데이터셋에서는 4608로 설정하여 모델을 학습시킵니다. 모델은 총 45 에포크 동안 학습된다.&lt;/p>
&lt;h3 id="text-to-audio-retrieval">Text-to-Audio Retrieval&lt;/h3>
&lt;p>&lt;strong>Audio and Text Encoders&lt;/strong> 텍스트-오디오 검색을 위해 가장 적합한 오디오와 텍스트 encoder를 찾기 위해 실험을 진행하였다. 이를 위해 두 오디오 encoder와 세 텍스트 encoder를 결합하고, AudioCaps와 Clotho 데이터셋에서 학습을 진행하였다. 이 실험의 목표는 최적의 encoder 조합을 찾는 것이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table2.png"
width="592"
height="214"
srcset="https://kurtkim.github.io/p/clap/images/table2_hueb35c23a5ca995604e43931c9ca5897a_39395_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table2_hueb35c23a5ca995604e43931c9ca5897a_39395_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="663px"
>&lt;/p>
&lt;p>HTSAT 오디오 encoder는 PANN보다 더 좋은 성능을 보이고, 텍스트 encoder는 RoBERTa가 BERT보다 우수하며, CLIP transformer는 가장 성능이 낮다. 또한, RoBERTa는 overfit이 덜 발생하지만, CLIP transformer는 overfit이 많아 일반화 성능이 낮다.&lt;/p>
&lt;p>&lt;strong>Dataset Scale&lt;/strong> HTSAT-RoBERTa 모델을 사용하여 텍스트-오디오 검색 실험을 수행하였다. 데이터셋 크기를 점차 늘렸지만, &amp;ldquo;AudioCaps + Clotho&amp;quot;에서 &amp;ldquo;LA.&amp;ldquo;로 확대해도 AudioCaps의 성능은 개선되지 않았다. 하지만 Clotho 세트에서의 성능은 향상되었다. 이는 AudioCaps가 사전 학습된 AudioSet와 유사한 오디오를 포함하고 있기 때문이며, 다른 출처의 데이터를 더 많이 받게 되면, 모델의 일반화는 증가하지만 AudioSet 데이터의 분포에서 벗어나게 된다. 따라서, AudioCaps의 성능은 떨어지지만, Clotho의 성능은 향상되었다. 이는 다양한 오디오 유형 간의 성능 유지에 대한 타협을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table3.png"
width="1358"
height="282"
srcset="https://kurtkim.github.io/p/clap/images/table3_hu33d594c9c09d66373764e22f05571ae3_107352_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table3_hu33d594c9c09d66373764e22f05571ae3_107352_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="481"
data-flex-basis="1155px"
>&lt;/p>
&lt;p>&lt;strong>Keyword-to-Caption and Feature Fusion&lt;/strong> feature fusion mechanism과 keyword-to-caption augmentation를 모델에 추가하면 성능이 향상된다. Clotho 데이터셋에서는 특히 효과적이다. AudioSet을 학습 세트에 추가하면 AudioCaps의 성능은 증가하지만 Clotho에서는 감소하는 것을 확인할 수 있다. 이는 AudioCaps와 Clotho 간의 성능 타협을 재확인한다. 또한, keyword-to-caption augmentation는 대부분의 지표에서 단순 템플릿 텍스트 프롬프팅보다 더 나은 성능을 보인다.&lt;/p>
&lt;p>최적 모델은 텍스트-오디오 검색에서 대부분의 지표에서 이전 방법보다 우수하며, 특히 AudioCaps에서 36.7%, Clotho에서 18.2%의 결과를 보여주었다. 대규모 데이터셋에서의 학습과 feature fusion은 모델 성능을 효과적으로 개선시킨다는 것을 입증하였다.&lt;/p>
&lt;h3 id="zero-shot-and-supervised-audio-classiﬁcation">Zero-shot and Supervised Audio Classiﬁcation&lt;/h3>
&lt;p>&lt;strong>Zero-shot Audio Classiﬁcation&lt;/strong> 모델의 일반화와 견고성을 평가하기 위해, 세 가지 주요 모델에 대해 zero-shot 오디오 분류 실험을 수행하였다. 이 모델들은 ESC50, VGGSound, Urbansound8K 데이터셋에서 평가되었고, top-1 정확도를 지표로 사용했다. &amp;ldquo;This a sound of label.&amp;rdquo; 형식의 텍스트 프롬프트를 사용하여 오디오를 분류하였다. 학습 데이터와 테스트 데이터셋간에 겹치는 부분은 제외하고 평가를 진행하였다.&lt;/p>
&lt;p>&lt;strong>Supervised Audio Classiﬁcation&lt;/strong> FSD50K와 VGGSound 데이터셋에서 오디오 encoder를 미세 조정하여 지도 학습 오디오 분류를 수행하였다. ESC50와 Urbansound8K는 데이터 유출 문제로 인해 실험을 수행하지 않았다. FSD50K 평가에는 mAP를 지표로 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table4.png"
width="664"
height="280"
srcset="https://kurtkim.github.io/p/clap/images/table4_hue3d9dda28790c2573ee91b7137dfa0ae_55869_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table4_hue3d9dda28790c2573ee91b7137dfa0ae_55869_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>세 가지 데이터셋에서 zero-shot 오디오 분류의 state-of-the-art를 보여주며, 이는 보이지 않는 데이터에 대한 모델의 높은 일반화 능력을 입증한다. feature fusion mechanism과 keyword-to-caption augmentation은 모델 성능을 향상시키는 데 기여하며, 우리의 지도 학습 오디오 분류 결과는 VGGSound에서 최고 성능을, FSD50K에서는 가장 가까운 성능을 보여주었다. 이 결과는 제안된 모델이 효과적인 오디오 표현을 학습한다는 것을 확인한다.&lt;/p>
&lt;h3 id="conclusion-and-futrue-work">Conclusion And Futrue Work&lt;/h3>
&lt;p>이 논문에서는 대규모 오디오-텍스트 데이터셋을 제안하고 언어-오디오 contrastive learning 패러다임을 개선하였다. LAION-Audio-630, keyword-tocaption augmentation가 있는 AudioSet, 그리고 feature fusion이 오디오 이해와 작업 성능을 향상시키며 가변 길이 데이터에서의 효과적인 학습을 가능하게 함을 보여주었다. 미래 연구는 더 큰 학습 데이터 수집과 오디오 합성, 분리 등의 downstream task 적용을 고려하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2211.06687.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/LAION-AI/CLAP" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>