<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on K2H Blog</title><link>https://kurtkim.github.io/tags/nlp/</link><description>Recent content in NLP on K2H Blog</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Sat, 30 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>BERT</title><link>https://kurtkim.github.io/p/bert/</link><pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/bert/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 양방향 인코더 표현을 사용하는 언어모델로, BERT는 레이블이 없는 텍스트에서 깊은 양방향 표현을 사전 학습함으로써, 하나의 출력 레이어만을 추가해서 다양한 작업에 맞게 미세조정 할 수 있다. 이 모델은 개념적으로 단순하면서도 실증적으로 강력하며, 다양한 자연어 처리 작업에서 새로운 최고 수준의 결과를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델의 사전 학습은 다양한 자연어 처리 작업의 향상에 크게 기여하였다. 이는 문장 전체를 분석하여 문장 간 관계를 예측하는 자연어 추론이나 재구성과 같은 문장 수준의 작업뿐만 아니라, 명명된 개체 인식이나 질문 응답과 같이 토큰 수준의 작업을 포함한다.&lt;/p>
&lt;p>사전 학습된 모델을 downstream tasks에 적용하는 데는 두 가지 전략이 있다: feature-based와 미세조정(fine-tuning)이다. feature-based 접근법은 ELMo와 같이 사전 훈련된 표현을 추가 특성으로 사용하고, 미세 조정 접근법은 GPT와 같이 작업 특정 파라미터를 최소화하고 사전 훈련된 모든 파라미터를 미세 조정한다. 이 두가지 접근 방식은 사전 학습을 하는동안 같은 목적 함수를 공유하며, 일반적인 언어 표현을 학습하기 위해 단방향 언어 학습 모델을 사용한다.&lt;/p>
&lt;p>현재의 기술들은 특히 미세 조정 접근법에 대해 사전 학습된 표현의 가능성을 제한한다고 주장한다. 표준 언어 모델이 단방향적이므로, 사전 훈련 동안 사용할 수 있는 아키텍처가 제한되기 때문이다. OpenAI GPT의 경우, 모든 토큰이 이전 토큰만을 주목하는 &amp;lsquo;왼쪽에서 오른쪽으로&amp;rsquo;의 아키텍처를 사용한다. 이러한 제한은 문장 수준 작업이나 양방향 맥락 통합이 중요한 토큰 수준 작업에 대해 불리하다.&lt;/p>
&lt;p>이 논문에서는 BERT(Bidirectional Encoder Representations from Transformers)를 제안하여 미세 조정 기반 접근법을 개선한다. BERT는 일부 토큰을 무작위로 마스크한 &amp;ldquo;masked language model&amp;quot;을 사용하여 단방향성 제약을 완화하고, 이를 통해 깊은 양방향 Transformer를 사전 학습한다. 그리고 &amp;ldquo;next sentence prediction&amp;rdquo; 작업을 통해 텍스트 쌍 표현을 함께 사전 학습한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>언어 표현에 대한 양방향 사전 훈련의 중요성을 강조한다. BERT는 마스크된 언어 모델을 사용해 깊은 양방향 표현을 사전 학습하는데, 이는 단방향 언어 모델을 사용하는 기존 방법과 대조적이다. 또한, 독립적으로 훈련된 언어 모델을 얕게 연결하는 방식과도 차별화된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>사전 학습된 표현은 작업별 아키텍처의 필요성을 줄이며, BERT는 다양한 작업에서 최고 수준의 성능을 달성하는 첫 미세 조정 기반 표현 모델이다. 이는 다수의 작업별 아키텍처들의 성능을 능가한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BERT는 11가지 자연어 처리(NLP) 작업에 대해 state-of-the-art 성능을 달성했다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>일반적인 언어 표현을 사전 학습하는 것은 오랜 역사를 가지고 있으며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략하게 검토한다.&lt;/p>
&lt;h3 id="unsupervised-feature-based-approaches">Unsupervised Feature-based Approaches&lt;/h3>
&lt;p>단어의 범용적인 표현 학습은 비신경망 및 신경망 방법을 포함한 여러 연구의 주제였다. 사전 학습된 단어 임베딩은 현대 NLP 시스템의 핵심 부분이며, 처음부터 학습된 임베딩보다 크게 개선된다. 단어 임베딩 벡터를 사전 훈련하기 위해, 언어 모델링 목표와 맥락에서 단어를 구별하는 목표가 사용되었다.&lt;/p>
&lt;p>이러한 접근법은 문장이나 문단 임베딩 등의 더 큰 단위로 확장되었다. 문장 표현을 훈련시키기 위해, 후보 다음 문장을 순위 매기는 연구, 이전 문장의 표현을 기반으로 다음 문장의 단어를 생성하는 연구, 또는 노이즈 제거 오토인코더에서 파생된 연구 등이 있었다.&lt;/p>
&lt;p>ELMo와 그 전임자는 단어 임베딩 연구를 다른 차원으로 확장해, 맥락에 따라 변하는 특징을 추출하였다. 이는 언어 모델의 왼쪽과 오른쪽 표현을 연결해 이루어진다. ELMo는 이를 기존의 작업 특정 아키텍처와 결합해 주요 NLP 벤치마크를 향상시켰다. 또한, 다른 연구들은 LSTM을 이용해 맥락적 표현을 학습하거나, 클로즈 작업을 통해 텍스트 생성 모델의 견고성을 향상시키는 방법을 제안하였다.&lt;/p>
&lt;h3 id="unsupervised-fine-tuning-approaches">Unsupervised Fine-tuning Approaches&lt;/h3>
&lt;p>첫 번째 연구들은 레이블이 없는 텍스트에서 단어 임베딩 파라미터만 사전 학습하였다.&lt;/p>
&lt;p>최근에는 문장이나 문서 인코더가 레이블이 없는 텍스트에서 사전 훈련되고, 지도학습의 다음 작업을 위해 미세조정되었다. 이 방법의 장점은 적은 양의 파라미터만 처음부터 학습하면 된다는 것이며, 이 때문에 OpenAI GPT는 여러 문장 수준 작업에서 최고 성능을 달성하였다. 이러한 모델을 사전 학습하기 위해 왼쪽에서 오른쪽으로의 언어 모델링과 오토인코더가 사용되었다.&lt;/p>
&lt;h3 id="transfer-learning-from-supervised-data">Transfer Learning from Supervised Data&lt;/h3>
&lt;p>대규모 데이터셋을 가진 자연어 추론과 기계 번역 등의 감독 학습 작업에서 효과적인 전이 학습이 보여주었다. 또한, 컴퓨터 비전 연구에서는 ImageNet으로 사전 훈련된 대규모 모델을 미세조정하여 전이 학습의 중요성을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="bert">BERT&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure1.png"
width="1218"
height="514"
srcset="https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>BERT는 사전 학습과 미세 조정 두 단계로 이루어진다. 사전 학습에서는 레이블이 없는 데이터로 모델을 학습하며, 미세 조정에서는 사전 훈련된 파라미터로 초기화된 BERT 모델을 downstream tasks의 레이블이 붙은 데이터로 미세 조정한다. 각 downstream tasks는 동일한 사전 훈련된 파라미터로 초기화되지만 별도의 미세 조정된 모델을 가진다.&lt;/p>
&lt;p>BERT는 다양한 작업에 걸친 통일된 아키텍처이며, 사전 학습된 아키텍처와 최종 downstream tasks 아키텍처 사이에는 거의 차이가 없다.&lt;/p>
&lt;h3 id="model-architecture">Model Architecture&lt;/h3>
&lt;p>BERT의 구조는 다중 레이어 양방향 Transformer 인코더로, Transformer의 널리 쓰이는 사용과 거의 동일하기 때문에, 모델 아키텍처의 상세한 설명은 &amp;ldquo;The Annotated Transformer&amp;quot;를 참조하도록 권장한다.&lt;/p>
&lt;p>이 연구에서는 레이어의 개수(Transformer 블록)를 $L$, 은닉의 크기를 $H$, self-attention 헤드 수를 $A$로 표시한다. $BERT_{BASE}$ (L=12, H=768, A=12, Total Parameters=110M)와 $BERT_{LARGE}$ (L=24, H=1024, A=16, Total Parameters=340M)의 두 모델 사이즈에 대한 결과를 비교한다.&lt;/p>
&lt;p>비교를 위해 $BERT_{BASE}$는 같은 모델 사이즈인 OpenAI GPT와 비교하였다. 그러나 중요한 점은, BERT Transformer는 양방향 self-attention을 사용하는 반면, GPT Transformer는 각 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 self-attention을 사용한다.&lt;/p>
&lt;h3 id="inputoutput-representations">Input/Output Representations&lt;/h3>
&lt;p>BERT의 입력 표현은 단일 문장과 문장 쌍(예: 〈질문, 답변〉)을 하나의 토큰 시퀀스에서 명확하게 표현할 수 있다. 여기서 &amp;ldquo;문장(senetence)&amp;ldquo;은 실제 문장이 아닌 텍스트의 일부를, &amp;ldquo;시퀀스(sequence)&amp;ldquo;는 BERT 입력 토큰을 의미하며, 이는 하나 또는 두 개의 문장일 수 있다.&lt;/p>
&lt;p>30,000개의 토큰 단어를 가진 WordPiece임베딩을 사용하며, 모든 문장의 첫번째 토큰은 특별한 분류 토큰([CLS])으로 시작한다. 이 토큰에 해당하는 마지막 은닉 상태(hidden state)는 분류 작업을 위해 총 시퀀스 표현으로 사용된다. 문장 쌍은 하나의 시퀀스로 묶이며, 특별한 토큰([SEP])과 학습된 임베딩을 사용해 문장을 구분한다. 입력 임베딩은 $E$로, [CLS] 토큰과 i번째 입력 토큰의 최종 은닉 벡터는 각각 $C \in R^H$, $T_i \in R^H$ 로 표기한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure2.png"
width="984"
height="308"
srcset="https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="319"
data-flex-basis="766px"
>&lt;/p>
&lt;p>주어진 토큰에 대한 입력 표현은 해당 토큰, 세그먼트, 위치 임베딩을 합하여 구성된다.&lt;/p>
&lt;h3 id="pre-training-bert">Pre-training BERT&lt;/h3>
&lt;p>BERT를 사전 학습하기 위해 전통적인 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로의 언어 모델을 사용하지 않았으며, 두 가지 비지도 학습 작업을 사용하여 BERT를 사전 학습하였다.&lt;/p>
&lt;h4 id="task-1-masked-lm">Task #1: Masked LM&lt;/h4>
&lt;p>깊은 양방향 모델은 왼쪽에서 오른쪽 혹은 오른쪽에서 왼쪽 모델보다 강력하다는 것이 직관적이다. 하지만, 표준 언어 모델은 양방향 조건을 적용하면 각 단어가 간접적으로 자신을 &amp;lsquo;볼&amp;rsquo; 수 있게 되어, 모델이 문맥 속의 목표 단어를 쉽게 예측할 수 있게 되므로, 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로만 훈련된다.&lt;/p>
&lt;p>깊은 양방향 표현을 훈련하기 위해, 입력 토큰의 일부를 무작위로 마스킹하고 그 마스킹된 토큰들을 예측하는 &amp;lsquo;Masked LM&amp;rsquo; 방법을 사용한다. 이 방법은 Cloze 작업으로 불리기도 한다. 마스킹된 토큰에 해당하는 최종 은닉 벡터는 어휘에 대한 출력 softmax에 공급되며, 모든 실험에서 각 시퀀스의 15% 토큰을 무작위로 마스킹한다. 이 방법은 denoising auto-encoders와 달리 마스킹된 단어만을 예측한다.&lt;/p>
&lt;p>양방향 사전 학습 모델을 얻는 방법은 [MASK] 토큰이 미세 조정 과정에서 나타나지 않는 문제로 인해 사전 훈련과 미세 조정 사이에 불일치를 유발한다. 이를 완화하기 위해, &amp;ldquo;마스킹된&amp;rdquo; 단어를 항상 [MASK] 토큰으로 교체하지 않는다. 토큰 위치의 15%를 임의로 선택하고, 선택된 토큰은 80% 확률로 [MASK] 토큰, 10% 확률로 임의의 토큰, 10% 확률로 원래 토큰으로 교체한다. 그 후, 크로스 엔트로피 손실을 사용하여 원래 토큰을 예측한다.&lt;/p>
&lt;h4 id="task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)&lt;/h4>
&lt;p>질문 응답(QA)과 자연어 추론(NLI) 같은 작업들은 두 문장 간의 관계를 이해하는 것에 기초하며, 이는 언어 모델링만으로는 직접적으로 캡처할 수 없다. 이를 해결하기 위해, 문장간 관계를 이해하는 모델을 위한 사전 훈련 과정에서, 어떤 하나의 언어를 사용하는 말뭉치로부터 생성될 수 있는 다음 문장 예측 과제를 2진화(binarized)된 다음 문장 예측 작업을 한다. 구체적으로, 각 사전 훈련 예제에서, 선택된 두 문장 A와 B는 50%의 확률로 실제 연속하는 문장이며, 나머지 50%는 말뭉치에서 임의로 선택된 문장이다. 이 간단한 방법이 QA와 NLI에 큰 도움이 된다는 것을 입증하였다.&lt;/p>
&lt;p>다음 문장 예측(NSP) 작업은 이전 연구와 밀접한 관련이 있지만, 이전 연구에서는 문장 임베딩만을 하위 작업에 전달했으나, BERT는 모든 매개변수를 최종 작업 모델 초기화에 사용한다.&lt;/p>
&lt;h4 id="pre-training-data">Pre-training data&lt;/h4>
&lt;p>사전 학습 절차는 대부분 언어 모델 사전 학습에 대한 기존 연구를 따른다. 이때 사용되는 말뭉치는 BooksCorpus(800M개의 단어)와 영어 위키백과(2,500M개의 단어)이다. 위키백과에서는 텍스트 부분만 추출하며, 긴 연속적인 시퀀스를 추출하기 위해 문장 수준이 아닌 문서 수준의 말뭉치 사용이 중요하다는 점을 강조한다.&lt;/p>
&lt;h3 id="fine-tuning-bert">Fine-tuning BERT&lt;/h3>
&lt;p>Transformer의 자기 self-attention mechanism을 활용한 BERT의 미세 조정은 단일 텍스트나 텍스트 쌍을 포함한 다양한 downstream tasks를 모델링하는데 효과적이다. 텍스트 쌍을 독립적으로 인코딩한 후 양방향 cross attention을 적용하는 것이 일반적이지만, BERT는 이 두 단계를 통합하여 self-attention으로 텍스트 쌍을 인코딩함으로써 두 문장 간의 양방향 cross attention을 효과적으로 포함시킨다.&lt;/p>
&lt;p>각 작업마다 BERT에 작업 특정 입력과 출력을 연결하고 모든 매개변수를 미세 조정한다. 입력에서, 사전 훈련된 문장 A와 B는 다양한 작업(표현 변경, 함축, 질문 답변, 텍스트 분류 등)의 입력 쌍에 상응한다. 출력에서, 토큰 표현은 토큰 수준 작업에, [CLS] 표현은 분류 작업에 사용된다.&lt;/p>
&lt;p>미세 조정은 사전 훈련에 비해 상대적으로 저렴하며, 이 논문의 모든 결과는 동일한 사전 훈련 모델을 기반으로 클라우드 TPU에서는 1시간, GPU에서는 몇 시간 안에 재현 가능하다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>11가지 자연어 처리(NLP) 작업에 대한 BERT 미세 조정 결과를 보여준다.&lt;/p>
&lt;h3 id="glue">GLUE&lt;/h3>
&lt;p>General Language Understanding Evaluation(GLUE) benchmark는 다양한 자연어 이해 작업의 모음이다.&lt;/p>
&lt;p>GLUE에서 미세 조정을 하기 위해, 입력 시퀀스를 표현하고 첫 번째 입력 토큰([CLS])에 대응하는 최종 벡터를 종합 표현으로 사용한다. 미세 조정시 도입되는 유일한 새로운 매개변수는 분류 레이어의 가중치이다. 그리고 이들을 사용해 standard classiﬁcation 손실을 계산한다.&lt;/p>
&lt;p>batch size 32로 데이터를 3 epoch 동안 미세 조정한다. 각 작업마다 최적의 미세 조정 학습률을 선택하며, $BERT_{LARGE}$는 작은 데이터셋에서 불안정할 때 랜덤 재시작을 사용하여 최적의 모델을 선택한다. 랜덤 재시작에서는 동일한 사전 학습 체크포인트를 사용하지만 데이터 셔플링과 분류기 레이어 초기화는 다르게 합니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table1.png"
width="1230"
height="272"
srcset="https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="452"
data-flex-basis="1085px"
>&lt;/p>
&lt;p>$BERT_{BASE}$와 $BERT_{LARGE}$는 모든 작업에서 월등한 성능을 보여주며, 평균 정확도를 각각 4.5%, 7.0% 향상시켰다. 가장 큰 GLUE 작업인 MNLI에서 BERT는 정확도를 4.6% 향상시켰고, 공식 GLUE 리더보드에서는 $BERT_{LARGE}$가 80.5의 점수로 OpenAI GPT의 72.8을 능가했다.&lt;/p>
&lt;p>$BERT_{LARGE}$가 모든 작업에서 $BERT_{BASE}$를 크게 능가하며, 특히 훈련 데이터가 매우 적은 작업에서 그렇다는 것을 발견하였다.&lt;/p>
&lt;h3 id="squad-v11">SQuAD v1.1&lt;/h3>
&lt;p>The Stanford Question Answering Dataset (SQuAD v1.1)는 10만 개의 크라우드 소싱 질문/답변 쌍의 컬렉션이다. 주어진 질문과 답변을 포함하는 위키백과의 문단이 주어지면, 그 문단 내에서 실제 답변의 위치나 범위를 정확하게 예측하는 것이 목표이다.&lt;/p>
&lt;p>질문 응답 작업에서는 입력 질문과 문단을 하나의 연결된 시퀀스로 표현하며, 각각 다른 임베딩을 사용한다. 미세 조정 과정에서는 시작과 끝 벡터만 추가적으로 도입되며, 답변의 시작 단어 확률은 해당 단어와 시작 벡터 간의 내적 후 softmax를 적용하여 계산된다.&lt;/p>
&lt;p>답변 범위의 시작과 끝 위치를 예측하는 공식이 사용되며, 그 중 최대 점수를 가진 범위가 최종 예측값이 된다. 훈련 목표는 정확한 시작과 끝 위치의 로그 가능도 합이며, learning rate 5e-5와 batch size 32로 3 epoch 동안 미세 조정이 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table2.png"
width="548"
height="488"
srcset="https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>BERT 모델은 다른 공개 데이터를 사용하여 학습된 모델들을 뛰어넘으며, 특히 TriviaQA에 먼저 미세 조정을 함으로써 성능을 향상시켰다. 단일 BERT 모델만으로도 최고의 앙상블 시스템을 능가하며, TriviaQA 데이터 없이도 모든 기존 시스템을 크게 앞서고있다.&lt;/p>
&lt;h3 id="squad-v20">SQuAD v2.0&lt;/h3>
&lt;p>SQuAD 2.0 작업은 제공된 문단에 짧은 답변이 존재하지 않을 수 있다는 가능성을 허용함으로써 SQuAD 1.1 문제 정의를 확장하였고, 이로 인해 문제가 더 현실적으로 변하였다.&lt;/p>
&lt;p>이 작업을 위해 SQuAD v1.1 BERT 모델을 간단하게 확장하였다. 답변이 없는 질문은 시작과 끝이 [CLS] 토큰에 있는 답변 범위로 취급하였다. 예측 시, 답변이 없는 범위의 점수와 최고의 비-null 범위의 점수를 비교하여, 특정 임계값을 넘을 경우 non-null 답변을 예측하였다. 이 모델에서는 TriviaQA 데이터를 사용하지 않았으며, learning rate 5e-5와 batch size 48로 2 epoch 동안 미세 조정을 수행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table3.png"
width="554"
height="384"
srcset="https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>BERT를 사용하지 않는 시스템들과의 결과를 비교하였다. 다른 모델들에 비해 F1 점수가 5.1점 향상되었다.&lt;/p>
&lt;h3 id="swag">SWAG&lt;/h3>
&lt;p>The Situations With Adversarial Generations (SWAG) dataset는 실제 상식 추론을 평가하는 113k개의 문장 쌍 완성 예제를 포함하고 있다. 주어진 문장에 대해, 작업은 네 가지 선택지 중 가장 그럴듯한 답을 선택하는 것이다.&lt;/p>
&lt;p>SWAG dataset에서 미세 조정을 할 때, 각각 주어진 문장과 가능한 연속성을 포함하는 네 개의 입력 시퀀스를 만든다. [CLS] 토큰 표현과 내적을 이루는 벡터는 각 선택지에 대한 점수를 나타내며, 이 점수는 softmax 레이어를 통해 정규화한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table4.png"
width="390"
height="304"
srcset="https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="307px"
>&lt;/p>
&lt;p>learning rate가 2e-5이고 batch size가 16인 상태로 모델을 3 epoch 동안 미세 조정하였다. $BERT_{LARGE}$는 ESIM+ELMo 모델을 +27.1%로, OpenAI GPT를 8.3%로 능가하였다.&lt;/p>
&lt;h2 id="ablation-studies">Ablation Studies&lt;/h2>
&lt;p>상대적인 중요성을 더 잘 이해하기 위해 BERT의 여러 면에 걸쳐서 ablation 실험을 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table5.png"
width="606"
height="244"
srcset="https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="596px"
>&lt;/p>
&lt;h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks&lt;/h3>
&lt;p>$BERT_{BASE}$의 동일한 사전 학습 데이터, 미세 조정 scheme, 그리고 hyperparameter를 사용하여 두 가지 사전 학습 목표를 평가함으로써 BERT의 깊은 양방향성의 중요성을 입증한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>No NSP: &amp;ldquo;다음 문장 예측(NSP)&amp;rdquo; 과제를 하지 않은, &amp;ldquo;Masked LM(MLM)&amp;ldquo;을 사용해 훈련된 양방향 모델&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LTR &amp;amp; No NSP: 왼쪽 컨텍스트만 있는 모델은 표준 LTR LM을 사용해 훈련되며, 이는 미세 조정 시에도 유지된다. 이 모델은 NSP 작업 없이 사전 훈련되었다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>NSP 작업을 제거하면 QNLI, MNLI, SQuAD 1.1에서 성능이 크게 저하된다. 또한, 양방향 표현을 훈련하는 것은 성능에 중요한 영향을 미치며, 특히 LTR 모델은 모든 작업에서 MLM 모델보다 성능이 떨어진다.&lt;/p>
&lt;p>SQuAD의 경우, 토큰 레벨 은닉 상태가 오른쪽 컨텍스트를 가지고 있지 않기 때문에 LTR 모델이 토큰 예측에서 성능이 떨어질 것이라는 것은 직관적으로 명확하다. 이를 개선하기 위해 무작위로 초기화된 BiLSTM을 추가했지만, 결과는 사전 훈련된 양방향 모델보다 훨씬 떨어진다. 또한, BiLSTM은 GLUE 작업에서의 성능을 저하시킨다.&lt;/p>
&lt;p>LTR과 RTL 모델을 별도로 훈련하는 것은 가능하지만, 이는 단일 양방향 모델보다 비용이 두 배 많이 들고, QA와 같은 작업에 대해 직관적이지 않다. 또한, 이 방식은 모든 계층에서 양방향 컨텍스트를 사용하는 모델보다 성능이 엄격하게 떨어진다.&lt;/p>
&lt;h3 id="effect-of-model-size">Effect of Model Size&lt;/h3>
&lt;p>모델 크기가 미세 조정 작업 정확도에 미치는 영향을 알아본다. BERT 모델을 같은 파라미터와 훈련 절차를 사용한 반면, 레이어의 수, 은닉 상태 개수, 어텐션 헤드 개수를 다르게 학습했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table6.png"
width="528"
height="286"
srcset="https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;p>GLUE 작업 결과에 따르면, 더 큰 모델은 레이블이 붙은 훈련 예시가 적은 데이터셋에서도 정확도를 개선하였다. 이는 이미 상대적으로 큰 모델을 기반으로 중요한 개선을 이루어내며, 예를 들어 $BERT_{BASE}$는 110M, $BERT_{LARGE}$는 340M의 파라미터를 가진다. 이는 기존 문헌에서 제시한 Transformer 모델 보다 훨씬 크다.&lt;/p>
&lt;p>모델 크기를 늘리는 것이 대규모 작업에서 성능을 향상시키는 것은 잘 알려져 있지만, 이 연구는 모델이 충분히 사전 훈련되었다면 작은 규모의 작업에서도 큰 개선을 가져올 수 있다는 것을 보여준다. 이전의 연구들은 사전 훈련된 모델의 크기를 늘리는 것이 혼합된 결과를 가져왔지만, 이 연구는 모델이 작업에 직접 미세 조정을 받고, 매우 적은 수의 무작위로 초기화된 추가 파라미터만 사용할 때, 작은 규모의 작업도 크고 표현력 있는 사전 훈련된 표현의 이점을 볼 수 있다.&lt;/p>
&lt;h3 id="feature-based-approach-with-bert">Feature-based Approach with BERT&lt;/h3>
&lt;p>지금까지의 BERT 결과는 모두 미세 조정 방식을 사용했다. 이 방식은 사전 학습된 모델에 분류 계층을 추가하고 모든 파라미터를 하류 작업에 맞게 조정하는 방법이다. 그러나, 사전 학습된 모델에서 고정 특징을 추출하는 특징 기반 접근법도 장점이 있다. 일부 작업은 Transformer 인코더 아키텍처로 표현하기 어려워 특정 작업용 모델이 필요하며, 훈련 데이터의 복잡한 표현을 미리 계산하고 이를 기반으로 저렴한 모델로 실험을 진행하면 계산적으로 이점이 있다.&lt;/p>
&lt;p>이 섹션에서는 BERT를 이름 인식(NER) 작업에 적용하여 두 가지 접근법을 비교한다. BERT 입력에는 대소문자를 구분하는 WordPiece 모델을 사용하고, 데이터에서 제공하는 최대 문서 컨텍스트를 포함한다. 이 작업은 일반적인 방식에 따라 태깅 작업으로 설정되지만, 출력에서는 CRF 계층은 사용하지 않는다. NER 레이블 세트에 대한 토큰 수준 분류기의 입력으로 첫 번째 서브토큰의 표현을 사용한다.&lt;/p>
&lt;p>미세 조정 방식을 제거하기 위해, BERT의 매개변수를 조정하지 않고 특징 기반 방식을 적용하여 활성화 함수를 추출한다. 이 문맥적인 임베딩은 랜덤하게 초기화된 두 계층의 768차원 BiLSTM에 입력으로 사용되며, 이는 분류 레이어 이전에 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table7.png"
width="550"
height="458"
srcset="https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="288px"
>&lt;/p>
&lt;p>$BERT_{LARGE}$는 state-of-the-art 방법들과 비슷한 수준의 성능을 보여준다. 가장 효과적인 방법은 사전 훈련된 Transformer의 상위 4개 계층에서 토큰 표현을 결합하는 것이며, 이는 전체 모델을 미세 조정한 것보다 F1에서 0.3만큼 뒤떨어진다. 이는 BERT가 미세 조정과 특징 기반 접근법 모두에 효과적임을 보여준다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>언어 모델과 전이 학습을 통한 최근 개선은 비지도 사전 학습이 언어 이해 시스템의 중요한 부분임을 보여준다. 이 결과는 low-resource tasks조차 깊은 단방향 아키텍처에서 이익을 얻을 수 있다는 것을 보여주었다. 이 논문에서는 이를 깊은 양방향 아키텍처로 일반화함으로써, 사전 훈련된 동일 모델이 다양한 NLP 작업을 성공적으로 처리할 수 있게 한다는 것을 보여준다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/bert" target="_blank" rel="noopener"
>Code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener"
>The Annotated Transformer&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-1</title><link>https://kurtkim.github.io/p/gpt-1/</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-1/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>자연어 이해는 텍스트의 함축, 질문에 대한 답변, 의미의 유사성 평가, 문서 분류 등 다양한 작업으로 구성되어 있다. 레이블이 지정된 데이터가 부족한 상황에서, 이 논문은 레이블이 없는 텍스트 데이터에 대해 언어 모델을 &lt;code>(생성적) 사전학습(generative pre-training)&lt;/code>하고, 이를 특정 작업에 &lt;code>미세조정(fine-tuning)&lt;/code>하는 방식을 제안한다. 이 방법은 모델 아키텍처에 최소한의 변경만을 요구하면서도 효과적인 전이를 달성하였고, 다양한 자연어 이해 벤치마크에서 우수한 성능을 보여주었다. 이 모델은 각 작업에 특별히 설계된 모델을 능가하며, 12개의 작업 중 9개에서 최고 성능을 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리(NLP)에서 지도 학습의 의존성을 줄이는 것은 중요한데, 이는 대부분의 딥러닝 방법이 수동 레이블링된 대량의 데이터가 필요하기 때문이다. 이런 상황에서 레이블이 없는 데이터에서 언어 정보를 추출할 수 있는 모델은 유용한 대안이 될 수 있으며, 비지도 학습을 통해 학습하는 것이 더 나은 결과를 얻는 경우도 있다. 이를 입증하는 가장 강력한 예는 사전 학습된 단어 임베딩이며, 이는 다양한 NLP 작업에서 성능 향상을 위해 널리 사용되고 있다.&lt;/p>
&lt;p>레이블이 없는 텍스트에서 단어 수준을 넘어서는 정보를 활용하는 것은 어려운 도전 과제이며, 이유는 다음과 같다. 첫째, 텍스트 표현을 학습하고 다른 곳에 유용하게 전이하는 최적화 목표가 무엇인지 확실하지 않다. 둘째, 학습된 표현을 어떤 작업에 가장 효과적으로 적용할 방법이 아직 확립되지 않았다. 이런 불확실성이 효과적인 준지도 학습 방법을 개발하는 것을 어렵게 한다.&lt;/p>
&lt;p>이 연구는 언어 이해 작업에 &lt;code>비지도 사전 학습(unsupervised pre-training)&lt;/code>과 &lt;code>지도 미세 조정(supervised fine-tuning)&lt;/code>을 결합하는 준지도 학습을 제안한다. 목표는 적은 조정으로 다양한 작업에 적용 가능한 표현을 학습하는 것이다. 레이블이 없는 대량의 텍스트와 수동으로 레이블링된 훈련 예제를 사용하며, 학습은 두 단계로 진행된다. 먼저, 레이블이 없는 데이터로 모델의 초기 파라미터를 학습하고, 그 다음으로 지도 학습을 통해 이 파라미터를 목표 작업에 맞게 조정한다.&lt;/p>
&lt;p>이 연구에서는 다양한 작업에서 뛰어난 성능을 보인 &lt;code>Transformer&lt;/code>모델을 사용한다. 이 모델은 텍스트의 장기적인 의존성을 처리하는 더 구조화된 메모리를 제공하므로 강한 전이 성능을 보여준다. 전이 단계에서는 작업 특정 입력 조정을 사용하여 텍스트 입력을 연속 토큰 시퀀스로 처리하며, 이 방식은 사전 학습된 모델의 구조를 최소한으로 변경하면서 효과적으로 미세 조정할 수 있음을 실험적으로 입증한다.&lt;/p>
&lt;p>이 연구는 자연어 추론, 질문 응답, 의미 유사성, 텍스트 분류 등 네 가지 언어 이해 작업에서 모델을 평가하였다. 제시된 모델은 각 작업에 특화된 모델들보다 더 우수한 성능을 보여주었고, 12개 작업 중 9개에서 최고 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="semi-supervised-learning-for-nlp">Semi-supervised learning for NLP&lt;/h3>
&lt;p>이 연구는 자연어에 대한 준지도 학습 범주에 속하며, 이는 시퀀스 라벨링이나 텍스트 분류와 같은 작업에 적용된다. 초기에는 레이블 없는 데이터를 사용해 단어나 구문 수준의 통계를 계산하였지만, 최근에는 레이블이 없는 말뭉치에서 훈련된 단어 임베딩을 활용하여 작업 성능을 향상시키는 방향으로 연구가 진행되고있다. 그러나 이 논문의 목표는 단어 수준 이상의 의미를 포착하는 것이며, 이를 위해 구문이나 문장 수준의 임베딩을 활용하여 텍스트를 벡터 표현으로 인코딩하는 방식을 채택하였다.&lt;/p>
&lt;h3 id="unsupervised-pre-training">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 사전 학습은 좋은 초기화 지점을 찾는 것을 목표로 하며, 이미지 분류, 음성 인식, 엔티티 구분, 기계 번역 등 다양한 작업에서 DNN의 훈련을 돕는데 사용되고있다.&lt;/p>
&lt;p>이 연구는 언어 모델링 목표를 사용하여 신경망을 사전 학습하고, 지도 학습으로 목표 작업에서 미세 조정하는 방식을 따른다. 이 방법은 LSTM을 사용하는 이전의 방법들이 제한적인 예측 능력을 가지는 반면, Transformer는 더 넓은 범위의 언어 구조를 포착할 수 있게 한다. GPT 모델은 자연어 추론, 패러프레이즈 감지, 스토리 완성 등 다양한 작업에서 효과를 보여주었으며, 다른 모델이 새로운 파라미터를 많이 필요로 하는 반면, GPT 모델은 아키텍처에 최소한의 변경만 필요로 한다.&lt;/p>
&lt;h3 id="auxiliary-training-objectives">Auxiliary training objectives&lt;/h3>
&lt;p>보조적인 비지도 학습 목표 추가는 준지도 학습의 변형 형태로, 다양한 NLP 작업을 통해 의미 역할 라벨링을 개선하는데 사용되었다. 최근에는 이러한 보조 목표를 목표 작업에 추가하여 시퀀스 라벨링 작업에서 성능을 향상시켰다. 이 연구에서도 비지도 사전 훈련이 이미 목표 작업과 관련된 다양한 언어적 요소를 학습한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="framework">Framework&lt;/h2>
&lt;p>학습은 큰 말뭉치에서 대용량 언어 모델을 학습하는 단계와 레이블이 달린 데이터를 활용해 모델을 목표 작업에 맞게 미세 조정하는 단계로 이루어진다.&lt;/p>
&lt;h3 id="unsupervised-pre-training-1">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 토큰 말뭉치 $U = \lbrace u_1, &amp;hellip; , u_n \rbrace $ 가 주어질때, 다음 Likelihood를 최대화하도록 표준언어모델링 목적함수를 사용한다:&lt;/p>
&lt;p>$$ L_1(U) = \sum_{i} \log{P} (u_i | u_{i-k}, &amp;hellip; , u_{i-1}, \theta) $$&lt;/p>
&lt;p>$k$는 context window의 크기이며, 조건부 확률 $P$는 parameter $\theta$를 가진 신경망을 사용하여 모델링된다. 이 parameter들은 stochastic gradient descent를 사용하여 학습된다.&lt;/p>
&lt;p>GPT 모델은 언어모델로 multi-layer Transformer decoder를 사용하며, 이 모델은 입력 컨텍스트 토큰에 대해 multi-headed self-attention을 적용한 후, position-wise feedforward layer를 적용하여 목표 토큰에 대한 출력 분포를 생성한다:&lt;/p>
&lt;p>$$ h_0 = UW_e + W_p $$
$$ h_l = \text{transformer_block}(h_{l-1}) \forall i \in [1, n] $$
$$ P(u) = \text{softmax}(h_n W^T_e) $$&lt;/p>
&lt;p>$U = (u_{i-k}, &amp;hellip; , u_{i-1}) $ 는 토큰의 컨텍스트 벡터이고, $n$은 layer의 수, $W_e$ 는 토큰 임베딩 행렬, $W_p$ 는 위치 임베딩 행렬이다.&lt;/p>
&lt;h3 id="supervised-ﬁne-tuning">Supervised ﬁne-tuning&lt;/h3>
&lt;p>모델을 학습한 후, parameter를 목표 작업에 맞게 조정한다. 레이블이 지정된 데이터셋 $C$ 는 입력 토큰 $x^1, &amp;hellip; , x^m $ 과 레이블 $y$로 구성된다. 입력은 사전 훈련된 모델을 통과하여 최종 transformer block의 활성값인 $h^m_l$ 을 얻으며, 이는 parameter $W_y$ 와 함께 선형 출력층으로 전달되어 $y$ 를 예측한다:&lt;/p>
&lt;p>$$ P(y|x^1, &amp;hellip; , x^m) = \text{softmax}(h^m_l W_y) $$&lt;/p>
&lt;p>이는 다음을 최대화 한다.&lt;/p>
&lt;p>$$ L_2(C) = \sum_{(x,y)} \log{P(y|x^1, &amp;hellip; , x^m)} $$&lt;/p>
&lt;p>추가로 미세 조정을 위한 보조 목표로 언어 모델링을 포함시키는 것은 지도 모델의 일반화를 향상시키고, 수렴을 가속화하는데 도움이 된다. 구체적으로, weight $\lambda$에 대해 다음을 최적화한다:&lt;/p>
&lt;p>$$ L_3(C) = L_2(C) + \lambda L_1(C) $$&lt;/p>
&lt;p>미세 조정 과정에서 추가 매개변수는 $W_y$ 와 구분자 토큰의 임베딩뿐이다.&lt;/p>
&lt;h3 id="task-speciﬁc-input-transformations">Task-speciﬁc input transformations&lt;/h3>
&lt;p>텍스트 분류같은 일부 작업들은 모델을 직접 미세 조정할 수 있지만, 질문 답변이나 텍스트 함의 같은 작업들은 구조화된 입력을 필요로 하는데, 이러한 입력에 대해 사전 학습된 모델은 별도의 수정 없이도 처리할 수 있다. 대신, 이런한 입력을 모델이 처리할 수 있는 순서가 있는 시퀀스로 변환한다. 이 접근법은 작업 간에 아키텍처를 크게 변경할 필요를 없애준다. 또한, 모든 변형에는 무작위로 초기화된 시작과 종료 토큰을 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure1.png"
width="1058"
height="450"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="564px"
>&lt;/p>
&lt;h4 id="textual-entailment">Textual entailment&lt;/h4>
&lt;p>텍스트 함의에서는, 전제 $p$와 가설 $h$를 구분자 &lt;code>$&lt;/code>로 연결한다.&lt;/p>
&lt;h4 id="similarity">Similarity&lt;/h4>
&lt;p>유사성 경우, 비교되는 두 문장의 순서는 정해져 있지 않으므로, 텍스트 두 개를 다른 순서로 이어붙여 각각을 독립적으로 처리하여 두 시퀀스 표현 $h^m_l$을 생성한다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning">Question Answering and Commonsense Reasoning&lt;/h4>
&lt;p>컨텍스트 문서 $z$, 질문 $q$, 가능한 답변들 $\lbrace a_k \rbrace$을 받는다. 각 가능한 답변을 문맥 문서와 질문에 연결하고, 구분자 토큰을 추가해 시퀀스 $[z; q;$ &lt;code>$&lt;/code>; $a_k]$ 를 만든다. 이 시퀀스들은 독립적으로 처리되고, softmax 계층을 통해 정규화되어 답변들에 대한 출력 분포를 생성한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;h4 id="unsupervised-pre-training-2">Unsupervised pre-training&lt;/h4>
&lt;p>언어 모델 학습에 BooksCorpus 데이터셋을 사용한다. 이는 다양한 장르의 7천개가 넘는 미발행 책들을 포함하며, 연속적인 긴 텍스트를 통해 모델이 long term depency를 학습할 수 있다. ELMo에서 사용된 1B Word Benchmark 데이터셋은 문장들이 서로 섞여 있어 long term depency를 학습하기 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table1.png"
width="1068"
height="192"
srcset="https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="556"
data-flex-basis="1335px"
>&lt;/p>
&lt;h4 id="model-speciﬁcations">Model speciﬁcations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">layer&lt;/td>
&lt;td style="text-align:center">12-layer decoder-only transformer with masked self-attention heads&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">state dimension&lt;/td>
&lt;td style="text-align:center">decoder: 768, attention heads: 12, position-wise FFN: 3072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">optimizer&lt;/td>
&lt;td style="text-align:center">Adam&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">learning rate&lt;/td>
&lt;td style="text-align:center">max: 2.5e-4, schedule: cosine annealing, warm-up step: 2,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">schedule&lt;/td>
&lt;td style="text-align:center">100 epochs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">64 random sample $\times$ 512 token/sample&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">weight initialization&lt;/td>
&lt;td style="text-align:center">$N(0, 0.02)$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">subword segmentation&lt;/td>
&lt;td style="text-align:center">BPE (40,000 merges)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">regularization&lt;/td>
&lt;td style="text-align:center">L2($w=0.01$)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">activation function&lt;/td>
&lt;td style="text-align:center">Gaussian Error Linear Unit(GELU)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">position embedding&lt;/td>
&lt;td style="text-align:center">learned positoin embeddings&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">pre-processing&lt;/td>
&lt;td style="text-align:center">cleaning: ftfy, tokenizer : spaCy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="fine-tuning-details">Fine-tuning details&lt;/h4>
&lt;p>명시되지 않은 것들은 사전학습에 사용된 hyperparameter를 재사용했다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Learning rate&lt;/td>
&lt;td style="text-align:center">max: 6.25e-5, warm-up: 0.2% of training&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">epochs&lt;/td>
&lt;td style="text-align:center">3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">auxiliary objective weight($\lambda$)&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="supervised-ﬁne-tuning-1">Supervised ﬁne-tuning&lt;/h3>
&lt;p>자연어 추론, 질문 응답, 의미론적 유사성, 텍스트 분류등의 평가를 진행하였고, 그 중 일부는 GLUE benchmark에 포함되어 있다.&lt;/p>
&lt;h4 id="natural-language-inference">Natural Language Inference&lt;/h4>
&lt;p>자연어 추론(NLI) 작업, 즉 텍스트 함의를 인식하는 것은 문장 쌍을 읽고, 그들 사이의 관계를 함의, 모순 또는 중립 중 하나로 판단하는 것으로, 이미지 캡션(SNLI), 텍스트 변환된 연설, 대중 소설, 정부 보고서(MNLI), 위키백과 기사(QNLI), 과학 시험(SciTail) 또는 뉴스 기사(RTE)를 포함한 다양한 출처의 다섯 개의 데이터셋을 사용해서 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table2.png"
width="1084"
height="324"
srcset="https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="334"
data-flex-basis="802px"
>&lt;/p>
&lt;p>다섯 가지 데이터셋 중 네 가지에서 좋은 성능을 보여주었으며, MNLI에서 1.5%, SciTail에서 5%, QNLI에서 5.8%, SNLI에서 0.6%의 성능 향상을 보였다. 이는 GPT 모델이 여러 문장을 더 잘 이해하고, 언어적 모호성의 측면을 처리할 수 있다는 것을 보여준다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning-1">Question answering and commonsense reasoning&lt;/h4>
&lt;p>질문 응답 작업은 한 문장이나 여러 문장을 이해하는 능력을 평가한다. 중고등학교 시험의 영어 지문과 질문이 포함된 RACE 데이터셋을 사용한 평가에서 좋은 성능을 보여주었다. 또한, 여러 문장의 이야기 중에서 올바른 결말을 고르는 Story Cloze 평가에서도 GPT 모델은 이전 최고 성능을 크게 능가하였다. 이 결과는 GPT 모델이 넓은 범위에 걸친 문맥 정보를 잘 처리할 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table3.png"
width="1078"
height="312"
srcset="https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="345"
data-flex-basis="829px"
>&lt;/p>
&lt;h4 id="semantic-similarity">Semantic Similarity&lt;/h4>
&lt;p>의미론적 유사성(또는 패러프레이즈 감지) 작업은 두 문장이 의미적으로 동일한지 여부를 판단한다. 뉴스 출처에서 수집된 Microsoft Paraphrase(MRPC), Quora Question Pairs(QQP), 그리고 Semantic Textual Similarity benchmark(STS-B) 데이터셋을 사용한다. 이 중 STSB와 QQP에서 좋은 성늘을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table4.png"
width="1076"
height="382"
srcset="https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;h4 id="classiﬁcation">Classiﬁcation&lt;/h4>
&lt;p>텍스트 분류로 사용한 데이터셋은 문법적으로 맞는지를 판단하는 Corpus of Linguistic Acceptability(CoLA)와 단순 이진분류 평가인 Stanford Sentiment Treebank(SST-2)을 사용하였다. CoLA에서 35.0 에서 45.4점으로, SST-2에서 68.9 에서 72.8점으로 상승하였으며, GLUE benchmark에서도 72.8점으로 이전 최고 성능을 크게 능가하였다.&lt;/p>
&lt;p>GPT모델은 평가한 12개의 데이터셋 중 9개에서 state-of-the-art를 달성하였다. 그리고 STS-B(약 5.7k)와 같은 작은 데이터셋부터 가장 큰 SNLI(약 550k)와 같은 크기의 다양한 데이터셋에서 잘 작동함을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="analysis">Analysis&lt;/h2>
&lt;h3 id="impact-of-number-of-layers-transferred">Impact of number of layers transferred&lt;/h3>
&lt;p>unsupervised pre-training에서 supervised target task로 transfer하는 layer 개수의 영향을 분석했다. MultiNLI와 RACE에서 성능을 관찰했고 transferring embeddings이 성능을 향상시킨다는 것과 각 transformer layer가 최대 9%까지 성능을 향상시킨다는 결과를 얻었다. 이는 pre-trained model의 각 layer가 target task를 푸는 데 유용한 기능을 포함함을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure2.png"
width="1070"
height="478"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;h3 id="zero-shot-behaviors">Zero-shot Behaviors&lt;/h3>
&lt;p>Trasformer를 사용한 language model이 pre-training에 효과적인 이유에 대한 가설로, Generative model이 학습하는 target tasks가 language modeling의 성능을 향상에 도움을 준다고 생각했고, 이를 검증하기 위해 pre-training 업데이트 횟수에 따른 target tasks의 성능을 fine-tuning없이 측정하였다.&lt;/p>
&lt;p>실험 결과 pre-training 업데이트 횟수에 따라 안정적 &amp;amp; 지속적으로 관련 taget task의 성능이 증가하는 것을 확인할 수 있었으며 이는 generative pre-training이 관련 task의 학습에 도움을 준다는 것을 의미한다. 반면, LSTM의 경우에는 업데이트 횟수에 따라 일관되게 안정적으로 증가하지 않고 분산을 가지면서 증가하는데, 이는 LSTM 보다 더 구조화된 transformer의 attentional memory가 transfer learning에 도움을 준다는 것을 의미한다.&lt;/p>
&lt;h3 id="ablation-studies">Ablation studies&lt;/h3>
&lt;p>세 가지 ablation study를 통해 다음의 결과를 얻었다. 첫째, 미세조정 시 보조 목적함수의 도움이 큰 데이터셋에서는 두드러지지만 작은 데이터셋에서는 그렇지 않다는 것을 확인하였다. 둘째, LSTM과 Transformer를 비교한 결과, LSTM은 오직 MRPC 데이터셋에서만 Transformer를 능가하는 것을 확인하였다. 마지막으로, 사전학습 없이 지도학습을 진행한 Transformer는 모든 작업에서 성능이 저하되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table5.png"
width="1074"
height="208"
srcset="https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="516"
data-flex-basis="1239px"
>&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>생성적 사전 학습과 미세조정을 사용한 모델을 통해 강력한 자연어 이해를 구현하였다. GPT 모델은 연속된 텍스트로 이루어진 다양한 말뭉치로 사전학습된 모델은 일반 지식(world knowledge)과 long term depency 처리하는 능력을 가질 수 있었다. 이를 통해, 우리는 지도학습 없이도 특정 작업의 성능을 향상시키는 것이 가능하다는 것을 보여주었으며, 특히 Trasformer 모델과 long term depency가 있는 텍스트 데이터셋이 이 접근법에서 잘 작동함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener"
>Code&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>