<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Music on K2H'log</title><link>https://kurtkim.github.io/tags/music/</link><description>Recent content in Music on K2H'log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Wed, 06 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/tags/music/index.xml" rel="self" type="application/rss+xml"/><item><title>MuLan</title><link>https://kurtkim.github.io/p/mulan/</link><pubDate>Wed, 06 Mar 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/mulan/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문은 &amp;ldquo;MuLan&amp;quot;이라는 새로운 음향 모델을 소개한다. MuLan은 44M 개의 음악 녹음과 자유 형식의 텍스트 주석을 학습하여 음악 오디오를 제한 없는 자연어 음악 설명과 직접 연결한다. 이 모델은 다양한 음악 장르와 텍스트 스타일과 호환되며, 오디오-텍스트 표현은 기존의 ontologie를 포함하면서 zero-shot 기능으로 발전하였다. 이 모델의 유연성은 전송 학습, zero-shot 음악 태깅, 음악 분야의 언어 이해 및 크로스-모달 검색 애플리케이션 등의 실험을 통해 입증되었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>분류기는 보통 사전에 정의된 클래스 목록으로 예제들을 레이블링하도록 학습되며, 이는 클래스 간 관계를 나타내는 구조화된 체계로 지정된다. 최근 신경 언어 모델링의 발전으로, 연구자들은 원시 콘텐츠 정보에 접근하기 위한 자연어 인터페이스를 탐색하고 있다. 이는 주로 시각과 오디오 이벤트 분야에서 이루어지며, 미디어 콘텐츠와 자연어 캡션을 함께 임베딩하는 것이 전이 학습, 크로스-모달 검색, 자동 캡셔닝, zero-shot 분류 등에서 효과적임을 보여주었다.&lt;/p>
&lt;p>이러한 연구의 성공은 대규모 학습 자료와 언어와 다른 모드 사이의 복잡한 관계를 모델링 할 수 있는 유연한 신경망 구조에 크게 의존한다. 특히, 시각 분야는 웹에서 대량의 캡션 이미지를 사용할 수 있어 이점을 얻었다. 그러나 환경 오디오 분야에서는 대규모 오디오-캡션 쌍이 덜 접근 가능하며, 이에 따라 작은 캡션 데이터셋에 의존하였다. 이러한 데이터셋들은 사운드를 묘사하는 언어의 다양성을 충분히 담지 못하며, zero-shot 설정에서의 성공은 여전히 부족하다.&lt;/p>
&lt;p>이 논문은 오디오와 자연어를 함께 임베딩하는 작업에 대해 다루되, 특히 음악 분야에 초점을 맞춘다. 목표는 모든 음악 개념을 관련 음악 오디오와 연결할 수 있는 유연한 언어 인터페이스를 제작하는 것이다. 이를 위해, 메타데이터, 댓글, 플레이리스트 데이터에서 추출한 텍스트 주석을 44M 개 이상의 인터넷 음악 비디오 학습 세트에 매핑하는 전략을 사용한다. 하지만 텍스트 데이터가 음악 콘텐츠를 정확히 참조하는 경우는 일부에 불과하므로, 음악 설명을 식별하기 위해 별도로 학습된 텍스트 분류기를 사용한 텍스트 사전 필터링을 탐색한다.&lt;/p>
&lt;p>이 논문에서는 대규모 데이터셋을 사용하여 자연어 인터페이스가 탑재된 새로운 세대의 음악 오디오 임베딩 모델인 &amp;ldquo;MuLan&amp;quot;을 학습시킨다. MuLan은 음악 오디오와 텍스트 간의 공유 임베딩 공간을 만들어내는 two-tower parallel encoder 구조를 사용한다. 이 모델은 다양한 음악 정보 검색 작업에서 최고 수준의 성능을 보이며, 텍스트에서 음악으로의 크로스-모달 검색, zero-shot 음악 태깅, 음악 도메인의 언어 이해 등의 기능을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Audio representation learning.&lt;/strong> 대규모로 일반적인 콘텐츠 표현을 사전 학습하여 전이 학습하는 방법이 여러 분야에서 주요 접근법이 되었다. 이는 오디오 표현 학습에도 적용되었으며, 일반 환경 오디오와 음악 오디오 모두에서 다양한 사전 훈련 메커니즘이 탐색되었다. ImageNet과 AudioSet에서 사전 학습된 오디오 스펙트로그램 변환기는 다양한 태깅 작업에서 최고의 성과를 보였으며, Million Song Database를 사용한 연구에서는 음악 오디오 표현 학습의 초기 기준선을 제시하였다.&lt;/p>
&lt;p>비지도 및 자기 지도 사전 학습에서는 구별적 모델과 생성적 모델 모두 성공적이었다. 구별적 학습은 같은 녹음에서 추출된 오디오 세그먼트에 더 높은 유사성을 부여하는 표현을 학습하는 방식을 사용하였다. 이와 비슷하게, SSAST는 생성적인 마스크된 스펙트로그램 패치 모델링을 탐색하였다. 생성 모델의 중간 임베딩이 downstream 분류를 위한 강력한 오디오 표현을 제공한다는 것이 확인되었으며, 사용자 상호작용 통계와 시각적 단서 같은 약한 감독도 검토되었다.&lt;/p>
&lt;p>이 연구는 음악 오디오와 약하게 연결된 풍부한 텍스트 주석을 활용한 크로스-모달 감독 방법을 개발하는 것에 집중하고 있다. 이를 통해 학습된 표현의 전이 학습 능력을 평가하고, 다양한 오디오 encoder 구조를 비교 분석하고 있다.&lt;/p>
&lt;p>&lt;strong>Cross-modal contrastive learning.&lt;/strong> 대규모 데이터를 활용한 대조 학습의 성공에 기반하여, 이미지-텍스트 모델에 오디오 타워를 추가하고 크로스-모달 정렬을 강화하는 삼중 모드 구조를 제안하였다. 오디오 분야에서는 오디오의 잠재적 표현과 관련 태그를 정렬하기 위해 대조 학습이 사용되었다. 후속 연구에서는 사전 학습된 비문맥적 단어 임베딩 모델을 사용하여 태그를 넘어 새로운 용어로 일반화하는 능력을 지원하였다. 이 방법은 음악 도메인을 위한 오디오-텍스트 쌍의 대규모 컬렉션을 채굴하는데 초점을 맞추고 있으며, 이를 통해 처음으로 임의적인 zero-shot 음악 태깅과 검색이 가능해졌다.&lt;/p>
&lt;p>&lt;strong>Music text joint embedding models.&lt;/strong> 콘텐츠 기반 음악 정보 검색은 풍부한 의미를 가진 텍스트와 음악의 광범위하고 세밀한 특성을 연결하는 것을 필요로 한다. 일반적인 접근법은 다중 레이블 분류 작업을 통해 텍스트 레이블 클래스의 의미를 음악에 적용하는 것이다. 이를 위해, 음악 비디오와 연관된 자연어 텍스트에서 대량의 어휘를 추출하고, 이를 통해 오디오 인코더를 학습시키며, 텍스트 레이블을 오디오 특성과 정렬시키는 방법이 사용되었다. 또한, 분류, 회귀, 메트릭 학습 등의 다양한 학습 작업을 통해 자유 형식의 텍스트와 음악 오디오를 정렬하는 방법이 탐색되었다.&lt;/p>
&lt;p>이 논문의 작업은 MuLaP와 가장 유사하며, 이 연구에서는 제작 음악 라이브러리에서 250K의 오디오-캡션 쌍을 채굴하여 multimodal Transformer를 학습시켰다. 그러나 그들의 초기 융합 방식은 전이 학습 응용에 임베딩의 활용성을 제한한다. 반면, 이 접근법은 임의의 음악 오디오에 대한 자연어 인터페이스를 제공하는 공동 임베딩 공간을 만들어, 크로스-모달 검색, zero-shot 태깅, 언어 이해 등의 가능성을 열어냈다.&lt;/p>
&lt;hr>
&lt;h2 id="proposed-approach">Proposed Approach&lt;/h2>
&lt;p>목표는 음악 오디오와 자유 형식의 자연어 텍스트에 대한 공유 임베딩 공간을 만드는 것으로, 이 공간에서는 근접성이 의미를 공유하는 지표로 작용한다. 이를 위해 크로스-모달 대조 학습과 두 타워 구조를 사용하며, 대규모의 (오디오, 텍스트) 쌍 훈련 데이터를 채굴하여 이 작업을 지원한다.&lt;/p>
&lt;h3 id="learning-framework">Learning Framework&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/figure1.png"
width="682"
height="410"
srcset="https://kurtkim.github.io/p/mulan/images/figure1_hu9b99b9c0e8c2595b9653bf7a0e387718_79893_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/figure1_hu9b99b9c0e8c2595b9653bf7a0e387718_79893_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="399px"
>&lt;/p>
&lt;p>각 MuLan 모델은 오디오와 텍스트 입력 모달 각각에 대한 별도의 임베딩 네트워크로 구성된다. 이 두 네트워크는 가중치를 공유하지 않지만, 같은 차원의 $l_2$-normalized embedding space에서 종료한다. 오디오 임베딩 네트워크는 log mel spectrogram 컨텍스트 윈도우를, 텍스트 임베딩 네트워크는 널 패딩 텍스트 토큰 시퀀스를 입력으로 받는다.&lt;/p>
&lt;p>음악 녹음과 관련 텍스트 요소를 가지고, (오디오, 텍스트) 쌍의 크로스-모달 학습 데이터셋을 구축한다. 각 녹음에서 log mel spectrogram을 계산하고, 텍스트 요소를 고정 길이로 조정한다. 그런 다음, 미니 배치는 무작위로 선택된 녹음과 그에 대응하는 텍스트 요소의 쌍으로 구성된다. 이 샘플링 방식은 학습 오디오와 모든 관련 텍스트를 전부 커버하기 위해 여러 에포크가 필요하다는 것을 의미한다. 각 예시에 대해 여러 텍스트 주석을 연결하는 실험도 진행했지만, 일반적으로는 잘 작동하지 않았다.&lt;/p>
&lt;p>$$ \sum_{i=1}^{B} - log \big[ {{h[f(x^{(i)}), g(t^{(i)})]}\over{\sum_{j \neq i} h[f(x^{(i)}), g(t^{(j)})] + h[f(x^{(j)}), g(t^{(i)})] }} \big] $$&lt;/p>
&lt;p>$h[a, b] = exp(a^T b/τ)$ 형태의 비평 함수 $h$는 $a, b ∈ \mathbb{R}^d$와 학습 가능한 온도 hyperparameter $τ ∈ (0, 1]$에 의존한다. 이 함수는 내적을 통해 코사인 유사도를 계산하며, 대상 오디오-텍스트 쌍에 대해 큰 값을, 비대상 쌍에 대해서는 0에 가까운 값을 생성하는 것이 목표이다. 또한, 1보다 작은 $τ$ 값은 $h$의 출력 범위를 확장한다. 이전 연구에 따르면, 대량의 배치 크기는 대조적 손실 최적화에 도움이 된다.&lt;/p>
&lt;h3 id="audio-embedding-network">Audio Embedding Network&lt;/h3>
&lt;p>오디오 임베딩에 검증된 Resnet-50 아키텍처를 적용하였다. 이를 위해 log mel spectrogram을 흑백 이미지로 처리하고, 학습 클립에서 무작위로 선택된 10초 윈도우를 입력으로 사용한다. 학습 중에는 SpecAugment를 적용하고, 마지막에는 시간과 mel 채널에 대한 평균 풀링과 L2 정규화된 linear fully connected layer를 적용한다. 이 모델은 AudioSet의 로지스틱 회귀를 사용하여 사전 학습되며, 최종 분류 계층은 미세 조정 전에 제거된다.&lt;/p>
&lt;p>Audio Spectrogram Transformer (AST)는 log mel spectrogram에서 추출된 패치의 토큰 시퀀스에 12개의 Transformer 블록을 적용하는 아키텍처이다. 학습 중에는 SpecAugment를 적용하고, positional encoding과 [CLS] 토큰을 추가한다. [CLS] 토큰 위치에서의 최종 인코딩에 linear fully-connected layer과 L2 정규화를 적용하여 오디오 임베딩 네트워크의 출력을 형성한다. 이 모델은 공개된 AST 체크포인트를 사용하여 사전학습을 시작한다.&lt;/p>
&lt;h3 id="text-embedding-network">Text Embedding Network&lt;/h3>
&lt;p>텍스트 임베딩 모델로는 일반적으로 사용되는 BERT 아키텍처를 사용한다. 이는 텍스트 입력을 토큰 시퀀스로 변환하고, [CLS] 토큰 임베딩을 오디오-텍스트 임베딩 공간으로 변환하는 역할을 한다. 이 모델은 공개 체크포인트를 사용하여 사전학습을 시작한다.&lt;/p>
&lt;h3 id="training-dataset-mining">Training Dataset Mining&lt;/h3>
&lt;p>5000만 개의 인터넷 음악 비디오에서 30초 클립을 추출하여 MuLan 임베딩 모델을 학습시키는 데 사용한다. 음악 오디오 감지기를 통해 음악 컨텐츠가 절반 미만인 클립을 제거한 후, 약 4400만 개의 클립, 즉 대략 370K 시간의 오디오가 남는다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table1.png"
width="648"
height="186"
srcset="https://kurtkim.github.io/p/mulan/images/table1_huc988009fa7bb561c10195b33fcb9b7fd_48533_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table1_huc988009fa7bb561c10195b33fcb9b7fd_48533_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="348"
data-flex-basis="836px"
>&lt;/p>
&lt;p>각 음악 비디오에 대해, 비디오 제목과 태그, 비디오 설명과 댓글, 그리고 데이터셋의 인터넷 음악 비디오에 연결된 재생목록 제목 등 노이즈가 많은 텍스트 데이터를 고려한다. 이러한 텍스트는 반드시 사운드트랙의 음악적 속성을 참조하고 있다는 것이 보장되지 않으며, 특히 댓글 데이터는 가장 많은 노이즈를 포함하고 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table2.png"
width="600"
height="154"
srcset="https://kurtkim.github.io/p/mulan/images/table2_hu12be41209accd48d06dcfc43168466a0_31265_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table2_hu12be41209accd48d06dcfc43168466a0_31265_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>노이즈가 많은 텍스트를 고려하여, 음악 서술적 주석으로 필터링된 SF 및 LF 텍스트 데이터를 사용하여 MuLan을 학습시켰다. 이를 위해, 사전 학습된 BERT 모델을 미세 조정하고, 이를 통해 LF 주석을 필터링하였다. 또한, SF 주석을 정리하기 위한 규칙 기반 필터링도 적용하였다. 하지만, 재생목록 제목과 필터링된 장형 주석은 데이터셋의 총 녹음 중 일부분만 사용할 수 있다.&lt;/p>
&lt;p>AudioSet을 오디오-텍스트 쌍의 세트로 변환하여, 이를 ASET으로 표기한다. 모든 예시를 포함시키고, 각 예시에 첨부된 레이블 문자열을 텍스트 주석으로 사용하여 약 2M 개의 10초 클립 학습 세트를 생성한다. 이 네 가지 다른 데이터 소스의 규모 불균형을 고려하여, SF:LF:PL:ASET을 2:2:1:1의 비율로 미니 배치를 구성한다. 이 방식으로, 규모가 작더라도 필터링된 LF 주석이 미니 배치의 1/3를 차지하게 된다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>Resnet-50과 AST 오디오 encoder를 사용하여 MuLan을 평가하며, 두 경우 모두 텍스트 encoder로 BERT-base-uncased를 사용한다. AudioSet, 단형 태그, 장형 문장, 재생목록 정보 등, 44M 음악 녹음에서 추출한 오디오-텍스트 쌍과 처리된 텍스트 레이블에 대해 모든 모델을 14 epoch 동안 학습시킨다. M-Resnet-50과 M-AST는 비슷한 성능을 보이므로, 더 나은 학습 효율성을 위해 M-Resnet-50을 선택적으로 사용한다.&lt;/p>
&lt;h3 id="evaluation-tasks">Evaluation Tasks&lt;/h3>
&lt;h4 id="zero-shot-music-tagging">Zero-shot Music Tagging&lt;/h4>
&lt;p>음악 클립의 오디오 임베딩과 태그 텍스트의 임베딩 간 코사인 유사도를 이용해 예측 점수를 정의한다. 컨텍스트 텍스트 encoder를 사용하여 유연한 예측 공간을 만들고, 크로스-모달 대조 학습을 통해 언어의 의미를 오디오 표현에 연결함으로써 보이지 않는 타겟 레이블에도 적용할 수 있다.&lt;/p>
&lt;p>MagnaTagATune과 AudioSet의 음악 부분을 이용한 두 가지 음악 태깅 벤치마크로 평가를 진행한다. MagnaTagATune에서는 상위 50개 태그 세트와 전체 188개 태그 세트를 고려하며, 각 오디오 클립을 10초짜리 3개 세그먼트로 분할해 클립 레벨 임베딩을 얻는다. AudioSet에서는 25개 장르 태깅 작업과 전체 음악 하위 트리를 포함한 141개 태깅 작업을 고려한다. 테스트 세트에서의 수신기 운영 특성 곡선(AUC-ROC)의 클래스 균형을 보고한다.&lt;/p>
&lt;p>AudioSet은 대조학습에 포함되고, 일부 MTAT 클래스는 AudioSet 온톨로지와 겹친다. 이로 인해 AudioSet과 MTAT 평가는 완전한 zero-shot이 아니다. 하지만, MuLan 학습 중에는 다양한 자유 형식의 언어 감독이 AudioSet 감독을 희석시킨다. 따라서 MuLan 모델과 기존 AudioSet 분류기를 비교함으로써, AudioSet 온톨로지를 넘어서는 클래스를 지원하는 유연한 자연 언어 인터페이스로 넘어가는 비용을 측정할 수 있다.&lt;/p>
&lt;h4 id="transfer-learning-with-linear-probes">Transfer Learning with Linear Probes&lt;/h4>
&lt;p>zero-shot 실험 외에도, 오디오 encoder를 downstream 태깅 작업에 적용하는 일반적인 피처 추출기로 평가한다. MagnaTagATune과 AudioSet 벤치마크를 다시 사용하고, 학습 데이터셋을 이용해 고정된 128차원 오디오 임베딩 위에 독립적인 클래스별 로지스틱 회귀 계층을 학습시킨다. 이 방법은 이전 전이 학습 연구의 평가 프로토콜을 따르므로, 성능을 직접 비교할 수 있다.&lt;/p>
&lt;h4 id="music-retrieval-from-text-queries">Music Retrieval from Text Queries&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table3.png"
width="634"
height="390"
srcset="https://kurtkim.github.io/p/mulan/images/table3_hubf8d334ad104ab5b9e994323df780b47_99837_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table3_hubf8d334ad104ab5b9e994323df780b47_99837_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>MuLan은 임베딩 공간에서 쿼리에 가장 가까운 음악 클립을 찾는 능력을 제공한다. 이는 컨텐츠 특징이 메타데이터 기반 방법보다 더 세밀하고 완전한 유사성 정보를 제공할 수 있는 음악 검색 애플리케이션에 중요하다. 전문가가 큐레이션한 7,000개의 재생 목록을 사용하여 평가하며, 각 재생 목록은 제목, 설명, 그리고 10-100개의 음악 녹음으로 구성되어 있다. 재생 목록 평가는 약 100K의 고유 녹음을 포함한다.&lt;/p>
&lt;p>전문가 큐레이션 재생 목록 데이터를 사용해 제목과 설명을 쿼리로 하는 두 개의 크로스-모달 검색 평가 세트를 만든다. 각 데이터셋에 대해 해당 재생 목록에 속하는 녹음을 검색 대상으로 사용하고, 100K 녹음 전체를 후보군으로 사용한다. AUC-ROC과 평균 정밀도(mAP)를 보고하며, zero-shot 태깅과 같은 임베딩 평균화 및 코사인 유사도 기반 점수 매기기 사용한다. 재생 목록 정보는 음악 태깅 벤치마크의 태그와는 다르게, 보다 세밀한 정보를 담고 있어 음악 검색 엔진에 제시되는 쿼리와 유사하다.&lt;/p>
&lt;h4 id="text-triplet-classiﬁcation">Text Triplet Classiﬁcation&lt;/h4>
&lt;p>텍스트 encoder는 도메인 내 음악 데이터와 크로스-모달 대조 손실을 사용해 미세 조정되었다. 텍스트 encoder가 음악 관련 텍스트를 얼마나 잘 이해하는지 평가하기 위해, 트리플렛 분류 작업을 통해 텍스트 임베딩을 직접 평가한다. 각 트리플렛은 (앵커, 긍정, 부정) 형태의 텍스트로 이루어져 있습니다. AudioSet 온톨로지를 사용한 첫 번째 테스트 세트와 전문가 큐레이션 재생 목록 데이터를 사용한 두 번째 테스트 세트를 구성하였다.&lt;/p>
&lt;h3 id="results-and-discussion">Results and Discussion&lt;/h3>
&lt;h4 id="music-tagging">Music Tagging&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table4.png"
width="646"
height="630"
srcset="https://kurtkim.github.io/p/mulan/images/table4_hu5a3de2cab49c8abeac5520f681303a61_139581_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table4_hu5a3de2cab49c8abeac5520f681303a61_139581_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="102"
data-flex-basis="246px"
>&lt;/p>
&lt;p>zero-shot 태깅 메트릭에서 MResnet-50과 M-AST는 비슷한 성능을 보였다. 하지만 학습 텍스트의 레이블 의미와 태깅 평가의 레이블 의미 사이에는 큰 차이가 있을 수 있어, 성능이 저하될 수 있다. 특히, MTAT 간격은 AudioSet보다 크게 나타났으며, 이는 비특정적인 의미나 여러 가지 의미를 가진 태그와 간단한 부정을 포함하는 태그에서 매우 나쁜 성능 때문이다. 이는 BERT의 알려진 문제로, 부정된 개념의 의미를 적절하게 모델링하지 못한다(&amp;ldquo;not rock&amp;quot;의 임베딩은 &amp;ldquo;rock&amp;quot;과 비슷하다).&lt;/p>
&lt;p>AudioSet만을 사용한 학습은 AudioSet 평가에서 가장 높은 AUC를 얻지만, 일반적으로 더 많은 데이터 소스를 포함하면 다른 모든 작업에서 성능이 향상된다. 무필터 데이터로의 학습은 필터링된 버전과 비슷한 성능을 달성하는데, 이는 원시 텍스트 데이터의 노이즈에도 불구하고 모델이 유용한 연관성을 학습하는 것을 보여준다. 텍스트 필터링이 너무 공격적이었을 수 있으며, 대비 학습의 높은 노이즈 허용성 때문에, 강하게 연결된 오디오-텍스트 쌍으로 제한하는 것의 이점이 유용한 쌍의 큰 세트를 잃는 것으로 상쇄되었을 수 있다.&lt;/p>
&lt;p>MuLan 오디오 임베딩에 선형 프로브를 적용하면 모든 태깅 작업에서 최고 수준의 전이 학습 성능을 달성한다. 이는 MuLan의 사전 학습된 오디오 encoder가 고품질 음악 오디오 임베딩을 계속 생성하면서 새로운 자연어 응용 프로그램을 지원한다는 것을 입증한다. 또한, 종단간 학습 기준선에 비해 선형 프로브 결과는 대부분이 더 우수하며, 최고 수준의 AST AudioSet 분류기를 약간만 뒤따른다.&lt;/p>
&lt;h4 id="music-retrieval-from-text-queries-1">Music Retrieval from Text Queries&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table5.png"
width="566"
height="258"
srcset="https://kurtkim.github.io/p/mulan/images/table5_hu62430557e012f8c71275209d360899b6_54993_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table5_hu62430557e012f8c71275209d360899b6_54993_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="526px"
>&lt;/p>
&lt;p>MuLan 모델은 쿼리 검색 평가 작업에서 놀랍게도 강인한 성능을 보여준다. 대규모 언어 자원으로 사전 학습된 BERT를 기반으로 하지만, AudioSet 클립과 레이블 주석만으로 학습을 하면 음악에 대한 도메인 내 자연어를 연결하는 능력이 제한적이다. 그러나 인터넷에서 추출한 대규모 단형태 태그를 포함하면 모델이 더 세분화된 음악 개념을 배울 수 있게 되고, 댓글과 재생 목록 데이터를 추가하면 더 복잡한 쿼리를 연결하는 데 도움이 된다. 또한, 필터링되지 않은 학습 텍스트를 사용하여도 유사한 성능을 달성하는 것으로 보아, 주석 노이즈에 대한 훈련의 강인함이 확인된다.&lt;/p>
&lt;h3 id="text-triplet-classiﬁcation-1">Text Triplet Classiﬁcation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/mulan/images/table6.png"
width="476"
height="368"
srcset="https://kurtkim.github.io/p/mulan/images/table6_hufba128f1e6743bcd07342508bb89e20b_70296_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/mulan/images/table6_hufba128f1e6743bcd07342508bb89e20b_70296_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="129"
data-flex-basis="310px"
>&lt;/p>
&lt;p>MuLan 텍스트 임베딩은 Sentence Transformer, SimCSE, Universal Sentence Embedding 등과 같은 기준선과 비교된다. 모든 기준선은 Transformer 기반 모델이며, MuLan 텍스트 encoder는 크로스-모달 손실로만 학습된다. 장형 텍스트 주석을 포함하면, 음악 도메인에 특화된 텍스트 임베딩 모델은 일반적인 문장 임베딩 모델을 능가한다. 놀랍게도, 텍스트 전용 미세 조정 손실을 사용하지 않아도 성공적인 특화가 이루어졌다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>약하게 연결된 텍스트와 오디오 데이터를 이용해 학습된 음악 오디오와 자연어 공동 임베딩 모델을 제시하였다. 이 모델은 다양한 응용에서 자연어 인터페이스의 유연성을 보여주며, 음악 태깅 벤치마크에서 최고 수준의 전이 학습 성능을 보여준다. 이는 음악 오디오에 대한 자유형 자연어 인터페이스를 구축하는 첫 시도로, 약한 신호와 절대적인 노이즈를 더 잘 구분하는 개선된 텍스트 필터링 방법을 통해 더욱 발전될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2208.12415.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/musiclm-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MusicLM</title><link>https://kurtkim.github.io/p/musiclm/</link><pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/musiclm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>MusicLM은 &amp;ldquo;a calming violin melody backed by a distorted guitar riff&amp;quot;과 같은 텍스트 설명을 바탕으로 고품질의 음악을 생성하는 모델이다. 이 모델은 sequenceto-sequence 모델링을 통해 음악을 생성하며, 음질과 텍스트 설명의 정확성에서 이전 모델들을 능가한다. 또한, 텍스트와 멜로디 모두에 조건을 두어 휘파람과 허밍된 멜로디를 텍스트 캡션에 따라 변형할 수 있다. 이를 지원하기 위해, 5.5k의 음악-텍스트 쌍 데이터셋인 MusicCaps를 공개하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>conditional neural 오디오 생성은 text-to-speech, lyrics-conditioned 음악 생성, MIDI 시퀀스로부터의 오디오 합성 등 다양한 분야에서 활용된다. 최근에는 text-to-image 생성 연구의 발전에 따라, 고수준의 캡션으로부터 오디오를 생성하는 연구가 진행되었다. 그러나 이러한 모델은 아직 단순한 음향 장면에 한정되어 있어, 풍부한 오디오 시퀀스를 생성하는 것은 여전히 도전 과제로 남아 있다.&lt;/p>
&lt;p>최근에 소개된 AudioLM은 오디오 합성을 discrete representation 공간에서의 언어 모델링 작업으로 취급하여 고해상도와 장기 일관성을 동시에 달성한다. 오디오 신호의 내용에 대한 가정 없이 오디오 전용 말뭉치에서 현실적인 오디오를 생성하는 방법을 학습한다. 이러한 시스템이 적절한 데이터에 학습된다면 더 풍부한 출력을 생성할 수 있을 것으로 보인다.&lt;/p>
&lt;p>고품질 오디오 합성의 본질적인 어려움 외에도, 오디오-텍스트 쌍 데이터의 부족이 주요 장애 요인이다. 이는 이미지 도메인과 대조적으로, 이미지 도메인에서는 대규모 데이터셋의 사용이 뛰어난 이미지 생성 품질에 크게 기여하였다. 또한, 오디오의 주요 특성을 단어로 명확하게 포착하거나 시퀀스 전체에 대한 캡션을 제공하는 것은 이미지를 설명하는 것보다 더욱 어렵다.&lt;/p>
&lt;p>이 연구에서는 텍스트 설명으로 고해상도 음악을 생성하는 MusicLM 모델을 소개한다. MusicLM은 AudioLM의 multi-stage autoregressive 모델링을 활용하고, 텍스트 조건부를 추가한다. 쌍 데이터의 부족함을 해결하기 위해, 공동 music-text 모델인 MuLan을 활용하여 음악과 텍스트 설명을 임베딩 공간에서 가까운 표현으로 투영한다. 이렇게 하면 학습 시간에 캡션의 필요성을 제거하고, 대규모 오디오 말뭉치에서 학습할 수 있다. 학습 시에는 오디오에서 계산된 MuLan 임베딩을, 추론 시에는 텍스트 입력에서 계산된 MuLan 임베딩을 사용한다.&lt;/p>
&lt;p>MusicLM은 라벨이 없는 대규모 음악 데이터셋에 학습되어 복잡한 텍스트 설명에 따른 일관성 있는 음악을 생성한다. 이를 평가하기 위해, 전문 음악가들이 준비한 5.5k의 예시를 포함하는 고품질 음악 캡션 데이터셋인 MusicCaps를 소개하고, 이를 공개하여 미래의 연구를 지원한다.&lt;/p>
&lt;p>이 연구의 실험은 MusicLM이 품질과 캡션 준수 면에서 이전 시스템을 능가한다는 것을 보여준다. 또한, 음악의 일부 측면을 단어로 설명하는 것이 어렵거나 불가능한 경우에도, 이 방법이 텍스트를 넘어서는 조건부 신호를 지원한다. 특히, 오디오 형태의 추가적인 멜로디를 받아들여 원하는 멜로디를 따르는 음악 클립을 생성하는 확장된 MusicLM을 소개한다.&lt;/p>
&lt;p>음악 생성과 관련된 위험성, 특히 창의적 콘텐츠의 부당한 사용을 인지하고 있다. 이에 따라, 대형 언어 모델에 대한 기억력 연구를 철저히 수행하였고, MuLan 임베딩을 MusicLM에 입력하면 생성된 토큰의 시퀀스가 학습 세트의 해당 시퀀스와 크게 다르다는 결과를 얻었다.&lt;/p>
&lt;p>이 작업의 주요 기여는 다음과 같다:&lt;/p>
&lt;ol>
&lt;li>텍스트 조건부 신호에 충실하게 몇 분 동안 일관된 고품질 음악을 생성하는 MusicLM 모델을 소개한다.&lt;/li>
&lt;li>멜로디와 같은 다른 조건부 신호로 방법을 확장하고, 텍스트 프롬프트에 따라 합성한다. 또한, 최대 5분 길이의 음악 클립을 일관되게 생성하는 것을 보여준다.&lt;/li>
&lt;li>text-to-music 생성 작업을 위한 첫 번째 평가 데이터셋인 MusicCaps를 공개한다. 이는 음악가들이 준비한 5.5k의 음악-텍스트 쌍으로 구성된 고품질 데이터셋이다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="background-and-related-work">Background and Related Work&lt;/h2>
&lt;p>다양한 도메인의 생성 모델링에서 Transformer 기반의 autoregressive 모델과 U-Net 기반의 diffusion 모델이 주도하고 있다. 이 섹션에서는 discrete 토큰을 다루는 autoregressive 생성 모델에 초점을 맞춰 MusicLM과 관련된 작업을 검토한다.&lt;/p>
&lt;h3 id="quantization">Quantization&lt;/h3>
&lt;p>자연어 처리, 이미지, 비디오 생성 등에서 autoregressively 하게 discrete 토큰의 시퀀스를 모델링하는 것이 효과적임이 입증되었다. 연속 신호에 대한 autoregressive 모델의 성공에는 양자화가 중요하며, 이는 컴팩트한 discrete 표현을 제공하면서도 높은 품질의 재구성을 가능하게 한다. VQ-VAEs는 다양한 도메인에서 낮은 비트레이트에서 뛰어난 재구성 품질을 보여주며, 많은 접근법의 기본 양자화 도구로 사용되었다.&lt;/p>
&lt;p>SoundStream은 일반 오디오를 낮은 비트레이트로 압축하면서도 높은 재구성 품질을 유지하는 neural audio codec이다. 이를 위해 residual vector quantization(RVQ)를 사용하여 비트레이트와 품질을 높이는 데 큰 계산 비용 없이 확장성을 제공한다. RVQ는 대상 비트레이트가 증가함에 따른 코드북 크기의 급증을 방지하며, 각 양자화기가 계층적 구조를 가지게 된다. 이는 고품질 재구성에 유리하며, 생성에도 바람직한 속성이다. 이 작업에서는 24kHz 음악을 6kbps로 고품질로 재구성할 수 있는 SoundStream을 오디오 토크나이저로 사용한다.&lt;/p>
&lt;h3 id="generative-models-for-audio">Generative Models for Audio&lt;/h3>
&lt;p>장기적인 일관성을 유지하면서 고품질 오디오를 생성하는 것은 어려운 문제지만, 최근에는 Jukebox와 PerceiverAR 같은 일련의 방법론이 이 문제를 해결하려 노력하였다. Jukebox는 높은 시간적 일관성을 달성하기 위해 VQVAEs의 계층을 제안하지만, 생성된 음악에서 아티팩트가 나타났다. 반면 PerceiverAR은 고품질 오디오를 달성하지만, 장기적인 시간적 일관성을 저해하였다.&lt;/p>
&lt;p>AudioLM은 계층적 토크나이징과 생성 체계를 사용하여 일관성과 고품질 합성 사이의 균형을 맞춘다. 이 방법은 의미 토큰과 음향 토큰 두 가지 유형을 구분하여 장기 구조를 모델링하고 미세한 음향 세부 사항을 포착한다. 이를 통해 AudioLM은 대본이나 기호적 음악 표현에 의존하지 않고도 일관되고 고품질의 음성과 피아노 음악을 생성할 수 있다.&lt;/p>
&lt;p>MusicLM은 AudioLM을 기반으로 하되, 추가적으로 (1) 생성 과정을 설명적인 텍스트에 조절하는, (2) 이 조절을 멜로디와 같은 다른 신호로 확장하는, 그리고 (3) 피아노 음악을 넘어 다양한 음악 장르의 긴 시퀀스를 모델링하는 세 가지 기여를 한다.&lt;/p>
&lt;h3 id="conditioned-audio-generation">Conditioned Audio Generation&lt;/h3>
&lt;p>텍스트 설명에서 오디오를 생성하는 것은 최근 연구 주제로 다루어졌다. DiffSound는 텍스트 인코더로 CLIP을 사용하고, 확산 모델을 적용하여 텍스트 임베딩에 따른 양자화된 멜 스펙트로그램 특성을 예측한다. 반면 AudioGen은 T5 인코더를 텍스트 임베딩에 사용하고, autoregressive transformer decoder로 EnCodec에 의해 생성된 오디오 코드를 예측한다. 두 연구 모두 AudioSet과 AudioCaps 같은 적절한 양의 쌍을 이룬 학습 데이터에 의존한다.&lt;/p>
&lt;p>텍스트에 기반한 음악 생성에 초점을 맞춘 연구 중, Mubert는 텍스트 프롬프트를 transformer로 임베딩하고, 이를 바탕으로 음악 태그를 선택하여 노래 생성 API에 쿼리한다. 반면 Riffusion은 안정적인 diffusion 모델을 음악-텍스트 데이터셋의 멜 스펙트로그램 음악 조각에 미세조정한다. 이 두 연구를 기준선으로 사용하여, 이 연구가 오디오 생성 품질과 텍스트 설명의 준수를 개선한다는 것을 보여준다.&lt;/p>
&lt;p>음악의 기호적 표현(예: MIDI)이 강력한 조절 형태로서 생성 과정을 주도하는 것이 보여졌다. 그러나 MusicLM은 허밍된 멜로디와 같은 방법을 통해 더 자연스럽고 직관적인 조절 신호를 제공하며, 이는 텍스트 설명과 결합될 수 있다.&lt;/p>
&lt;h3 id="text-conditioned-image-generation">Text-Conditioned Image Generation&lt;/h3>
&lt;p>텍스트 조건의 이미지 생성 모델은 구조적 개선과 대량의 고품질 쌍 학습 데이터 덕분에 큰 진전을 이루었다. 이 방법은 transformer 기반의 autoregressive 접근법과 확산 기반 모델을 포함하며, 이는 텍스트 프롬프트에서 비디오를 생성하는 것으로 확장되어 왔습니다.&lt;/p>
&lt;p>이 연구의 접근법은 DALL·E 2와 가장 유사하며, 둘 다 텍스트 인코딩을 위해 CLIP에 의존한다. 하지만, DALL·E 2가 diffusion 모델을 decoder로 사용하는 반면, 이 연구는 AudioLM을 기반으로 한 deocer를 사용한다. 또한, 오디오 전용 데이터셋에서 학습이 가능하도록 텍스트 임베딩을 음악 임베딩으로 매핑하는 사전 모델을 생략하고, 추론 시에 음악 임베딩을 텍스트 임베딩으로 단순히 대체한다.&lt;/p>
&lt;h3 id="joint-embedding-models-for-music-and-text">Joint Embedding Models for Music and Text&lt;/h3>
&lt;p>MuLan은 음악과 텍스트의 공동 임베딩 모델로, 각 모달리티에 대한 임베딩 탑을 포함한다. 이 탑들은 대조 학습을 통해 두 모달리티를 128차원의 공유 임베딩 공간으로 매핑한다. 텍스트 임베딩 네트워크는 대규모 텍스트 데이터에서 사전 학습된 BERT를 사용하며, 오디오 탑은 ResNet-50 변형을 사용한다.&lt;/p>
&lt;p>MuLan은 음악 클립과 그에 대응하는 텍스트 주석의 쌍에 학습된다. 데이터 품질에 대한 요구가 낮아, 약한 연관성만 가진 음악-텍스트 쌍에서도 상호 연관성을 학습할 수 있다. 이로 인해 음악을 자연 언어 설명에 연결하여 검색이나 zero-shot 음악 태깅에 활용할 수 있다. 이 작업에서는 Huang et al. (2022)의 사전 학습된 모델을 사용한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>이 섹션에서는 MusicLM과 그 구성 요소에 대해 설명한다. 섹션 3.1은 오디오 표현을 제공하는 모델에 대해, 그리고 섹션 3.2는 이를 텍스트 조건의 음악 생성에 어떻게 활용하는지에 대해 다룬다.&lt;/p>
&lt;h3 id="representation-and-tokenization-of-audio-and-text">Representation and Tokenization of Audio and Text&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/figure1.png"
width="644"
height="334"
srcset="https://kurtkim.github.io/p/musiclm/images/figure1_hu6e041e7e12f30cda1cfeed8c59f97029_66923_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/figure1_hu6e041e7e12f30cda1cfeed8c59f97029_66923_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="462px"
>&lt;/p>
&lt;p>conditional autoregressive 음악 생성을 위해 세 가지 모델을 사용해 오디오 표현을 추출한다. AudioLM의 접근법을 따라 SoundStream의 self-supervise 오디오 표현과 w2vBERT를 각각 음향 토큰과 의미 토큰으로 사용하며, 조절을 위해 MuLan의 음악 임베딩과 텍스트 임베딩을 활용한다. 이 모델들은 독립적으로 사전 학습되고 고정되어, sequence-to-sequence 모델링에 필요한 이산적인 오디오와 텍스트 표현을 제공한다.&lt;/p>
&lt;p>&lt;strong>SoundStream.&lt;/strong> 24 kHz monophonic 오디오를 위해 스트라이딩 요소 480의 SoundStream 모델을 사용하며, 이로 인해 50 Hz의 임베딩이 생성된다. 이 임베딩들은 RVQ에 의해 학습된 양자화로 인해 6 kbps의 비트레이트를 가지게 되며, 1초의 오디오는 600개의 토큰으로 표현된다. 이 토큰들을 &amp;ldquo;acoustic token&amp;quot;이라 부르며, 이는 $A$로 표기된다.&lt;/p>
&lt;p>&lt;strong>w2v-BERT.&lt;/strong> 600M parameter를 가진 w2v-BERT masked-language-modeling (MLM) 모듈의 중간 계층을 사용한다. 이 모델을 사전 학습하고 고정한 후, 7번째 계층에서 임베딩을 추출하고, 이를 k-means의 중심을 사용해 양자화한다. 결과적으로, 오디오의 모든 초당 25개의 의미 토큰을 생성하며, 이는 $S$로 표기된다.&lt;/p>
&lt;p>&lt;strong>MuLan.&lt;/strong> MusicLM을 학습시키기 위해, MuLan의 오디오 임베딩 네트워크에서 오디오 시퀀스의 표현을 추출한다. 이 표현은 연속적이지만, 오디오와 조절 신호가 동질적인 이산 토큰 기반 표현을 가지도록 MuLan 임베딩을 양자화한다. 이는 조절 신호를 autoregressively 하게 모델링하는 연구를 지원한다.&lt;/p>
&lt;p>MuLan은 10초 오디오 입력에 작동하며, 긴 오디오 시퀀스를 처리하기 위해 1초 간격으로 10초 윈도우에서 오디오 임베딩을 계산하고 평균화한다. 이후 1024 크기의 어휘를 가진 12개의 벡터 양자화기를 사용하는 RVQ로 이산화하여, 오디오 시퀀스에 대해 12개의 MuLan 오디오 토큰 $M_A$를 생성한다. 추론 시에는 텍스트 프롬프트에서 MuLan 텍스트 임베딩을 추출하고 같은 RVQ로 양자화하여 12개의 토큰 $M_T$를 얻는다.&lt;/p>
&lt;p>학습 중에 $M_A$에 조건을 부여하는 것은 학습 데이터를 쉽게 확장할 수 있도록 하며, 대조적인 손실로 학습된 MuLan 같은 모델을 활용해 잡음이 많은 텍스트 설명에 대한 강건성을 높이는 두 가지 이점이 있다.&lt;/p>
&lt;h3 id="hierarchical-modeling-of-audio-representations">Hierarchical Modeling of Audio Representations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/figure2.png"
width="1326"
height="394"
srcset="https://kurtkim.github.io/p/musiclm/images/figure2_hu5f857058def17e8cc3de10acbfba69e5_117180_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/figure2_hu5f857058def17e8cc3de10acbfba69e5_117180_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="336"
data-flex-basis="807px"
>&lt;/p>
&lt;p>discrete 오디오 표현을 AudioLM과 결합해 텍스트 조건의 음악 생성을 달성한다. 이를 위해, 각 단계가 decoder-only Transformer에 의해 autoregressively 하게 모델링되는 계층적인 sequenceto-sequence 모델링 작업을 제안한다.&lt;/p>
&lt;p>첫 번째 단계는 의미 모델링 단계로, MuLan 오디오 토큰에서 의미 토큰 $S$로의 매핑을 학습한다. 이는 분포 $p(S_t | S_{&amp;lt;t}, M_A)$를 모델링함으로써 이루어지며, 여기서 $t$는 시간 단계에 해당하는 시퀀스 내의 위치이다. 두 번째 단계는 음향 모델링 단계로, 음향 토큰 $A_q$는 MuLan 오디오 토큰과 의미 토큰 모두에 의해 조건이 부여되어 예측되며, 이는 분포 $p(A_t | $A_{&amp;lt;t}, S, M_A)$를 모델링한다.&lt;/p>
&lt;p>긴 토큰 시퀀스를 피하고자, AudioLM은 음향 모델링 단계를 대략적인 단계와 세부적인 단계로 분할하는 방법을 제안하였으며, 이 연구에서도 이를 따랐다. 대략적인 단계에서는 SoundStream RVQ의 첫 네 단계를, 세부적인 단계에서는 나머지 여덟 단계를 모델링한다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental Setup&lt;/h2>
&lt;h3 id="models">Models&lt;/h3>
&lt;p>AudioLM의 의미 단계와 음향 단계 모델링에 decoder-only Transformer를 사용한다. 이 모델은 24개 layer, 16개 attention head, 1024의 embedding dimension, 4096의 차원의 feed-forward layer, 0.1의 dropout, 그리고 relative positional embedding으로 구성되어 있으며, 각 단계는 430M의 parameter를 가진다.&lt;/p>
&lt;h3 id="training-and-inference">Training and Inference&lt;/h3>
&lt;p>MusicLM의 다른 구성 요소를 학습시키기 위해 사전 학습된 MuLan에 의존한다. SoundStream과 w2v-BERT는 FMA 데이터셋에서, 토크나이저와 의미 및 음향 모델링 단계의 모델은 24 kHz에서 280k시간의 음악을 포함하는 5M 개의 오디오 클립 데이터셋에서 학습된다. 각 단계는 학습 데이터를 여러 번 반복하여 학습하며, 의미와 음향 단계에서는 각각 30초와 10초의 임의 오디오를 사용한다. 세부적인 음향 모델링 단계는 3초 자르기에서 학습된다.&lt;/p>
&lt;p>추론 시에는 MuLan이 학습한 오디오와 텍스트 사이의 임베딩 공간을 사용하며, $M_A$를 $M_T$로 대체한다. 그 후, $M_T$가 주어졌을 때 $A$를 얻기 위해 위에서 설명한 단계를 따른다. 모든 단계에서 autoregressive 샘플링을 위해 온도 샘플링을 사용하며, 의미 모델링 단계는 1.0, 대략적인 음향 모델링 단계는 0.95, 세밀한 음향 모델링 단계는 0.4의 온도를 사용한다. 이 값들은 생성된 음악의 다양성과 시간적 일관성 사이의 좋은 균형을 위해 선택되었다.&lt;/p>
&lt;h3 id="evaluation-dataset">Evaluation Dataset&lt;/h3>
&lt;p>MusicLM 평가를 위해 고품질 음악 캡션 데이터셋인 MusicCaps를 준비하였다. 이 데이터셋은 AudioSet의 5.5k 음악 클립과 전문 음악가들의 영어 설명이 포함되어 있다. 각 음악 클립에는 자유 형식의 캡션과 장르, 기분, 템포 등을 설명하는 음악 측면이 포함되어 있다.&lt;/p>
&lt;p>MusicCaps는 AudioSet의 오디오 클립과 텍스트 설명을 포함한 AudioCaps를 보완한다. AudioCaps는 음악이 아닌 내용이 포함되어 있지만, MusicCaps는 전문가의 상세 주석이 포함된 음악에만 초점을 맞추고 있다. AudioSet의 학습 및 평가 분할로부터 다양한 장르의 예시를 추출하였으며, 1k 예시로 구성된 장르별 균형 분할도 제공한다.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>MusicLM을 평가하기 위해, 오디오 품질과 텍스트 설명 충실도라는 음악 생성의 두 가지 중요한 측면을 측정하는 다양한 메트릭을 사용한다.&lt;/p>
&lt;p>&lt;strong>Fréchet Audio Distance (FAD).&lt;/strong>&lt;/p>
&lt;p>Fréchet Audio Distance는 사람의 인식과 잘 맞는 오디오 품질 지표이다. 이 점수가 낮은 모델은 신뢰할 수 있는 오디오를 생성할 것으로 예상되지만, 생성된 샘플이 제공된 텍스트 설명을 반드시 따르는 것은 아니다.&lt;/p>
&lt;p>공개적으로 사용 가능한 두 가지 오디오 임베딩 모델, 즉 음성 데이터에 학습된 Trill 2와 YouTube-8M 오디오 이벤트 데이터셋에 학습된 VGGish 3에 기반한 FAD를 보고한다. 학습 데이터의 차이로 인해, 이 두 모델은 오디오 품질의 다른 측면을 측정하게 될 것으로 예상한다.&lt;/p>
&lt;p>&lt;strong>KL Divergence (KLD).&lt;/strong> 텍스트 설명과 음악 클립 사이에는 다대다 관계가 있어, 오디오 파형 수준에서 직접 비교는 불가능하다. 입력 텍스트 충실도를 평가하기 위해, AudioSet에서 다중 레이블 분류를 위해 학습된 LEAF 분류기를 사용하여 생성된 음악과 참조 음악의 클래스 예측을 계산하고, 이들 사이의 KL-Divergence를 측정한다. KL-Divergence가 낮으면, 생성된 음악은 분류기에 따라 참조 음악과 유사한 음향 특성을 가질 것으로 예상된다.&lt;/p>
&lt;p>&lt;strong>MuLan Cycle Consistency (MCC).&lt;/strong> 음악-텍스트 임베딩 모델인 MuLan을 사용해 음악-텍스트 쌍의 유사성을 측정한다. MusicCaps의 텍스트 설명과 그에 기반한 생성된 음악에서 임베딩을 계산하고, 이들 간의 평균 코사인 유사성을 MCC 메트릭으로 정의한다.&lt;/p>
&lt;p>&lt;strong>Qualitative evaluation.&lt;/strong> 생성된 샘플이 텍스트 설명을 얼마나 잘 따르는지 평가하기 위해 A대 B 인간 평가 작업을 설정하였다. 평가자들은 텍스트 설명과 두 가지 다른 모델에 의해 생성된 음악 샘플을 비교하며, 샘플에 대한 강한, 약한 또는 무관한 선호도를 선택한다. 음악 품질은 이미 FAD 메트릭으로 평가되었으므로, 이를 고려하지 않도록 지시받았다.&lt;/p>
&lt;p>참조 음악과 n개의 다른 모델의 출력을 고려해 총 n + 1개의 조건을 설정하고, 이들 간의 쌍을 비교한다. 각 조건이 얼마나 선호되는지를 승리의 수로 계산하여 결과를 집계하고 순위를 매긴다. 샘플은 평가 데이터의 장르 균형 1k 부분 집합에서 선택된다.&lt;/p>
&lt;p>&lt;strong>Training data memorization.&lt;/strong> 큰 언어 모델은 학습 데이터의 패턴을 기억할 수 있다. 이를 바탕으로 MusicLM이 음악 세그먼트를 얼마나 기억하는지 연구한다. 학습 세트에서 무작위로 선택한 예시에 대해, MuLan 오디오 토큰과 첫 번째 의미 토큰 시퀀스를 포함한 프롬프트를 모델에 제공한다. greedy decoding을 통해 의미 토큰의 연속을 생성하고, 이를 데이터셋의 대상 토큰과 비교한다. 생성된 토큰과 대상 토큰이 전체 샘플링 세그먼트에서 완전히 일치하는 예시의 비율을 측정한다.&lt;/p>
&lt;p>음향적으로 유사한 오디오 세그먼트가 다른 토큰 시퀀스로 이어질 수 있다는 관찰을 기반으로, 근사치 일치를 감지하는 방법을 제안한다. 생성된 토큰과 대상 토큰의 의미 토큰 수 히스토그램을 계산하고, 이들 사이의 일치 비용을 측정한다. 이를 위해 먼저 의미 토큰 쌍 간의 거리 행렬을 계산하고, 그 후 Sinkhorn 알고리즘을 사용해 최적의 전송 문제를 해결한다. 부정 쌍을 구성하고 이들의 일치 비용 분포를 측정해 일치 임계값을 보정한다. 이 임계값은 0.01% 미만의 거짓 긍정 근사치 일치를 초래하는 0.85로 설정된다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>MusicLM을 평가하기 위해, 최근의 기준선인 Mubert와 Riffusion과 비교한다. Mubert API를 쿼리하고 Riffusion 모델에서 추론을 실행하여 오디오를 생성한다. 이 논문과 함께 공개적으로 발표하는 평가 데이터셋인 MusicCaps에서 평가를 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musiclm/images/table1.png"
width="600"
height="170"
srcset="https://kurtkim.github.io/p/musiclm/images/table1_hu932e6e5791924bf2b016ff6504760e37_25595_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musiclm/images/table1_hu932e6e5791924bf2b016ff6504760e37_25595_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="352"
data-flex-basis="847px"
>&lt;/p>
&lt;p>&lt;strong>Comparison to baselines.&lt;/strong> 오디오 품질을 측정하는 FAD 메트릭에 따르면, MusicLM은 Mubert와 Riffusion보다 더 나은 점수를 얻었다. 또한, 입력 텍스트 설명에 대한 충실도를 측정하는 KLD와 MCC에 따라, MusicLM은 기준선에 비해 텍스트 설명에서 더 많은 정보를 캡처하는 능력을 보여준다.&lt;/p>
&lt;p>텍스트 충실도 평가를 보완하기 위해 인간 청취 테스트를 진행하였다. 참가자들은 두 개의 10초 클립과 텍스트 캡션을 보고, 어떤 클립이 캡션의 텍스트를 가장 잘 설명하는지 5점 척도로 평가한다. 총 1200개의 평가를 수집하였고, 각 소스는 600개의 쌍별 비교에 참여하였다. MusicLM은 두 가지 기준선에 비해 확실히 선호되지만, 실제 참조 음악과의 차이는 여전히 존재한다. 청취 연구의 자세한 내용은 부록 B에서 확인할 수 있다.&lt;/p>
&lt;p>실제 참조가 MusicLM보다 선호되는 경우, 이는 주로 아래의 패턴 때문이다: (1) 캡션은 매우 상세하고, 여러 악기나 비음악적인 측면을 설명한다; (2) 캡션은 오디오의 재생 순서를 설명한다; (3) 부정적인 표현이 사용되는데, 이는 MuLan에 잘 반영되지 않는다.&lt;/p>
&lt;p>결론적으로, 이 방법론은 MusicCaps의 자유형 텍스트 캡션에서 세부적인 정보를 잘 포착하며, KLD와 MCC 메트릭은 텍스트 설명에 대한 충실도를 정량적으로 측정하고, 이는 인간의 평가와 일치한다는 것을 확인하였다.&lt;/p>
&lt;p>&lt;strong>Importance of semantic tokens.&lt;/strong> 의미론적 모델링과 음향 모델링을 분리하는 것의 유용성을 검증하기 위해, Transformer 모델을 학습시켜 MuLan 토큰에서 음향 토큰을 직접 예측하였다. FAD 메트릭은 비슷하지만, 의미론적 모델링 단계를 제거하면 KLD와 MCC 점수가 악화됨을 발견하였다. 이는 의미 토큰이 텍스트 설명에 따른 순응을 돕는다는 것을 나타낸다. 또한, 샘플을 들어보니 장기 구조에서의 저하가 관찰되었다.&lt;/p>
&lt;p>&lt;strong>Information represented by audio tokens.&lt;/strong> 추가 실험을 통해 의미론적 토큰과 음향 토큰이 어떤 정보를 포착하는지 연구하였다. 첫 번째 실험에서는 텍스트 토큰과 의미론적 토큰을 고정하고, 음향 모델링 단계를 반복하여 샘플을 생성하였다. 결과적으로 샘플들은 다양했지만, 장르나 리듬, 멜로디 등에서 공통점을 보였습니다. 두 번째 실험에서는 텍스트 토큰만 고정하고 의미론적 토큰과 음향 토큰을 생성했을 때, 멜로디와 리듬에서 더 큰 다양성을 보였다. 이 연구의 샘플들은 동반 자료에서 확인할 수 있다.&lt;/p>
&lt;p>&lt;strong>Memorization analysis.&lt;/strong> 의미론적 토큰 프롬프트의 길이를 변화시킬 때 일치도를 보고합니다. 정확한 일치의 비율은 매우 작은 것을 확인했으며, 근사치 일치(τ = 0.85 사용)의 경우, 프롬프트 길이가 증가함에 따라 일치하는 예시의 비율이 증가한다. 또한, 일치 점수가 낮은 시퀀스는 토큰 다양성이 낮은 것으로 확인되었다. 이러한 패턴은 의미론적 토큰이 정확하게 일치하더라도, 음향 모델링이 생성된 샘플에 추가적인 다양성을 도입한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="extensions">Extensions&lt;/h2>
&lt;p>&lt;strong>Melody conditioning.&lt;/strong> MusicLM은 텍스트 설명과 함께 허밍, 노래, 휘파람, 악기 연주 등의 형태로 제공되는 멜로디에 기반한 음악을 생성할 수 있도록 확장되었다. 이를 위해, 멜로디는 같지만 음향이 다른 오디오 쌍으로 구성된 데이터셋을 만들고, 공통 임베딩 모델을 학습시켜 같은 멜로디를 가진 오디오 클립의 임베딩이 서로 가까워지도록 하였다.&lt;/p>
&lt;p>MusicLM의 멜로디를 추출하기 위해, 멜로디 임베딩을 양자화하고 결과 토큰 시퀀스를 MuLan 오디오 토큰과 연결한다. 추론 과정에서는, 입력 오디오 클립에서 멜로디 토큰을 계산하고 이를 MuLan 텍스트 토큰과 연결한다. 이러한 방식으로, MusicLM은 입력 오디오 클립의 멜로디를 따르면서도 텍스트 설명을 준수하는 음악을 성공적으로 생성할 수 있다.&lt;/p>
&lt;p>&lt;strong>Long generation and story mode.&lt;/strong> MusicLM은 시간 차원에서 자기회귀적 생성을 사용하여 학습 시 사용한 것보다 더 긴 시퀀스를 만들 수 있다. 의미론적 모델링은 30초 시퀀스에서 학습되며, 더 긴 시퀀스를 위해 15초 간격으로 진행하고, 15초를 접두사로 사용하여 추가적인 15초를 생성한다. 이 방법으로 몇 분 동안 일관성을 유지하는 긴 오디오 시퀀스를 생성할 수 있다.&lt;/p>
&lt;p>소소한 수정을 통해 MusicLM은 시간이 지나면서 텍스트 설명이 변화하는 동안 긴 오디오 시퀀스를 생성할 수 있게 되었다. 이를 스토리 모드라고 부르며, 여러 텍스트 설명에서 $M_T$를 계산하고 15초마다 조건 신호를 변경한다. 이 방법으로, 모델은 텍스트 설명에 따라 음악 컨텍스트를 변경하면서도 템포가 일관되고 의미론적으로 타당한 부드러운 전환을 생성한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>MusicLM은 텍스트 조건에 충실하면서 몇 분 동안 일관된 24 kHz의 고품질 음악을 생성하는 모델이다. 이 모델은 뮤지션들이 준비한 5.5k 음악-텍스트 쌍의 고품질 데이터셋인 MusicCaps에서 기존 모델을 능가하는 성능을 보여준다.&lt;/p>
&lt;p>이 방법의 한계 중 일부는 MuLan으로부터 비롯되며, 이는 모델이 부정을 잘못 이해하고 텍스트에 기술된 시간 순서를 정확하게 따르지 않는다는 점을 포함한다. 또한, 정량적 평가의 추가적인 개선이 필요하며, MCC 점수는 MuLan에 의존하기 때문에 이 방법에 유리하다.&lt;/p>
&lt;p>미래의 연구는 가사 생성, 텍스트 조건화 및 보컬 품질 개선, 고수준 노래 구조(서론, 구절, 후렴구 등)의 모델링, 그리고 더 높은 샘플 속도에서 음악 모델링에 초점을 맞출 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="broader-impact">Broader Impact&lt;/h2>
&lt;p>MusicLM은 텍스트를 기반으로 고품질 음악을 생성해 인간의 창의적 음악 작업을 돕지만, 여러 위험 요소가 있다. 학습 데이터의 편향이 생성된 샘플에 반영될 수 있으며, 이것은 대표성이 부족한 문화에 대한 음악 생성의 적절성과 문화적 강탈 문제를 불러일으킬 수 있다.&lt;/p>
&lt;p>창의적 콘텐츠의 잘못된 사용 가능성을 인지하며, 책임있는 모델 개발 원칙에 따라 의미 모델링에 초점을 맞춘 연구를 수행하였다. 기억된 예시는 극히 적었고, 예시의 1%만 대략적으로 일치하였다. 음악 생성과 관련된 위험을 해결하기 위한 미래 연구의 필요성을 강조하며, 현재 모델 공개 계획은 없다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2301.11325.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/musiclm-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MusicGEN</title><link>https://kurtkim.github.io/p/musicgen/</link><pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/musicgen/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>conditional music generation을 위한 &amp;ldquo;MusicGEN&amp;quot;이라는 언어 모델을 개발하였다. 이 모델은 여러 스트림의 압축된 이산 음악 표현을 다루며, 효율적인 토큰 교차 패턴과 single-stage transformer를 사용해 여러 모델을 계층적으로 구성하거나 업샘플링할 필요가 없다. 이 방법을 통해 텍스트 설명이나 멜로디 특징에 따라 높은 품질의 음악 샘플을 생성할 수 있음을 입증하였다. 실증적 평가를 통해 제안된 접근법이 기존 벤치마크보다 우수하다는 것을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>text-to-music은 텍스트 설명을 바탕으로 음악을 생성하는 작업이다. 이 과정은 long range sequence를 모델링하고 full frequency spectrum을 사용해야 하므로 어렵다. 또한, 다양한 악기의 하모니와 멜로디를 포함하는 음악은 복잡한 구조를 가지며, 이로 인해 음악 생성 과정에서는 멜로디 오류를 범할 여지가 거의 없다. 키, 악기, 멜로디, 장르 등 다양한 요소를 제어할 수 있는 능력은 음악 창작자에게 필수적이다.&lt;/p>
&lt;p>self-supervised audio representation, sequential modeling, audio synthesis 등의 최근 연구 진보가 새로운 모델 개발을 가능하게 한다. 최근 연구들은 오디오 신호를 같은 신호를 표현하는 여러 이산 토큰의 스트림으로 나타내는 것을 제안하였는데, 이를 통해 고품질의 오디오 생성과 효과적인 오디오 모델링이 가능해졌다. 그러나 이는 여러 parallel dependent stream을 동시에 모델링해야한다는 비용을 수반한다.&lt;/p>
&lt;p>Kharitonov et al. 과 Kreuk et al. 은 음성 토큰의 다중 스트림을 병렬로 모델링하는 지연 접근법을 제안하였다. Agostinelli et al. 은 음악 세그먼트를 다양한 세부성의 이산 토큰 시퀀스로 표현하고 이를 autoregressive 모델로 모델링하는 방식을 제안하였다. Donahue et al. 은 비슷한 접근법을 가요 생성 작업에 적용했고, Wang et al. 은 문제를 두 단계로 해결하는 방법을 제안하였다: 첫 번째 토큰 스트림만 모델링한 후, non-autoregressive 방식으로 나머지 스트림을 모델링한다.&lt;/p>
&lt;p>이 연구에서는 텍스트 설명에 따른 고품질 음악을 생성하는 &amp;ldquo;MusicGEN&amp;quot;이라는 단순하고 조절 가능한 모델을 소개한다. 이 모델은 음향 토큰의 병렬 스트림을 모델링하는 프레임워크를 제안하며, 스테레오 오디오 생성을 추가 비용 없이 확장할 수 있다. 또한, 비지도 멜로디 조건 설정을 통해 생성된 샘플의 제어력을 향상시키고, 주어진 조화와 멜로디 구조에 맞는 음악을 생성할 수 있다. MusicGEN은 평가에서 100점 만점에 84.8점의 높은 점수를 받았으며, 이는 최고 기준선의 80.5점보다 우수한 성능을 보여준다. 마지막으로, 인간 평가에 따르면 MusicGEN은 주어진 조화 구조에 잘 맞는 멜로디를 가진 고품질 샘플을 생성하며, 텍스트 설명을 충실히 따른다.&lt;/p>
&lt;p>&lt;strong>Our contribution:&lt;/strong> 32 kHz에서 고품질 음악을 생성하는 간단하고 효율적인 모델, MusicGEN을 제안한다. 이 모델은 효율적인 코드북 교차 전략을 통해 일관된 음악을 생성하며, 텍스트와 멜로디 조건에 모두 부합하는 단일 모델을 제공한다. 생성된 오디오는 제공된 멜로디와 일치하고 텍스트 조건 정보에 충실하다. 또한, 주요 설계 선택에 대한 광범위한 객관적 평가와 인간 평가를 제공한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>MusicGEN은 텍스트나 멜로디에 의존하는 autoregressive transformer-based decoder이다. 이 모델은 양자화된 오디오 토큰을 사용하며, 이는 고해상도 복구를 가능하게 한다. 병렬 스트림은 Residual Vector Quantization (RVQ)를 통해 생성되며, 각 스트림은 다양한 코드북에서 생성된 이산 토큰으로 구성된다. 이 연구에서는 다양한 코드북 교차 패턴에 적용 가능한 새로운 모델링 프레임워크를 소개하며, 이를 통해 양자화된 오디오 토큰의 내부 구조를 활용한다. MusicGEN은 텍스트나 멜로디를 기반으로 한 조건부 생성을 지원한다.&lt;/p>
&lt;h3 id="audio-tokenization">Audio tokenization&lt;/h3>
&lt;p>Residual Vector Quantization (RVQ)를 사용하여 양자화된 latent space와 adversarial reconstruction 손실을 가진 EnCodec을 사용한다. 이는 오디오 무작위 변수를 연속 텐서로 인코딩하고, 이를 다시 양자화하여 병렬 이산 토큰 시퀀스를 생성한다. RVQ에서는 각 양자화기가 이전 양자화기의 양자화 오류를 인코딩하므로, 다른 코드북의 양자화 값은 일반적으로 독립적이지 않다. 이 과정에서 첫 번째 코드북이 가장 중요하게 작용한다.&lt;/p>
&lt;h3 id="codebook-interleaving-patterns">Codebook interleaving patterns&lt;/h3>
&lt;p>&lt;strong>Exact flattened autoregressive decomposition.&lt;/strong> autoregressive 모델은 일정한 길이 $S$를 가진 이산 랜덤 시퀀스 $U$가 필요하며, 이 시퀀스는 {$1, &amp;hellip;, N$}$^S$에서 선택된다. 관례적으로 시퀀스의 시작은 $U_0 = 0$, 즉 특별 토큰으로 표현된다. 이를 통해 분포를 모델링한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, p_t (U_{t−1}, &amp;hellip;, U_0) \triangleq \mathbb{P} [U_t | U_{t−1}, &amp;hellip;, U_0] $$&lt;/p>
&lt;p>auto-regressive density $p$를 이용해 랜덤 변수의 두 번째 시퀀스인 $\tilde{U}$를 만든다. 이때, $\tilde{U}_0 = 0$으로 초기화하고, $t &amp;gt; 0$인 모든 경우에 대해 재귀적으로 정의한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, \mathbb{P} \big[\tilde{U}_t | \tilde{U}_{t−1}, &amp;hellip;, \tilde{U}_0 \big] = p_t (\tilde{U}_{t−1}, &amp;hellip;, \tilde{U}_0) $$&lt;/p>
&lt;p>$U$와 $\tilde{U}$가 같은 분포를 가진다는 것이 바로 확인된다. 이는 딥러닝 모델로 $p$의 완벽한 추정치 $\tilde{p}$를 맞출 수 있다면, $U$의 분포도 정확히 맞출 수 있다는 것을 의미한다.&lt;/p>
&lt;p>EnCodec 모델로부터 얻은 $Q$ 표현의 문제는 각 시간 단계마다 $K$개의 코드북이 있다는 점이다. 이를 해결하기 위해 $Q$를 펼쳐 $S = d \cdot f_r \cdot K$로 설정할 수 있다. 이 방식은 첫 번째 시간 단계의 각 코드북을 순차적으로 예측한다. 이론적으로 $Q$의 분포를 정확하게 모델링할 수 있지만, 복잡성이 증가하고 가장 낮은 샘플 속도 $f_r$에서 얻는 이익이 일부 손실된다.&lt;/p>
&lt;p>여러 가지 flattening 방법이 가능하며, 모든 $\hat{p_t}$ 함수를 한 모델로 추정할 필요는 없다. 예를 들어, MusicLM은 두 개의 모델을 사용해 첫 번째 $K/2$ 코드북과 나머지 $K/2$ 코드북을 각각 모델링한다. 이렇게 해도 autoregressive step의 수는 $df_r \cdot K$로 동일하다.&lt;/p>
&lt;p>&lt;strong>Inexact autoregressive decomposition.&lt;/strong> 일부 코드북이 병렬로 예측되는 autoregressive 분해를 고려하는 것이 가능하다. 즉, $V_0 = 0$을 정의하고, 모든 $t$와 $k$에 대해 $V_{t, k} = Q_{t, k}$로 시퀀스를 설정한다. 이때, 코드북 인덱스 $k$를 생략하면, 시간 $t$에서 모든 코드북이 연결된 것을 의미한다.&lt;/p>
&lt;p>$$ p_{t, k} (V_{t−1}, &amp;hellip;, V_0) \triangleq \mathbb{P} [V_{t, k} | V_{t−1}, \dot, &amp;hellip;, V_0] $$&lt;/p>
&lt;p>재귀적으로 $\tilde{V}_0 = 0$을 다시 정의하고, 모든 $t &amp;gt; 0$에 대해 이를 정의한다.&lt;/p>
&lt;p>$$ \forall t &amp;gt; 0, \mathbb{P} \big[\tilde{V}_{t, k} \big] = p_{t, k} (\tilde{V}_{t−1}, &amp;hellip;, \tilde{V}_0) $$&lt;/p>
&lt;p>일반적으로 정확한 분포 $p_{t,k}$를 가정하더라도 $\tilde{V}$는 $V$와 동일한 분포를 따르지 않는다. 실제로, 모든 $t$에 대해 $(V_{t,k})$ $k$가 $V_{t−1}, &amp;hellip;, $V_0$에 조건부로 독립인 경우에만 적절한 생성 모델을 가진다. $t$가 증가함에 따라 오류가 누적되고 두 분포는 점점 멀어진다. 이 분해법은 부정확하지만 원래의 프레임 속도를 유지하므로, 학습과 추론이 특히 긴 시퀀스에 대해 크게 가속화된다.&lt;/p>
&lt;p>&lt;strong>Arbitrary codebook interleaving patterns.&lt;/strong> 다양한 분해 실험을 진행하고, 부정확한 분해의 영향을 측정하기 위해 코드북 교차 패턴을 사용한다. 모든 시간 단계와 코드북 인덱스의 쌍을 나타내는 $\Omega$ 집합을 고려하며, 코드북 패턴은 $P_0 = \emptyset$으로 시작해 $P_s$가 $\Omega$ 의 부분집합인 시퀀스이다. 이 패턴은 $\Omega$를 모델링하는 데 사용되며, 모든 위치를 병렬로 예측합니다. 실용적으로, 각 $P_s$에서 코드북 인덱스가 최대 한 번만 나타나는 패턴으로 제한한다.&lt;/p>
&lt;p>&amp;ldquo;parallel&amp;rdquo; 패턴과 같은 여러 분해를 쉽게 정의할 수 있다. 이 패턴은 다음과 같이 주어진다.&lt;/p>
&lt;p>$$ P_s = \lbrace (s, k) : k \in \lbrace 1, &amp;hellip;, K \rbrace \rbrace $$&lt;/p>
&lt;p>코드북 사이에 &amp;ldquo;delay&amp;quot;를 도입하는 것도 가능하다.&lt;/p>
&lt;p>$$ P_s = \lbrace (s − k + 1, k) : k \in \lbrace 1, &amp;hellip;, K \rbrace , s − k \geq 0 \rbrace $$&lt;/p>
&lt;p>다양한 코드북 패턴의 장단점을 실증적으로 평가하여, 병렬 코드북 시퀀스 모델링의 중요성을 강조한다.&lt;/p>
&lt;h3 id="model-conditioning">Model conditioning&lt;/h3>
&lt;p>&lt;strong>Text conditioning.&lt;/strong> 입력 오디오에 대응하는 텍스트를 표현하는 세 가지 주요 방법에 대해 실험하였다: T5 인코더를 사용하는 Kreuk et al. 의 방법, 지시기반 언어 모델을 사용하는 Chung et al. 의 방법, 그리고 공동 텍스트-오디오 표현인 CLAP을 사용하는 방법이다. 이 세 가지 방법 모두 조건부 오디오 생성 테스트에서 사용되었다.&lt;/p>
&lt;p>&lt;strong>Melody conditioning.&lt;/strong> 텍스트보다는 다른 오디오 트랙이나 휘파람, 허밍 등에서 얻은 멜로디 구조를 조건으로 삼는 것이 음악에 더 적합하다. 이를 위해 입력의 chromagram과 text description에 동시에 조건을 부여하여 멜로디 구조를 제어하는 실험을 진행하였다. 하지만 raw chromagram에 조건을 부여하면 과적합이 발생해 원본 샘플이 재구성되는 문제가 발생하였다. 이를 해결하기 위해 각 시간 단계에서 주요 time-frequency 빈도를 선택하는 정보 병목 방법을 도입하였다. 이는 supervised proprietary 데이터가 필요 없는 unsupervised 학습 방법으로, 데이터 수집 비용을 줄이는 효과가 있다.&lt;/p>
&lt;h3 id="model-architecture">Model architecture&lt;/h3>
&lt;p>&lt;strong>Codebook projection and positional embedding.&lt;/strong> 코드북 패턴에 따라 각 패턴 단계에서는 일부 코드북만 사용된다. 각 코드북은 최대 한 번만 사용되거나 아예 사용되지 않는다. 코드북이 사용되면, 해당 값은 학습된 임베딩 테이블을 통해 표현되고, 사용되지 않으면 특별 토큰으로 표시된다. 이렇게 변환된 각 코드북의 기여를 합산하며, 첫 번째 입력은 모든 특별 토큰의 합이 된다. 마지막으로, 현재 단계를 인코딩하기 위해 사인 임베딩을 합산한다.&lt;/p>
&lt;p>&lt;strong>Transformer decoder.&lt;/strong> 입력값은 여러 layer와 차원을 가진 transformer를 통해 처리된다. 각 layer는 causal self-attention block으로 구성되고, 조건부 신호 $C$에 따라 cross-attention block을 사용한다. 멜로디 조건을 사용할 경우, 조건부 텐서 $C$를 transformer 입력의 접두어로 사용한다. layer는 fully connected block으로 끝나며, 이 block은 linear layer, ReLU, 그리고 다시 linear layer로 구성된다. 각 block은 residual skip 연결로 래핑되고, 각 block에는 layer normalization가 적용된다.&lt;/p>
&lt;p>&lt;strong>Logits prediction.&lt;/strong> transformer decoder의 출력은 패턴 단계에서 $Q$의 값에 대한 logit 예측으로 변환된다. 코드북이 존재하면, 코드북 특정 linear layer를 적용하여 logit 예측을 얻는다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental setup&lt;/h2>
&lt;h3 id="models-and-hyperparameters">Models and hyperparameters&lt;/h3>
&lt;p>&lt;strong>Audio tokenization model.&lt;/strong> 32 kHz 단음 오디오를 위한 비인과적인 5층 EnCodec 모델을 사용하며, 이는 50 Hz의 frame rate와 initial hidden size 64를 가진다. 4개의 양자화기를 가진 RVQ로 임베딩을 양자화하고, 오디오 시퀀스에서 무작위로 잘린 1초 오디오 세그먼트로 모델을 학습시킨다.&lt;/p>
&lt;p>&lt;strong>Transformer model.&lt;/strong> 300M, 1.5B, 3.3B parameter의 크기를 가진 autoregressive transformer 모델을 학습시킨다. 이 모델들은 긴 시퀀스의 처리 속도와 메모리 사용량을 개선하기 위해 memory efficient Flash attention을 사용한다. 이 기술은 xFormers 패키지에서 구현되어 있다.&lt;/p>
&lt;p>이 모델들은 전체 음악 트랙에서 무작위로 샘플링된 30초 오디오 클립을 학습 데이터로 사용한다. 각 모델은 1M 단계 동안 AdamW optimizer를 사용하여 학습되며, batch size는 192, $\beta_1$은 0.9, $\beta_2$는 0.95, weight decay는 0.1, 그리고 gradient clipping은 1.0의 값을 가진다.&lt;/p>
&lt;p>300M parameter 모델의 경우, D-Adaptation 기반의 automatic step-size를 사용하여 모델의 수렴을 개선한다. 이 방법은 모델의 크기가 더 큰 경우에는 별다른 효과가 없었다.&lt;/p>
&lt;p>모델의 learning rate은 4000 step의 warmup을 가진 cosine learning rate을 따르며, 이동 평균은 0.99의 decay를 사용하여 계산된다.&lt;/p>
&lt;p>각 모델은 각각 32, 64, 96의 GPU를 사용하여 학습되며, mixed precision 방식을 사용한다. 더욱이, bfloat16이 시스템에서 불안정성을 초래하므로 float16을 사용한다.&lt;/p>
&lt;p>마지막으로, 샘플링 과정에서는 top-k 샘플링 방법을 사용하여 상위 250개의 토큰만을 유지하고, 이 토큰들의 확률 분포를 이용하여 샘플링을 진행한다. 이때의 temperature 값은 1.0이다.&lt;/p>
&lt;p>&lt;strong>Text preprocessing.&lt;/strong> Kreuk et al. 은 불용어를 제거하고 텍스트를 표제어화하는 텍스트 정규화 방법을 제안하였다. 음악 데이터셋에서는 음악 키, 템포, 악기 유형 등의 추가 정보를 텍스트 설명에 병합하는 실험을 진행하였다. 또한, 단어 dropout을 텍스트 augmentation 전략으로 사용하였다. 최종 모델에서는 0.25 확률로 정보 병합, 0.5 확률로 텍스트 dropout, 0.3 확률로 단어 dropout을 적용하였다.&lt;/p>
&lt;p>&lt;strong>Codebook patterns and conditioning.&lt;/strong> 30초의 오디오를 1500개의 autoregressive step으로 변환하는 &amp;ldquo;delay&amp;rdquo; 교차 패턴을 사용한다. 텍스트 조건 부여에는 T5 텍스트 encoder를 사용하며, 필요에 따라 멜로디 조건 부여를 추가한다. FLAN-T5와 CLAP를 실험하고, 각 텍스트 encoder의 성능을 비교하였다. 멜로디 조건 부여에는 chromagram을 계산하고 양자화하는 방법을 사용하였다. 학습 중에는 조건을 일정 확률로 드롭하고, 추론 시에는 가이드 스케일을 적용한다.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>&lt;strong>Training datasets.&lt;/strong> 20K 시간의 라이선스 음악, 내부 데이터셋의 10K 고품질 음악 트랙, 그리고 ShutterStock과 Pond5 음악 데이터 컬렉션을 사용하여 MusicGEN을 학습시킨다. 이 데이터셋들은 모두 텍스트 설명, 장르, BPM, 태그 등의 메타데이터와 함께 32 kHz로 샘플링된 전체 길이의 음악을 포함하며, 오디오는 모노로 다운믹스된다.&lt;/p>
&lt;p>&lt;strong>Evaluation datasets.&lt;/strong> MusicCaps 벤치마크에서 평가하였다. 이 벤치마크는 전문 음악가들이 준비한 5.5K의 샘플과 장르별로 균형을 이루는 1K의 샘플로 구성되어 있다. 균형되지 않은 샘플에서 객관적 지표를 보고하고, 질적 평가를 위해 장르 균형 샘플에서 예제를 추출하였다. 또한, 멜로디 평가와 소거 연구를 위해 학습 세트와 아티스트가 중복되지 않는 528개의 음악 트랙으로 구성된 평가 세트를 사용하였다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>&lt;strong>Baselines.&lt;/strong> Riffusion과 Mousai, 두 가지 text-to-music 생성 모델과 MusicGEN을 비교한다. 오픈소스 Riffusion 모델을 이용해 추론을 실행하고, Mousai의 경우 저자들이 제공한 오픈소스를 이용해 이 연구의 데이터셋으로 모델을 학습시켜 비교하였다. 가능한 경우, MusicLM과 Noise2Music과도 비교하였다.&lt;/p>
&lt;p>&lt;strong>Evaluation metrics.&lt;/strong> 객관적인 지표인 Fréchet Audio Distance(FAD), Kullback-Leiber Divergence(KL), 그리고 CLAP 점수를 이용하여 제안한 방법을 평가한다. FAD 점수는 생성된 오디오의 타당성을 나타내며, KL-Divergence는 원본 음악과 생성된 음악 사이의 레이블 확률을 비교한다. 이때, KL이 낮을수록 생성된 음악이 원본 음악과 유사한 개념을 가지고 있다고 판단한다. 마지막으로, CLAP 점수는 트랙 설명과 생성된 오디오 사이의 정렬을 정량화한다.&lt;/p>
&lt;p>인간 평가자들을 활용하여 overall quality(OVL)과 relevance to the text input(REL)을 평가하는 연구를 진행하였다. 평가자들은 제공된 오디오 샘플의 품질과 텍스트와의 일치도를 각각 1에서 100의 범위로 평가하였다. 이 평가는 Amazon Mechanical Turk 플랫폼에서 모집한 평가자들을 통해 진행되었고, 각 샘플은 최소 5명의 평가자에 의해 평가되었다. 잡음 주석과 이상치는 CrowdMOS 패키지를 통해 필터링하였으며, 모든 샘플은 공정성을 위해 -14dB LUFS에서 정규화되었다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>text-to-music 생성 작업에 대한 제안된 방법의 결과를 제시하며, 이전의 연구와 비교한다. 또한, 멜로디 특징에 기반한 음악 생성 능력을 평가하고, 스테레오 오디오 생성을 위해 코드북 패턴을 확장하는 방법을 설명한다.&lt;/p>
&lt;h3 id="comparison-with-the-baselines">Comparison with the baselines&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table1.png"
width="1132"
height="372"
srcset="https://kurtkim.github.io/p/musicgen/images/table1_hu82e130725237512cebe1a4c4f1872ed9_104459_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table1_hu82e130725237512cebe1a4c4f1872ed9_104459_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;p>제안된 방법과 Mousai, Riffusion, MusicLM, 그리고 Noise2Music와의 비교 한다. Noise2Music과 MusicLM의 공식 구현이 없으므로, 각각의 원고에서 보고된 FAD만을 보고한다. 인간 연구에서는 MusicCaps의 악기만을 사용한 40개 샘플을 사용하였으며, MusicGEN의 chromagram 학습에서 누수를 방지하기 위해 테스트 시간 동안 보류된 세트에서 무작위로 chromagram을 샘플링하였다.&lt;/p>
&lt;p>결과적으로 MusicGEN은 오디오 품질과 텍스트 설명 준수 측면에서 인간 청취자들에게 더 높은 평가를 받았다. Noise2Music은 MusicCaps에서 FAD 측면에서 가장 우수했으며, 텍스트 조건부로 학습된 MusicGEN이 뒤를 이었다. 멜로디 조건을 추가하면 객관적 지표가 저하되지만, 인간 평가에는 큰 영향을 미치지 않았다.&lt;/p>
&lt;p>낮은 평가를 받은 모델에 대해서는 FAD가 주관적 평가와 상관관계가 있지만, 높은 평가를 받은 모델에는 그러지 않았다. 또한, MusicCaps의 많은 샘플이 &amp;ldquo;noisy&amp;rdquo; 녹음이라는 설명을 포함하고 있어, 오디오 품질 향상이 일정 수준을 넘어설 경우 FAD가 악화될 수 있다는 사실을 발견하였다.&lt;/p>
&lt;h3 id="melody-evaluation">Melody evaluation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table2.png"
width="974"
height="192"
srcset="https://kurtkim.github.io/p/musicgen/images/table2_hu84589e5fca9f397abb8c20fca19cbe40_51604_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table2_hu84589e5fca9f397abb8c20fca19cbe40_51604_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="507"
data-flex-basis="1217px"
>&lt;/p>
&lt;p>텍스트와 멜로디를 동시에 고려하는 MusigGEN을 객관적, 주관적 지표로 평가하였다. 이를 위해 새로운 지표인 chroma cosine-similarity를 도입하였으며, 이는 참조 샘플과 생성된 샘플의 chroma 사이의 average cosine-similarity를 측정한다. 또한, 인간 연구를 통해 생성된 음악과 멜로디 사이의 관계를 평가하였다. 결과적으로 MusigGEN은 주어진 멜로디를 따르는 음악을 성공적으로 생성하며, chroma를 떨어뜨려도 성능이 유지되는 것으로 나타났다.&lt;/p>
&lt;h3 id="fine-tuning-for-stereophonic-generation">Fine-tuning for stereophonic generation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/figure2.png"
width="1160"
height="518"
srcset="https://kurtkim.github.io/p/musicgen/images/figure2_hu82a0dfe475666588aef20e9ef9119079_204671_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/figure2_hu82a0dfe475666588aef20e9ef9119079_204671_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;p>스테레오 데이터로의 생성을 확장하려는 실험을 진행하였다. 동일한 EnCodec 토크나이저를 사용하여 왼쪽과 오른쪽 채널에 독립적으로 적용하였고, 사전 학습된 단일 음향 MusicGEN 모델을 스테레오 오디오를 포함하는 데이터셋으로 미세 조정하였다. &amp;ldquo;delay&amp;rdquo; 패턴을 재사용하고, &amp;ldquo;stereo delay&amp;quot;과 &amp;ldquo;stereo partial delay&amp;rdquo; 두 가지 변형을 도입하였다. 이 간단한 전략을 통해 추가적인 계산 비용 없이 스테레오 오디오를 생성할 수 있었다. 스테레오 출력을 모노로 다운믹싱하면, 모노 모델과 거의 동등한 품질을 느낄 수 있었다. 전반적으로 스테레오 오디오가 모노보다 높게 평가되었으며, &amp;ldquo;stereo partial delay&amp;quot;이 &amp;ldquo;stereo delay&amp;quot;에 비해 전반적인 품질과 텍스트 관련성에서 약간의 향상을 보였습니다.&lt;/p>
&lt;h3 id="ablation">Ablation&lt;/h3>
&lt;p>다양한 코드북 패턴의 제거 연구와 모델 크기, 기억 연구 결과를 소개한다. 이 모든 연구는 보류된 평가 세트에서 임의로 샘플링된 30초 분량의 1K 샘플을 사용하여 수행되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table4.png"
width="1022"
height="318"
srcset="https://kurtkim.github.io/p/musicgen/images/table4_huf62e92f64d1bdb73d42bde58b852d6de_88931_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table4_huf62e92f64d1bdb73d42bde58b852d6de_88931_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="321"
data-flex-basis="771px"
>&lt;/p>
&lt;p>&lt;strong>The effect of the codebook interleaving patterns.&lt;/strong> 여러 코드북 패턴을 평가하였다. 이는 &amp;ldquo;delay&amp;rdquo;, &amp;ldquo;partial delay&amp;rdquo;, &amp;ldquo;parallel&amp;rdquo;, &amp;ldquo;coarse first&amp;rdquo;, &amp;ldquo;Partial flattening&amp;rdquo;, &amp;ldquo;flattening&amp;rdquo; 등의 패턴을 포함한다. 이 중 &amp;ldquo;flattening&amp;rdquo; 패턴은 생성을 개선하지만 높은 계산 비용이 들며, 간단한 &amp;ldquo;delay&amp;rdquo; 방식을 사용하면 비슷한 성능을 더 적은 비용으로 달성할 수 있음을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/musicgen/images/table5.png"
width="1076"
height="196"
srcset="https://kurtkim.github.io/p/musicgen/images/table5_hud8b26f41b0e31e8e2f3e6f87f76a2efe_54637_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/musicgen/images/table5_hud8b26f41b0e31e8e2f3e6f87f76a2efe_54637_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="548"
data-flex-basis="1317px"
>&lt;/p>
&lt;p>&lt;strong>The effect of model size.&lt;/strong> 다양한 모델 크기(300M, 1.5B, 3.3B parameter)에 대한 결과를 보고하였다. 모델 크기를 크게하면 성능이 향상되지만, 학습과 추론 시간이 더 길어진다. 주관적인 품질은 1.5B에서 최적이며, 더 큰 모델은 텍스트 프롬프트를 더 잘 이해한다.&lt;/p>
&lt;p>&lt;strong>Memorization experiment.&lt;/strong> 우리는 MusicGEN의 기억능력을 분석하였다. 학습 세트에서 20,000개의 예제를 무작위로 선택하고, 각각에 대해 원본 오디오에 해당하는 프롬프트를 모델에 입력하였다. greedy decoding을 사용하여 5초 길이의 오디오를 생성하고, 생성된 오디오와 원본 오디오가 일치하는 비율을 보고하였다. 또한, 80% 이상 일치하는 경우의 비율도 보고하였다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>&lt;strong>Audio representation.&lt;/strong> 최근 연구는 음악 신호를 압축된 표현으로 변환하고 이를 기반으로 생성 모델을 적용하는 방식을 주로 사용하고 있다. Lakhotia et al. 은 k-means를 사용한 음성 표현의 양자화를, Défossez et al. 과 Zeghidour et al. 은 residual vector quantization를 사용한 원시 파형에 대한 VQ-VAE 적용을 제안하였다. 이러한 방법들은 텍스트에서 오디오로의 생성에 활용되고 있다.&lt;/p>
&lt;p>&lt;strong>Music generation.&lt;/strong> 음악 생성은 다양한 방법으로 연구되어 왔다. Dong et al. 은 GAN을 사용한 심볼symbolic릭 음악 생성을, Bassan et al. 은 symbolic 음악의 비지도학습 분할을 제안하였다. Ycart et al. 은 RNN을 이용한 polyphonic 음악 모델링을, Ji et al. 은 음악 생성에 대한 딥러닝 방법들을 포괄적으로 조사하였다.&lt;/p>
&lt;p>Dhariwal et al. 은 hierarchical VQ-VAE를 사용하여 음악 샘플을 discrete representation으로 변환하고, 이를 통해 음악을 생성하는 방법을 제안하였다. Gan et al. 은 주어진 비디오에 대한 음악을 생성하면서 미디 노트를 예측하는 방법을, Agostinelli et al. 은 의미 토큰과 음향 토큰을 사용하여 음악을 표현하는 방법을 제안하였다. Donahue et al. 은 이러한 접근법을 노래 동반 생성 작업에 적용하였다.&lt;/p>
&lt;p>diffusion 모델을 사용하는 것은 대안적인 접근법이다. Schneider et al., Huang et al., Maina, Forsgren and Martiros는 이를 텍스트에서 음악으로 변환하는 작업에 적용하였다. Schneider et al.과 Huang et al. 은 오디오 생성과 샘플링 비율 증가에 diffusion 모델을 사용하였다. Forsgren et al. Martiros는 5초 오디오 세그먼트 생성과 장기 시퀀스 생성을 위해 spectrogram을 이용한 diffusion 모델을 미세조정하였다.&lt;/p>
&lt;p>&lt;strong>Audio generation.&lt;/strong> 텍스트에서 오디오로 변환하는 연구가 다양하게 진행되었다. Yang et al. 은 오디오 spectrogram을 VQ-VAE를 이용해 표현하고, 이를 기반으로 텍스트 CLIP 임베딩에 조건화된 diffusion 모델을 적용하였다. Kreuk et al. 과 Sheffer와 Adi는 각각 transformer 언어 모델과 이미지-오디오 생성을 위한 접근법을 제안하였다. 또한, Huang et al. 과 Liu et al. 은 텍스트-오디오 작업을 위해 latent diffusion 모델을 사용하면서 이를 다양한 작업에 확장하는 방법을 제안하였다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>텍스트와 멜로디에 따라 조절 가능한 음악 생성 모델인 MusicGEN을 소개하였다. 이 모델은 단순한 코드북 교차 전략을 통해 고품질의 음악 생성을 가능하게 하고, autoregressive 시간 단계를 줄일 수 있다. 또한 모델 크기, 조건화 방법, 텍스트 전처리 기법의 영향에 대한 포괄적인 연구를 제공하며, 생성된 오디오의 멜로디를 제어하는 chromagram 기반 조건화를 소개하였다.&lt;/p>
&lt;p>&lt;strong>Limitations&lt;/strong> 이 모델의 생성 방법은 조건에 따른 세밀한 제어를 허용하지 않아 CF guidance에 주로 의존한다. 텍스트 조건에 대한 데이터 augmentation은 상대적으로 간단하지만, 오디오 조건에 대한 데이터 augmentation와 guidance에 대해 추가 연구가 필요하다.&lt;/p>
&lt;p>&lt;strong>Broader impact.&lt;/strong> 대규모 생성 모델은 윤리적 도전을 제시한다. 모든 학습 데이터가 권리 소유자와 합의 하에 이루어지며, 데이터셋의 다양성 부족 문제를 인지하고 있다. 이 문제를 해결하기 위해 단순화된 모델을 사용하여 새로운 데이터셋에 대한 응용을 확대하고 있다. 또한, 이러한 모델이 아티스트에 대한 불공정한 경쟁을 일으킬 수 있음을 인지하며, 이 문제를 해결하기 위해 열린 연구를 통해 모든 참가자가 모델에 동등하게 접근할 수 있도록 노력하고 있다. 고급 제어 기법을 통해, 모델이 음악 애호가와 전문가 모두에게 도움이 될 수 있도록 하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2306.05284.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/audiocraft" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>CLAP</title><link>https://kurtkim.github.io/p/clap/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/clap/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 연구에서는 contrastive learning을 통해 자연어 설명과 오디오 데이터를 결합한 오디오 표현을 개발하는 방법을 제안한다. 이를 위해 633,526개의 오디오-텍스트 쌍을 모은 LAION-Audio-630K를 공개하고, 이를 활용해 오디오와 텍스트를 처리하는 모델을 구축하였다. 이 모델은 텍스트-오디오 검색에서 우수한 성능을 보였고, zero-shot 오디오 분류에서는 state-of-the-art를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>오디오는 텍스트, 이미지와 더불어 중요한 정보 유형이지만, 세부적인 주석이 필요한 오디오 작업은 데이터 수집이 노동 집약적이어서 사용 가능한 데이터가 제한적이다. 이 때문에 많은 감독 없이 다양한 오디오 작업에 적합한 효과적인 오디오 표현을 만드는 것은 어려운 과제이다.&lt;/p>
&lt;p>Contrastive learning 방식은 인터넷에서 모아진 대규모 노이즈 데이터로 모델을 학습시키는데 효과적이다. 최근 제안된 Contrastive Language-Image Pretraining (CLIP) 방식은 텍스트와 이미지를 shared latent space에 투영하여 학습한다. 이 방식은 데이터 주석에 제약받지 않으며, ImageNet 데이터셋의 변형에 대해 zero-shot 설정에서 높은 정확도를 보여준다. 또한, 오디오와 자연어도 중복 정보를 포함하며, 이를 통해 crossmodal 정보의 오디오 표현을 형성할 수 있다. 이러한 모델 학습은 쌍으로 된 오디오와 텍스트 데이터만을 필요로 하므로 수집이 상대적으로 쉽다.&lt;/p>
&lt;p>최근 연구들은 텍스트-오디오 검색 작업에 대한 대조적(contrastive) 언어-오디오 사전 학습 모델을 제안하였다. 일부 연구는 오디오 encoder로 Pretrained Audio Neural Network (PANN)을, 텍스트 encoder로 BERT를 사용하며, 다른 연구는 성능 향상을 위해 HTSAT와 RoBERTa를 추가로 앙상블하였다. 또한, AudioClip과 WaveCLIP과 같은 연구들은 이미지-오디오(또는 이미지-오디오-언어) 사전 학습 모델에 초점을 맞추었다. 이러한 모든 모델들은 오디오 도메인에서의 대조적 학습에 큰 잠재력을 보여주고 있다.&lt;/p>
&lt;p>현재의 언어-오디오 대조적 학습 연구들은 전체적인 강점을 아직 다 보여주지 못하였다. 모델들은 대부분 작은 데이터셋에서 학습되었고, 오디오/텍스트 encoder의 선택과 hyperparameter 설정에 대한 충분한 조사가 없었다. 또한, 모델들은 다양한 길이의 오디오를 처리하는 데 어려움을 겪었으며, 텍스트-오디오 검색에만 초점을 맞추고 downstream task에서의 오디오 표현을 평가하지 않았다. 이러한 문제점들을 해결하고 더 많은 하downstream task에 대한 일반화 능력을 발견하는 것이 필요하다.&lt;/p>
&lt;p>이 논문에서는 이전 연구를 바탕으로, 위의 문제점들을 개선하기 위해 데이터셋, 모델 설계, 실험 설정에 대한 기여를 한다:&lt;/p>
&lt;ul>
&lt;li>이 논문에서는 633,526개의 오디오-텍스트 쌍을 포함하는 현재 가장 큰 공개 오디오 캡션 데이터셋인 LAION-Audio-630K를 공개하고, 학습 과정을 돕기 위해 키워드-캡션 모델을 활용해 AudioSet의 레이블을 캡션으로 확장하였다. 이 데이터셋은 다른 오디오 작업에도 활용될 수 있다.&lt;/li>
&lt;li>이 논문에서는 대조적 언어-오디오 사전 학습 파이프라인을 구축하고, 이를 위해 두 개의 오디오 인코더와 세 개의 텍스트 인코더를 선택하였다. 또한, 성능 향상과 variable-length inputs 처리를 위해 feature fusion mechanism을 활용하였다.&lt;/li>
&lt;li>이 논문에서는 텍스트-오디오 검색과 zero-shot 및 지도 오디오 분류와 같은 downstream task에 대한 모델의 포괄적인 실험을 수행하였다. 데이터셋의 확장, keyword-to-caption augmentation, feature fusion이 모델 성능을 향상시키는 데 도움이 된다는 것을 보여주었다. 이를 통해 텍스트-오디오 검색과 오디오 분류 작업에서 state-of-the-art를 달성하였다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="laion-audio-630k-and-training-dataset">LAION-Audio-630K And Training Dataset&lt;/h2>
&lt;h3 id="laion-audio-630k">LAION-Audio-630K&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table1.png"
width="664"
height="206"
srcset="https://kurtkim.github.io/p/clap/images/table1_huc915c892c5fa79f7a24207ab27da5b09_35590_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table1_huc915c892c5fa79f7a24207ab27da5b09_35590_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="322"
data-flex-basis="773px"
>&lt;/p>
&lt;p>LAION-Audio-630K는 총 4,325.39시간에 걸친 633,526쌍의 오디오-텍스트 데이터셋을 수집하였다. 이 데이터셋은 사람의 활동, 자연 소리, 오디오 효과 등을 포함하며, 공개적으로 사용 가능한 여러 웹사이트에서 수집하였다. 현재로서는 LAION-Audio-630K가 공개적으로 이용 가능한 가장 큰 오디오-텍스트 데이터셋이다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>이 논문에서는 모델 성능이 데이터셋의 크기와 유형에 따라 어떻게 변화하는지 테스트하기 위해, 세 가지 학습 세트 설정을 사용하였다. 이들 설정은 AudioCaps+Clotho (약 55K 샘플), LAION-Audio-630K (약 630K 샘플), Audioset (1.9 백만 오디오 샘플)을 포함하며, 모든 중복 데이터는 제외하였다.&lt;/p>
&lt;h3 id="dataset-format-and-preprocessing">Dataset Format and Preprocessing&lt;/h3>
&lt;p>이 작업에서 사용된 모든 오디오 파일은 48kHz sample rate의 mono channel로 전처리되었다. 레이블만 있는 데이터셋의 경우, 템플릿이나 키워드-캡션 모델을 사용해 레이블을 캡션으로 확장하였다. 이를 통해 대조적 언어-오디오 사전 학습 모델의 학습에 더 많은 데이터를 활용할 수 있게 되었고, 총 오디오 샘플 수는 2.5M개로 증가하였다.&lt;/p>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;h3 id="contrastive-language-audio-pretraining">Contrastive Language-Audio Pretraining&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/figure1.png"
width="680"
height="972"
srcset="https://kurtkim.github.io/p/clap/images/figure1_huef2bf2bcce39fef884bc41272d03fdcb_166045_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/figure1_huef2bf2bcce39fef884bc41272d03fdcb_166045_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="69"
data-flex-basis="167px"
>&lt;/p>
&lt;p>오디오 데이터 $X_i^a$와 텍스트 데이터 $X_i^t$의 입력을 각각 처리하기 위해 두 개의 encoder를 사용한다. 여기서 $(X_i^a, X_i^t)$는 $i$로 색인된 오디오-텍스트 쌍 중 하나이다. 오디오 임베딩 $E_i^a$와 텍스트 임베딩 $E_i^t$는 각각 오디오 encoder $\mathbf{f}_{audio}(\cdot)$와 텍스트 encoder $\mathbf{f}_{text}(\cdot)$에 의해 얻어지며, projection layer를 사용한다:&lt;/p>
&lt;p>$$ E_i^a = \text{MLP}_{audio}(\mathbf{f}_{audio}(X_i^a)) $$&lt;/p>
&lt;p>$$ E_i^t = \text{MLP}_{text}(\mathbf{f}_{text}(X_i^t)) $$&lt;/p>
&lt;p>오디오/텍스트 projection layer는 2-layer multilayer perceptron(MLP)이다. 이는 ReLU activation function을 사용하여 encoder ouptput을 동일한 차원 $D$로 매핑한다(i.e., $E_i^a, E_i^t \in \mathbb{R}^D$). 이로 인해 오디오 데이터와 텍스트 데이터의 관계를 더 잘 파악할 수 있다.&lt;/p>
&lt;p>이 모델은 오디오와 텍스트 임베딩 간의 contrastive learning을 통해 학습되며, 이 때 다음의 loss function를 사용한다:&lt;/p>
&lt;p>$$ \mathbf{L} = {{1}\over{2N}} \sum_{i=1}^N (log {{exp(E_i^a \cdot E_i^t / \gamma)}\over{\sum_{j=1}^N exp(E_i^a \cdot E_j^t / \gamma)}} + log {{exp(E_i^t \cdot E_a^t / \gamma)}\over{\sum_{j=1}^N exp(E_i^t \cdot E_j^a / \gamma)}})$$&lt;/p>
&lt;p>$\gamma$는 손실을 조정하는 학습 가능한 parameter이며, 로그항은 오디오-텍스트 또는 텍스트-오디오 변환을 고려한다. $N$은 일반적으로 데이터 수를 나타내지만, 효율적인 학습을 위해 학습 시 배치 크기로 사용된다.&lt;/p>
&lt;p>학습된 모델의 임베딩$(E^a, E^b)$은 다양한 문맥에서 활용되어, 각 작업에 따른 성능을 향상시키는 데 도움을 준다.&lt;/p>
&lt;h3 id="downstream-tasks-in-inference-stage">Downstream Tasks in Inference Stage&lt;/h3>
&lt;p>&lt;strong>Text-to-Audio Retrieval&lt;/strong> target 오디오 임베딩 $E_p^a$는 cosine similarity 함수를 사용하여 $M$개의 텍스트 임베딩 $E^t = \lbrace E_1^t, &amp;hellip;, E_M^t \rbrace$중에서 가장 가까운 텍스트 임베딩 $E_q^t$를 찾아, 가장 잘 매치되는 텍스트를 결정할 수 있다. 이는 오디오와 텍스트 간의 가장 적합한 대응을 찾는데 사용된다.&lt;/p>
&lt;p>&lt;strong>Zero-shot Audio Classiﬁcation&lt;/strong> $M$개의 오디오 클래스 $C = \lbrace C_1, &amp;hellip;, C_M \rbrace$에 대해, $M$개의 프롬프트 텍스트 $X^t = \lbrace X_1^t, &amp;hellip;, X_M^t \rbrace$를 구성하고, 주어진 오디오 $X_p^a$에 대해 코사인 유사도를 통해 $X^t$중에서 가장 최적의 매치 $X_q^t$를 찾는다. 이 방법의 장점은 오디오 카테고리가 제한되지 않고, 분류 작업을 텍스트-오디오 검색 작업으로 변환할 수 있다는 점이다.&lt;/p>
&lt;p>&lt;strong>Supervised Audio Classiﬁcation&lt;/strong> 모델 학습 후, 주어진 오디오 $X_p^a$의 임베딩 $E_p^a$은 projection layer를 추가하고 미세조정하여 고정 카테고리 분류 작업으로 매핑될 수 있다.&lt;/p>
&lt;h3 id="audio-encoders-and-text-encoders">Audio Encoders and Text Encoders&lt;/h3>
&lt;p>PANN과 HTSAT 두 모델을 오디오 encoder로 선택하였다. PANN은 CNN 기반, HTSAT은 transformer 기반 모델이며, 둘 다 바로 앞(penultimate) layer의 output $L$을 MLP layer으로 보낸다다. 각각의 output 차원 $L_{PANN} = 2048$, $L_{HTSAT} = 768$이다.&lt;/p>
&lt;p>텍스트 encoder로 CLIP transformer, BERT, RoBERTa를 선택하였다다. output 차원은 각각 $L_{CLIP} = 512$, $L_{BERT} = 768$, $L_{RoBERTa} = 768$이며, 오디오와 텍스트 output을 모두 512 차원으로 매핑하기 위해 2layer MLP를 적용하였다.&lt;/p>
&lt;h3 id="feature-fusion-for-variable-length-audio">Feature Fusion for Variable-Length Audio&lt;/h3>
&lt;p>오디오는 길이가 가변적인 특성을 가지기 때문에, 전체 오디오를 인코더에 입력하고 임베딩의 평균을 출력하는 전통적인 방식은 계산 효율이 떨어진다. 따라서 대략적인 전역 정보와 랜덤 샘플링된 지역 정보를 결합하여 다양한 길이의 오디오에 대해 일정한 계산 시간 내에서 학습하고 추론한다.&lt;/p>
&lt;p>$T$초의 오디오와 ﬁxed chunk duration $d = 10$초에 대해:&lt;/p>
&lt;ul>
&lt;li>$T \leq d$인 경우: 먼저 입력을 반복한 다음, 그것을 0값으로 채운다. 예를 들어, 3초의 입력은 $3 \times 3 = 9$초로 반복되고, 1초의 0 값으로 패딩된다. 이 방식은 짧은 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다.&lt;/li>
&lt;li>$T &amp;gt; d$인 경우: 먼저 입력을 $T$에서 $d$초로 다운샘플링하여 전역 입력으로 사용한다. 그런 다음 입력의 앞 ${1}\over{3}$, 중간 ${1}\over{3}$, 뒤 ${1}\over{3}$에서 각각 무작위로 $d$초 클립을 슬라이스하여 지역 입력으로 사용한다. 우리는 이 $4 \times d$ 입력들을 오디오 encoder의 첫 번째 layer로 보내어 초기 특징을 얻고, 그런 다음 세 개의 지역 특징들이 시간 축에서 3-stride를 가진 다른 2D-Convolution 계층에 의해 하나의 특징으로 변환된다. 마지막으로, 지역 특징 $X_{local}^a$와 전역 특징 $X_{global}^a$는 다음과 같이 결합된다:&lt;/li>
&lt;/ul>
&lt;p>$$ X_{fusion}^a = \alpha X_{global}^a + (1 - \alpha)X_{local}^a $$&lt;/p>
&lt;p>여기서 $\alpha = \mathbf{f}_{AFF}(X_{global}^a, X_{local}^a)$는 두 입력의 결합 요소를 학습하기 위한 두 개의 분기를 가진 CNN 모델인 attention feature fusion (AFF)에 의해 얻어진 요소이다. &amp;ldquo;slice &amp;amp; vote&amp;rdquo; 방법과 비교하여, 특징 결합은 첫 몇 개의 layer에서만 오디오 슬라이스를 처리하기 때문에 학습 시간을 절약한다. 이 방식은 긴 오디오 입력에 대해 효과적으로 처리할 수 있게 해준다.&lt;/p>
&lt;h3 id="keyword-to-caption-augmentation">Keyword-to-Caption Augmentation&lt;/h3>
&lt;p>일부 데이터셋에서는 오디오에 대응하는 키워드로 레이블이나 태그를 사용한다. 이러한 키워드를 바탕으로 사전 학습된 언어 모델 T5를 사용하여 캡션을 생성하며, output 문장에서 편향을 제거하는 후처리를 진행한다. 예를 들어, &amp;ldquo;여성&amp;quot;과 &amp;ldquo;남성&amp;quot;을 &amp;ldquo;사람&amp;quot;으로 교체하여 성별 편향을 제거한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>제안한 모델에 대해 세 가지 실험을 수행한다. 다양한 encoder를 사용해 최적의 조합을 찾고, 다양한 데이터셋 크기에서 특징 결합과 keyword-to-caption augmentation을 적용해 효과를 검증하며, 오디오-텍스트 검색과 텍스트-오디오 검색에서의 성능을 평가한다. 마지막으로 최적의 모델로 zero-shot과 지도 오디오 분류 실험을 수행한다.&lt;/p>
&lt;h3 id="hyperparameters-and-training-details">Hyperparameters and Training Details&lt;/h3>
&lt;p>AudioCaps, Clotho, LAIONAudio-630K, AudioSet 데이터셋을 사용해 모델을 학습시킨다. 오디오 데이터는 10-second input length, 480 hop size, 1024 window size, 64 mel-bins으로 처리하며, 입력은 $(T = 1024, F = 64)$의 형태를 가진다. 텍스트 데이터는 최대 토큰 길이를 77로 토큰화한다.&lt;/p>
&lt;p>10초보다 긴 오디오는 무작위로 10초 세그먼트로 분할한다. 학습 중에는 $\beta_1 = 0.99, \beta_2 = 0.9$의 Adam optimizer를 사용하고, warm-up과 learning rate $10^{−4}$의 cosine learning rate decay를 사용한다. AudioCaps+Clotho 데이터셋에서는 batch size를 768로, LAION-Audio-630K를 포함하는 학습 데이터셋에서는 2304로, AudioSet을 포함하는 학습 데이터셋에서는 4608로 설정하여 모델을 학습시킵니다. 모델은 총 45 에포크 동안 학습된다.&lt;/p>
&lt;h3 id="text-to-audio-retrieval">Text-to-Audio Retrieval&lt;/h3>
&lt;p>&lt;strong>Audio and Text Encoders&lt;/strong> 텍스트-오디오 검색을 위해 가장 적합한 오디오와 텍스트 encoder를 찾기 위해 실험을 진행하였다. 이를 위해 두 오디오 encoder와 세 텍스트 encoder를 결합하고, AudioCaps와 Clotho 데이터셋에서 학습을 진행하였다. 이 실험의 목표는 최적의 encoder 조합을 찾는 것이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table2.png"
width="592"
height="214"
srcset="https://kurtkim.github.io/p/clap/images/table2_hueb35c23a5ca995604e43931c9ca5897a_39395_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table2_hueb35c23a5ca995604e43931c9ca5897a_39395_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="663px"
>&lt;/p>
&lt;p>HTSAT 오디오 encoder는 PANN보다 더 좋은 성능을 보이고, 텍스트 encoder는 RoBERTa가 BERT보다 우수하며, CLIP transformer는 가장 성능이 낮다. 또한, RoBERTa는 overfit이 덜 발생하지만, CLIP transformer는 overfit이 많아 일반화 성능이 낮다.&lt;/p>
&lt;p>&lt;strong>Dataset Scale&lt;/strong> HTSAT-RoBERTa 모델을 사용하여 텍스트-오디오 검색 실험을 수행하였다. 데이터셋 크기를 점차 늘렸지만, &amp;ldquo;AudioCaps + Clotho&amp;quot;에서 &amp;ldquo;LA.&amp;ldquo;로 확대해도 AudioCaps의 성능은 개선되지 않았다. 하지만 Clotho 세트에서의 성능은 향상되었다. 이는 AudioCaps가 사전 학습된 AudioSet와 유사한 오디오를 포함하고 있기 때문이며, 다른 출처의 데이터를 더 많이 받게 되면, 모델의 일반화는 증가하지만 AudioSet 데이터의 분포에서 벗어나게 된다. 따라서, AudioCaps의 성능은 떨어지지만, Clotho의 성능은 향상되었다. 이는 다양한 오디오 유형 간의 성능 유지에 대한 타협을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table3.png"
width="1358"
height="282"
srcset="https://kurtkim.github.io/p/clap/images/table3_hu33d594c9c09d66373764e22f05571ae3_107352_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table3_hu33d594c9c09d66373764e22f05571ae3_107352_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="481"
data-flex-basis="1155px"
>&lt;/p>
&lt;p>&lt;strong>Keyword-to-Caption and Feature Fusion&lt;/strong> feature fusion mechanism과 keyword-to-caption augmentation를 모델에 추가하면 성능이 향상된다. Clotho 데이터셋에서는 특히 효과적이다. AudioSet을 학습 세트에 추가하면 AudioCaps의 성능은 증가하지만 Clotho에서는 감소하는 것을 확인할 수 있다. 이는 AudioCaps와 Clotho 간의 성능 타협을 재확인한다. 또한, keyword-to-caption augmentation는 대부분의 지표에서 단순 템플릿 텍스트 프롬프팅보다 더 나은 성능을 보인다.&lt;/p>
&lt;p>최적 모델은 텍스트-오디오 검색에서 대부분의 지표에서 이전 방법보다 우수하며, 특히 AudioCaps에서 36.7%, Clotho에서 18.2%의 결과를 보여주었다. 대규모 데이터셋에서의 학습과 feature fusion은 모델 성능을 효과적으로 개선시킨다는 것을 입증하였다.&lt;/p>
&lt;h3 id="zero-shot-and-supervised-audio-classiﬁcation">Zero-shot and Supervised Audio Classiﬁcation&lt;/h3>
&lt;p>&lt;strong>Zero-shot Audio Classiﬁcation&lt;/strong> 모델의 일반화와 견고성을 평가하기 위해, 세 가지 주요 모델에 대해 zero-shot 오디오 분류 실험을 수행하였다. 이 모델들은 ESC50, VGGSound, Urbansound8K 데이터셋에서 평가되었고, top-1 정확도를 지표로 사용했다. &amp;ldquo;This a sound of label.&amp;rdquo; 형식의 텍스트 프롬프트를 사용하여 오디오를 분류하였다. 학습 데이터와 테스트 데이터셋간에 겹치는 부분은 제외하고 평가를 진행하였다.&lt;/p>
&lt;p>&lt;strong>Supervised Audio Classiﬁcation&lt;/strong> FSD50K와 VGGSound 데이터셋에서 오디오 encoder를 미세 조정하여 지도 학습 오디오 분류를 수행하였다. ESC50와 Urbansound8K는 데이터 유출 문제로 인해 실험을 수행하지 않았다. FSD50K 평가에는 mAP를 지표로 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/clap/images/table4.png"
width="664"
height="280"
srcset="https://kurtkim.github.io/p/clap/images/table4_hue3d9dda28790c2573ee91b7137dfa0ae_55869_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/clap/images/table4_hue3d9dda28790c2573ee91b7137dfa0ae_55869_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>세 가지 데이터셋에서 zero-shot 오디오 분류의 state-of-the-art를 보여주며, 이는 보이지 않는 데이터에 대한 모델의 높은 일반화 능력을 입증한다. feature fusion mechanism과 keyword-to-caption augmentation은 모델 성능을 향상시키는 데 기여하며, 우리의 지도 학습 오디오 분류 결과는 VGGSound에서 최고 성능을, FSD50K에서는 가장 가까운 성능을 보여주었다. 이 결과는 제안된 모델이 효과적인 오디오 표현을 학습한다는 것을 확인한다.&lt;/p>
&lt;h3 id="conclusion-and-futrue-work">Conclusion And Futrue Work&lt;/h3>
&lt;p>이 논문에서는 대규모 오디오-텍스트 데이터셋을 제안하고 언어-오디오 contrastive learning 패러다임을 개선하였다. LAION-Audio-630, keyword-tocaption augmentation가 있는 AudioSet, 그리고 feature fusion이 오디오 이해와 작업 성능을 향상시키며 가변 길이 데이터에서의 효과적인 학습을 가능하게 함을 보여주었다. 미래 연구는 더 큰 학습 데이터 수집과 오디오 합성, 분리 등의 downstream task 적용을 고려하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2211.06687.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/LAION-AI/CLAP" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>