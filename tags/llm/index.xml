<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on K2H Blog</title><link>https://kurtkim.github.io/tags/llm/</link><description>Recent content in LLM on K2H Blog</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Mon, 29 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>OPT</title><link>https://kurtkim.github.io/p/opt/</link><pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/opt/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>대규모 언어 모델들은 높은 계산 비용 때문에 복제하기 어렵다. 이를 해결하기 위해, Open Pre-trained Transformers (OPT)를 제시한다. 이는 125M에서 175B의 parameter 범위를 가진 사전 학습된 transformer 모델들을 포함하며, 이들은 완전하게 그리고 책임감 있게 관심 있는 연구자들과 공유될 것이다. OPT-175B는 GPT-3와 비교할 수 있으나, 개발하는 데 필요한 탄소 발자국은 1/7밖에 되지 않는다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 텍스트 컬렉션에 학습된 거대 언어 모델은 텍스트 생성 및 zero-shot, few-shot 학습 등 놀라운 기능을 보여준다. 그러나 현재로서는 완전한 모델 접근이 풍부한 자원을 가진 몇몇 연구소에만 제한되어 있다. 이 제한된 접근은 대형 언어 모델이 어떻게 그리고 왜 작동하는지 연구하는 능력을 제한하고, 견고성, 편향, 독성 등의 문제를 개선하는 데 있어 진전을 방해하고 있다.&lt;/p>
&lt;p>125M에서 175B parameter 범위의 decoder 기반 사전 학습된 transformer인 Open Pretrained Transformers (OPT)를 소개하고 있다. OPT 모델은 GPT-3 계열 모델의 성능과 크기를 대략 맞추도록 학습되었으며, 최신 데이터 수집 및 효율적 학습 방법을 적용하였다. 이 모델은 대규모 연구를 가능하게 하고, 거대 언어 모델의 영향력을 연구하는 다양한 의견을 수렴하기 위해 개발되었다. risk, harm, bias, toxicity 등의 정의는 연구 커뮤니티 전체가 공동으로 명시해야 하며, 이는 모델들이 연구에 사용 가능할 때만 가능하다.&lt;/p>
&lt;p>125M부터 66B parameter 사이의 모든 모델을 공개하며, 요청에 따라 OPT-175B에 대한 연구 접근 권한을 제공한다. 학계 연구자, 정부 및 학계의 조직, 산업 연구소에 접근 권한이 부여된다. 모델 생성 로그북과 OPT-175B를 992개의 80GB A100 GPU에서 학습시키는 데 사용된 코드베이스인 metaseq도 공개된다. 이를 통해, 우리는 GPT-3의 탄소 발자국의 1/7만큼의 에너지를 사용해 OPT-175B를 개발할 수 있었다. 이는 큰 성과이지만, 이렇게 큰 모델을 만드는 에너지 비용은 중요하며, 이를 계속 복제하면 LLM들의 컴퓨팅 발자국이 계속 증가할 것이다.&lt;/p>
&lt;p>전체 AI 커뮤니티가 책임있는 AI와 LLM 사용에 대한 명확한 지침을 개발하기 위해 협력해야한다고 생각한다. 더 넓은 AI 커뮤니티가 이 모델에 접근하고 재현 가능한 연구를 수행하여 전체 필드를 발전시키는 것이 필요하다. OPT-175B와 작은 규모의 기준선 출시를 통해, 이러한 기술의 윤리적 고려사항에 대한 다양한 의견을 더욱 들을 수 있을 것을 기대한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;h3 id="models">Models&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table1.png"
width="596"
height="416"
srcset="https://kurtkim.github.io/p/opt/images/table1_hufcbf75edd82a2c14e8b21e79a0243c1b_69598_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table1_hufcbf75edd82a2c14e8b21e79a0243c1b_69598_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;p>125M 개에서 175B 개의 parameter를 가진 8개의 transformer 언어 모델 결과를 제시한다. 모델과 hyperparameter는 학습의 안정성을 위해 대부분 Brown et al. (2020)의 연구를 따르며, 배치 크기 조정은 주로 계산 효율성 향상을 위한 것이다.&lt;/p>
&lt;h3 id="training-setup">Training Setup&lt;/h3>
&lt;p>가중치 초기화는 평균 0, 표준 편차 0.006의 정규 분포를 사용하며, Megatron-LM 코드베이스의 설정을 따른다. 출력 layer의 표준 편차는 총 layer 수에 따라 조정되고, 모든 편향 항은 0으로 초기화된다. 모든 모델은 ReLU 활성화 함수를 사용하며, 시퀀스 길이는 2048로 설정하여 학습된다.&lt;/p>
&lt;p>AdamW optimizer를 사용하며, 이때 ($\beta_1$, $\beta_2$)는 (0.9, 0.95)로 설정하고, weight decay는 0.1이다. linear learning rate schedule을 따라, OPT-175B에서 첫 2000단계 동안 0에서 maximum learning rate까지 상승하고, 작은 기준선에서는 375M 토큰 동안 상승 후, 300B 토큰 동안 maximum learning rate의 10%로 감소시킨다. 학습 과정 중에 learning rate을 몇 번 변경하였으며, 배치 크기는 모델 크기에 따라 0.5M에서 4M까지 설정하고 학습 과정 동안 일정하게 유지한다.&lt;/p>
&lt;p>전반적으로 0.1의 드롭아웃을 사용하며, 임베딩에는 드롭아웃을 적용하지 않는다. gradient norm은 일반적으로 1.0에서 제한하나, 중간에 몇 번 1.0에서 0.3으로 줄여야 하는 경우가 있었다. 또한, gradient를 계산할 때 오버플로우/언더플로우 위험을 줄이기 위해 gradient across all rank를 사용하였다.&lt;/p>
&lt;h3 id="pre-training-corpus">Pre-training Corpus&lt;/h3>
&lt;p>사전 학습 코퍼스는 RoBERTa, Pile, 그리고 PushShift.io Reddit에서 사용된 데이터셋을 결합한 것을 포함한다. 이 코퍼스는 대부분 영어 텍스트이지만, CommonCrawl을 통해 비영어 데이터도 일부 포함되어 있다.&lt;/p>
&lt;p>모든 데이터셋에서 중복된 문서를 제거하기 위해, Jaccard 유사도가 .95 이상인 문서를 MinhashLSH를 통해 필터링하였다. 특히 Pile 데이터셋에서는 중복 문서가 많이 발견되어, 이를 사용하는 연구자들에게 추가적인 중복 제거 처리를 권장한다.&lt;/p>
&lt;p>모든 코퍼스를 GPT-2 byte level BPE 토크나이저를 사용하여 토큰화한다. 최종 코퍼스는 대략 180B 토큰을 포함하고 있다.&lt;/p>
&lt;p>&lt;strong>RoBERTa&lt;/strong> RoBERTa 코퍼스의 BookCorpus와 Stories 하위 집합을 포함시키고, 2021년 9월 28일까지 크롤링된 뉴스 기사를 포함한 업데이트된 CCNews를 사용하였다. 이 코퍼스는 원래 RoBERTa CCNews와 같은 방식으로 전처리 되었다.&lt;/p>
&lt;p>&lt;strong>The Pile&lt;/strong> Pile의 일부 하위 집합인 CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO, 그리고 Wikipedia를 포함시켰다. 그러나 gradient norm의 급증을 초래하는 경향이 있어 불안정성을 높이는 Pile의 다른 하위 집합은 제외하였다. 모든 하위 집합은 추가적인 ad-hoc 공백 정규화를 거쳤다.&lt;/p>
&lt;p>&lt;strong>PushShift.io Reddit&lt;/strong> Baumgartner et al. (2020)이 생성하고 Roller et al. (2021)이 이전에 사용한 Pushshift.io 코퍼스의 일부를 포함시켰다. 대화 트리를 언어 모델이 접근 가능한 문서로 변환하기 위해, 우리는 각 스레드에서 가장 긴 댓글 체인을 추출하고 트리의 모든 다른 경로를 제거하였다. 이로 인해 코퍼스는 약 66% 감소했다.&lt;/p>
&lt;h3 id="training-efﬁciency">Training Efﬁciency&lt;/h3>
&lt;p>완전히 분할된 데이터 병렬과 Megatron-LM Tensor 병렬성을 활용하여 992개의 80GB A100 GPU에서 OPT-175B를 학습시켰다. 이로써 GPU 당 최대 147 TFLOP/s의 이용률을 달성하였다. 모든 호스트에서 Adam 상태를 분할하여 FP32로 유지하고, 모델 가중치는 FP16으로 유지하였다. 언더플로우를 방지하기 위해 동적 손실 스케일링을 사용하였다.&lt;/p>
&lt;h3 id="training-processes">Training Processes&lt;/h3>
&lt;p>&lt;strong>Hardware Failures&lt;/strong> OPT-175B 학습 도중에는 컴퓨팅 클러스터에서 상당한 수의 하드웨어 실패가 발생하였다. 총 2달 동안 하드웨어 실패로 인해 최소 35번의 수동 재시작이 이루어졌으며, 100개 이상의 호스트가 교체되었다. 수동 재시작 시에는 학습이 일시 중단되고, 문제가 있는 노드를 탐지하기 위해 일련의 진단 테스트가 수행되었다. 이후 문제가 있는 노드는 격리되고, 마지막으로 저장된 체크포인트에서 학습이 재개되었다. 교체된 호스트 수와 수동 재시작 횟수의 차이를 고려할 때, 하드웨어 실패로 인한 자동 재시작이 70번 이상 이루어진 것으로 추정된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure1.png"
width="598"
height="418"
srcset="https://kurtkim.github.io/p/opt/images/figure1_huc020b8f94bd6f8270b5a5a7e0e51aa4b_52130_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure1_huc020b8f94bd6f8270b5a5a7e0e51aa4b_52130_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure2.png"
width="594"
height="422"
srcset="https://kurtkim.github.io/p/opt/images/figure2_hu74794a1377f0c0a44cdd659cf8772acd_42166_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure2_hu74794a1377f0c0a44cdd659cf8772acd_42166_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>&lt;strong>Loss Divergences&lt;/strong> 학습 과정에서 loss divergence 문제가 있었다. 손실이 발산할 때, learning rate를 낮추고 이전 체크포인트에서 재시작하면 학습이 계속될 수 있었다. loss divergence, dynamic loss 스칼라가 0으로 떨어지는 현상, 그리고 마지막 layer의 activation $l^2$-norm이 급증하는 것 사이에 상관관계가 있다는 것을 확인하였다. 이를 바탕으로 dynamic loss 스칼라가 &amp;ldquo;healthy&amp;rdquo; 상태에서, 그리고 activation norm이 무제한으로 증가하지 않는 지점에서 재시작하였다. 학습 초기에는 gradient clipping을 1.0에서 0.3으로 낮추는 것이 안정성에 도움이 되었다.&lt;/p>
&lt;p>&lt;strong>Other Mid-ﬂight Changes&lt;/strong> loss divergence을 처리하기 위해 몇 가지 실험적 변경을 시행하였다. 이에는 바닐라 SGD로의 전환, dynamic loss 스칼라의 재설정, 그리고 Megatron의 새 버전으로의 전환 등이 포함되었다. 이러한 변화들은 최적화의 빠른 정체, 일부 발산의 회복, 그리고 activation norm의 압력 감소와 처리량 향상에 도움이 되었다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluations">Evaluations&lt;/h2>
&lt;h3 id="prompting--few-shot">Prompting &amp;amp; Few-Shot&lt;/h3>
&lt;p>HellaSwag, StoryCloze, PIQA, ARC Easy와 Challenge, OpenBookQA, WinoGrad, WinoGrande, 그리고 SuperGLUE 등 문헌에서 사용하는 16개의 표준 NLP 작업에서 모델을 평가하였다. GPT-3의 프롬프트와 실험 설정을 따라서 주로 GPT-3와 비교하였고, 가능한 경우에는 다른 LLM의 성능도 포함시켰다.&lt;/p>
&lt;p>성능을 정확도로 보고하며, 평가 지표의 일관성을 위해 MultiRC와 ReCoRD의 F1은 생략하였다. SuperGLUE의 Winograd Schema Challenge 작업에서는 객관식 질문으로 작업을 구성하였고, 이는 성능에 영향을 미친다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure3.png"
width="582"
height="434"
srcset="https://kurtkim.github.io/p/opt/images/figure3_hu4ac69ec78235bded1ae62ea40f91eb0b_58046_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure3_hu4ac69ec78235bded1ae62ea40f91eb0b_58046_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="321px"
>&lt;/p>
&lt;p>&lt;strong>Zero-shot&lt;/strong> 전반적으로, 평균 성능은 GPT-3의 추세를 따르는 것으로 보인다. 그러나 작업에 따라 성능은 크게 달라질 수 있다. MultiRC와 WIC를 평균에서 의도적으로 제외하였다. 이 데이터셋들은 GPT-3 또는 OPT를 체계적으로 우대하는 것으로 보인다.&lt;/p>
&lt;p>모델의 성능은 10개 작업에서 GPT-3와 비슷했고, 3개 작업에서는 성능이 떨어졌다. 일부 작업에서는 검증 세트 크기가 작아서 모델의 행동이 예측 불가능했다. WIC에서는 OPT 모델이 GPT-3 모델을 능가했으며, MultiRC에서는 GPT-3 결과를 복제하지 못하였다. BoolQ와 WSC에서는 OPT와 GPT 모델이 대부분 클래스 정확도 주변에서 변동했음을 알 수 있다.&lt;/p>
&lt;p>Chinchilla와 Gopher는 parameter 크기에 따라 일관된 성능을 보였지만, PaLM은 모든 설정에서 더 우수한 성능을 보였다. 이는 parameter 수를 제어하더라도 마찬가지였다. PaLM의 높은 성능은 주로 사전 학습 데이터의 품질과 다양성 때문이라고 추정된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure4.png"
width="590"
height="436"
srcset="https://kurtkim.github.io/p/opt/images/figure4_hufc5e96ef5c25de0dfc31b5ce629c954a_87764_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure4_hufc5e96ef5c25de0dfc31b5ce629c954a_87764_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="135"
data-flex-basis="324px"
>&lt;/p>
&lt;p>&lt;strong>One-shot and Few-shot&lt;/strong> 평균 multi-shot in-context 성능은 OPT 모델이 GPT-3 모델과 비슷하게 수행함을 보여준다. 그러나 작업별로 결과를 분석하면, zero-shot과 같은 10개의 데이터셋에서 두 모델이 비슷한 성능을 보이는 반면, 일부 다른 데이터셋에서는 모델 크기에 따라 성능이 일관되지 않음을 보여준다. 특히, MultiRC에서는 OPT 모델이 GPT3 모델에 비해 성능이 떨어진다. 이러한 결과는 우리의 평가 설정이 Brown et al. (2020)과 다를 수 있음을 시사한다.&lt;/p>
&lt;h3 id="dialogue">Dialogue&lt;/h3>
&lt;p>대화 모델의 핵심 요소인 LLM에 초점을 맞춰, OPT-175B를 여러 오픈 소스 대화 데이터셋에서 평가하였다. 이는 ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, Blended Skill Talk, 그리고 최근의 Wizard of Internet 데이터셋을 포함한다. 주로 미세 조정된 BlenderBot 1과 Reddit 2.7B 같은 기존 오픈 소스 대화 모델과 비교하였으며, 또한 미세 조정된 R2C2 BlenderBot과도 비교하였다.&lt;/p>
&lt;p>Perplexity와 Unigram F1 (UF1) 겹침을 보고하며, 모든 Perplexities는 GPT-2 토큰화기의 공간에서 정규화된다. 대화 작업에 대해 감독되고 미감독된 모델들을 구분한다. OPT-175B는 최대 32토큰까지의 탐욕적 디코딩을 사용하며, &amp;ldquo;Person 1:&amp;ldquo;과 &amp;ldquo;Person 2:&amp;ldquo;의 대화 라인만을 번갈아 가며 사용한다. 나머지 모델들은 BlenderBot 1의 생성 parameter를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table2.png"
width="1232"
height="294"
srcset="https://kurtkim.github.io/p/opt/images/table2_hu60a06193089582f8990a0d1a27068df8_80020_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table2_hu60a06193089582f8990a0d1a27068df8_80020_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="419"
data-flex-basis="1005px"
>&lt;/p>
&lt;p>OPT-175B는 모든 작업에서 unsupervised Reddit 2.7B 모델을 크게 능가하며, ConvAI2 데이터셋에서는 supervised BlenderBot 1 모델과 비슷한 성능을 보인다. 하지만, 모든 모델이 unsupervised Wizard-of-Internet 데이터셋에서는 OPT-175B가 가장 낮은 Perplexity를 보이지만, UF1은 Wizard-ofWikipedia supervised 모델들보다 낮다.&lt;/p>
&lt;p>unsupervised OPT-175B 모델의 평가가 ConvAI2 데이터셋에서 BlenderBot 1과 경쟁력이 있었다. 이는 데이터셋의 유출을 의심케 하지만, 사전 학습 말뭉치에서는 어떤 겹침도 찾지 못했다. OPT-175B는 공개되지 않은 ConvAI2 테스트 세트와 MSC 데이터셋에서도 좋은 성능을 보여주었으며, 이는 모델이 여러 PersonaChat과 유사한 데이터셋에 잘 일반화되고 있음을 보여준다. OPT-175B가 대화를 거치면서 일관된 페르소나를 유지하는 강력한 능력을 가지고 있음이 확인되었다.&lt;/p>
&lt;hr>
&lt;h2 id="bias--toxicity-evaluations">Bias &amp;amp; Toxicity Evaluations&lt;/h2>
&lt;p>OPT-175B의 잠재적인 문제를 파악하기 위해, 혐오 발언 탐지, stereotype 인식, toxic 콘텐츠 생성 등과 관련된 벤치마크를 평가하였다. 이 벤치마크들은 단점이 있을 수 있지만, OPT-175B의 한계를 이해하는데 도움을 준다. 주로 GPT-3 Davinci와 비교하였는데, 이 벤치마크들은 Brown et al. (2020)에 포함될 수 있을 때까지 사용되지 않았다.&lt;/p>
&lt;h3 id="hate-speech-detection">Hate Speech Detection&lt;/h3>
&lt;p>Mollas et al. (2020)의 ETHOS 데이터셋을 사용해, OPT-175B가 특정 영어 문장이 인종차별적인지, 성차별적인지 판별하는 능력을 측정하였다. zero-shot, one-shot, few-shot 이진 케이스에서는 모델에게 텍스트가 인종차별적이거나 성차별적인지 판단하고 yes/no로 응답하도록 했고, few-shot 다중 클래스 설정에서는 yes/no/neither로 응답하도록 하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table3.png"
width="592"
height="236"
srcset="https://kurtkim.github.io/p/opt/images/table3_hub55f8b4388eff5af052ad41772d7ecc7_39414_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table3_hub55f8b4388eff5af052ad41772d7ecc7_39414_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="602px"
>&lt;/p>
&lt;p>OPT175B는 one-shot에서 few-shot 설정에서 모두 Davinci보다 훨씬 더 좋은 성능을 보였다. 이는 Davinci API를 통한 평가가 추가적인 안전 제어 메커니즘을 도입하고 있거나, 사전 학습 데이터셋에 포함된 통제되지 않은 소셜 미디어 토론이 이러한 분류 작업에 도움을 주는 귀납적 bias를 제공했기 때문으로 추측된다.&lt;/p>
&lt;h3 id="crows-pairs">CrowS-Pairs&lt;/h3>
&lt;p>CrowSPairs는 마스크 언어 모델을 위해 개발된 벤치마크로, 9가지 카테고리(gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance, socioeconomic status)의 문장 내 bias를 측정한다. 각 예시는 한 그룹에 대한 stereotype 또는 anti-stereotype을 나타내는 문장 쌍으로, 모델이 stereotype 표현을 선호하는 정도를 측정한다. 높은 점수는 모델이 더 큰 bias를 보이는 것을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table4.png"
width="580"
height="468"
srcset="https://kurtkim.github.io/p/opt/images/table4_hu7cde3cccfb2ff574d4954c18dd90cc83_74905_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table4_hu7cde3cccfb2ff574d4954c18dd90cc83_74905_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="123"
data-flex-basis="297px"
>&lt;/p>
&lt;p>OPT175B는 종교를 제외한 대부분 카테고리에서 더 많은 stereotypical bias을 보였다. 이는 학습 데이터의 차이 때문으로, Reddit 말뭉치가 stereotype과 차별적인 텍스트의 발생률이 더 높다고 나타났다. 이런 데이터가 OPT-175B의 주요 학습 원천이기 때문에, 모델은 더 많은 차별적 연관성을 배웠을 수 있고, 이는 CrowS-Pairs에서의 성능에 직접적인 영향을 미친다.&lt;/p>
&lt;h3 id="stereoset">StereoSet&lt;/h3>
&lt;p>직업, 성별, 종교, 인종의 4가지 카테고리에서 stereotypical bias을 측정하기 위해, 우리는 StereoSet을 사용한다. 이 도구는 문장 내 bias 측정뿐만 아니라, 추가적인 맥락을 포함하는 모델의 능력을 테스트하기 위한 문장 간 bias 측정도 포함한다. bias 탐지와 언어 모델링 능력 사이의 잠재적인 교환 관계를 고려하기 위해, StereoSet은 두 가지 지표를 포함한다.&lt;/p>
&lt;p>Language Modeling Score(LMS)와 Stereotype Score(SS)를 결합해 Idealized Context Association Test score(ICAT)를 만든다. 문자 수가 아닌 토큰 수로 점수를 정규화하는데, 이 방법이 여러 모델의 측정치를 개선한다고 보고되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table5.png"
width="566"
height="694"
srcset="https://kurtkim.github.io/p/opt/images/table5_hu440f9e1cbb2a349639c30b4932f23c58_103130_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table5_hu440f9e1cbb2a349639c30b4932f23c58_103130_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="195px"
>&lt;/p>
&lt;p>Davinci와 OPT-175B는 전체적으로 비슷한 점수를 보여주었다. Davinci는 직업과 인종 분야에서, OPT-175B는 성별과 종교 분야에서 더 뛰어난 성능을 보였다. OPT175B는 SS 지표에서 전반적으로 더 좋은 성능을 보였고, Davinci는 LMS 지표에서 일반적으로 더 뛰어난 성능을 보였다.&lt;/p>
&lt;h3 id="realtoxicityprompts">RealToxicityPrompts&lt;/h3>
&lt;p>RealToxicityPrompts 데이터셋을 이용해 OPT-175B가 toxic 언어로 응답하는 경향을 평가하였다. RTP에서 무작위로 샘플링한 10,000개의 프롬프트 각각에 대해, nucleus 샘플링을 사용하여 생성된 연속성의 평균 toxicity rate을 보고했습니다. 또한, 비교를 위해 Davinci와 PaLM에서의 toxicity rate을 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure5.png"
width="596"
height="400"
srcset="https://kurtkim.github.io/p/opt/images/figure5_hu0476d36e1e24bd5bdeda38192a6f51ad_66046_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure5_hu0476d36e1e24bd5bdeda38192a6f51ad_66046_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="357px"
>&lt;/p>
&lt;p>OPT-175B는 PaLM이나 Davinci보다 높은 toxicity rate을 보였다. 프롬프트의 toxic이 증가할수록 모든 모델이 toxic을 가진 연속성을 생성할 가능성이 증가하는 것을 확인하였다. 사전 학습 말뭉치에 통제되지 않은 소셜 미디어 텍스트가 포함되어 있다는 점이 toxic 텍스트 생성과 탐지 경향을 높일 수 있다. 이는 downstream 응용 프로그램의 요구에 따라 바람직하지 않을 수도 있으므로, OPT-175B의 미래 응용은 이를 고려해야 한다.&lt;/p>
&lt;h3 id="dialogue-safety-evaluations">Dialogue Safety Evaluations&lt;/h3>
&lt;p>대화 안전성 평가 두 가지를 통해 OPT-175B를 비교하였다. SaferDialogues는 명백한 안전 실패에서 회복하는 능력을, Safety Bench Unit Tests는 모델의 응답의 안전성을 측정한다. 이는 주제의 민감성에 따라 4단계로 분류됩니다. 이 결과는 기존 오픈 소스 대화 모델과 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table6.png"
width="588"
height="230"
srcset="https://kurtkim.github.io/p/opt/images/table6_hu52305b5ec9c40f76468aa62df8ee6326_43286_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table6_hu52305b5ec9c40f76468aa62df8ee6326_43286_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="613px"
>&lt;/p>
&lt;p>두 실험 결과에 따르면, OPT-175B는 SaferDialogues와 Unit Tests에서 Reddit 2.7B 모델과 유사한 성능을 보여주었다. 안전하고 적대적인 설정에서 OPT-175B는 약간 더 높은 성능을 보여주었다. 정제된 대화 데이터셋에서 미세 조정된 모델들은 전반적으로 더 낮은 toxic을 가진 것으로 확인되었다. 따라서, OPT-175B를 대화용으로 활용하는 미래의 실험은 안전 프로파일을 향상시키기 위해 정제된 데이터셋에서 미세 조정을 포함해야 한다는 결론을 내렸다.&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>다양한 규모의 모든 출시된 모델에 대해 폭넓게 평가하였다. GPT-3 모델에서 사용된 표준 평가 데이터셋에 대한 성능은 비슷했으며, 안전성, 편향, 포괄성 등의 평가에서도 대체적으로 비슷한 성능을 보여주었다. 그러나 이러한 평가는 모델의 전체적인 한계를 완전히 반영하지는 못할 수 있다. 특히 OPT-175B는 다른 LLMs에서 지적된 동일한 한계를 보여주었다.&lt;/p>
&lt;p>OPT-175B는 명령형 지시문이나 간결한 질문에 잘 작동하지 않는다는 것을 발견하였다. 지시문의 실행보다는 대화의 시뮬레이션을 생성하는 경향이 있다. 이러한 한계는 InstructGPT와 같은 지시문 학습에 대한 미래의 연구를 통해 완화될 수 있을 것이다.&lt;/p>
&lt;p>OPT-175B는 반복적인 경향이 있고 쉽게 루프에 빠질 수 있다는 것을 발견하였다. 한 번의 생성만 샘플링할 때 샘플링이 반복을 완전히 제거하지 못했다. 미래의 연구에서는 반복을 줄이고 다양성을 향상시키는 전략, 예를 들어 unlikelihood training이나 best-ﬁrst decoding을 통합할 필요가 있다.&lt;/p>
&lt;p>OPT-175B는 다른 LLM과 마찬가지로 사실적으로 부정확한 문장을 생성할 수 있다. 이는 정보의 정확성이 중요한 분야에서 특히 문제가 될 수 있다. 그러나 최근의 연구들은 검색 기반 모델이 LLM의 사실적 정확성을 향상시킬 수 있음을 보여주었다. 따라서, OPT-175B도 미래에 검색 기반 확장의 이점을 누릴 것으로 예상한다.&lt;/p>
&lt;p>OPT-175B는 무해한 프롬프트를 제공받았을 때도 toxic한 언어를 생성하고 해로운 stereotype을 강화하는 경향이 높다고 확인되었다. 또한 적대적인 프롬프트는 쉽게 찾을 수 있었다. toxic과 bias에 대한 대응책에 대한 많은 연구가 있으며, OPT-175B의 미래 사용은 이러한 접근법을 적용해야 할 수 있다. 그러나 이번 첫 릴리즈에서는 GPT-3의 복제를 주요 목표로 두었기 때문에, 이러한 완화책을 적용하지 않았다.&lt;/p>
&lt;p>이 기술이 상업적 배포에는 아직 준비되지 않았다고 생각한다. 더 많은 신중함이 필요하며, 이상적으로는 재현성과 복제성을 보장하기 위해 더 간결하고 일관된 평가 설정을 가지고 있어야 한다. 프롬프트 스타일과 문맥 학습에 대한 차이점은 다른 결과를 이끌어낼 수 있다. OPT 모델의 공개 릴리즈는 이러한 중요한 문제에 대한 연구를 촉진할 것으로 기대한다.&lt;/p>
&lt;hr>
&lt;h2 id="considerations-for-release">Considerations for Release&lt;/h2>
&lt;p>AI 파트너십과 NIST의 지침에 따라, OPT-175B 학습 과정의 모든 세부사항을 공개하고, 연구자들이 모델 가중치에 접근하고 작은 기준선 세트를 사용할 수 있게 한다. OPT-175B의 개발 생명주기에 대한 완전한 책임을 지며, LLM 개발에 대한 투명성을 높여 LLM의 한계와 위험을 이해하는 데 중점을 두고 있다.&lt;/p>
&lt;p>일상적인 학습 과정의 세부사항을 공유함으로써 OPT-175B 학습에 사용된 컴퓨팅 리소스와 대규모에서의 불안정성을 관리하는 데 필요한 인력을 공개한다. 이런 세부사항은 대게 이전 연구에서 생략되었지만, ad-hoc 디자인 결정 과정을 공개함으로써 미래의 모델 개발에서 이러한 방식을 개선하고 실험적 강인성을 높이는 데 기여하길 희망한다.&lt;/p>
&lt;p>개발 코드베이스를 공개함으로써, 논문에서 명시적으로 언급되지 않은 구현 세부 사항에 대한 명확성을 제공하려고 한다. 현재의 코드베이스는 파이프라인 병렬성을 사용하지 않고 175B 이상의 parameter를 가진 decoderonly transformer를 NVIDIA GPU에서 학습시키는 유일한 오픈 소스 구현이다.&lt;/p>
&lt;p>175B 규모의 실험을 가능하게 하기 위해, 연구자들에게 OPT-175B의 parameter에 직접 접근할 수 있게 했다. 이는 LLM에 대한 책임 있는 AI 연구를 촉진하고, 이 규모의 연구가 환경에 미치는 영향을 줄이기 위한 것이다. 대규모 언어 모델 배포의 윤리적, 사회적 위험을 다루는 연구가 증가하고 있다. 비상업적 라이센스를 가진 연구 커뮤니티만 OPT-175B에 접근하게 하여, 상업적 배포 전에 먼저 LLM의 한계를 파악하는 데 초점을 맞추고자 한다.&lt;/p>
&lt;p>이 규모의 모델을 재현하는데는 상당한 컴퓨팅 및 탄소 비용이 발생한다. OPT-175B는 추정 75톤의 탄소 배출량으로 개발되었으며, 다른 모델들은 더 많은 양을 사용하였다. 이러한 추정치는 표준화되지 않았고, AI 시스템의 전체 탄소 발자국은 모델 학습뿐만 아니라 실험과 추론 비용도 포함한다. 로그북을 공개하여 이론적 탄소 비용 추정치와 전체 개발 수명주기를 고려한 추정치 사이의 차이를 강조하고자 한다. 또한, 점점 복잡해지는 이 시스템들의 제조 탄소를 이해하고, 환경에 대한 규모의 영향을 측정할 때 고려해야 할 추가 요인을 정의하는 데 이 논문이 도움이 될 수 있기를 희망한다.&lt;/p>
&lt;p>다양한 스케일에서 기준선을 설정함으로써, 연구 커뮤니티가 이 모델들의 영향력과 한계를 스케일만으로 연구할 수 있도록 돕고자 한다. 일부 LLM은 사용된 학습 데이터 양에 비해 학습이 부족했을 수 있으며, 이는 더 많은 데이터를 추가하고 계속 학습하면 성능이 향상될 수 있음을 의미한다. 또한, 175B보다 훨씬 작은 규모에서 기능 변화가 발생할 수 있다는 증거가 있으므로, 다양한 연구 활용을 위해 더 넓은 스케일 범위를 검토해야 한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>transformer 아키텍처와 BERT의 출시 이후, NLP 분야는 self-supervised 학습을 통한 LLM 사용으로 크게 변화하였다. T5와 MegatronLM 같은 여러 가면 언어 모델들은 규모를 통해 지속적으로 성능을 향상시켰다. 이는 모델의 parameter 수 증가뿐만 아니라 사전 학습 데이터의 양과 품질 향상으로 이루어졌다.&lt;/p>
&lt;p>auto-regressive 언어 모델은 모델 크기가 크게 증가하였고, 이로 인해 생성 유창성과 품질이 대폭 향상되었다. 많은 큰 모델들이 학습되었지만, 이들은 대부분 비공개 소스로, 내부적으로 또는 유료 API를 통해만 접근 가능하다. 그러나 비영리 연구 조직에서는 LLM을 오픈 소스화하는 노력이 있으며, 이러한 모델들은 OPT 모델과 다르기 때문에, 커뮤니티가 다양한 사전 학습 전략을 비교할 수 있다.&lt;/p>
&lt;p>LLM의 주요 평가 기준은 프롬프트 기반이며 이는 특정 작업에 대한 미세 조정 없이도 많은 작업을 평가하는 편리함 때문이다. 프롬프트는 오래된 역사를 가지고 있고, 최근에는 모델에 대한 지식 탐색 또는 다양한 NLP 작업 수행에 사용되었다. 또한, 작은 모델에서 프롬프트 동작을 유도하거나, 프롬프트의 유연성을 개선하고, 프롬프트가 어떻게 작동하는지 이해하는 연구도 있다.&lt;/p>
&lt;p>모델을 지시 스타일의 프롬프트에 대응하게 미세조정하는 것이 이익을 보였지만, 효과적인 프롬프트 엔지니어링은 여전히 해결되지 않은 연구 과제이다. 프롬프트 선택에 따라 결과는 크게 달라지며, 모델은 프롬프트를 우리가 기대하는 만큼 완전히 이해하지 못하는 것으로 보인다. 또한, 개발 세트 없이 프롬프트를 작성하는 것은 어려움이 있다. 이러한 문제를 해결하려 하지 않고, 단지 OPT-175B의 평가만을 목표로 하며, OPT-175B의 전체 릴리스가 미래의 연구를 돕길 바란다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>125M에서 175B parameter까지 다양한 크기의 auto-regressive 언어 모델 모음인 OPT를 소개하다. 이 연구의 목표는 GPT-3 클래스의 모델을 복제하고 최신 데이터 큐레이션 및 학습 효율성 모범 사례를 적용하는 것이다. 모델의 여러 제한 사항과 책임감 있는 공개에 대한 고려 사항을 논의하였다. 우리는 AI 커뮤니티가 책임감 있는 LLM 가이드라인 개발에 협력하고, 이러한 유형의 모델에 대한 넓은 접근이 기술의 윤리적 고려 사항을 정의하는 다양한 목소리를 늘리길 희망한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2205.01068.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/metaseq" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>PaLM</title><link>https://kurtkim.github.io/p/palm/</link><pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/palm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>few-shot learning 예제를 사용하는 대형 언어 모델은 다양한 자연어 작업에서 뛰어난 성능을 보여준다. 이를 더 깊이 이해하기 위해, 540B parameter의 densely activated transformer 언어 모델인 Pathways Language Model(PaLM)을 학습시켰다.&lt;/p>
&lt;p>새로운 ML 시스템인 Pathways를 사용해 PaLM을 학습시키고, 수백 개의 언어 이해 및 생성 벤치마크에서 state-of-the-art의 few-shot learning 결과를 달성하였다. PaLM 540B는 다단계 추론 작업과 BIG-bench 벤치마크에서 인간 평균 성능을 능가하는 성과를 보여주었다. 모델 규모가 커짐에 따라 성능이 급격히 향상된 작업도 있었다. 또한 PaLM은 다국어 작업과 소스 코드 생성에서도 강력한 능력을 가지고 있다. bias와 toxicity 대한 분석과 함께, 거대 언어 모델과 관련된 윤리적 고려 사항에 대해 논의하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 이해와 생성을 위한 대규모 신경망들은 다양한 작업에서 놀라운 결과를 보여주고있다. BERT나 T5 같은 모델들은 대량의 텍스트를 통해 사전 학습되고, 특정 작업에 맞게 미세 조정된다. 이들 모델은 다양한 자연어 작업에서 state-of-the-art를 보여주지만, 모델을 미세 조정하는 데 많은 수의 작업 특정 학습 예제가 필요하고, 일부 모델 parameter를 작업에 맞게 업데이트하는 복잡성이 증가한다는 단점이 있다.&lt;/p>
&lt;p>GPT-3는 극도로 큰 autoregressive 언어 모델이 소수의 예측을 위해 사용될 수 있음을 보여주었다. 이 모델은 자연어 작업 설명과 작업 완료 방법을 보여주는 몇 가지 예시만 제공받아 학습된다. 대규모 작업 특정 데이터 수집이나 모델 parameter 업데이트 없이도 매우 강력한 결과를 달성하였다.&lt;/p>
&lt;p>GPT-3 이후에도 GLaM, Gopher, Chinchilla, Megatron–Turing NLG, LaMDA와 같은 강력한 대규모 autoregressive 언어 모델들이 개발되어 state-of-the-art를 계속 밀어내고 있다. 이들 모델은 모두 transformer 아키텍처의 변형이며, 모델의 크기 확대, 학습된 토큰 수 증가, 더 깨끗한 데이터셋 사용, 희소 활성화 모듈을 통한 계산 비용 없는 모델 용량 증가 등의 방법으로 개선되었다.&lt;/p>
&lt;p>이 연구에서는 780B 개의 고품질 텍스트 토큰에 대해 540B 개의 parameter를 가진 densely activated autoregressive transformer를 학습시키는 언어 모델링 개선을 계속하였다. 이는 새로운 ML 시스템인 Pathways를 사용하여 수천 개의 accelerator chip에서 매우 큰 신경망을 효율적으로 학습시키는 데 성공하였다. 이 새로운 모델인 PaLM은 수백 개의 자연어, 코드, 수학적 추론 작업에서 breakthrough performance를 달성하였다.&lt;/p>
&lt;p>이 연구에서 주요 결론은 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Eﬃcient scaling&lt;/strong> 이 연구에서는 새로운 ML 시스템인 Pathways를 대규모로 처음 사용하였다. 이를 통해, 6144개의 TPU v4 칩에서 540B parameter 언어 모델을 이전에는 도달할 수 없었던 효율 수준에서 학습시켰다. 이전의 대부분의 대규모 언어 모델들은 단일 TPU 시스템에서 학습되거나 GPU 클러스터 또는 여러 TPU v3 pods에 걸쳐 확장되었다. 두 개의 TPU v4 Pods에 걸쳐 6144개의 칩으로 PaLM 540B의 학습을 확장하면서 매우 높은 효율성을 달성하였다.&lt;/li>
&lt;li>&lt;strong>Continued improvements from scaling&lt;/strong> 자연어, 코드, 수학적 추론 작업 등 수백 가지 작업에 대해 PaLM을 평가하고, 대부분의 벤치마크에서 상당한 차이로 state-of-the-art를 달성하였다. 이는 대규모 언어 모델로부터의 scaling 개선이 아직도 정체되지 않았음을 보여준다. 가장 널리 평가된 29개의 영어 언어 이해 벤치마크 중 28개에서 최고 작업별 결과에 비해 state-of-the-art를 보여주었다.&lt;/li>
&lt;li>&lt;strong>Breakthrough capabilities&lt;/strong> 이 연구에서는 다양한 어려운 작업에 대해 언어 이해와 생성에서 breakthrough capabilities를 보여준다. 특히, multi-step 수학적 또는 상식적 추론이 필요한 일련의 추론 작업에 대해 평가하였다. 모델 scaling과 사슬 형태의 생각 유도를 결합하면, 간단한 소수 평가가 넓은 범위의 추론 작업에서 state-of-the-art를 능가하거나 매치할 수 있음을 보여주었다. 또한, 최근 출시된 150개 이상의 새로운 언어 이해와 생성 작업을 포함하는 BIG-bench에서 breakthrough performance을 보여주었다. PaLM이 복잡한 추론 체인을 명확하게 해석하고 설명하는 능력을 탐색하였다.&lt;/li>
&lt;li>&lt;strong>Discontinuous improvements&lt;/strong> 8B, 62B, 540B의 세 가지 다른 parameter 규모에서의 결과를 제시하여 scaling 행동을 이해한다. 일반적으로, 62B에서 540B로의 scaling은 8B에서 62B로의 scaling과 유사한 성능을 가져온다. 그러나 특정 작업에 대해서는, 62B에서 540B로의 scaling이 정확도에서 drastic jump를 가져오는 것을 관찰하였다. 이는 대규모 언어 모델의 새로운 능력이 충분한 규모를 달성하면 나타날 수 있음을 제안한다.&lt;/li>
&lt;li>&lt;strong>Multilingual understanding&lt;/strong> 이 연구에서는 다양한 언어에서의 기계 번역, 요약, 그리고 질문 응답을 포함한 다국어 벤치마크에 대한 철저한 평가를 수행하였다. 비영어 데이터의 비율이 상대적으로 작음에도 불구하고, PaLM 모델은 비영어 요약 작업에서 이전에 미세 조정된 state-of-the-art와의 격차를 메우며, 번역 작업에서 이전의 state-of-the-art를 능가하였다. 다국어 데이터 비율 증가의 영향을 이해하기 위해 추가적인 연구가 필요하다.&lt;/li>
&lt;li>&lt;strong>Bias and toxicity&lt;/strong> distributional bias와 toxicity에 대한 모델 성능을 평가하였다. 성별과 직업에 대한 bias에서, 모델 규모가 커짐에 따라 성능이 개선되었다. 인종/종교/성별 프롬프트 연속성에서는 모델이 스테레오타입을 거짓으로 확증하는 가능성을 보여주었다. toxicity 분석에서는 62B와 540B 모델이 8B 모델에 비해 약간 더 높은 toxicity 수준을 보여주었다. 모델이 생성한 연속성의 toxicity은 프롬프트 텍스트의 toxicity과 높게 상관되었다. 향후 연구에서는 이러한 벤치마크를 비영어 언어로 확장하고 잠재적 위험을 더 철저히 고려할 계획이다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>PaLM은 다음과 같은 수정을 가진 표준 Transformer 모델 아키텍처의 decoder-only setup으로 사용한다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>SwiGLU Activation&lt;/strong> MLP 중intermediate activation에 SwiGLU activation을 사용한다. 이는 표준 ReLU, GeLU, Swish activation에 비해 품질을 크게 향상시키기 때문이다. 이는 MLP에서 세 개의 행렬 곱셈이 필요하다는 것을 의미하지만, 이는 품질 개선을 보여준다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Layers&lt;/strong> 각 Transformer block에서 표준 &amp;ldquo;serialized&amp;rdquo; 형식 대신 &amp;ldquo;parallel&amp;rdquo; 형식을 사용한다. 특히, 표준 serialized 형식은 다음과 같이 작성할 수 있다:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$ y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x)))) $$&lt;/p>
&lt;p>반면에, parallel 형식은 다음과 같이 작성할 수 있다:&lt;/p>
&lt;p>$$ y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x)) $$&lt;/p>
&lt;p>parallel 형식은 MLP와 Attention 입력 행렬 곱셈이 융합될 수 있어 대규모 규모에서 학습 속도를 약 15% 더 빠르게 한다. 실험에서는 8B 규모에서는 약간의 품질 저하가 있었지만, 62B 규모에서는 품질 저하가 없었으므로, 540B 규모에서 parallel layer의 효과는 품질에 영향을 주지 않을 것으로 추정하였다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Multi-Query Attention&lt;/strong> 표준 Transformer 형식은 $k$개의 attention head를 사용하며, 각 타임스텝의 입력 벡터는 &amp;ldquo;query&amp;rdquo;, &amp;ldquo;key&amp;rdquo;, &amp;ldquo;value&amp;rdquo; 텐서로 선형적으로 투영된다. 이 방식은 모델 품질과 학습 속도에 중립적인 효과를 가지지만, decoding 시간에 비용 절약을 가져온다. 이는 standard multi-headed attention이 auto-regressive decoding 시에 accelerator 하드웨어에서 낮은 효율성을 보이기 때문이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RoPE Embeddings&lt;/strong> 긴 시퀀스 길이에서 더 나은 성능을 보이는 RoPE 임베딩을 사용한다. 이는 절대적 또는 상대적 포지션 임베딩 대신에 사용되었다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Shared Input-Output Embeddings&lt;/strong> 입력과 출력 임베딩 행렬을 공유한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>No Biases&lt;/strong> 어떤 dense kernel이나 layer norm에서도 bias를 사용하지 않았다. 이는 큰 모델의 학습 안정성을 증가시키는 것으로 나타났다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Vocabulary&lt;/strong> 256k 토큰의 SentencePiece 어휘를 사용하여 학습 말뭉치의 많은 언어를 지원한다. 이 어휘는 학습 데이터에서 생성되었으며, 학습 효율성을 향상시킨다. 어휘는 완전히 손실 없이 되돌릴 수 있으며, 공백을 완전히 보존하고, 어휘 외의 유니코드 문자를 UTF-8 바이트로 분할한다. 숫자는 항상 개별 숫자 토큰으로 분할된다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="model-scale-hyperparameters">Model Scale Hyperparameters&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table1.png"
width="1056"
height="210"
srcset="https://kurtkim.github.io/p/palm/images/table1_hude476ed3e91ed00e06b0e7cb7b21a5f2_44541_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table1_hude476ed3e91ed00e06b0e7cb7b21a5f2_44541_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="502"
data-flex-basis="1206px"
>&lt;/p>
&lt;p>이 연구에서는 540B, 62B, 8B parameter의 세 가지 다른 모델 규모를 비교한다. 이 모델들은 standard dense transformer이므로, 토큰 당 FLOP 수는 parameter 수와 대략적으로 동일하다. 이 모델들은 같은 데이터와 어휘를 사용하여 동일하게 학습되었다.&lt;/p>
&lt;hr>
&lt;h2 id="training-dataset">Training Dataset&lt;/h2>
&lt;p>PaLM 사전 학습 데이터셋은 다양한 자연어 사용 사례를 대표하는 7800억 토큰의 말뭉치로 구성되어 있다. 이 데이터셋은 웹페이지, 책, 위키백과, 뉴스 기사, 소스 코드, 소셜 미디어 대화를 섞어 만들었다. 모든 모델을 데이터의 1 epoch 학습시키고, 데이터를 반복하지 않도록 혼합 비율을 선택하였다.&lt;/p>
&lt;p>사전 학습 데이터셋은 자연어 데이터뿐만 아니라 코드도 포함한다. 이 코드는 GitHub의 오픈 소스 저장소에서 얻은 것이며, 라이선스에 따라 필터링하였다. 또한 파일 이름 확장자에 따라 24개의 일반적인 프로그래밍 언어 중 하나로 제한하였고, 중복 파일을 제거하였다. 이 결과, 196GB의 소스 코드가 생성되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table2.png"
width="780"
height="322"
srcset="https://kurtkim.github.io/p/palm/images/table2_hu3b61d835e2368f25411df1d314c5133d_56956_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table2_hu3b61d835e2368f25411df1d314c5133d_56956_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>최종 PaLM 데이터셋 혼합물을 생성하는 데 사용된 다양한 데이터 소스의 비율을 보여주며, 데이터 오염을 확인하고, 학습 데이터셋과 평가 데이터 사이의 중복을 분석한다.&lt;/p>
&lt;hr>
&lt;h2 id="training-infrastructure">Training Infrastructure&lt;/h2>
&lt;p>학습 및 평가 코드베이스는 JAX와 T5X를 기반으로 하며, 모든 모델은 TPU v4 Pods에서 학습된다. PaLM 540B는 데이터 센터 네트워크를 통해 연결된 두 개의 TPU v4 Pods에서 학습되며, 이는 모델과 데이터 병렬성의 조합을 사용한다. 이 시스템은 파이프라인 병렬성 없이 학습을 6144개의 칩으로 효율적으로 확장할 수 있게 해준다.&lt;/p>
&lt;p>이전의 비슷한 규모에서 모델 학습은 두 가지 접근법을 사용했다. LaMDA와 GLaM은 파이프라인 병렬성이나 DCN을 활용하지 않고 단일 TPU 시스템에서 학습되었고, Megatron-Turing NLG 530B는 여러 가지 병렬성을 사용하여 A100 GPU에서, Gopher는 파이프라이닝을 사용하여 DCN-연결된 TPU v3 Pods에서 학습되었다.&lt;/p>
&lt;p>파이프라이닝은 일반적으로 DCN과 함께 사용되며, 추가적인 병렬화를 제공한다. 그러나 이는 학습 배치를 &amp;ldquo;micro-batches&amp;quot;로 분할하지만, 중요한 단점이 있다. 첫째, 많은 장치가 유휴(idle) 상태인 동안 발생하는 시간 오버헤드가 있다. 둘째, 미니 배치 내의 각 마이크로 배치에 대해 메모리에서 가중치를 다시 로드해야 하므로 높은 메모리 대역폭이 필요하다. 이러한 문제를 해결하기 위한 전략을 통해 PaLM 540B의 학습을 6144 칩으로 효율적으로 확장할 수 있었다.&lt;/p>
&lt;p>각 TPU v4 Pod는 모델 parameter의 전체 복사본을 포함하며, 각 가중치 텐서는 모델 병렬성과 완전분할 데이터 병렬성을 사용하여 칩으로 분할된다. forward pass에서 가중치가 모두 모아지고, 각 layer에서 activation 텐서가 저장된다. backward pass에서는 나머지 activation이 rematerialized되며, 이는 더 큰 배치 크기에서 더 높은 학습 처리량을 결과로 내기 때문이다.&lt;/p>
&lt;p>Pathways 시스템을 사용하여 단일 TPU v4 Pod를 넘어서 학습을 확장한다. PaLM 540B는 Pathways의 클라이언트-서버 아키텍처를 사용하여 pod 레벨에서 데이터 병렬성을 달성한다. Python 클라이언트는 배치의 절반을 각 pod에 할당하고, 각 pod는 gradient를 계산하기 위해 병렬로 계산을 수행한다. 그 후, pod들은 gradient를 원격 pod에 전송하고, 각 pod는 gradient를 누적하고 parameter를 업데이트하여 다음 타임스텝에 대한 parameter를 얻는다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure2.png"
width="872"
height="342"
srcset="https://kurtkim.github.io/p/palm/images/figure2_hu61787b504635cdb77d202cdfb13ee7cd_88893_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure2_hu61787b504635cdb77d202cdfb13ee7cd_88893_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>Python 클라이언트는 분할된 데이터플로우 프로그램을 구성하고, 이 프로그램은 각 pod에서 계산과 최적화 업데이트를 수행하고, gradient를 다른 pod로 전송한다. Pathways 시스템의 디자인은 프로그램 실행을 수천 개의 accelerator 칩으로 확장할 수 있게 한다. 이는 원격 서버로 작업을 발송하는 데 걸리는 지연 시간을 감추고, 데이터 전송의 관리 비용을 분산시킨다.&lt;/p>
&lt;p>two-way pod-level 데이터 병렬성의 도전적인 측면은 cross-pod gradient 전송에 대한 높은 학습 처리량을 달성하는 것이다. 이는 데이터 센터 네트워크를 통해 모든 호스트가 gradient를 동시에 전송하는 매우 폭발적인 작업량을 초래한다. 이로 인한 도전을 극복하기 위해, 데이터를 작은 청크로 분해하고 다양한 DCN 링크를 통해 라우팅하는 등의 최적화를 수행한다. 이러한 최적화를 통해, 학습 중 단일 pod에 비해 약 1.95배의 처리량을 달성하였다. 이론적인 2배 처리량에 비한 성능 차이는 backward pass와 cross-pod gradient 축소 사이의 중첩이 부족하기 때문에 발생하며, 이 문제는 향후 작업에서 해결할 예정이다.&lt;/p>
&lt;h3 id="training-eﬃciency">Training Eﬃciency&lt;/h3>
&lt;p>언어 모델의 accelerator 효율성은 대게 hardware FLOPs utilization(HFU)로 측정된다. 이는 주어진 장치에서 관찰된 FLOPs와 이론적인 최대 FLOPs 사이의 비율을 나타낸다. 하지만 이 방법에는 문제가 있다. 첫째, 실행된 하드웨어 FLOPs의 수는 시스템과 구현에 따라 달라진다. 둘째, 하드웨어 FLOPs 측정은 그것들을 세거나 추적하는 방법에 의존적이다. 결국, 학습 시스템의 목표는 가능한 많은 하드웨어 FLOPs를 사용하는 것이 아니라 초당 토큰의 높은 처리량을 달성하는 것이다.&lt;/p>
&lt;p>HFU는 LLM 학습 효율성에 대한 일관된 척도가 아니라는 문제점을 인식하였다. 따라서, model FLOPs utilization(MFU)이라는 새로운 효율성 척도를 제안한다. MFU는 관찰된 처리량이 피크 FLOPs에서 운영하는 시스템의 이론적 최대 처리량에 대한 비율이다. 이 척도는 다양한 시스템에서의 학습을 공정하게 비교할 수 있게 해준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure3.png"
width="1214"
height="506"
srcset="https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;p>PaLM 540B 모델의 model FLOPs utilization(MFU)을 제시하고, 이전의 큰 모델들과 비교하였다. MFU는 다양한 모델 parameter 수, 아키텍처, 모델 품질의 맥락에서 모델과 시스템을 비교하는데 유용하다.&lt;/p>
&lt;p>GPT-3의 MFU는 21.3%, Gopher는 32.5%, Megatron–Turing NLG 530B는 self-attention 없이 29.7%, 있으면 30.2%이다. 반면, PaLM 540B는 self-attention 없이 45.7%, 있으면 46.2%의 MFU를 달성하였다.&lt;/p>
&lt;p>PaLM은 병렬성 전략과 XLA TPU 컴파일러 최적화, 그리고 &amp;ldquo;parallel layers&amp;quot;의 사용 등으로 인해 높은 accelerator 이용률을 달성하였다. 이로써 PaLM은 LLM 학습 효율성에서 중요한 진전을 나타내는 것으로 보여진다.&lt;/p>
&lt;hr>
&lt;h2 id="training-setup">Training Setup&lt;/h2>
&lt;p>모델 학습은 large transformer 언어 모델에 대한 상당히 표준적인 설정을 따랐다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Weight initialization&lt;/strong> 커널 가중치는 &amp;ldquo;fan-in variance scaling&amp;quot;을 사용하여 초기화하며, 입력 임베딩은 layer normalization가 적용되지 않기 때문에 $E ∼ N(0, 1)$으로 초기화된다. 입력과 출력 임베딩 레이어가 공유되므로, pre-softmax 출력 logit은 임베딩 크기의 제곱근의 역수로 스케일링된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Optimizer&lt;/strong> 이 모델은 Adafactor optimizer를 사용하여 학습되었으며, 이는 parameter 행렬의 평균 제곱근으로 learning rate을 조정하는 Adam과 사실상 동일하다. 가중치 초기화가 ${{1}\over{\sqrt{n}}}$에 비례하기 때문에, 이는 learning rate를 수동으로 축소하는 것과 비슷한 효과를 가진다. 하지만, 다른 스케일에서 작동하는 parameter 행렬들이 동일한 비율로 learning rate을 축소하지 않게 하는 이점이 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Optimization hyperparameters&lt;/strong> 처음 10,000 단계에는 $10^{-2}$의 Adafactor learning rate을 사용하고, 이후에는 단계 번호에 따라 learning rate을 감소시킨다. 모멘텀은 $\beta_1 = 0.9$로 설정하고, 두 번째 순서 모멘트 보간 값은 $\beta_2 = 1.0 - k^{-0.8}$로 계산된다. 이 방법은 희귀 임베딩 토큰의 두 번째 순간을 더 정확하게 추정할 수 있어 안정적이다. 그리고, 모든 모델에서 1.0의 global norm gradient clipping을 사용하며, 학습 중에는 현재 learning rate의 2배에 해당하는 dynamic weight decay를 사용한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Loss function&lt;/strong> 이 모델은 표준 언어 모델링 손실 함수, 즉 모든 토큰의 average log probability를 사용하여 학습된다. 또한, softmax normalizer 값인 $log(Z)$가 0에 가깝게 만드는 auxiliary loss인 $z$ 손실을 사용하며, 이는 학습 안정성을 높이는 데 도움이 된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sequence length&lt;/strong> 모든 모델은 2048 토큰의 시퀀스 길이로 작동하며, 입력 예제들은 이 길이에 맞춰 연결되고 분할된다. 각 예제는 특별한 [eod] 토큰으로 구분되며 패딩 토큰은 사용되지 않는다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Batch size&lt;/strong> 학습 도중 모든 모델의 배치 크기를 점진적으로 증가시킨다. 큰 모델의 경우, 초기에는 배치 크기를 512로 설정하고, 학습이 진행됨에 따라 이를 2048까지 늘린다. 이런 방식은 학습 초기에는 작은 배치 크기가, 후반에는 큰 배치 크기가 더 효율적이기 때문이며, 또한 큰 배치 크기는 TPU 효율성을 높이는데 도움이 된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Bitwise determinism&lt;/strong> 이 모델은 체크포인트에서 완전 재현이 가능하며, 이는 JAX+XLA+T5X가 제공하는 비트 단위 결정적 모델링 프레임워크와, 단계 번호만으로 학습 배치의 내용을 결정하는 결정적 데이터셋 파이프라인 덕분이다. 따라서 모델이 한 번의 실행에서 특정 단계까지 학습되었다면, 그 체크포인트에서 다시 시작해도 동일한 결과를 보장한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dropout&lt;/strong> 이 모델은 드롭아웃 없이 학습되었지만, 대부분의 경우에는 0.1의 드롭아웃을 사용하여 미세조정 한다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="training-instability">Training Instability&lt;/h3>
&lt;p>가장 큰 모델을 학습하면서 gradient clipping이 적용되어 있음에도 불구하고, 불규칙한 간격으로 20번가량 손실이 급증하는 현상을 관찰하였다. 이는 작은 모델에서는 발견되지 않았으며, 큰 모델의 학습 비용 때문에 이 문제를 완화하기 위한 명확한 전략을 세우지 못하였다.&lt;/p>
&lt;p>손실 증가 문제를 완화하기 위해, 손실 증가가 시작되기 전 체크포인트에서 학습을 재시작하고, 손실 증가가 관찰된 데이터 배치를 건너뛰는 전략을 사용했다. 이 방법은 손실 증가가 특정 데이터 배치와 모델 파라미터의 특정 상태의 조합으로 발생한다는 것을 보여주며, &amp;ldquo;bad data&amp;rdquo; 때문이 아님을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;h3 id="english-nlp-tasks">English NLP tasks&lt;/h3>
&lt;p>PaLM 모델은 이전 연구와 동일한 29개의 영어 벤치마크를 이용하여 평가한다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Open-Domain Closed-Book Question Answering tasks:&lt;/strong> TriviaQA, Natural Questions, Web Questions&lt;/li>
&lt;li>&lt;strong>Cloze and Completion tasks:&lt;/strong> LAMBADA, HellaSwag, StoryCloze&lt;/li>
&lt;li>&lt;strong>Winograd-style tasks:&lt;/strong> Winograd, WinoGrande&lt;/li>
&lt;li>&lt;strong>Common Sense Reasoning:&lt;/strong> PIQA, ARC, OpenBookQA&lt;/li>
&lt;li>&lt;strong>In-context Reading Comprehension:&lt;/strong> DROP, CoQA, QuAC, SQuADv2, RACE&lt;/li>
&lt;li>&lt;strong>SuperGLUE&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Natural Language Inference (NLI):&lt;/strong> Adversarial NLI&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table4.png"
width="968"
height="1244"
srcset="https://kurtkim.github.io/p/palm/images/table4_hua13741bf4df7c3dc8993730568ef574d_274515_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table4_hua13741bf4df7c3dc8993730568ef574d_274515_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="186px"
>&lt;/p>
&lt;p>PaLM 540B는 대부분의 작업에서 이전 state-of-the-art를 능가하였다. 특히, 읽기 이해와 NLI 작업에서 더욱 두드러졌다. 이는 모델 크기 뿐 아니라, 사전 학습 데이터셋, 학습 전략, 학습 중 관찰된 토큰 수 등이 중요하게 작용했음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table5.png"
width="494"
height="226"
srcset="https://kurtkim.github.io/p/palm/images/table5_hu3527a0d28514c866b7132b05a4581fea_30514_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table5_hu3527a0d28514c866b7132b05a4581fea_30514_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="524px"
>&lt;/p>
&lt;p>PaLM 540B가 자연어 이해와 자연어 생성 작업에서 평균 점수를 5점 이상 향상시켰다. 특히, PaLM 62B는 GPT-3 175B를 두 카테고리에서 모두 능가하였다.&lt;/p>
&lt;h4 id="massive-multitask-language-understanding">Massive Multitask Language Understanding&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table6.png"
width="914"
height="170"
srcset="https://kurtkim.github.io/p/palm/images/table6_hu0ffda79964b359911c752ba11766fe13_34544_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table6_hu0ffda79964b359911c752ba11766fe13_34544_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="537"
data-flex-basis="1290px"
>&lt;/p>
&lt;p>PaLM 모델은 다양한 주제를 다루는 대규모 다중작업 언어 이해 벤치마크에서 평가되었고, 평균 점수를 약 2점 향상시켰다. PaLM 540B는 &amp;ldquo;Other&amp;quot;을 제외한 모든 카테고리에서 Chinchilla 모델을 능가하였다.&lt;/p>
&lt;h4 id="finetuning">Finetuning&lt;/h4>
&lt;p>SuperGLUE 벤치마크에서 PaLM 모델을 미세조정하는 실험을 진행했고, 일반적으로 15K 단계 이내에 수렴했다. 이 과정에서는 Adafactor optimizer를 사용하고 batch size는 32였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table7.png"
width="1228"
height="180"
srcset="https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="682"
data-flex-basis="1637px"
>&lt;/p>
&lt;p>PaLM이 가장 뛰어난 성능을 보여주는 모델과 경쟁력 있게 성능을 내는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table8.png"
width="990"
height="136"
srcset="https://kurtkim.github.io/p/palm/images/table8_hu20a202c16920982ef7d5644d60c68753_29808_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table8_hu20a202c16920982ef7d5644d60c68753_29808_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="727"
data-flex-basis="1747px"
>&lt;/p>
&lt;p>또한 few-shot과 미세조정 결과 사이에 여전히 큰 차이가 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table9.png"
width="1272"
height="178"
srcset="https://kurtkim.github.io/p/palm/images/table9_hu7179d5d88d289d45789eec86efa95118_49867_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table9_hu7179d5d88d289d45789eec86efa95118_49867_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="714"
data-flex-basis="1715px"
>&lt;/p>
&lt;p>PaLM이 최첨단 모델과 경쟁력 있으며, 리더보드에서 가장 뛰어난 성능을 내는 decoder-only autoregressive 언어 모델을 크게 능가하는 것을 보여준다.&lt;/p>
&lt;h3 id="big-bench">BIG-bench&lt;/h3>
&lt;p>BIG-bench는 대규모 언어 모델에 대한 도전적인 작업을 목표로 하는 벤치마크로, 다양한 언어 모델링 작업을 포함한다. 이 벤치마크에서 PaLM 모델 계열은 few-shot 평가를 수행하였고, 텍스트 작업에 초점을 두었다. 인간의 성능도 같은 지표로 측정되었으며, 이를 통해 &amp;ldquo;best&amp;quot;와 &amp;ldquo;average&amp;rdquo; 인간 성능이 계산되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure3.png"
width="1214"
height="506"
srcset="https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;p>PaLM 모델 계열은 BIG-bench에서 상당한 성능을 나타냈으며, GPT-3, Gopher, Chinchilla를 크게 능가하였다. 특히, 5-shot PaLM 540B는 동일한 작업을 수행한 인간의 평균 점수보다 높은 점수를 얻었다. 또한, 규모에 따른 PaLM 모델의 성능은 log-linear 행동을 보였으며, 이는 추가적인 스케일링이 성능 향상을 가져올 가능성을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure4.png"
width="1282"
height="490"
srcset="https://kurtkim.github.io/p/palm/images/figure4_hu2b8641c88a546db059f15eb12490f615_62019_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure4_hu2b8641c88a546db059f15eb12490f615_62019_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="627px"
>&lt;/p>
&lt;p>BIG-bench에서 PaLM이 특별히 눈에 띄는 성능을 보인 몇 가지 작업을 강조한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>goal step wikihow&lt;/strong> 목표는 이벤트 간의 목표-단계 관계에 대해 추론하는 것이다.
Input: ”clean silver,” which step should be done ﬁrst? (a) dry the silver (b) handwash the silver.
Answer: (b) handwash the silver.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>logical args&lt;/strong> 목표는 문단에서 올바른 논리적 추론을 예측하는 것이다.
Input: Students told the substitute teacher they were learning trigonometry. The substitute told them that instead of teaching them useless facts about triangles, he would instead teach them how to work with probabilities. What is he implying? (a) He believes that mathematics does not need to be useful to be interesting. (b) He thinks understanding probabilities is more useful than trigonometry. (c) He believes that probability theory is a useless subject.
Answer: (b) He thinks understanding probabilities is more useful than trigonometry.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>english proverbs&lt;/strong> 목표는 어떤 속담이 텍스트 구절을 가장 잘 설명하는지 추측하는 것이다.
Input: Vanessa spent lots of years helping out on weekends at the local center for homeless aid. Recently, when she lost her job, the center was ready to oﬀer her a new job right away. Which of the following proverbs best apply to this situation? (a) Curses, like chickens, come home to roost. (b) Where there is smoke there is ﬁre (c) As you sow, so you shall reap.
Answer: (c) As you sow, so you shall reap.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>logical sequence&lt;/strong> 목표는 논리적인 순서대로 배열하는 것이다.
Input: Which of the following lists is correctly ordered chronologically? (a) drink water, feel thirsty, seal water bottle, open water bottle (b) feel thirsty, open water bottle, drink water, seal water bottle (c) seal water bottle, open water bottle, drink water, feel thirsty.
Answer: (b) feel thirsty, open water bottle, drink water, seal water bottle.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>navigate&lt;/strong> 목표는 간단한 네비게이션 지시를 따르고, 어디에 도착할지 파악하는 것이다.
Input: If you follow these instructions, do you return to the starting point? Always face forward. Take 6 steps left. Take 7 steps forward. Take 8 steps left. Take 7 steps left. Take 6 steps forward. Take 1 step forward. Take 4 steps forward.
Answer: No.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>mathematical induction&lt;/strong> 목표는 실제 세계의 수학과 상충하더라도 수학적 귀납법 규칙에 따라 논리적 추론을 수행하는 것이다.
nput: It is known that adding 2 to any odd integer creates another odd integer. 2 is an odd integer. Therefore, 6 is an odd integer. Is this a correct induction argument (even though some of the assumptions may be incorrect)?
Answer: Yes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure5.png"
width="1150"
height="762"
srcset="https://kurtkim.github.io/p/palm/images/figure5_hu7f70e194740d88da067dccae36cdf0ef_190827_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure5_hu7f70e194740d88da067dccae36cdf0ef_190827_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="362px"
>&lt;/p>
&lt;p>goal step wikihow와 logical args에 대한 성능은 log-linear 스케일링 곡선을 따르며, PaLM 540B 모델은 최고의 인간 성능에 가까워진다. 영어 속담과 논리적 순서에 대한 성능도 강력하지만, 개선 곡선은 불연속적이다. 특히, 특정 규모에 도달하면서만 특정 능력이 나타나는 것이 확인되었다. PaLM 62B에서 25%에서 PaLM 540B의 87%로 크게 개선된 영어 속담 성능은 매우 흥미로운 결과이다.&lt;/p>
&lt;p>불연속성에 대한 예로, PaLM의 논리적 순서 작업에서 8b, 62b, 540b 모델에 대한 정확도가 각각 13%, 25%, 87%였다. 이에 따라, 540b에 대한 예상 정확도는 37%였지만, 실제 정확도는 87%로, 불연속성은 +50%였다. 전체 150개 작업 중 25%의 작업에서 불연속성이 +10% 이상, 15%의 작업에서 +20% 이상 나타났으며, 이는 스케일에서의 불연속적인 개선이 일반적인 현상임을 보여준다.&lt;/p>
&lt;p>모든 작업에서 규모가 이익을 가져다주는 것은 아니다. 네비게이션과 수학적 귀납 작업에서 PaLM 540B는 PaLM 62B를 살짝 능가하지만, 두 모델 모두 최고의 인간 성능에서는 아직 멀리 떨어져 있다. 이는 작업의 예제 수준의 난이도에 큰 변동성이 있다는 것을 나타낸다. 특히, 수학적 귀납 작업에서는 올바른 가정과 잘못된 가정을 가진 예제들이 있어, 모델들이 가정의 정확성에 대한 문제를 해결하는데 어려움을 겪는 것으로 보인다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure6.png"
width="1268"
height="490"
srcset="https://kurtkim.github.io/p/palm/images/figure6_hu8e7975e3396d9807f159e8f50bd39708_72597_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure6_hu8e7975e3396d9807f159e8f50bd39708_72597_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="258"
data-flex-basis="621px"
>&lt;/p>
&lt;p>PaLM 540B가 전반적으로 인간 평가의 평균 성능을 능가하지만, 개별 작업의 35%에서는 인간의 평균 성능이 더 높다는 것을 보여준다. 이는 BIG-bench에서 아직도 상당한 개선 여지가 있다는 것을 의미한다.&lt;/p>
&lt;p>PaLM 540B는 여러 언어의 표현 및 큰 양의 정보를 기억하는 능력 등을 통해 인간의 평균 성능을 능가하는 일부 작업에서 뛰어난 성과를 보여준다. 그 중에는 원인과 결과를 판단하는 작업도 포함되어 있다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>cause and eﬀect (one sentence no prompt)&lt;/strong> 하나의 문장으로 된 서브태스크에서, 이벤트들은 두 가지 다른 순서로 하나의 문장으로 결합되며, 각 문장의 log-likelihood는 모델로 점수화된다. 프롬프트는 제공되지 않는다.
Input A: I washed the car because my car got dirty.
Input B: My car got dirty because I washed the car.
Higher-Likelihood Sentence: I washed the car because my car got dirty.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>cause and eﬀect (two sentence)&lt;/strong> 두 문장의 서브태스크에서는, 모델에게 두 가지 이벤트가 보여지고, 어떤 문장이 다른 이벤트를 일으킨 원인에 해당하는지 선택해야 한다.
Input: For each example, two events are given. Which event caused the other? (a) My car got dirty. (b) I washed the car.
Correct Prediction: (a) My car got dirty.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>모든 PaLM 모델이 한 문장 프롬프트 없는 작업에서 잘 수행되었고, 특히 8B 모델은 80% 이상의 정확도를 보여주었다. 그러나 두 문장 버전의 작업에서는 작은 모델의 성능이 떨어졌다. 대신 540B 모델은 이 작업에서 90% 이상의 높은 정확도를 보여, 대규모 모델이 언어 모델링 능력을 향상시킬 수 있음을 입증하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure7.png"
width="1262"
height="418"
srcset="https://kurtkim.github.io/p/palm/images/figure7_hubd021db6bae7cc2617562c762d44270e_60551_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure7_hubd021db6bae7cc2617562c762d44270e_60551_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="301"
data-flex-basis="724px"
>&lt;/p>
&lt;p>24개의 BIG-bench 작업 중 가벼운 평가 대상인 BIG-bench Lite의 상세 평가 결과를 보여준다. 일부 작업들은 해결되었거나 거의 해결된 상태이지만, 인간 평가의 최고 성능 점수에 비해 다른 일부 작업은 아직 해결되지 않았다.&lt;/p>
&lt;p>BIG-bench 데이터를 모델이 암기하여 성과를 달성한 것이 아닌지 확인하기 위해 여러 단계를 거쳤다. BIG-bench 작업 파일의 고유한 canary 문자열이 PaLM 학습 데이터에 없음을 확인했고, BIG-bench 데이터셋은 학습 데이터 수집 시점에 인터넷에 없었다. 대부분의 BIG-bench 작업들은 새로운 벤치마크이며, 모델의 우수한 성능을 보인 작업들을 임의로 점검하여 정보 유출이 없음을 확인하였다.&lt;/p>
&lt;h3 id="reasoning">Reasoning&lt;/h3>
&lt;p>PaLM은 여러 단계의 산술이나 상식적인 논리적 추론을 필요로 하는 추론 작업에서 평가된다. 언어 모델은 다양한 작업을 수행할 수 있지만, 여러 단계의 추론을 필요로 하는 작업을 수행하는 데에는 어려움이 있다. 이 작업에서는 두 가지 주요 추론 벤치마크 카테고리를 평가한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Arithmetic reasoning&lt;/strong> 이 작업들은 대부분 초등학교 수준의 자연어 수학 문제를 포함하며, 여러 단계의 논리적 추론이 필요하다. 수학은 대체로 간단하며, 어려운 부분은 자연어를 수학식으로 변환하는 것이다. 이 연구에서는 모델 자체가 수학을 수행하는 계산기 형태와 직접 추론 형태를 모두 평가했하였다.
Input: Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
Answer: The answer is 11.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Commonsense reasoning&lt;/strong> 이 작업들은 강한 세계 지식을 필요로 하는 질문 응답 작업이며, 세계에 대한 여러 논리적 추론을 연결하는 것을 필요로 한다. 이는 단순히 사실에 기반한 질문 응답이 아니다.
Input: Q: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?
Answer Choices: (a) take time (b) dawdle (c) go slowly (d) ocean (e) slow down Answer: The answer is (e) slow down.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure8.png"
width="1150"
height="496"
srcset="https://kurtkim.github.io/p/palm/images/figure8_hue021553084c21e4b466167a44884c546_155083_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure8_hue021553084c21e4b466167a44884c546_155083_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="556px"
>&lt;/p>
&lt;p>최근 연구들은 대형 언어 모델이 최종 답변을 생성하기 전에 중간 추론 단계를 생성하면 정확도가 크게 향상될 수 있음을 보여주었다. 이 기술을 &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅이라고 부릅니다. few-shot 설정에서, 중간 추론 단계는 수동으로 작성되고, 모델은 테스트 예시에 대한 자신의 &amp;ldquo;chain-of-thought&amp;quot;을 생성한다. 생성된 &amp;ldquo;chain-of-thought&amp;quot;은 오류 분석과 모델 해석에 유용할 수 있지만, 평가에는 최종 답변만 사용된다.&lt;/p>
&lt;h4 id="results">Results&lt;/h4>
&lt;p>이 연구에서는 모델 규모와 &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅만으로도 다양한 산술 및 상식 추론 작업에서 최첨단의 정확도를 달성할 수 있음을 보여준다. 이전의 많은 연구들은 도메인 특정 아키텍처, 작업 특정 미세조정, 작업 특정 검증자를 결합했지만, 이 연구에서는 단순히 few-shot 프롬프팅을 통해 작업들을 표현했다. 산술 추론 데이터셋의 경우, 사후 외부 계산기를 사용해 모델 예측을 보강했지만, 이는 어떤 데이터셋에서도 성능을 5% 이상 향상시키지 않았다.&lt;/p>
&lt;p>&amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅을 사용해, PaLM의 성능을 산술 데이터셋인 GSM8K, SVAMP, MAWPS, AQuA와 상식 추론 데이터셋인 CommonsenseQA와 StrategyQA에서 평가하였다. 이 프롬프팅 설정은 오직 8-shot 예시만을 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table10.png"
width="800"
height="254"
srcset="https://kurtkim.github.io/p/palm/images/table10_hubb89b4be08e9d85f51629839e28e57bb_54803_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table10_hubb89b4be08e9d85f51629839e28e57bb_54803_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="755px"
>&lt;/p>
&lt;p>GSM8K에서 PaLM의 결과를 강조하며, 이전 state-of-the-art인 Cobbe et al. (2021)이 모델 미세조정, &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅, 외부 계산기, 작업 특정 검증자를 사용하였다. 외부 계산기와 결합된 8-shot &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅을 사용한 PaLM 540B는 58%의 성능을 달성해 이전 state-of-the-art인 55%를 능가하였다. 이는 &amp;ldquo;chain-of-thought&amp;rdquo; 없는 PaLM 540B와 &amp;ldquo;chain-of-thought&amp;quot;이 있는 PaLM 62B를 크게 능가하였다. PaLM 62B 모델이 잘못 처리한 문제들은 대체로 의미 이해, 한 단계 누락, 그리고 다른 오류들에 속하며, 540B 모델 크기로 확장하면 이러한 오류들의 대부분이 수정되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure10.png"
width="1264"
height="532"
srcset="https://kurtkim.github.io/p/palm/images/figure10_huea06ca2898ddb563d235af1fa303cd49_171272_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure10_huea06ca2898ddb563d235af1fa303cd49_171272_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="570px"
>&lt;/p>
&lt;p>7개의 추론 데이터셋에서, PaLM 540B+&amp;ldquo;chain-of-thought&amp;quot;을 이용한 8-shot 예측은 4개의 작업에서 최고의 정확도를 달성하였으며, 나머지 3개의 작업에서는 state-of-the-art에 근접한 결과를 보여주었다. GSM8K에는 중간 추론 단계가 포함되었지만 다른 벤치마크에는 포함되지 않았다. state-of-the-art과 모델 확장이 모든 작업에서 크게 도움이 되었으며, 두 기술 없이는 PaLM이 한 가지 작업에서만 최고 수준을 달성했을 것이다. 데이터 오염이 없었음을 n-gram 겹침 분석을 통해 확인하였다.&lt;/p>
&lt;h3 id="code-tasks">Code Tasks&lt;/h3>
&lt;p>최근 연구에서 대형 언어 모델이 경쟁 프로그래밍, 코드 완성, 자연어 명세에서 프로그램 합성 등의 코딩 작업에 유용함이 보여졌다. 이번 섹션에서는 PaLM 모델이 다양한 코딩 작업에서 뛰어난 결과를 달성하는 것을 보여준다.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Text-to-code.&lt;/strong> 자연어 설명이 주어진 상태에서 코드를 작성하는 세 가지 작업을 고려한다. HumanEval과 MBPP 데이터셋에서는, 모델에게 몇 문장의 영어 설명과 소량의 입력-출력 예시가 주어지며, 주로 단일 함수인 짧은 파이썬 프로그램을 생성하는 것이 목표이다. 또한, GSM8K 데이터셋에서 파생된 GSM8K-Python 작업을 소개한다. 이 작업에서는 올바른 답을 제공하는 대신 올바른 해결책을 반환하는 파이썬 프로그램을 생성하는 것이 목표이다. 데이터셋의 문제 중 네 개를 few-shot 예시로 사용하기 위해 수동으로 파이썬 프로그램으로 변환하였다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Code-to-code.&lt;/strong> TransCoder는 C++ 프로그램을 파이썬으로 번역하는 작업이다. 데이터셋에서 Python과 C++ 모두에 나타나는 함수들을 수집하고, 이 중 세 가지 다른 유형의 함수를 few-shot 프롬프트로 사용하며, 나머지는 테스트 세트를 만드는 데 사용하였다. 또한, 컴파일에 실패하는 C 프로그램을 성공적으로 컴파일할 수 있도록 수정하는 DeepFix 코드 수리 작업에서도 평가하였다. 결함 있는 코드에 대한 컴파일러 오류를 모델에 제공하고, 1260개의 프로그램에 대해 테스트하였다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>pass@k 메트릭을 사용해 결과를 보고하며, 이는 모델이 $k$개의 코드 샘플을 제공하고 그 중 하나라도 문제를 해결하면 문제가 해결된 것으로 간주한다. 간단히 문제를 해결하는 샘플의 비율을 보고하며, 이를 측정하기 위해 MBPP와 GSM8K의 테스트 데이터를 사용한다. 1개의 샘플일 경우 greedy decoding을, 그 이상일 경우 nucleus sampling을 사용한다.&lt;/p>
&lt;p>PaLM 모델을 LaMDA 137B 파라미터 모델과 초기 Codex 모델 12B와 비교한다. LaMDA는 GitHub의 코드에 대해 학습되지 않았지만, 코드 관련 웹 문서를 일부 포함하여 프로그램 합성 능력을 가지며, Codex 모델은 HumanEval 데이터셋에서의 결과만을 보고한다.&lt;/p>
&lt;p>다른 데이터셋에서 Codex 결과를 얻기 위해, OpenAI Davinci Codex API를 사용했다. 이는 2021년 9월 1일부터 2022년 3월 10일까지 진행되었고, 가장 최신 버전인 Davinci 모델 버전 1을 사용했다. Davinci Codex 모델에 대한 많은 정보는 공개되지 않아 성능 차이의 원인을 이해하는 것은 어렵지만, 이 비교는 고려하는 작업의 본질적인 어려움을 이해하는 데 유용하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table12.png"
width="1074"
height="404"
srcset="https://kurtkim.github.io/p/palm/images/table12_hu85d3a19441c4936a062e570c3986b2d2_95505_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table12_hu85d3a19441c4936a062e570c3986b2d2_95505_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="265"
data-flex-basis="638px"
>&lt;/p>
&lt;p>&lt;strong>Datasets&lt;/strong> PaLM 모델은 학습 세트에 GitHub 코드를 포함하며, 총 39B 개의 코드 토큰이 사전 학습 데이터셋에 있다. Python 프로그래밍을 테스트하는 평가를 위해, ExtraPythonData라는 추가 데이터셋을 수집했고, 이는 사전 학습에 사용되지 않은 GitHub에서 5.8B 개의 토큰을 수집한 것이다. 이 데이터는 Java, HTML, Javascript, Python, C, PHP, C#, C++ 등의 언어를 포함하고 있다.&lt;/p>
&lt;p>&lt;strong>PaLM 540B&lt;/strong> PaLM 모델은 모든 작업에서 LaMDA보다 높은 성능을 보여주며, HumanEval에서는 Codex 12B와 비슷한 수준이다. 이는 동일한 모델이 코드와 자연어 작업 모두에서 뛰어난 성능을 보여주는 첫 번째 큰 언어 모델이라는 점에서 중요하다. PaLM은 Python 코드 토큰 약 2.7B 개로 학습되었는데, 이는 Codex 모델의 Python 토큰 1000억 개에 비해 50배 적다. 그럼에도 불구하고 PaLM은 비슷한 성능을 보여주어, 다른 프로그래밍 언어와 자연어 데이터로부터의 전이와 큰 모델이 작은 모델보다 효율적일 수 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table11.png"
width="700"
height="226"
srcset="https://kurtkim.github.io/p/palm/images/table11_hu2faab6ab491b7073a9ff2c2dc15711d8_32278_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table11_hu2faab6ab491b7073a9ff2c2dc15711d8_32278_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="743px"
>&lt;/p>
&lt;p>&lt;strong>PaLM-Coder&lt;/strong> PaLM 모델을 Python 코드와 다양한 언어의 코드, 그리고 자연어에 대해 미세 조정한 결과, PaLM-Coder 540B의 성능이 크게 향상되었다. 이는 미세 조정을 하지 않은 모델에 비해 HumanEval에서 +12%, MBPP에서 +5%의 절대적인 성능 향상을 보여주었다. 또한, 모델의 규모가 증가함에 따라 성능이 계속 향상되는 것을 확인하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure12.png"
width="1214"
height="528"
srcset="https://kurtkim.github.io/p/palm/images/figure12_hu37ce5c724d96c0b24bd0ad2b4311a1f6_174562_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure12_hu37ce5c724d96c0b24bd0ad2b4311a1f6_174562_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>GSM8K-Python 데이터셋에 대해, PaLM-Coder 540B는 8-shot 프롬프트에서 pass@1 점수 57.5를 얻었고, 반면에 PaLM 540B 모델은 pass@1 점수 58.1을 달성하였다.&lt;/p>
&lt;p>&lt;strong>DeepFix Code Repair&lt;/strong> PaLM-Coder 540B 모델은 DeepFix 코드 수정 작업에서 82.1%의 컴파일률을 달성하여 뛰어난 성능을 보여주었다. 이는 이전 작업에서 달성한 71.7%보다 높은 결과이다. 프롬프트는 다양한 일반적인 오류를 포함한 두 쌍의 깨진 및 수정된 C 프로그램을 손으로 작성하였으며, 이후 모델이 수정된 전체 코드를 예측하게 하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table13.png"
width="1174"
height="328"
srcset="https://kurtkim.github.io/p/palm/images/table13_hu42e80d5d41f68f17a097f82c44f9348b_98064_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table13_hu42e80d5d41f68f17a097f82c44f9348b_98064_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="357"
data-flex-basis="859px"
>&lt;/p>
&lt;p>코드 수정에서는 이상적으로 깨진 코드의 작은 부분만 수정하고 싶기 때문에 모델이 변경한 코드의 양을 평가하는 것이 중요하다. PaLM은 가장 작은 편집을 생성하는 반면, PaLM-Coder는 작은 정규화된 편집 거리를 가진 편집에 대해 가장 높은 성공률을 보여주었다. 반면에 Davinci Codex는 변경된 라인 수가 적은 편집에서 가장 높은 성공률을 보였다. 이는 PaLM-Coder가 더 많은 라인에 대해 적은 수의 문자를 변경하는 경향이 있음을 의미한다.&lt;/p>
&lt;p>&lt;strong>Discussion&lt;/strong> 소프트웨어 개발에서 언어 모델 기반 시스템을 사용할 때, 생성된 코드가 잘못되거나 미묘한 버그를 도입할 위험이 있다. 개발자들은 제안된 코드를 프로그램에 추가하기 전에 검토해야 하지만, 항상 미묘한 버그를 찾을 수는 없다. 코드 제안은 테스트 스위트로 확인할 수 있지만, 소수의 테스트 케이스로부터 솔루션이 기능적으로 올바르다는 것을 추론하는 것은 항상 안전하지 않다. 이에 따라, 기능적 정확성에 대한 더 철저한 테스트가 필요하다.&lt;/p>
&lt;p>기능적 정확성은 소스 코드 품질의 한 가지 측면일 뿐이며, 언어 모델이 생성한 코드 제안은 읽기 쉽고, 견고하고, 빠르고, 안전해야 한다. DeepFix는 PaLM-Coder의 현재 예측과 관련된 문제를 보여주는데, 수정된 프로그램은파일되지만 입력의 형식과 크기에 대한 가정에 의존하기 때문에 반드시 안전한 것은 아니다. 이러한 제안은 더 일반적인 상황에서는 원치 않을 수 있다. 개발자가 제안된 코드를 이해하고 신뢰하는 것은 여전히 해결되지 않은 문제이며, 가독성과 보안성을 평가하는 이전의 연구가 있지만, 이 분야는 아직 초기 단계에 있다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>기계 번역은 텍스트를 한 언어에서 다른 언어로 변환하는 작업이다. GPT-3 같은 거대 언어 모델들은 병렬 텍스트에 대해 명시적으로 학습받지 않았음에도 불구하고 번역 능력을 보여주었다. 이번 섹션에서는 다양한 언어 쌍에 대해 PaLM의 번역 능력을 평가하며, 이 과정에서 WMT에서 제공하는 언어 쌍을 주로 사용할 예정이다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>English-centric language pairs&lt;/strong> 이전 모델들이 주로 다루었던 전통적인 언어 쌍은 영어를 포함하고, 병렬 데이터의 양에 따라 고자원, 중자원, 저자원으로 구분한다. 이번 분석에서는 WMT'14의 영어-프랑스어(고자원), WMT'16의 영어-독일어(중자원), 그리고 WMT'16의 영어-루마니아어(저자원)를 언어 쌍으로 사용한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Direct language pairs&lt;/strong> 번역 시스템이 영어를 거치지 않고 어떤 언어 쌍이든 직접 번역하는 능력이 점점 중요해지고 있다. 이를 테스트하기 위해, 프랑스어와 독일어 사이의 직접 번역 능력을 WMT'19 데이터를 사용해 확인한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Extremely-low resource language pairs&lt;/strong> 모든 언어 쌍은 병렬 데이터가 없어 제로-리소스 상태이다. 그러나 단일 언어 데이터가 적은 언어, 예를 들어 이 연구에서 선택한 카자흐스탄어는 흥미로운 점이 있다. 프랑스어와 독일어는 각각 240억, 260억의 토큰을 가지고 있는 반면, 카자흐스탄어는 1.34억 토큰만 가지고 있다. 이를 평가하기 위해 WMT'19의 영어-카자흐스탄어를 사용하였다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table14.png"
width="928"
height="330"
srcset="https://kurtkim.github.io/p/palm/images/table14_huadf9b0c805cd9cbcff3b8cc9aa8b69db_70951_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table14_huadf9b0c805cd9cbcff3b8cc9aa8b69db_70951_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="674px"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Evaluation on English-centric language pairs&lt;/strong> 전통적인 영어 중심 언어 쌍에서 0-shot, 1-shot, few-shot 설정에서 PaLM을 평가하였다. 이 모델은 GPT-3와 FLAN과 같은 다른 모델들을 능가하며, 때때로 최대 13 BLEU 점수 차이를 보여주었다. 독일어-영어와 루마니아어-영어에서는 감독된 기준선을 능가했지만, 이 기준들이 최근 변경된 WMT 작업에 따라 오래되었을 수 있음을 인정한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure15.png"
width="1298"
height="706"
srcset="https://kurtkim.github.io/p/palm/images/figure15_hu4d6f9fa9e4694490d447df1b45fe040a_205909_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure15_hu4d6f9fa9e4694490d447df1b45fe040a_205909_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="441px"
>&lt;/p>
&lt;p>모델 크기를 8B에서 62B, 그리고 540B로 확대하면서 0-shot 번역의 결과에서 급격한 BLEU 점수 상승이 관찰되었다. 특히, 영어-독일어는 13 BLEU, 영어-프랑스어는 17 BLEU 증가를 보였습니다. 이는 &amp;ldquo;power law&amp;rdquo; 법칙에 따르지 않는 현상이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table15.png"
width="686"
height="270"
srcset="https://kurtkim.github.io/p/palm/images/table15_huf0a4711c70b039caa45845db4276e543_39524_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table15_huf0a4711c70b039caa45845db4276e543_39524_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="609px"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Evaluation on direct and extremely-low resource language pairs&lt;/strong> PaLM은 직접적이고 극도로 저자원 언어 쌍에서의 성능을 평가하였다. WMT'19에서 가장 높은 점수를 받은 제출물을 활용하였다. 이 도전적인 상황에서 PaLM은 프랑스어-독일어에서만 지도 성능을 맞출 수 있었지만, 독일어-프랑스어와 카자흐스탄어-영어에서는 강력한 성능을 보여주었다.&lt;/li>
&lt;/ul>
&lt;h4 id="further-ﬁndings-and-analysis">Further ﬁndings and analysis&lt;/h4>
&lt;p>결과는 다음과 같은 관찰로 정리할 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Translation quality is better when translating into English rather than out of English.&lt;/strong> 모든 영어 중심 언어 모델에서 관찰되는 공통적인 패턴이며, PaLM의 성능을 살펴보면서 비슷하게 나타난다. 다국어 데이터를 우선시하면 이 효과가 완화될 것으로 추정한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompts can deliver even more value than a single example.&lt;/strong> 대부분의 경우, 언어 이름을 사용하여 번역을 유도하는 0-shot 설정이 입력-출력 예시만을 사용하는 1-shot 및 few-shot 설정보다 더 높은 성능을 보여주었다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Generalist models relying solely on self-supervision can match specialized models at smaller scales.&lt;/strong> 대부분의 전용 번역 기준은 parameter가 1B개 미만으로, 가장 큰 PaLM 설정보다 두 자릿수가 작다. 그러나, 대형 번역 모델이 다양한 작업에 적응할 수 있음을 확인했으므로, specialist도 generalist로 활용될 수 있다. 이로 인해, 자원이 풍부한 상황에서는 specialist를 학습시킬지, 아니면 generalist를 학습시킬지에 대한 질문이 제기된다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="multilingual-natural-language-generation">Multilingual Natural Language Generation&lt;/h3>
&lt;p>자연어 생성은 텍스트나 비언어적 정보를 입력으로 받아 이해하기 쉬운 텍스트를 자동 생성하는 과제이다. 그러나 과거에는 비슷한 크기의 모델들에 대해 few-shot 조건부 자연어 생성에 대한 탐구가 없었다. 대형 언어 모델들(GPT-3, GLaM, Gopher, LaMDA, Megatron-Turing NLG) 중 어느 것도 이런 과제에 대한 결과를 보고하지 않았다.&lt;/p>
&lt;p>이 연구는 few-shot 모델링을 위한 첫 번째 대형 언어 모델 벤치마크를 제시하며, 비교 대상으로 LaMDA 137B를 사용하였다. 이 모델은 이전 연구에서 벤치마크 결과를 보고하지 않았지만 테스트는 할 수 있었다.&lt;/p>
&lt;p>미세 조정을 위한 이전 최고 성능은 주로 T5, mT5, BART 등의 encoder-decoder 모델에서 나왔다. 이들 모델은 PaLM보다 작지만, 채우기를 위해 학습된 모델들은 종종 더 큰 decoder-only 언어 모델을 능가한다. 따라서, 이 연구에서는 대규모 모델이 decoder-only 언어 모델의 약점을 보완할 수 있는지를 중요하게 비교하고 있다.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong> 우리는 PaLM을 GEM 벤치마크의 여섯 가지 작업(세 가지 요약, 세 가지 데이터-텍스트 생성)으로 평가하였다. 이는 체코어, 영어, 독일어, 러시아어, 스페인어, 터키어, 베트남어 등의 언어를 포함한 데이터셋을 사용하였다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>MLSum&lt;/strong> 다중 문장으로 뉴스 기사를 요약 [독일어/스페인어]&lt;/li>
&lt;li>&lt;strong>WikiLingua&lt;/strong> WikiHow의 단계별 지시사항을 매우 간결한 문장으로 요약 [영어/스페인어/러시아어/터키어/베트남어 → 영어]&lt;/li>
&lt;li>&lt;strong>XSum&lt;/strong> 한 문장으로 뉴스 기사를 요약 [영어]&lt;/li>
&lt;li>&lt;strong>Clean E2E NLG&lt;/strong> 주어진 키-값 속성 쌍을 바탕으로, 레스토랑을 한 두 문장으로 설명 [영어]&lt;/li>
&lt;li>&lt;strong>Czech Restaurant response generation&lt;/strong> 대화 맥락과 대화 행동 표현을 바탕으로, 스마트 어시스턴트가 제공할 응답 생성 [체코어]&lt;/li>
&lt;li>&lt;strong>WebNLG 2020&lt;/strong> 주어-동사-목적어 삼중체를 문법적이고 자연스럽게 한 문장 이상으로 표현 [영어/러시아어]&lt;/li>
&lt;/ul>
&lt;p>모델의 추론 시간을 줄이기 위해, 테스트 세트가 5,000개를 초과하면 균일하게 샘플링한다.&lt;/p>
&lt;p>&lt;strong>Metrics&lt;/strong> Gehrmann et al. 의 제안에 따라 ROUGE-2, ROUGE-L, BLEURT-20 결과를 보고하며, 이 섹션의 본문은 ROUGE-2의 F-측정에 초점을 맞춘다.&lt;/p>
&lt;p>&lt;strong>Few-shot evaluation methodology&lt;/strong> PaLM은 few-shot 추론에 사용되며, 작업 특정 프롬프트를 입력에 연결하고 출력 프롬프트를 출력에 추가한다. 요약을 위한 긴 입력은 2048 토큰으로 줄이고, few-shot 예시들은 두 줄의 공백으로 분리한다. 모든 few-shot 예시들은 훈련 데이터에서 무작위로 추출된다.&lt;/p>
&lt;p>&lt;strong>Finetuning methodology&lt;/strong> 미세조정 시, decoder만 사용하며, 입력과 목표를 연결하지만, 손실은 목표 부분에서만 계산한다. 연결된 시퀀스는 2048 토큰으로 잘라내고, 목표를 위해 512 토큰을 예약한다. 이 과정은 요약 작업에서만 필요하다.&lt;/p>
&lt;p>PaLM 미세조정은 $5×10^-5$ 의 learning rate와 optimizer 리셋을 사용하며, 검증 세트에서 가장 좋은 ROUGE 점수를 보인 모델을 선택한다. 추론은 $k = 10$의 top-k 샘플링으로 수행되고, T5 XXL 기준선은 PaLM과 동일한 parameter로 미세조정하며, beam size 4의 beam-search를 사용해 디코딩한다.&lt;/p>
&lt;h4 id="results-1">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table16.png"
width="1204"
height="608"
srcset="https://kurtkim.github.io/p/palm/images/table16_hu5ef0d5c12b11ec602efe11f8b7ed8764_143466_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table16_hu5ef0d5c12b11ec602efe11f8b7ed8764_143466_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="475px"
>&lt;/p>
&lt;p>1-shot과 미세조정의 비교는 ROUGE-2의 F-측정을 사용한다.&lt;/p>
&lt;p>이 연구는 few-shot 모델링에 초점을 맞추고 있고, 이러한 작업에 대한 공개된 few-shot 결과는 없지만, 이 결과들로부터 몇 가지 흥미로운 교훈을 얻을 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Eﬀectiveness of ﬁnetuning&lt;/strong> 요약 작업에서, 미세조정된 540B PaLM은 모든 영어 생성 작업에서 최상의 성과를 보여주며, 이는 그것의 대규모 스케일을 통해 아키텍처적 단점을 극복할 수 있다는 것을 보여준다. 62B 버전도 최상의 결과에 가깝고, 540B는 그것을 초과한다. decoder 전용 LM의 미세조정이 작업 특정 훈련 데이터가 많을 때 모든 작업에 대한 최적의 접근법이 아닐 수 있다는 것을 인지하고 있지만, 이것이 few-shot 예측에 대한 중요한 상한선 역할을 한다고 믿는다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Generation quality of English vs. non-English&lt;/strong> PaLM은 6개의 요약 작업 중 4개에서 새로운 미세조정 state-of-the art를 달성하였다. 그러나 비영어 요약에서는 최고 기록에 못 미치며, 비영어 생성에서 few-shot과 미세조정 사이의 차이는 더 크다. 이는 PaLM이 비영어 입력 처리에는 능하지만 비영어 출력 생성에는 덜 능하다는 것을 보여주며, 이는 향후 비영어 텍스트의 큰 부분에 대한 사전 학습을 통해 개선될 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>1-shot vs. ﬁnetuning gap&lt;/strong> 데이터-텍스트 결과에서, few-shot 결과는 요약과 비슷한 추세를 보이지만, 최상의 미세조정 결과와의 차이는 크게 줄어든다. FLAN은 instruction tuning 후 E2E-NLG에서 33.2, WebNLG에서 48.0의 점수를 보고하는 반면, PaLM은 어떠한 튜닝 없이 35.2와 44.4를 얻었다. 그러나 데이터-텍스트 작업은 그 크기가 작고 사전 학습 말뭉치와 크게 다르기 때문에, 미세조정 벤치마크로서의 가치가 제한적일 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Few-shot summarization&lt;/strong> 다양한 PaLM 규모에서의 few-shot 요약 결과를 비교하면, 8B에서 62B로, 그리고 62B에서 540B로 크게 향상되는 것을 볼 수 있다. 그러나, few-shot과 미세조정 사이의 차이는 아직도 크며, 1-shot 성능은 비영어 작업의 T5-base나 T5-large, 영어 작업의 T5-small와 같은 작은 미세조정 모델과 비슷하다. 이는 큰 언어 모델로의 few-shot 요약 첫 시도이므로, 조건부 생성 작업에 대한 few-shot과 미세조정 모델 사이의 간극을 좁히는 데 중요한 시작점이 될 것이라 믿는다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="multilingual-question-answering">Multilingual Question Answering&lt;/h3>
&lt;p>TyDiQA-GoldP 벤치마크를 사용해 다국어 질문 응답에 대한 모델을 few-shot 설정과 미세조정 설정에서 평가했다. few-shot 설정에서는 문맥, 질문, 답변을 새 줄 문자로 구분하고, &amp;ldquo;Q:&amp;ldquo;와 &amp;ldquo;A:&amp;ldquo;로 각각 질문과 답변을 표시했다. 미세조정에서는 영어 SuperGLUE 미세조정 실험과 동일한 hyperparameter를 사용했으며, 가장 좋은 전체 체크포인트에서의 결과를 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table7.png"
width="1228"
height="180"
srcset="https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="682"
data-flex-basis="1637px"
>&lt;/p>
&lt;p>few-shot과 미세조정 품질 사이에는 평균적으로 큰 차이가 있다는 것을 보여준다. 그러나 스와힐리어와 핀란드어 같은 특정 언어들에서는 이 차이가 적다. 프롬프트 엔지니어링과 다국어 데이터셋에 대한 다작업 적응 연구가 few-shot 결과를 더욱 개선할 수 있을 것으로 보인다.&lt;/p>
&lt;p>PaLM 540B는 비영어 데이터의 학습 비율이 적음에도 불구하고 이 작업에서 매우 경쟁력 있는 결과를 보여준다. mT5와 ByT5는 비영어 텍스트에 대해 PaLM의 6배와 1.5배 만큼 학습되었음에도 불구하고, PaLM 540B는 mT5 XXL을 능가하고 ByT5 XXL에게는 능가당하였다. 이러한 결과는 사전 학습 데이터셋에서 비영어 데이터 비율을 늘리거나, 구조적 단점이나 귀납적 편향을 극복하는 방법을 통해 더욱 개선될 수 있을 것으로 보인다.&lt;/p>
&lt;h3 id="analysis">Analysis&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure16.png"
width="1268"
height="500"
srcset="https://kurtkim.github.io/p/palm/images/figure16_hu30ac0e0e215b045dcc508a472f8d2be3_223216_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure16_hu30ac0e0e215b045dcc508a472f8d2be3_223216_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>PaLM 모델의 few-shot 성능에 대한 분석을 제시한다. 세 가지 모델(8B, 62B, 540B)을 다섯 가지 다른 작업(RTE, Natural Questions, Lambada, Story Cloze, Trivia QA)에서 연구하였으며, 이들 작업은 지식 중심에서 추론 중심까지 다양하다. Trivia QA와 Natural Questions은 문맥 문서 없이 질문만을 입력으로 제공되는 &amp;ldquo;closed book&amp;rdquo; 방식이다.&lt;/p>
&lt;p>0-shot, 1-shot, 5-shot, 8-shot 학습을 평가하여 모델에 더 많은 예제가 제공될수록 대부분의 작업과 모델에서 성능이 향상되는 것을 확인하였다. 하지만 Trivia QA 작업에서는 1-shot 학습이 모든 모델 크기에서 5-shot 및 8-shot 학습을 능가하는 예외적인 결과를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure17.png"
width="1124"
height="314"
srcset="https://kurtkim.github.io/p/palm/images/figure17_hua7a3805379910bdae83a5243740e90f9_119000_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure17_hua7a3805379910bdae83a5243740e90f9_119000_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="357"
data-flex-basis="859px"
>&lt;/p>
&lt;p>이 연구에서는 다양한 모델 체크포인트에서의 few-shot 학습 성능을 분석했다. 대부분의 작업에서 체크포인트 간 성능에 큰 차이를 보이지 않았지만, Web Questions 작업에서는 체크포인트 간에 큰 성능 변동을 보였다. 가장 높은 성능을 보인 PaLM 540B는 학습 토큰 7700억 개의 체크포인트에서 최고 결과를 보였지만, 그 이후의 체크포인트에서는 성능이 감소했다. 이 연구의 모든 결과는 동일한 체크포인트에서 평가되었다.&lt;/p>
&lt;hr>
&lt;h2 id="memorization">Memorization&lt;/h2>
&lt;p>신경망이 학습 데이터를 기억하는 것은 overfit의 일종이며, 이는 주로 작은 학습 세트를 여러 번 반복할 때 발생한다. 그러나 PaLM 같은 모델은 780B 토큰의 말뭉치를 한 번만 훑어내려도, 모델의 큰 용량 때문에 학습 데이터의 상당 부분을 기억할 수 있다. 더욱이, 웹에서 추출된 말뭉치에는 중복되는 텍스트가 많이 있어, 학습 과정에서 약간 변형된 구절들이 여러 번 나타날 수 있습니다.&lt;/p>
&lt;p>PaLM 모델이 학습 데이터를 얼마나 잘 기억하고 있는지를 분석한다. 학습 예제에서 무작위로 선택한 100개의 토큰 시퀀스로 모델을 실행하고, 모델이 학습 예제와 정확히 일치하는 50개 토큰을 얼마나 자주 생성하는지 측정한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure18.png"
width="1290"
height="338"
srcset="https://kurtkim.github.io/p/palm/images/figure18_hu379c2c297c002a4e3d032ea44dd1f4cf_104085_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure18_hu379c2c297c002a4e3d032ea44dd1f4cf_104085_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="381"
data-flex-basis="915px"
>&lt;/p>
&lt;p>(a)는 세 가지 크기의 모델이 훈련 데이터를 얼마나 잘 기억하는지를 보여준다. 8B 모델은 1.6%의 데이터, 540B 모델은 2.4%의 데이터를 정확히 재현할 수 있었다. 또한 학습 데이터와 같은 분포에서 추출된 보류 중인 데이터에 대한 기억율도 평가했으며, 이는 일부 보류 중인 예제가 학습 세트 예제와 매우 유사하기 때문에 0% 이상이었다.&lt;/p>
&lt;p>학습 데이터에서 예제가 정확히 몇 번 보였는지에 따른 기억율을 보여주는 (b)에 따르면, 한 번만 본 예는 가장 큰 모델에서 0.75%의 기억을 가지고, 500번 이상 본 예제는 40% 이상의 기억율 보였다. 이는 학습 과정에서 전체 문서에 대해 중복을 제거하고, 100 토큰 범위에서 기억을 평가했기 때문이다.&lt;/p>
&lt;p>(c)는 학습 데이터 말뭉치별로 모델의 기억율을 보여준다. 학습에서 예제의 정확한 중복, 거의 중복, 또는 템플릿화의 양이 가장 큰 영향을 미쳤다. 코드 말뭉치는 표준 라이센스 문자열, 다른 곳에서 복사된 공유 코드 스니펫, 자동 생성된 코드 등을 포함하고 있고, 반면 책 말뭉치는 주로 고유한 텍스트를 포함하고 있다.&lt;/p>
&lt;p>이 결과들로부터, memorization에 대해 다음과 같은 결론을 내릴 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>더 큰 모델이 더 작은 모델보다 높은 기억율을 보이며, 이는 이전 연구의 결과와 일치한다. 기울기와 결정계수($R^2$) 값이 모두 비슷하게 나타났다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>흔한 템플릿과 표준 문구에 대해 모델이 정확히 일치하는 연속을 생성하므로, 일정 수준의 &amp;ldquo;memorization&amp;quot;이 예상된다. 그러나 학습 데이터에 대한 기억율은 보류 중인 데이터보다 상당히 높아, 이는 모델이 실제로 데이터의 일부를 기억한다는 것을 보여준다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>예제가 기억될 확률은 학습에서 그 예제의 독특함과 강하게 연관되어 있다. 한 번만 본 예제는 여러 번 본 예제보다 기억될 가능성이 적다. 이는 이전 연구들과 일치하는 결과이다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>대부분의 기억 사례는 우려를 불러일으키지 않을 것 같은 공식적인 텍스트였으며, 이야기나 뉴스 기사, 사실 등도 기억되었다. 추출 가능한 기억된 내용의 양은 학습 데이터, 모델 크기, 그리고 추출을 수행하는 사람이 학습 데이터를 얼마나 알고 있는지에 따라 달라진다. 하지만 단순히 추출 가능한 학습 데이터의 양을 측정하는 것만으로는 이 기억이 문제가 될 수 있는지에 대한 정보를 얻을 수 없다.&lt;/p>
&lt;p>기억이 문제가 되는지는 데이터셋의 특성과 사용 목적에 따라 다르다. 큰 언어 모델을 사용할 때는 항상 신중해야 한다. 생성 시점의 기억을 방지하는 한 방법은 학습 데이터 위에 블룸 필터를 구현하고, 학습 데이터셋에서 그대로 나온 시퀀스를 생성하지 않게 제한하는 것이다. 하지만 이 방법도 완벽하지 않으며, 최선의 대응 전략은 큰 언어 모델을 언제, 어떻게 사용할지 신중하게 결정하는 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="dataset-contamination">Dataset Contamination&lt;/h2>
&lt;p>이전 연구들은 벤치마크 평가 세트와 학습 데이터 사이에 높은 수준의 데이터 중복률이 있다고 보고했다. 그러나 많은 벤치마크는 웹에서 맥락을 가져와 생성된 질문에 대한 답을 만들도록 요청하는 방식으로 구성되었다. 이러한 작업에 대해 평가 시점에 맥락이 제공되므로, 모델이 이전에 맥락에 대해 학습했더라도 평가 시간에 불공정한 이점은 주지 않는다.&lt;/p>
&lt;p>단순히 고차 n-gram 중복을 찾는 것이 아니라, 29개의 주요 영어 NLP 벤치마크 작업에 대해 통계를 계산하고 각각의 예제를 수동으로 검토하여 오염된 예제의 비율이 높은 것을 파악하였다. 이는 각 데이터셋이 어떻게 구성되었는지를 고려하여 수행되었다.&lt;/p>
&lt;p>29개의 벤치마크 작업을 대략 네 가지 카테고리로 나눌 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Wholesale contamination&lt;/strong> 데이터셋의 상당 부분이 오픈 웹에 나타나는 데이터셋이며, 이것들을 오염되었다고 간주한다. 예: SQuADv2, Winograd.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Constructed from web&lt;/strong> 질문+답변(또는 접두사+연속)이 오픈 웹에서 자동으로 추출된 데이터셋으로, 많은 평가 예제가 학습 데이터에 있을 가능성이 높으며, 이것들을 오염되었다고 간주한다. 예: Web Questions, ReCoRD, Lambada.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Context on web&lt;/strong> 맥락은 웹에서 가져왔지만 질문은 그렇지 않은 질문 응답 데이터셋이며, 이것들을 오염되지 않았다고 간주한다. 예: BoolQ, Multirc, ANLI.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>No signiﬁcant overlap&lt;/strong> 학습 데이터와 중복되는 부분이 없는 데이터셋으로, 어떤 대규모 학습 코퍼스에서도 기대할 수 있는 공통 n-gram은 제외한다. 예: StoryCloze, OpenbookQA.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>29개의 벤치마크 세트 중 10개가 첫 두 카테고리에 속한다는 것을 확인하였다. 이들 중 일부만이 학습 데이터에서 발견되었다. 이는 학습 코퍼스가 웹 데이터의 일부만 포함하고 있기 때문이다. 따라서 각 데이터셋을 &amp;ldquo;contaminated&amp;rdquo; 부분과 &amp;ldquo;clean&amp;rdquo; 부분으로 나눌 수 있었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table18.png"
width="1046"
height="480"
srcset="https://kurtkim.github.io/p/palm/images/table18_hu6141fbf5b1fbce561b456895b53c3ea5_102997_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table18_hu6141fbf5b1fbce561b456895b53c3ea5_102997_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="523px"
>&lt;/p>
&lt;p>깨끗한 부분에서 긍정적인 정확도 변화와 부정적인 정확도 변화를 보여주는 세트의 수가 동일함을 확인하였다. 이는 데이터 오염이 결과에 큰 영향을 미치지 않음을 의미한다. 만약 540B 모델이 평가 세트의 대부분을 단순히 암기했다면, 깨끗한 부분에서 8B 모델보다 더 큰 부정적인 변화를 보였을 것이다. 하지만, 8B와 540B 모델은 깨끗한 검증 세트와 전체 검증 세트 사이에 비슷한 수의 부정적인 변화를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table19.png"
width="1122"
height="352"
srcset="https://kurtkim.github.io/p/palm/images/table19_hu5e24c026733eac3a635037bdd59222b5_75005_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table19_hu5e24c026733eac3a635037bdd59222b5_75005_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;p>기계 번역에 대해 분석을 수행했고, 데이터 오염은 발견되지 않았지만, 학습 데이터에서 발생하는 목표 참조 문장이 일부 있었다는 것을 확인하였다. 결과적으로, 학습 데이터와 높은 n-gram 중복을 가진 문장을 제거하여 &amp;ldquo;clean&amp;rdquo; 부분집합을 만들었다. 대부분의 세트에서 깨끗한 세트와 전체 세트 사이의 BLEU 점수는 비슷했으며, 이는 기억력에 의한 차이가 주요 요인이 아니라는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="exploring-explanations">Exploring Explanations&lt;/h2>
&lt;p>&amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅이 다단계 추론 작업의 예측 정확도를 크게 향상시키는 것을 보여주었다. 이 방법은 모델이 올바른 답을 내는 이유를 규명하는 과학적 관심사, 사용자의 신뢰도 조절, 그리고 설명 자체가 필요한 상황(예: 농담 설명) 등에 유용하게 사용될 수 있다.&lt;/p>
&lt;p>chain-of-thought 프롬프팅을 사용한 PaLM의 설명적 언어 생성 능력을 보여주려 한다. 제시한 예시들은 논리적 추론, 세계 지식, 추상적 언어 이해, 사전적 언어 이해 등을 복합적으로 필요로 힌다. &amp;ldquo;Explaining a Joke&amp;quot;과 &amp;ldquo;Logical Inference&amp;quot;이라는 두 가지 작업을 통해 모델 output을 보여준다. 각 작업에 대해, 원하는 output 스타일을 보여주는 예시들을 작성하였다. 이 예시들은 저자들이 작성하고 선택했지만, 여전히 PaLM의 언어 이해 능력을 획기적으로 보여주는 결과라고 믿는다. 이는 이 분석이 어떻게 수행되었는지에 관한 여러 핵심 요인들 때문이다.&lt;/p>
&lt;ol>
&lt;li>모든 예측은 동일한 2-shot 예시를 통해 생성되며, 이는 평가하는 예시의 내용과는 무관하게 오직 스타일에만 연관이 있다. 게다가, 모든 예시 프롬프트는 예시 평가 이전에 작성되었고, 모델 output의 검토를 바탕으로 수정된 적은 없다.&lt;/li>
&lt;li>모든 output은 temperature sampling이 아닌 greedy decoding으로부터 나온다. 그 이유는 각 output이 exponential space에서 가능한 많은 output 중 하나가 아니라 모델의 표준 1-best 예측이기 때문입니다.&lt;/li>
&lt;li>이 작업들의 목적이 모델에게 철저한 자연어 설명을 생성하도록 유도하는 것이기 때문에, greedy decodin이 단순한 통계적 상관 관계나 &amp;ldquo;lucky guesses&amp;quot;을 통해 완전히 정확한 설명을 생성할 확률은 극히 낮다.&lt;/li>
&lt;li>프롬프트가 저자들에 의해 작성되었기 때문에, 이는 직접적인 데이터 오염과 기억이 주요 요인이 될 가능성을 완화시킨다.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure19.png"
width="1238"
height="1444"
srcset="https://kurtkim.github.io/p/palm/images/figure19_hubcf5d0a6345a739cd8e445105c9963a2_581191_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure19_hubcf5d0a6345a739cd8e445105c9963a2_581191_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="85"
data-flex-basis="205px"
>&lt;/p>
&lt;p>이 섹션에서 가장 큰 가치를 이러한 예시들을 단순히 읽는 것에서 얻을 수 있다고 믿는다. 비록 이 결과들이 철저한 정량적 분석을 의미하지는 않지만, 이것이 심층적인 언어 이해의 정말 놀라운 수준을 보여준다고 말한다.&lt;/p>
&lt;hr>
&lt;h2 id="representational-bias-analysis">Representational Bias Analysis&lt;/h2>
&lt;p>사전 학습된 언어 모델들은 데이터의 편향을 포함하고 확대한다는 것이 입증되었다. 모델의 구조를 공유하는 것의 중요성도 강조되었다. 이 섹션에서는 PaLM이 사회 집단과 관련된 편향과 개방형 언어 생성에서의 toxicity를 분석한다. 이 분석은 모델의 잠재적 위험을 개요화하는 데 도움이 되지만, 가능한 위험을 제대로 조정하고 맥락화하며 완화하기 위해선 도메인 및 작업별 분석이 필수적이다.&lt;/p>
&lt;h3 id="distributional-bias-in-social-groups">Distributional bias in social groups&lt;/h3>
&lt;h4 id="gender-and-occupation-bias">Gender and occupation bias&lt;/h4>
&lt;p>대용어 해결은 언어 시스템의 중요한 능력이다. 영어에서는 대명사가 의미적 성별로 표시되며, 이는 대용어 해결 성능에 영향을 미친다. 우리는 &amp;ldquo;nurse&amp;quot;와 &amp;ldquo;electrician&amp;quot;와 같은 직업 명사의 성별 편향을 측정하는 Winogender 벤치마크를 사용하여 PaLM의 이러한 편향에 대해 평가한다.&lt;/p>
&lt;p>다중 선택 점수화는 Winogender에 대해 일반적으로 사용되며, 각 가능한 답변을 모델이 그 답변을 생성할 확률로 점수화한다. 이 점수화 방법은 올바른 답변을 생성할 모델의 절대 확률이 낮더라도 예시가 올바르게 점수화될 수 있다. 이 방법은 널리 쓰이지만, 특히 0-shot 설정에서의 모델 성능을 과대평가한다는 것을 발견하였다. 540B 모델의 다중 선택 점수화와 생성적 출력의 예시가 있다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/exp10.1.1.png"
width="1078"
height="338"
srcset="https://kurtkim.github.io/p/palm/images/exp10.1.1_hu7bc34adf4cff411851be4b5d83bcc5bd_66932_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/exp10.1.1_hu7bc34adf4cff411851be4b5d83bcc5bd_66932_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;p>0-shot 생성 케이스에서, 모델은 작업을 이해하지 못하고 다중 선택 시험을 흉내 낸다. 생성 점수화에서는 대소문자를 구분하지 않는 정확한 문자열 일치를 사용하며, 모델 output은 문장 부호나 줄 바꿈에서 잘린다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure20.png"
width="712"
height="494"
srcset="https://kurtkim.github.io/p/palm/images/figure20_hu747af002f527b9e6f676207dda56d339_85807_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure20_hu747af002f527b9e6f676207dda56d339_85807_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="345px"
>&lt;/p>
&lt;p>모델 규모가 커질수록 정확도가 향상되며, PaLM 540B는 1-shot과 few-shot 설정에서 최고 state-of-the-art를 달성하였다. 특히, 더 엄격한 생성 점수화 방법을 사용해도 4-shot 설정에서 84.7%의 정확도를 보였다. 하지만 이 성능은 아직 작업에 맞춘 모델이나 인간의 성능보다 낮다.&lt;/p>
&lt;p>Winogender를 고정관념적 또는 &amp;ldquo;gotcha&amp;rdquo; 부분집합으로 나누어 분산 정확도를 보고한다. 고정관념적 주석에서는 성별과 직업이 일치하고, &amp;ldquo;gotcha&amp;rdquo; 주석에서는 반대이다. 성별 중립적인 대명사는 중립 분할의 일부이다. 모든 경우에서, 올바른 예측은 제공된 맥락에서 분명하게 추론될 수 있다. 모델이 통계적 단축에 얼마나 의존하는지를 측정하는 강력한 척도이다. 모든 경우에서, few-shot 예시들은 전체 예시 세트에서 무작위로 샘플링되며, 평가 중인 현재 예시는 제외된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure21.png"
width="1270"
height="302"
srcset="https://kurtkim.github.io/p/palm/images/figure21_hudb77948ac7043909c9cc6ee4536fa94f_107095_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure21_hudb77948ac7043909c9cc6ee4536fa94f_107095_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="420"
data-flex-basis="1009px"
>&lt;/p>
&lt;p>고정관념적 예시에서의 정확도가 &amp;ldquo;gotcha&amp;rdquo; 예시보다 높으며, 여성에 대한 &amp;ldquo;gotcha&amp;rdquo; 예시에서 정확도가 가장 낮다. shot의 수가 증가함에 따라 이러한 분할 간의 성능 차이가 개선되는 것을 볼 수 있다. 성능의 차이는 학습 세트에서 영어 대명사의 빈도 차이와 관련이 있을 수 있지만, 정확도와 직업 순위 사이에는 명확한 관계를 찾지 못하였다.&lt;/p>
&lt;h4 id="toxicity-and-bias">Toxicity and bias&lt;/h4>
&lt;p>모델이 &amp;ldquo;성별, 종교, 인종 및 민족 신분&amp;quot;과 같은 특정 용어를 참조할 때 자주 함께 나타나는 단어를 분석한다. 각 프롬프트에 대해 800개의 출력을 생성하고, 불용어를 제거하고 형용사와 부사만 선택한다. 이 분석은 어떠한 수동 인간 라벨링도 없이 투명하게 이루어진다.&lt;/p>
&lt;p>정체성 그룹을 참조하지 않는 설명적인 단어의 수를 줄이기 위해, 첫 번째 완전한 문장에서만 형용사와 부사의 수를 계산하였다.&lt;/p>
&lt;p>이 방법을 통해 특정 차원, 특히 이슬람에 대한 bias가 더 잘 드러나는 것을 확인하였다. 인종 신분 용어는 서로 함께 나타나는 경향이 있으며, 프롬프트 언어의 작은 변화가 결과에 큰 변화를 가져온다. 예를 들어, &amp;ldquo;The term was&amp;rdquo; 프롬프트를 사용하면 Latinx는 폭력적이거나 공격적인 어조와 함께 많이 등장한다.&lt;/p>
&lt;p>&amp;ldquo;Indian&amp;quot;이 &amp;ldquo;White&amp;quot;와 많이 동시에 나타났다. 이는 &amp;ldquo;White&amp;quot;라는 표현이 백인 정복자를 지칭하는 데 일반적으로 사용되는 미국 기원의 내용에서 비롯된 것으로 보인다. 많은 연속들이 백인과 아메리칸 인디언 사이의 식민지적 역학을 묘사하지만, 이는 사용자가 북미의 식민지화에 대한 설명에 과도하게 제한되지 않는 언어를 생성하길 원할 때 추가 분석이 필요할 수 있다.&lt;/p>
&lt;p>결과를 검토할 때, 정체성 용어가 모호성을 해소하지 않는다는 것을 알아두는 것이 중요하다. 예를 들어 &amp;ldquo;Indian&amp;quot;은 아메리칸 인디언과 인도 출신 사람을 구분하지 않는다. 또한 &amp;ldquo;Black&amp;quot;과 &amp;ldquo;White&amp;quot;는 종종 인종 신분 외의 것을 참조하며, &amp;ldquo;White&amp;quot;는 백인이 설명될 때 일반적으로 사용되지 않아, &amp;ldquo;White&amp;quot;와 함께 나타나는 용어를 비교하는 것이 복잡할 수 있다.&lt;/p>
&lt;p>62B와 540B 모델은 매우 유사한 동시 출현 횟수를 보여주며, 인종, 종교, 성별 차원에서 상위 10개 단어 중 70%가 동일하다. 이로 인해, 학습 데이터가 모델의 크기보다 결과에 더 큰 영향을 미친다고 판단하였다.&lt;/p>
&lt;p>동시 출현 분석은 용어가 어떻게 다른 용어와 관련되어 나타나는지를 파악하는데 중요하다. 정체성 용어가 있는 프롬프트 템플릿을 사용하여 모델 완성의 toxicity를 분석하는 접근법을 사용하였다. 이슬람에 대한 상위 용어로 &amp;ldquo;terrorist&amp;quot;를 확인했고, 이슬람과 무신론을 포함하는 프롬프트에서 더 높은 toxicity 점수를 보여준다. 이를 통해 모델 완성이 무슬림에 대한 부정적인 고정관념을 잘못 확인하는 가능성을 파악할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure22.png"
width="1214"
height="728"
srcset="https://kurtkim.github.io/p/palm/images/figure22_hu8c0f5177ee767f3f5198ddeea975db15_112115_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure22_hu8c0f5177ee767f3f5198ddeea975db15_112115_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="400px"
>&lt;/p>
&lt;p>동시 출현 횟수를 계산하는 것 외에도, 계속되는 내용의 toxicity를 분류하기 위해 Perspective API를 사용하다. 이 API는 텍스트가 무례하거나 불쾌하거나 사람들이 대화를 떠나게 만들 가능성을 측정한다. 모델 응답의 toxicity 확률 분포를 보면, 이슬람과 유대교는 &amp;ldquo;All { practitioners } are&amp;quot;이라는 프롬프트에 이어 toxicity 반응을 생성할 확률이 더 높다. 또한, 특정 무해한 언급에 대해 높은 toxicity를 부여하는 Perspective API의 사회적 bias에 의존하고 있다.&lt;/p>
&lt;p>bias와 toxicity 평가는 모든 언어 모델에 대해 완전히 적용되지는 않지만, 잠재적인 위험에 대한 중요한 통찰력을 제공한다. 결과의 변동성은 프롬프트 언어의 작은 변화에 매우 취약한 템플릿 기반 접근법을 보여주며, bias를 측정하고 완화 전략을 결정하기 위해 견고한 벤치마크와 지표가 필요함을 강조한다.&lt;/p>
&lt;h3 id="toxicity-in-open-ended-generation">Toxicity in open-ended generation&lt;/h3>
&lt;p>Toxicity degeneration는 언어 모델이 toxicity로 인식하는 텍스트를 만드는 것이다. 이를 평가하기 위해, RealToxicityPrompts 데이터셋을 활용하고, Perspective API를 통해 계속되는 내용에 toxicity 확률을 부여하다. 그 후, 프롬프트가 toxicity일 가능성에 따른 모델 응답의 toxicity 확률 분포를 연구하다.&lt;/p>
&lt;p>무작위로 추출한 1만 개의 프롬프트에 대해 각각 25개의 연속문을 생성하였다. 이때는 최대 128개의 디코딩 단계를 사용하였고, top-k 샘플링과 1.0의 온도를 적용하였다. 하지만 여러 디코딩 단계를 사용하더라도, 첫 번째 완전한 문장의 toxicity 지표만을 보고하였다. 이는 인간의 단일장 연속문을 기준으로 하며, 텍스트 길이에 따라 toxicity 점수가 증가하는 경향 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure23.png"
width="1010"
height="716"
srcset="https://kurtkim.github.io/p/palm/images/figure23_hu9507a657f886eacb1117fcb81854c403_139911_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure23_hu9507a657f886eacb1117fcb81854c403_139911_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="338px"
>&lt;/p>
&lt;p>다양한 모델 크기에 따른 toxicity probability of the prompt(TPP) 함수로서의 toxicity probability of the continuation(TPC)을 보여준다. TPC는 TPP와 함께 증가하는 경향이 있지만, 프롬프트의 toxicity나 인간 기준선보다는 일관되게 낮다. 8B 모델과 더 큰 모델들(62B와 540B) 사이에서 toxicity 확률이 증가하였고, 이는 toxicity 수준과 모델 크기 사이에 상관관계가 있음을 시사한다. 모델의 TPC는 인간의 TPC보다 TPP와 더 일관성이 있고, 이는 모델이 프롬프트 스타일에 크게 영향을 받아, 프롬프트와 유사한 toxicity 수준의 연속문을 생성할 가능성이 높다는 것을 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table20.png"
width="646"
height="210"
srcset="https://kurtkim.github.io/p/palm/images/table20_hu89fe36b7efe5b4f9a75bf57640c81d05_30699_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table20_hu89fe36b7efe5b4f9a75bf57640c81d05_30699_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="307"
data-flex-basis="738px"
>&lt;/p>
&lt;p>TPC는 이전 연구보다는 낮지만, 이는 첫 번째 완전한 문장에 toxicity 측정을 제한했기 때문일 뿐, 모델이 toxicity한 내용을 생성하는 경향이 낮다는 것을 의미하지는 않는다. 무작위로 샘플링된 프롬프트와 연속문의 길이 때문에 이전 작업과의 직접적인 비교는 어렵다.&lt;/p>
&lt;h3 id="limitations">Limitations&lt;/h3>
&lt;p>이 섹션의 공정성 분석은 영어 데이터에만 국한되어 있지만, PaLM은 다양한 언어 데이터에 대해 학습되고 평가되기 때문에 주요한 한계를 가지고 있다. 전 세계적으로 사용되는 언어 기술에 대한 편향 벤치마크의 개발과 활용이 중요하며, 서구 세계에서 개발된 공정성 평가는 다른 지역-문화 맥락으로 쉽게 이동할 수 없을 수 있다. 따라서, 현재 측정 가능한 것 이상의 잠재적인 bias가 존재할 수 있음을 인지해야 한다.&lt;/p>
&lt;p>영어 언어 기술의 편향성에 대한 연구가 증가하고 있지만, 공정성 벤치마크의 표준화, NLP의 편향 측정과 관련된 해의 이해, 그리고 포괄적인 방식으로 다양한 정체성을 다루는 것에 대한 표준이 부족하다. 이러한 이유로, 공정성 평가는 한계를 가지고 있으며, 측정 가능한 것 이상의 잠재적 위험이 존재한다. 이 논문의 평가는 대명사 해결과 공존 분석과 같은 인기있는 작업에 제한되어 있으며, 이러한 벤치마크는 번역, 코드 생성, 상식 추론 등의 작업에서의 편향 유형을 대표할 수 있다.&lt;/p>
&lt;p>bias는 구체적인 응용 프로그램, 학습 과정, 그리고 보호 조치에 따라 시스템에 영향을 미칠 수 있다. 모델 사용 방식에 따라 사전 학습된 모델의 bias가 다양한 영향을 미칠 수 있으며, 모델 미세 조정 후의 downstream task 평가에 어떤 영향을 미치는지는 명확하지 않다. 그래서 배포 전에 응용 프로그램에서의 공정성 격차를 평가하기 위한 적절한 조치를 취하는 것이 중요하다.&lt;/p>
&lt;hr>
&lt;h2 id="ethical-considerations">Ethical Considerations&lt;/h2>
&lt;p>대규모 고품질 언어 모델링은 건강관리와 교육 등 실제 세계 응용 프로그램의 가능성을 제공한다.&lt;/p>
&lt;p>최근 연구에서는 웹 텍스트에 대해 학습된 대규모 언어 모델이 사회적 bias를 악화시키거나, 개인 정보를 노출하거나, downstream에서 해를 입힐 수 있다는 여러 잠재적 위험을 지적하였다. 이러한 bias를 완전히 제거하는 것은 불가능할 수 있으므로, 이런 부적절한 연관성과 위험을 분석하고 기록하는 것이 중요하다. 이를 위해, 데이터셋과 모델 출력의 철저한 분석을 수행하고, PaLM의 사용자들에게 더 큰 투명성을 제공하기 위해 데이터시트와 모델 카드를 제공한다.&lt;/p>
&lt;p>학습 데이터와 PaLM 모델이 다양한 사회적 stereotype과 toxicity를 반영하고 있다는 것을 보여준다. 그러나 이러한 연관성을 제거하는 것은 간단하지 않으며, 자동화 도구를 이용해 toxicity를 갖는 콘텐츠를 필터링하면 소외된 그룹의 콘텐츠가 과도하게 배제될 수 있다. 이러한 bias를 효과적으로 처리하고 그 영향을 연구하는 것이 필요하며, PaLM을 실제 작업에 사용할 때는 추가적인 공정성 평가를 수행해야 한다.&lt;/p>
&lt;p>공정성 분석은 범위가 좁아 다양한 잠재적 위험을 전체적으로 설명하지 못한다. 성별, 인종, 민족, 종교 등의 축을 따라 bias를 분석하지만, 이는 영어 데이터와 모델 출력에만 적용된다. 성적 지향성, 장애 등의 다른 사회적 불평등 축이나 비서구 사회 문화 맥락에서 중요한 편향은 고려하지 않았다. 따라서, 잠재적 위험을 의미있게 평가하려면, 대상 응용 분야와 사회 문화 맥락에 관련된 불평등의 축을 따라 공정성 분석이 필요하다.&lt;/p>
&lt;p>이 논문의 분석은 데이터와 모델의 편향에 초점을 맞추지만, 이들이 실제로 어떻게 사용되는지에 따라 downstream에서의 피해는 달라질 수 있다. 예를 들어, toxicity 콘텐츠가 학습 데이터에 포함되어 있는 것은 바람직하지 않아 보일 수 있지만, PaLM이 toxicity 콘텐츠를 감지하는데 사용된다면, 이러한 콘텐츠에 대한 사전 학습은 중요하다고 볼 수 있다.&lt;/p>
&lt;p>PaLM의 언어 능력은 학습 데이터와 평가 벤치마크의 언어 한계에 의해 제한될 수 있다. 벤치마크 평가는 종종 언어 이해 능력의 전체 복잡성을 완전히 포착하지 못하며, 그들이 측정하려는 것과 실제로 측정하는 것 사이에 차이가 있다. 따라서, 다른 실세계 응용 프로그램 상황에서 동일한 성능 수준이 보장되지 않을 수 있다.&lt;/p>
&lt;p>PaLM은 평가한 벤치마크에서 다국어 능력을 보여주지만, 대부분은 영어로 이루어진 벤치마크이다. 비영어 언어에서의 성능과 bias에 대한 더욱 견고한 평가가 필요하다. 학습 데이터셋의 웹 페이지는 품질을 평가하기 위해 필터링되었는데, 이로 인해 일상적인 언어, 코드 스위칭, 방언의 다양성 등이 과도하게 배제되었을 수 있다. 또한, PaLM은 특정 시점의 언어 사용을 나타내서 현재의 일상 언어나 속어를 모델링하는 작업에 성능이 떨어질 수 있다. 표준 벤치마크는 언어 데이터의 다양한 측면을 포착하거나 구분하지 않아, 이 부분에서 PaLM의 능력을 평가하는 것은 어렵다.&lt;/p>
&lt;p>모델의 다양한 대표성 bias와 능력 차이를 완화한 후에도, 인간의 언어 행동을 모방하는 대규모 언어 모델이 악용될 가능성이 있다는 것을 기억하는 것이 중요하다. 이러한 고품질 언어 생성 능력은 오보 캠페인 등의 악의적인 용도로 사용될 수 있고, 온라인에서 소외된 그룹을 괴롭히는 데도 사용될 수 있다. 이러한 위험은 PaLM 뿐만 아니라 대부분의 대규모 언어 모델에 존재하므로, 이러한 악의적인 용도를 방지할 수 있는 확장 가능한 해결책에 대한 노력이 필요하다.&lt;/p>
&lt;p>소프트웨어 개발 지원을 위한 PaLM-Coder의 배포는 복잡성과 윤리적 고려사항을 수반한다. 언어 모델 기반 제안이 정확하고 견고하며 안전하고 보안이 확보된 것을 보장하고, 개발자들이 이를 확신하는 것은 아직 해결되지 않은 문제이다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>대규모 언어 모델링을 통해 자연 언어 능력이 크게 발전하였다. 이는 시퀀스에서 다음 토큰을 예측하거나 마스킹된 영역을 예측하는 방법으로, 인터넷, 책, 포럼에서 얻은 방대한 데이터에 적용되었다. 이로 인해 고급 언어 이해와 생성 능력을 가진 모델이 개발되었고, 데이터, 매개변수, 계산량의 확장을 통해 모델 품질의 예측 가능한 향상이 이루어졌다.&lt;/p>
&lt;p>Transformer 아키텍처는 현대 accelerator에서 높은 효율성을 보여주며 언어 모델의 기본적인 접근법이 되었다. 4년 동안, 최대 모델의 크기와 계산량은 몇 배로 증가했다. BERT, GPT 시리즈 등 다양한 모델이 등장하며 언어 이해와 모델링 성능이 크게 향상되었다. 또한, 코드 이해 및 생성, 대화 응용 등 여러 분야에서도 개선이 이루어졌다. 최근에는 언어 모델이 지시사항을 따르도록 하는 연구를 통해 이러한 모델의 유용성과 신뢰성이 더욱 향상되었다.&lt;/p>
&lt;p>큰 모델들은 단일 accelerator에 효율적으로 학습하거나 적용하기 어렵다. 이에 따라, 모델 텐서를 가속기 간에 분할하거나, 모델 계층을 accelerator 간에 분리하고 activation을 파이프라인화하는 기술이 등장하였다. 여러 연구들이 모델 규모를 늘리면서 통신 오버헤드를 제한하는 것을 목표로 하고 있으며, PaLM은 Pathways 인프라를 통해 데이터와 모델 병렬화를 혼합하여 사용한다.&lt;/p>
&lt;p>모델을 효율적으로 확장하기 위한 아키텍처 변형이 제안되었다. 대량의 텍스트를 임베딩하여 모델 크기를 줄이는 검색 모델, 다른 예시가 parameter의 다른 부분 집합을 사용하게 하는 모델 희소성, 그리고 극도로 긴 시퀀스로 효율적인 학습을 가능하게 하는 시퀀스 길이의 희소성 등이 포함된다. 이러한 연구의 개선 사항들이 미래의 Pathways 언어 모델에 통합될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="open-questions-in-scaling">Open Questions in Scaling&lt;/h2>
&lt;p>few-shot learning 기반으로 학습하는 대규모 언어 모델의 품질 향상은 모델의 깊이와 너비, 학습된 토큰의 수, 학습 코퍼스의 품질, 그리고 계산량 증가 없이 모델 용량을 증가시키는 방법 등 네 가지 주요 요인에 의해 이루어졌다. 이 중 하나인 학습 코퍼스의 품질이 주요 요인으로 작용할 수 있음이 나타났으며, 신중한 데이터 필터링을 통한 few-shot learning 기반으로 한 학습 향상이 매우 중요함이 밝혀졌다.&lt;/p>
&lt;p>학습 비용이 높아서, 모델의 깊이와 너비와 학습된 토큰 수의 효과를 분리하는 연구를 수행하지 못했다. 즉, &amp;ldquo;7T 토큰으로 학습된 62B parameter 모델과 780B 토큰으로 학습된 540B parameter 모델은 어떻게 비교될까?&amp;ldquo;라는 질문에 대한 답을 아직 찾지 못했습니다. 이러한 모델은 PaLM 540B와 비슷한 학습 비용을 가지지만, 추론 비용이 그 크기에 비례하기 때문에 더 작은 모델이 선호될 것이다.&lt;/p>
&lt;p>최근 연구에서는 1.4T 토큰의 데이터로 학습된 70B parameter 모델인 Chinchilla와 300B 토큰의 데이터로 학습된 280B parameter 모델인 Gopher를 비교하였다. 두 모델은 유사한 학습 비용을 가지지만, Chinchilla는 다양한 언어 작업에서 Gopher를 큰 차이로 능가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table21.png"
width="1220"
height="524"
srcset="https://kurtkim.github.io/p/palm/images/table21_hu0ab69c11a7bb3e9b00bcdf6dc9b1dde9_136056_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table21_hu0ab69c11a7bb3e9b00bcdf6dc9b1dde9_136056_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="558px"
>&lt;/p>
&lt;p>Chinchilla와 PaLM, 두 모델의 결과를 비교하였다. 두 모델은 58개의 BIG-bench 작업과 9개의 영어 NLP 작업에서 비슷한 결과를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure24.png"
width="1194"
height="504"
srcset="https://kurtkim.github.io/p/palm/images/figure24_hu1393194a6f844254c2dc8a997000725c_109049_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure24_hu1393194a6f844254c2dc8a997000725c_109049_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>Chinchilla는 BIG-bench에서 PaLM의 스케일링 곡선을 약간 능가하고, 9개의 영어 NLP 작업에서는 약간 미치지 못하는 반면, Gopher는 두 스케일링 곡선 모두를 크게 미치지 못했다. Gopher와 Chinchilla는 동일한 학습 코퍼스를 사용했지만, PaLM은 다른 코퍼스를 사용하여 비교가 복잡해졌다. 이 결과는 Gopher가 그 크기의 모델에 대해 학습이 부족했음을 보여주지만, &amp;ldquo;X 크기의 모델이 Y 토큰으로 훈련되면 PaLM 540B와 어떻게 비교될까?&amp;ldquo;라는 질문에 대한 답을 추론하기에는 충분하지 않다. 이것이 어려운 질문이 된 이유는 여러 가지이다:&lt;/p>
&lt;ol>
&lt;li>강력한 결론을 도출하려면 큰 규모의 실험이 필요하며, 이는 높은 계산 비용을 요구한다.&lt;/li>
&lt;li>더 작은 모델이 더 적은 TPU 칩으로 학습된다면, 학습 시간이 비례적으로 증가할 것이다. 같은 수의 TPU 칩으로 학습된다면, 배치 크기를 크게 늘리지 않으면 TPU 계산 효율성을 유지하기 어렵다. PaLM 540B의 배치 크기는 이미 4M 토큰인데, 이보다 더 큰 배치 크기가 효율성을 유지할 수 있을지는 불확실하다.&lt;/li>
&lt;li>웹에서는 무한한 양의 고품질 텍스트 데이터가 있지는 않다. PaLM에서는 780B 토큰 이후 일부 데이터가 반복되는 것을 확인했고, 이는 학습의 종료 지점으로 설정한 이유이다. 반복된 데이터의 가치와 보지 않은 데이터의 가치를 비교하는 것은 불확실하지만, 새로 갱신된 데이터셋에서 더 오래 학습하면 성능이 향상되는 것을 확인하였다.&lt;/li>
&lt;/ol>
&lt;p>향후 연구에서는 다양한 작업에 잘 적용되는 뛰어난 언어 모델을 만드는 데 영향을 미치는 여러 요인들 사이의 균형에 대해 조사할 계획이다. 이는 모델 아키텍처, 사전 학습 작업, 최적화 설정 등의 추가적인 요인에 대한 연구를 포함한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>이 연구에서는 고품질, 다양한 텍스트로 학습된 대규모 언어 모델인 PaLM을 사용하여 few-shot 언어 이해와 생성의 가능성을 확장하였다. 이 모델은 29개의 주요 영어 NLP 작업 중 28개에서 state-of-the-art를 달성했으며, 150개 이상의 새로운 언어 작업을 포함하는 BIG-bench에서는 인간의 평균 성능을 능가하였다. 또한 소스 코드 이해, 다국어 NLP, 기계 번역 등 다양한 분야에서도 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>이 연구의 중요한 결과 중 하나는 복수 단계의 논리적 추론을 필요로 하는 작업에서 뛰어난 성능을 보였다는 것이다. 다양한 산술 및 상식 추론 작업에서 state-of-the-art를 달성하였으며, 이는 단순히 모델 규모 확대 뿐 아니라, 예측 전에 논리적 추론 과정을 명시적으로 생성하도록 하는 방식을 통해 이루어졌다. PaLM은 농담을 설명하고 복잡한 시나리오에 대한 질문에 답하는 등의 작업에서 논리적 추론 과정을 구체적으로 표현할 수 있었다.&lt;/p>
&lt;p>이 연구 결과는 few-shot 언어 이해를 위한 모델 규모 확대의 효과가 아직 정체되지 않았음을 보여준다. 동일한 학습 방법을 사용한 다른 모델들과 비교했을 때, 규모의 증가와 성능 향상이 log-linear 관계를 보였고, 특정 벤치마크에서는 더 큰 모델로 확대했을 때 불연속적인 성능 향상이 관찰되었다. 이는 특정 언어 모델의 기능이 충분한 규모에서만 나타나며, 미래의 모델에서는 추가적인 능력이 나타날 수 있음을 시사한다.&lt;/p>
&lt;p>추론 작업에서의 뛰어난 성능은 중요한 의미를 가지고 있다. 모델이 예측을 설명하는 자연어를 생성하는 것은 사용자가 모델의 예측 이유를 이해하는 데 도움이 되며, 더불어 모델에게 명확한 추론 과정을 생성하도록 요청함으로써 예측의 품질이 크게 향상될 수 있음을 보여준다. 즉, 모델의 언어 생성능력은 언어 생성을 크게 필요로 하지 않는 분류나 회귀와 같은 작업에서도 매우 유익할 수 있다.&lt;/p>
&lt;p>few-shot 언어 모델링의 규모를 확대하는 목표를 달성했지만, 미래 모델에 대한 최적의 네트워크 구조와 훈련 방식에 대한 여전히 많은 미해결 문제가 있다. PaLM은 Google의 ML 확장 미래 비전인 Pathways 설립의 첫 단계일 뿐이다. 이 규모 확대 능력을 잘 알려진 full-attention transformer 모델에서 보여주었으며, 더 넓은 목표는 다양한 새로운 구조와 학습 방식을 탐구하고, 가장 유망한 시스템을 Pathways의 확장 능력과 결합하는 것이다. PaLM은 여러 모달리티에 걸친 일반화 능력을 가진 대규모, 모듈화된 시스템을 개발하는 우리의 최종 목표에 강한 기반을 제공한다고 믿는다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2204.02311.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/PaLM-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Minerva</title><link>https://kurtkim.github.io/p/minerva/</link><pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/minerva/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델은 자연어 이해를 필요로 하는 작업에서 탁월한 성과를 보였지만, 수량적 추론을 필요로 하는 작업에서는 어려움을 겪었다. 이를 해결하기 위해, 일반 자연어 데이터에 대해 사전 학습된 후 기술적인 내용에 대해 추가 학습된 Minerva라는 큰 언어 모델을 제안한다. 이 모델은 외부 도구 없이도 기술 벤치마크에서 state-of-the-art를 보여주며, 물리학, 생물학, 화학, 경제학 등 대학 수준의 문제 200개 이상을 풀어보았을 때, 그 중 거의 1/3을 정확하게 해결할 수 있었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>인공 신경망은 다양한 분야에서 큰 성과를 내었습니다. 특히, 거대 언어 모델은 다양한 자연어 작업에서 우수한 성능을 보였지만, 수학이나 과학 등 정량적 추론을 필요로 하는 문제 해결에서는 어려움을 겪었다.&lt;/p>
&lt;p>정량적 추론 문제는 언어 모델의 다양한 능력을 평가하는 중요한 분야이다. 이는 자연어 이해, 세계 지식 회상, 계산 알고리즘 적용, 수학 토큰 조작 등의 능력을 요구하며, 과학과 기술 분야에서 사람들의 작업을 지원하는 견고한 정량적 추론 해결사로서의 연구를 검증하는 기회를 제공한다.&lt;/p>
&lt;p>이전 연구에서는 대규모 언어 모델이 특정 도메인 데이터셋에서 학습 후 수학과 프로그래밍 문제에서 뛰어난 성능을 보여주었다. 이 연구에서는 이런 접근법을 외부 도구 없이 독립적인 해결책을 제공해야 하는 정량적 추론 문제에 적용하였고, 이는 수학, 과학, 공학 문제 등을 포함한다.&lt;/p>
&lt;h3 id="our-contribution">Our Contribution&lt;/h3>
&lt;p>Minerva라는 언어 모델을 제안한다. 이 모델은 자연어로 표현된 과학 및 수학 문제를 처리하고, 올바른 LATEX 표기법으로 단계별 해답을 생성하는 능력을 보여주며, 여러 정량적 추론 작업에서 뛰어난 성능을 보여주었다.&lt;/p>
&lt;p>Minerva는 과학과 수학 데이터를 포함하는 고품질 데이터셋으로 추가 학습된 PaLM 언어 모델을 기반으로 한다. 우리는 사전 학습된 모델을 사용하여 기술 데이터셋에서 학습을 계속하였고, MATH, GSM8k, MMLU 데이터셋 등에서 최고 수준의 성능을 보였다. 이 모델은 이러한 평가 데이터셋에서 명시적인 학습 없이도 강인한 성능을 보여주었다.&lt;/p>
&lt;p>이 논문의 핵심 novelty는 자연어와 형식적 수학 언어를 병행하는 대규모 학습 데이터셋이다. 이 데이터는 arXiv와 신중하게 처리된 웹 페이지에서 수집되었다. 이 연구는 데이터 품질과 모델 크기를 향상시킴으로써 정량적 추론 벤치마크에서 달성 가능한 성능에 대한 새로운 기준을 설정하였다.&lt;/p>
&lt;p>정량적 추론 벤치마크의 범위를 확장하기 위해, MIT의 OpenCourseWare에서 과학과 수학의 대학 수준 문제 200개 이상으로 데이터셋을 구축하였다. 이를 통해 순수 수학적 환경을 넘어 우리 모델의 사고 과정에서의 정량적 추론 능력을 측정하였다.&lt;/p>
&lt;h3 id="related-works">Related Works&lt;/h3>
&lt;p>자연어로 표현된 정량적 추론 문제를 해결하는 것은 활발히 연구되는 분야이다. 스크래치패드나 사고의 연결 고리를 사용한 프롬프트 언어 모델은 보이지 않는 문제의 단계별 해결책을 출력할 수 있다. GSM8k 작업은 모델 출력을 재정렬하기 위해 학습된 검증자를 사용하면 성능이 향상될 수 있음을 보여주었다. 이 연구에서는 외부 도구에 의존하지 않는 독립적인 모델에 초점을 맞추었다.&lt;/p>
&lt;p>언어 모델을 평가하는 표준 방법은 문제 당 한 가지 해결책을 탐욕적으로 샘플링하는 것이다. 하지만 최근 연구에서는 문제 당 여러 해결책을 샘플링하고 필터링하는 것이 더 유리하다는 것을 보여주었다. 특히, 다수결 투표 방식이 탐욕적 디코딩보다 성능을 크게 향상시킨다는 것을 확인하였다.&lt;/p>
&lt;p>Drori et al. (2021)은 OpenAI의 davinci-002 모델을 MATH 데이터셋의 일부로 평가하였다. 하지만 문제의 하위 집합에 초점을 맞추고 문제 형식의 변경으로 인해, 이 연구와 논문의 결과를 직접 비교하는 것은 어렵다.&lt;/p>
&lt;p>&lt;strong>Code generation.&lt;/strong> 코드 생성 모델을 수학 문제에 적용하는 것은 활발한 연구 분야이다. PaLM은 학습 데이터셋에 코드가 포함된 거대 언어 모델이 좋은 성능을 보일 수 있음을 보여주었고, Codex 모델은 MATH 문제에 대한 코드 해결책을 생성할 수 있다. 이러한 해결책들은 외부 라이브러리에 의존하지만, 이 논문의 접근법은 모델이 자체 추론 능력만으로 답을 도출하는 능력을 직접 연구한다.&lt;/p>
&lt;p>&lt;strong>Formal mathematics.&lt;/strong> 수학은 자연어를 기반으로 발전했지만, 공리적인 기초를 통해 수학적 사고를 시뮬레이션할 수 있다. 이는 Coq, Isabelle, HOL4, Lean, Metamath, Mizar 같은 특수 프로그래밍 언어를 통해 가능하며, 이들은 컴퓨터를 이용한 논리적, 수학적 사고의 시뮬레이션을 지원한다. 또한, 증명 보조 도구와 자동 정리 증명기의 자동화에 대한 연구는 기계 학습 방법과의 통합으로 큰 이익을 얻었다.&lt;/p>
&lt;p>&lt;strong>Language models applied to formal and synthetic mathematical problems.&lt;/strong> 이전 연구에서는 언어 모델을 학습시켜 수학적 표현을 예측하는 방법을 사용하였다. 이러한 예측 모델은 증명 검색을 안내하는 데 사용할 수 있다. 거대 언어 모델은 자연어 모델링에 뛰어나지만, 형식 언어의 경우, 수학 공식의 그래프 구조 정보를 유지하는 모델, 예를 들어 GNNs,이 여전히 경쟁력이 있다.&lt;/p>
&lt;p>&lt;strong>Modelling mathematics as a discipline of natural language.&lt;/strong> 새로운 벤치마크 데이터셋은 고급 수학 주제를 포함하며, 이 분야에서 언어 모델은 다른 유형의 모델로부터 제한적인 경쟁을 받고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="training-and-evaluation">Training and Evaluation&lt;/h2>
&lt;h3 id="mathematical-training-dataset">Mathematical Training Dataset&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table1.png"
width="1126"
height="176"
srcset="https://kurtkim.github.io/p/minerva/images/table1_huf80f382bf912273569a8cab6823b82ea_38036_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table1_huf80f382bf912273569a8cab6823b82ea_38036_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="639"
data-flex-basis="1535px"
>&lt;/p>
&lt;p>Minerva 모델은 수학적 내용으로 필터링된 웹페이지와 arXiv 사전 인쇄 서버의 논문에서 추출한 데이터셋에서 학습되었다. 이 데이터셋은 일반적인 자연어 데이터도 포함하고 있다. 수학 웹페이지 데이터셋은 MathJax 형식의 수학 표현이 있는 페이지를 수집하여 만들었고, 대부분의 HTML 태그를 제거하지만 수학 표기법을 유지하는 과정을 거쳤다. 이로 인해 모델은 학습 중에 전체 수학 공식을 볼 수 있으며, 계산과 기호 조작을 요구하는 작업에서 잘 수행하게 된다.&lt;/p>
&lt;h3 id="models-and-training-procedure">Models and Training Procedure&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table2.png"
width="894"
height="176"
srcset="https://kurtkim.github.io/p/minerva/images/table2_hud92a0004aa2106af00d2120898e8b2e4_39164_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table2_hud92a0004aa2106af00d2120898e8b2e4_39164_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="507"
data-flex-basis="1219px"
>&lt;/p>
&lt;p>이 논문의 방법은 PaLM 사전 학습된 decoder-only transformer 언어 모델로 시작하여, 이를 autoregressive 목표를 사용하여 수학 데이터셋에서 더욱 학습(미세 조정)하는 것이다. 가장 큰 모델은 540B parameter를 가지며, 26B 토큰에서 미세 조정되었다. 이 모델은 학습이 부족하지만, 우수한 성능을 보여주었다.&lt;/p>
&lt;h3 id="evaluation-datasets">Evaluation Datasets&lt;/h3>
&lt;p>주로 few-shot 평가에 초점을 맞추며, 평가를 위해 입력을 1024 토큰으로 자르고 모델을 사용하여 최대 512 토큰을 생성한다. 문제당 한 번 샘플링할 때에는 탐욕적으로, 여러 번 샘플링할 때에는 핵심 샘플링을 사용한다. 생성 작업에서, 모델은 사고의 연결 고리를 답변으로 생성하고 최종 답변을 표시하며, 최종 답변이 실제 답변과 일치하면 해결책을 올바르다고 평가한다. 정확성 평가는 SymPy 라이브러리를 사용하여 수학적으로 동등한 답변을 올바르게 식별한다.&lt;/p>
&lt;p>기존 데이터셋들은 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>MATH: 주로 사용하는 데이터셋은 Hendrycks et al. (2021)이 제시한 중고등학교 수학 문제 12K 데이터셋이다. 문제 설명은 LATEX로 작성되어 있으며, 네 개의 무작위 예제를 포함하는 고정 4-shot 프롬프트로 모델을 프롬프트한다.&lt;/li>
&lt;li>GSM8k: 중학교 수학 단어 문제 데이터셋은 Cobbe et al. (2021)이 제시했으며, 모델은 Wei et al. (2022)의 사고의 연결 고리 프롬프트를 사용하여 평가된다.&lt;/li>
&lt;li>MMLU-STEM: 과학, 기술, 공학, 수학 (STEM)에 초점을 맞춘 MMLU 데이터셋의 일부를 사용한다. 각 작업에 대해 5-shot 프롬프트를 사용하고, 단계별 해결책이 포함된 예제로 모델을 프롬프트한다. 수학적 추론을 포함하는 주제에 대해 객관식 MATH 프롬프트를 사용하고, 나머지 주제에 대해 단계별 해결책을 추가한 5-shot 프롬프트를 사용한다.&lt;/li>
&lt;/ul>
&lt;h3 id="undergraduate-level-stem-problems">Undergraduate-Level STEM Problems&lt;/h3>
&lt;p>Minerva의 과학적 추론 능력을 평가하기 위해, 대학 수준의 STEM 문제 세트를 수집하였다. 이 문제들은 대부분 다단계 추론을 포함하고 있다. MIT의 공개 강좌 자료를 사용하여 자동으로 검증 가능한 해결책을 가진 문제들을 수집하였다. 총 272개의 문제를 수집했으며, 이 중 191개는 numeric solution을 가지고 81개는 symbolic solution을 가진다.&lt;/p>
&lt;h3 id="inference-time-techniques">Inference-Time Techniques&lt;/h3>
&lt;p>여러 해결책을 샘플링하고 다수결로 하나를 선택함으로써 탐욕적 디코딩을 상당히 능가할 수 있다는 것을 발견하였다. 이는 가장 흔한 답변을 선택하는 방법으로, maj1@k라고 표시한다. 이 알고리즘의 변형은 가장 흔한 답변 $n$개를 선택하는 것을 포함한다. 이 방법이 성능을 향상시키는 이유는 일반적으로 올바른 답변 방법이 매우 적기 때문이다.&lt;/p>
&lt;p>다수결과 pass@k를 비교하면, pass@k는 $k$개의 샘플 중 하나가 문제를 해결하면 작업이 해결된 것으로 간주된다. 반면, 다수결 성능은 빠르게 포화되며, MATH의 경우 $k = 64$, GSM8k의 경우 $k = 16$에서 이미 대부분의 정확도를 달성하였다. 이는 다수결이 모델링된 분포에서 가장 흔한 답변을 선택하기 때문이며, pass@k의 성능 향상은 분포의 꼬리에서 발생하므로 $k$가 증가함에 따라 계속 개선될 수 있다.&lt;/p>
&lt;p>Log-likelihood는 샘플을 재정렬하는 데 사용할 수 있는 또 다른 지표이다. 우리는 다수결이 Log-likelihood 재정렬보다 훨씬 더 잘 수행된다는 것을 발견하였다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/figure4.png"
width="1232"
height="512"
srcset="https://kurtkim.github.io/p/minerva/images/figure4_hubac6b2a53cf43efbd7e3d93d43dc83f5_102071_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/figure4_hubac6b2a53cf43efbd7e3d93d43dc83f5_102071_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="577px"
>&lt;/p>
&lt;p>MMLU 평가는 주제별로 표준 5-shot 프롬프트를 사용하고 가장 높은 점수의 답변을 선택하며, 다수결로 평가할 때는 사고의 연결 고리 프롬프트를 사용하여 16개의 모델 답변을 샘플링한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table3.png"
width="962"
height="472"
srcset="https://kurtkim.github.io/p/minerva/images/table3_hu53fb9a6500ea6d5812a664d1596e7023_103625_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table3_hu53fb9a6500ea6d5812a664d1596e7023_103625_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="203"
data-flex-basis="489px"
>&lt;/p>
&lt;p>Minerva 62B를 폴란드의 국가 수학 시험에 적용해 보았는데, 이는 2021년 국가 평균인 57%의 점수를 달성하였고, 540B 모델은 65%의 점수를 달성하였다.&lt;/p>
&lt;p>최신 OpenAI 언어 모델인 davinci-002의 결과를 포함시켰고, 이는 모든 기술적 작업에서 state-of-the-art를 보였다. 대부분의 작업들에서 이전 결과에 비해 상당한 향상이 있었다.&lt;/p>
&lt;p>이 논문은 few-shot 평가에 집중했고, Minerva를 MATH에서 미세 조정해 보았지만 개선 사항을 발견하지 못하였다. 그러나, MATH에서 PaLM을 미세 조정할 때는 상당한 개선이 있었다. 이는 비지도 학습 데이터셋의 품질과 다양성이 향상됨에 따라 표준 미세 조정의 효용성이 감소한다는 것을 보여준다.&lt;/p>
&lt;h3 id="basic-arithmetic">Basic arithmetic&lt;/h3>
&lt;p>Minerva 540B가 10자리 수 덧셈에서 80% 이상, 18자리 수 덧셈에서 20% 이상의 정확도를 보였다.&lt;/p>
&lt;hr>
&lt;h2 id="performance-analysis">Performance Analysis&lt;/h2>
&lt;h3 id="model-mistakes">Model Mistakes&lt;/h3>
&lt;p>Minerva 8B와 Minerva 62B의 성능을 비교하여 모델이 만드는 오류 유형을 파악하려고 했다. 두 모델 모두가 높은 확신을 가진 216개의 문제를 선정하였고, 이 중에서 상위 답변이 15% 이상의 표를 받았으며, Minerva 8B는 정확하고 Minerva 62B는 부정확했던 경우와 그 반대 경우를 분석하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table4.png"
width="530"
height="306"
srcset="https://kurtkim.github.io/p/minerva/images/table4_hu78717e1cda1af8ed8d9bc2a7627c83e5_42217_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table4_hu78717e1cda1af8ed8d9bc2a7627c83e5_42217_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>8B 모델의 주요 오류는 부정확한 추론이나 계산에 관련되어 있었으며, 대부분은 산술적 실수였다. 너무 짧은 해결책은 드물었고, 일부 경우에는 실제로 존재하지 않는 수학적 사실을 만들어내기도 했다.&lt;/p>
&lt;p>62B 모델이 틀린 경우, 주로 추론과 계산에서의 오류가 발생하였다. 결론적으로, 62B Minerva 모델은 8B 모델의 기술을 대부분 유지하면서 추론과 계산의 견고성을 향상시킨다는 것을 확인하였다.&lt;/p>
&lt;h3 id="false-positives">False Positives&lt;/h3>
&lt;p>이 논문의 접근법은 문제의 최종 답변의 정확성을 자동으로 확인할 수 있지만, 모델의 추론 과정을 자동으로 검증할 수는 없다. 이로 인해, 추론이 부정확하거나 불완전하더라도 최종 답변이 맞는 &amp;ldquo;false positives&amp;quot;의 가능성이 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table5.png"
width="818"
height="146"
srcset="https://kurtkim.github.io/p/minerva/images/table5_huef1c0d130804bd0bed9d3c481e8cecf7_22209_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table5_huef1c0d130804bd0bed9d3c481e8cecf7_22209_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;p>MATH에서 무작위로 선택한 100개의 질문과 62B 모델에서 샘플링한 답변을 수동으로 검토하여 거짓 긍정 비율을 결정하였다. 전반적으로 거짓 긍정 비율은 낮았지만, 난이도가 높아질수록 증가하는 경향을 보였다.&lt;/p>
&lt;p>평가 지표로 pass@1과 다수결 투표를 중점적으로 사용한다. 이는 그들이 거짓 긍정에 덜 취약하기 때문이다. 62B 모델의 pass@256 정확도는 84.5%이지만, 이 중 거짓 긍정의 비율이 30%로 추정된다. 거짓 긍정을 제외하면, pass@256 정확도는 약 68%로 추정된다.&lt;/p>
&lt;hr>
&lt;h2 id="memorization">Memorization&lt;/h2>
&lt;p>머신러닝 모델의 성능이 진짜 분석 능력을 보여주는지, 아니면 단순히 학습 데이터를 암기한 결과인지를 판단하는 것이 중요하다. 이는 모델이 중간 사실들을 암기하는 것이 성능에 큰 영향을 미치기 때문이다. 모델이 문제와 답변을 암기하는 강력한 암기와, 동일한 질문에 대한 다양한 답변을 암기하는 약한 암기를 모두 검토하려고 한다.&lt;/p>
&lt;p>모델이 학습 데이터에서 암기한 정보를 얼마나 잘 활용하는지 평가하기 위해, 우리는 세 가지 분석을 수행한다: 학습 코퍼스에서 문제와 solution 검색, 문제 변형에 대한 모델의 강인성 평가, 그리고 실제 solution과 모델이 생성한 solution 사이의 유사도 측정. 그 결과, 모델의 성능이 암기에 크게 의존하고 있다는 증거는 찾을 수 없었다.&lt;/p>
&lt;h3 id="training-and-evaluation-dataset-overlap">Training and Evaluation Dataset Overlap&lt;/h3>
&lt;p>올바른 답변을 생성한 문제들 중 다수결 점수가 가장 높은 100개의 문제를 선택하여 암기 가능성을 평가하였다. 이들 각각에 대해 BLEU 점수를 계산하고, 점수가 가장 높은 250개 문서를 수동으로 검토했다. 많은 문서가 수학 문제와 해답이 있는 숙제 도움 사이트에서 나왔지만, 고려 중인 문제와는 일치하지 않았다. 이 분석은 이러한 문제들이 데이터 수집 과정을 통과하지 못했다는 결론을 도출하였다.&lt;/p>
&lt;h3 id="performance-on-modiﬁed-math-problems">Performance on Modiﬁed MATH Problems&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/figure5.png"
width="1250"
height="390"
srcset="https://kurtkim.github.io/p/minerva/images/figure5_hudf6acb557fd345f038a788211189f340_96024_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/figure5_hudf6acb557fd345f038a788211189f340_96024_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="320"
data-flex-basis="769px"
>&lt;/p>
&lt;p>암기를 더 깊게 조사하기 위해, 다수결 투표로 올바르게 답변한 문제 20개를 임의로 선택해 수정하였다. 문제의 표현을 바꾸거나 문제에 나타난 숫자를 변경하고 solution을 수정했다. 수정 전후의 solution 정확도를 비교한 결과, 두 경우 모두 수정 전후의 정확도가 상관관계를 보이며, 암기가 최소한임을 나타내었다.&lt;/p>
&lt;h3 id="bleu-score-between-ground-truth-and-generated-solutions">BLEU Score Between Ground Truth and Generated Solutions&lt;/h3>
&lt;p>실제 답변과 모델이 생성한 답변 사이의 BLEU 점수를 계산하여 solution의 암기를 검사하였다. 5,000개의 테스트 질문 중 160개가 BLEU 점수가 80 이상인 샘플을 가지고 있었으며, 일반적으로 이들은 짧은 solution 이었다. 답변의 유사성이 성능에 어떤 영향을 미치는지 이해하기 위해, 특정 BLEU 점수 이상의 샘플을 제거하고 다수결 투표 정확도를 다시 계산하였다. 결과적으로, 성능이 실제 답변과 매우 유사한 모델 출력에 의해 결정되지 않음을 확인했다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions-and-discussion">Conclusions and Discussion&lt;/h2>
&lt;p>이 연구에서는 자연어로 표현된 수학적 추론을 활용하여 문제를 해결하는 양적 추론 방법을 채택하였다. 거대 언어 모델을 고품질의 수학 데이터셋에 학습시켜 논리적 추론, 수치 계산, 심볼 조작 작업에 강력한 성능을 보임을 입증하였다. 코드 생성 모델과 형식적 방법 등 다른 접근법들과 결합해 양적 문제를 해결하는 에이전트를 제작하는 것이 최종 목표이다.&lt;/p>
&lt;h3 id="limitations-of-our-approach">Limitations of Our Approach&lt;/h3>
&lt;p>양적 추론 접근법은 몇 가지 한계를 가지고 있다. 첫째, 모델의 답변의 정확성을 자동으로 검증할 수 없다. 둘째, 모델은 외부 도구를 사용할 수 없어 복잡한 수치 계산을 수행하는 능력이 제한적이다. 셋째, 대량의 데이터를 통해 학습된 모델이므로, 획득한 특정 능력에 대해 직접 통제할 수 있는 부분이 거의 없다.&lt;/p>
&lt;h3 id="societal-impact">Societal Impact&lt;/h3>
&lt;p>일반적인 상황에서 양적 추론 문제를 해결할 수 있는 인공 신경망은 큰 사회적 영향력을 가질 수 있다. 하지만 현재로서는 Minerva 모델이 이 목표에 도달하기엔 먼 상태로, 성능이 인간에 비해 떨어지며 출력의 정확성을 자동으로 검증할 수 없다. 이러한 문제가 해결되면, 모델은 광범위한 긍정적 영향을 미칠 것으로 예상되며, 접근성이 좋고 저렴한 수학 튜터로서 교육 불평등을 개선하는 데에 활용될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2206.14858.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/gair-nlp/abel" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LaMDA</title><link>https://kurtkim.github.io/p/lamda/</link><pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/lamda/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>LaMDA는 최대 137B의 parameter를 가진 대화용 언어 모델이다. 이 모델은 모델 확장만으로는 안전성과 사실적 근거에 대한 개선이 제한적이라는 문제를 해결하기 위해, 주석이 달린 데이터로 미세 조정하고 외부 지식 소스를 참조하는 방식을 사용한다. 이를 통해 모델의 안전성을 향상시키고, 사실에 근거한 응답을 생성하는 데 성공하였다. 또한, 이 모델은 교육 및 콘텐츠 추천 분야에서의 활용 가능성을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델 사전 학습은 자연어 처리 연구에서 유망하며, 레이블이 없는 텍스트를 사용해 모델과 데이터셋의 크기를 확장하면 성능이 향상된다. 이를 통해 GPT-3와 같은 모델은 few-shot 학습 예제로도 높은 성능을 보여준다.&lt;/p>
&lt;p>대화형 모델은 텍스트의 long-term dependency를 표현하는 능력을 활용하여 언어 모델을 효과적으로 활용한다. 모델의 크기가 커짐에 따라 대화 품질도 향상되는 강한 상관관계가 있다.&lt;/p>
&lt;p>LaMDA는 transformer 기반의 언어 모델로, 대화를 위해 설계되었다. 이는 대량의 공개 대화 데이터와 웹 문서로 사전 학습되었고, 잠재적인 응답을 생성, 필터링, 재정렬하여 최고 품질의 응답을 제공하는 다양한 작업을 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure1.png"
width="1054"
height="358"
srcset="https://kurtkim.github.io/p/lamda/images/figure1_hu79814309481c86c02f59b14c3d04a8e2_76180_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure1_hu79814309481c86c02f59b14c3d04a8e2_76180_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="294"
data-flex-basis="706px"
>&lt;/p>
&lt;p>LaMDA와 함께 모델 스케일링의 이점을 연구한 결과, 스케일링만으로는 품질이 향상되지만, 안전성과 실제 연관성은 인간의 성능에 미치지 못하였다. 그러나 스케일링과 미세 조정을 결합하면 모든 지표에서 크게 향상되었고, 특히 품질 격차는 인간 수준에 가까워졌다.&lt;/p>
&lt;p>&amp;ldquo;quality&amp;rdquo; 지표는 &amp;ldquo;sensibleness&amp;rdquo;, &amp;ldquo;speciﬁcity&amp;rdquo;, &amp;ldquo;interestingness&amp;quot;의 세 가지 요소에 기반하며, 이를 바탕으로 응답이 얼마나 합리적이고 구체적이며 흥미로운지 주석 데이터를 수집한다. 이 데이터를 사용하여 후보 응답을 재정렬하는 discriminator를 미세 조정한다.&lt;/p>
&lt;p>&amp;ldquo;safety&amp;rdquo; 지표는 모델이 생성하는 위험한 응답을 줄이기 위해 도입되었다. 이를 위해 안전 목표를 설정하고, 다양한 군중의 작업자들을 통해 대화 응답을 레이블링한다. 이 레이블을 통해 위험한 응답을 감지하고 제거하는 discriminator를 미세 조정한다. 이는 고수준에서 AI의 가치를 조정하는 과정으로 볼 수 있다.&lt;/p>
&lt;p>&amp;ldquo;groundedness&amp;rdquo; 지표는 모델이 검증 가능한 정보를 포함하는 응답을 알려진 출처에 근거하여 생성하도록 하기 위해 도입되었다. 이는 사용자나 외부 시스템이 응답의 유효성을 판단하는데 도움이 된다. 이 목표를 달성하기 위해, 정보 검색 시스템과 같은 외부 도구를 사용하여 사실을 조사하는 군중의 작업자들의 행동을 모델이 흉내내도록 학습시킨다.&lt;/p>
&lt;p>교육과 콘텐츠 추천 분야에서 LaMDA의 사용을 연구하였다. 특정 응용 프로그램에 LaMDA를 적용하기 위해 몇 번의 응용 프로그램 특정 대화를 사전 조건으로 설정했다. 실험 결과, 사전 학습만 받은 LaMDA 모델과 미세 조정된 LaMDA 모델 모두 그들의 예상 응용 프로그램 역할에 잘 적응할 수 있었으며, 특히 미세 조정된 LaMDA 모델이 더욱 도움이 되었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>&lt;strong>Language models and dialog models:&lt;/strong> 언어 모델은 최근 NLP 응용 분야에서의 성공 덕분에 주목받고 있다. 이 연구는 모델 스케일링이 품질, 안전성, 실제 연관성 지표를 어느 정도 향상시키는 것을 보여주지만, 미세 조정과 스케일링을 결합하면 모든 지표에서 성능이 크게 향상된다는 것을 보여준다.&lt;/p>
&lt;p>이 연구는 언어 모델을 대화 모델링에 적용하는 최근의 연구와 밀접하게 연관되어 있다. 대화 데이터만을 학습하는 미세 조정 단계는 이전 연구와 관련이 있다. 또한, 군중 작업자가 주석을 단 데이터에 미세 조정을 사용하여 흥미로움을 향상시키는 방법을 사용하였다. 그러나 이 연구의 목표는 사용자와의 추가적인 상호작용보다는 모델의 출력의 흥미로움을 극대화하는 것이다.&lt;/p>
&lt;p>순수 스케일링이 오픈 도메인 대화 모델 성능에 제한적인 영향을 미치는 것은 최근 연구와 일치하며, 이는 실제 연관성의 문제에 중점을 둔다. 최근 스케일링 연구는 질문-응답 작업의 성능이 모델 크기에 따라 향상된다는 것을 발견했는데, 이는 미세 조정 전의 사전 학습된 LaMDA에 대한 연구 결과와 일치한다.&lt;/p>
&lt;p>이 연구의 접근법은 언어 모델을 검색 시스템을 통해 향상시키는 데 초점을 맞춘 연구와 연관이 있다. 대부분의 기존 연구는 대화 생성보다는 오픈 도메인 질문-응답에 초점을 맞추고 있으며, 모델 자체가 중간 도구를 사용하도록 학습된다. 이러한 접근법은 RNNLM, RAG, REALM, FiD 등의 아키텍처를 포함하며, 최근의 연구는 신경 모델의 검색과 순위 지정 능력을 확장하고 발전시키고 있다. 이 접근법은 또한 영화 티켓 대화를 위한 외부 API를 사용하도록 모델을 미세 조정하는 연구와도 비교할 수 있다.&lt;/p>
&lt;p>연구 결과는 최근 대화의 실제 연관성에 대한 연구와 일부 유사하다. 외부 지식 베이스에 접근하는 것은 모델이 출처가 없는 내용을 환영하는 비율을 줄이는 것으로 나타났다. 질문-응답 시스템의 정확도는 추론 단위와 응답 생성기를 분리함으로써 개선된다. 검색 엔진과 언어 모델을 결합하면 더 사실적으로 근거를 둔 응답을 제공하는 것으로 나타났다. 알려진 출처의 정보로 생성된 응답을 보강함으로써, 안전성이나 품질에 대한 향상을 희생하지 않고 모델을 실제 연관성에 대해 세부 조정할 수 있다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Dialog metrics:&lt;/strong> 대화 모델에 대한 효과적인 지표를 정의하는 것은 아직 해결되지 않은 연구 주제이다. 이 연구의 접근법은 인간 같은 지표를 주장한 이전 연구에 의해 영감을 받았다. 많은 자동화된 지표들이 연구되었지만, 이러한 지표들은 인간의 판단과 잘 연관되지 않을 수 있다. 따라서 대화 모델링에 대한 더 신뢰할 수 있는 지표는 인간의 평가를 필요로 한다.&lt;/li>
&lt;/ul>
&lt;p>이전 연구는 다양한 대화 품질 평가를 하나의 지표로 결합하려 했으나, 이 연구에서는 각각의 평가 요소를 따로 고려한다. sensibleness, speciﬁcity 외에 interestingness, safety, groundedness 등의 새로운 지표를 추가했고, 이런 다양한 지표 사용의 장점은 특정 지표가 낮은 응답을 분석해 개선 방법을 찾는 것이 가능하다는 점이다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Safety and safety of dialog models:&lt;/strong> 언어 모델의 부적절하고 위험한 행동에 대해 많은 연구가 이루어져 왔으며, toxicity, bias, inappropriately revealing personally identifying information (PII) 등의 문제가 발견되었다. 대규모 언어 모델과 관련된 21가지 위험을 식별하였고, 이 문제를 해결하기 위한 다양한 방법이 제안되었음에도 불구하고, 이 문제를 의미있게 해결하는 것은 여전히 활발한 연구 분야이다.&lt;/li>
&lt;/ul>
&lt;p>대화 모델에 대한 문제도 논의되었는데, bias, offensiveness, hate speech 등이 학습 데이터와 모델 output에서 발견되었다. 대화 모델은 학습 데이터의 bias을 배우고 확대할 수 있다. 이를 해결하기 위해, 안전한 output을 감지하는 별도의 layer를 학습하는 방법이 사용되었고, 미세 조정이 효과적이었다. 크기를 늘리는 것은 toxicity 지표에 영향을 미치지 않지만, 안전 평가에서의 미세 조정은 영향을 미친다는 것이 확인되었다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Groundedness metrics:&lt;/strong> 권위 있는 외부 소스와 모델의 output이 일치하는지를 판단하는 대중들에게 실제성을 평가하도록 요청함으로써 실제성을 평가한다. 최근에 제안된 AIS 프레임워크는 외부 세계에 관련된 언어 모델의 output을 더 정확하게 평가하는 방법을 제시하며, 이는 정보의 이해와 식별, 그리고 정보의 출처 판별의 두 단계로 이루어진다. 또한, 최근의 연구에서는 Q2 지표를 통해 자동 평가의 가능성을 다시 제시하였다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="lamda-pre-training">LaMDA pre-training&lt;/h2>
&lt;p>LaMDA는 공개 대화 데이터와 웹 문서를 기반으로 사전 학습되어 텍스트의 다음 토큰을 예측하도록 설계되었다. 이로 인해 LaMDA는 미세 조정 전에도 일반 언어 모델로 사용될 수 있다.&lt;/p>
&lt;p>사전 학습 데이터셋은 총 2.97B 개의 문서, 1.12B 개의 대화, 13.39B 개의 대화 발화로 구성되어 있고, 대부분이 영어이다. SentencePiece 라이브러리를 통해 2.81T byte pair encoding(BPE) 토큰으로 토큰화하였다. 이는 Meena의 학습 세트인 40B 단어에 비해 훨씬 큰 규모이다.&lt;/p>
&lt;p>가장 큰 LaMDA 모델은 Meena보다 약 50배 많은 137B개의 non-embedding parameter를 가지고 있다. 이 모델은 decoder-only Transformer 언어 모델을 사용하며, 64개의 layer와 relative attention, gated-GELU activation 등을 특징으로 한다.&lt;/p>
&lt;p>LaMDA는 총 57.7일 동안 1024개의 TPU-v3 칩에서 사전 학습되었고, 배치당 256K 토큰을 사용하였다. Lingvo 프레임워크를 통해 123 TFLOPS/sec의 성능을 달성하였다. 또한, 모델 스케일링의 효과를 측정하기 위해 2B-parameter와 8B-parameter의 작은 모델도 학습하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure2.png"
width="826"
height="428"
srcset="https://kurtkim.github.io/p/lamda/images/figure2_hu194ec68ae1ded1ca77e52945a85c89f5_111247_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure2_hu194ec68ae1ded1ca77e52945a85c89f5_111247_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="463px"
>&lt;/p>
&lt;p>미세 조정 전의 모델을 &amp;ldquo;PT&amp;quot;라 부르며, PT는 Meena와 같은 샘플링-랭킹 전략을 사용한다. 총 16개의 독립적인 후보 응답을 샘플링하고, log-likelihood와 length를 기반으로 점수가 가장 높은 후보를 최종 output으로 선택한다.&lt;/p>
&lt;hr>
&lt;h2 id="metrics">Metrics&lt;/h2>
&lt;h3 id="foundation-metrics-quality-safety-and-groundedness">Foundation metrics: Quality, Safety and Groundedness&lt;/h3>
&lt;p>&lt;strong>Sensibleness, Speciﬁcity, Interestingness (SSI):&lt;/strong> overall quality score는 sensibleness, speciﬁcity, and interestingness (SSI)의 평균이다.&lt;/p>
&lt;p>Adiwardana et al.은 Meena의 품질을 측정하기 위해 sensibleness와 speciﬁcity의 평균인 SSA 지표를 제안하였다.&lt;/p>
&lt;p>&amp;ldquo;sensibleness&amp;rdquo; 점수는 모델의 응답이 문맥에 맞고 이전의 발언과 모순되지 않는지를 측정한다. 그러나, sensibleness만으로 모델을 평가하면 모델이 짧고 일반적이며 지루한 응답을 생성하는 것을 보상할 수 있다. 예를 들어, 모든 질문에 &amp;ldquo;I don’t know&amp;quot;라고 답하는 GenericBot 알고리즘은 sensibleness에서 70%의 점수를 얻었다.&lt;/p>
&lt;p>&amp;ldquo;speciﬁcity&amp;rdquo; 점수는 응답이 주어진 문맥에 특정한지를 측정한다. 예를 들어, &amp;ldquo;Me too&amp;quot;는 다양한 문맥에 사용될 수 있으므로 특이성 점수가 0이지만, &amp;ldquo;Me too. I love Eurovision songs&amp;quot;라는 응답은 문맥에 특정하므로 1점을 받는다. Meena는 이 SSA 지표에서 인간 성능과의 격차를 줄였다.&lt;/p>
&lt;p>모델의 성능이 향상됨에 따라, sensibleness와 speciﬁcity만으로는 대화 모델의 품질을 충분히 측정할 수 없다는 것을 확인하였다. 예를 들어, &amp;ldquo;How do I throw a ball?&amp;ldquo;라는 질문에 대해, &amp;ldquo;You can throw a ball by ﬁrst picking it up and then throwing it&amp;quot;은 답변은 합리적이고 특이하지만, 더 깊고 만족스러운 답변은 &amp;ldquo;One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm down and up again, extending your elbow and then releasing the ball upwards&amp;quot;이다.&lt;/p>
&lt;p>&amp;ldquo;interestingness&amp;quot;이라는 세 번째 점수는 대화의 흥미로움을 측정한다. 이는 무리 작업자에 의해 0/1 레이블로 측정되며, &amp;ldquo;누군가의 주목을 끄는&amp;rdquo; 또는 &amp;ldquo;호기심을 불러일으키는&amp;rdquo; 것, 또는 예상치 못하거나 재치 있거나 통찰력 있는 응답을 흥미롭다고 판단한다.&lt;/p>
&lt;p>&lt;strong>Safety:&lt;/strong> 대화 모델은 높은 품질(SSI) 점수를 얻을 수 있지만 사용자에게 위험할 수 있다. 그래서 위험한 모델 output을 측정하기 위한 새로운 안전 지표를 개발하였다. 이 지표는 피해의 위험을 줄이고 불공정한 편향을 방지하는 Google의 AI 원칙을 따른다.&lt;/p>
&lt;p>&lt;strong>Groundedness:&lt;/strong> 언어 모델이 잘못된 주장을 생성하는 경향이 있기 때문에, LaMDA가 가능한 한 알려진 출처와 연관된 응답을 생성하도록 하여 필요한 경우 확인할 수 있도록 하려고 한다.&lt;/p>
&lt;p>&amp;ldquo;groundedness&amp;quot;은 외부 세계에 대한 주장을 포함하는 응답 중에서 권위 있는 외부 출처에 의해 지지될 수 있는 주장의 비율로 정의된다.&lt;/p>
&lt;p>&amp;ldquo;informativeness&amp;quot;는 모든 응답 중에서 알려진 출처로부터 지지받는 외부 세계 정보를 전달하는 응답의 비율로 정의된다. 이는 &amp;ldquo;groundedness&amp;quot;과 분모 항에서만 다르다. 예를 들어, &amp;ldquo;That’s a great idea&amp;quot;와 같은 외부 세계 정보를 전달하지 않는 응답은 실재성에는 영향을 미치지 않지만 정보성에는 영향을 미친다. &amp;ldquo;Rafael Nadal is the winner of Roland Garros 2020&amp;quot;는 실재성 있는 응답의 예이다.&lt;/p>
&lt;p>마지막으로, &amp;ldquo;citation accuracy&amp;quot;를 외부 세계에 대한 명확한 주장을 포함하는 모든 응답 중에서 출처의 URL을 인용하는 모델 응답의 비율로 정의한다. 이는 &amp;ldquo;말은 네 다리가 있다&amp;quot;와 같은 잘 알려진 사실에 대한 주장은 제외합니다.&lt;/p>
&lt;h3 id="role-speciﬁc-metrics-helpfulness-and-role-consistency">Role-speciﬁc metrics: Helpfulness and Role consistency&lt;/h3>
&lt;p>기본 메트릭(quality, safety, groundedness)은 대화 에이전트의 중요한 속성을 측정한다. 이는 에이전트의 특정 역할에 의존하지 않는다. 도움이 되는지와 역할의 일관성은 에이전트가 특정 역할을 가진 대화 응용 프로그램에서 측정된다.&lt;/p>
&lt;p>&lt;strong>Helpfulness:&lt;/strong> 사용자가 정보 검색 시스템을 통해 독립적으로 조사한 정보가 올바르고, 사용자가 도움이 된다고 판단하는 경우, 모델의 응답은 helpful으로 표시된다. helpful 응답은 사용자가 올바르고 유용하다고 판단하는 응답의 부분 집합이다.&lt;/p>
&lt;p>&lt;strong>Role consistency:&lt;/strong> 모델의 응답이 대상 역할을 수행하는 에이전트가 말할 것 같다면, 그것은 role consistent가 있다고 표시된다. 이는 대화 내에서의 자체 일관성과는 다르며, 대화 외부의 에이전트의 역할 정의와의 일관성을 의미한다.&lt;/p>
&lt;hr>
&lt;h2 id="lamda-ﬁne-tuning-and-evaluation-data">LaMDA ﬁne-tuning and evaluation data&lt;/h2>
&lt;p>&lt;strong>Quality (Sensibleness, Speciﬁcity, Interestingness):&lt;/strong> 품질(SSI)을 향상시키기 위해, 작업자들에게 LaMDA와 14~30턴에 걸친 대화를 요청하여 6400개의 대화를 수집히였다. 작업자들은 각 응답이 합리적(sensible)이고, 특정(speciﬁc)하고, 흥미로운지(interesting)를 평가하고, &amp;ldquo;yes&amp;rdquo;, &amp;ldquo;no&amp;rdquo;, &amp;ldquo;maybe&amp;quot;로 레이블한다. 응답이 합리적이거나 특정하지 않으면, 특이성과 흥미로움을 &amp;ldquo;no&amp;quot;로 간주한다. 모든 응답은 5명의 다른 작업자에 의해 레이블이 붙여지고, 5명 중 적어도 3명이 &amp;ldquo;yes&amp;quot;라고 표시하면 응답이 합리적이고, 특정하며, 흥미로운 것으로 간주된다.&lt;/p>
&lt;p>최대 3번의 대화 턴을 가진 1477개의 대화로 구성된 Mini-Turing Benchmark(MTB) 데이터셋에 대한 모델의 응답을 기반으로 평가한다. 이 대화들은 모델에 공급되어 다음 응답을 생성한다. 모든 응답은 5명의 작업자 중 적어도 3명이 &amp;ldquo;yes&amp;quot;라고 표시하면 합리적이고, 특정하고, 또는 흥미로운 것으로 레이블이 붙는다.&lt;/p>
&lt;p>&lt;strong>Safety:&lt;/strong> safety를 위한 미세 조정을 위해, safety 목표를 정의하고, 이를 바탕으로 다양한 작업자들을 이용해 사람이 만든 프롬프트에 대한 LaMDA의 응답을 주석 처리하는 구조화된 접근법을 사용한다.&lt;/p>
&lt;p>작업자들에게 LaMDA와 5~10턴에 걸친 대화를 요청하여 8K 대화를 수집하였다. 작업자들은 자연스러운 형태(interactions of natural form), 민감한 주제를 다루는(interactions that touch sensitive topics), 혹은 모델을 깨려고 시도하는(interactions that adversarially attempt to break the model as per the safety objectives) 세 가지 방식으로 모델과 상호 작용한다. 각 응답에 대해, 작업자들은 문맥을 고려하여 safety 목표를 위반하는지 평가하고, &amp;ldquo;yes&amp;rdquo;, &amp;ldquo;no&amp;rdquo;, &amp;ldquo;maybe&amp;quot;로 레이블한다. 각 safety 목표에 대해 &amp;ldquo;no&amp;quot;로 표시한 작업자가 적어도 2명인 경우, 응답에는 safety 점수 1이 부여된다. 그렇지 않으면 점수는 0으로 지정된다.&lt;/p>
&lt;p>1458턴의 1166개 대화로 구성된 보류 샘플 데이터셋을 사용해 safety를 평가한다. 이 대화들은 모델에 입력되어 다음 응답을 생성한다. 각 safety 목표를 &amp;ldquo;no&amp;quot;라고 표시한 무리 작업자가 적어도 2명인 경우, 응답에는 점수 1이 부여되고, 그렇지 않으면 점수는 0이다.&lt;/p>
&lt;p>&lt;strong>Groundedness:&lt;/strong> SSI와 safety처럼, 작업자들에게 모델과 상호작용하면서 정보 탐색을 위한 대화로 이끌도록 요청하여 4K 대화를 수집하였다.&lt;/p>
&lt;p>작업자들에게 모델의 대화 턴이 외부 세계에 대한 주장을 하는지 평가하도록 요청하였다. 공개적으로 인정받지 않은 사람들에 대한 주장은 제외하고, 이는 모델이 즉흥적인 인물을 대신하여 사실적인 주장을 할 수 있기 때문이다. 이러한 주장은 외부 소스에 기반을 두는 것을 필요로 하지 않는다.&lt;/p>
&lt;p>작업자들에게 주장이 사실인지 물어본다. 3명의 작업자 모두 주장이 사실임을 안다면, 그것을 상식으로 가정하고 외부 지식 소스를 확인하지 않는다.&lt;/p>
&lt;p>확인이 필요한 주장을 포함하는 발언에 대해, 작업자들에게 조사를 위한 검색 쿼리를 기록하도록 요청한다. 그리고 외부 지식 검색 시스템에서의 결과를 포함하여 모델의 응답을 수정하도록 요청한다. 오픈 웹의 내용이 검색 결과에 포함되면, 작업자들에게 출처를 인용하는 URL을 포함하도록 요청한다.&lt;/p>
&lt;p>다양한 주제를 다루는 784턴의 대화를 포함하는 평가 데이터셋을 이용하여 실제성을 평가한다. 이 맥락들은 모델에 공급되어 다음 응답을 생성한다. 각 응답에 대해, 작업자들은 모델의 응답이 사실적인 주장을 포함하고, 이 주장이 알려진 소스를 통해 검증될 수 있는지 평가한다. 모든 응답은 3명의 다른 작업자에 의해 레이블이 붙으며, 최종 실제성, 정보성, 인용 정확성 레이블은 다수결에 의해 결정된다. 모든 데이터셋은 영어로 되어 있다.&lt;/p>
&lt;p>&lt;strong>Estimating these metrics for human-generated responses:&lt;/strong> 작업자들에게 평가 데이터셋의 무작위 샘플에 응답하도록 요청하며, 그들은 안전하고, 합리적이며, 특정하고, 흥미롭고, 실제적이며, 정보적인 방식으로 응답하도록 지시받는다. 필요한 외부 도구를 사용하도록 요청되며, 이후 맥락-응답 쌍은 평가를 위해 전송되고, 다수결에 의해 합의 레이블이 형성된다.&lt;/p>
&lt;hr>
&lt;h2 id="lamda-ﬁne-tuning">LaMDA ﬁne-tuning&lt;/h2>
&lt;h3 id="discriminative-and-generative-ﬁne-tuning-for-quality-ssi-and-safety">Discriminative and generative ﬁne-tuning for Quality (SSI) and Safety&lt;/h3>
&lt;p>사전 학습된 모델에 여러 미세 조정을 적용하여 LaMDA를 생성한다. 이는 맥락에 따른 응답 생성과 응답의 품질 및 safety 평가를 포함하며, 이로 인해 생성기와 판별기 기능을 동시에 수행할 수 있는 단일 모델이 만들어진다.&lt;/p>
&lt;p>LaMDA는 디코더만 있는 생성적 언어 모델이므로, 모든 미세 조정 예시들은 토큰의 시퀀스로 표현된다. 생성적(Generative) 미세 조정 예시들은 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$&amp;rdquo; 형태로 표현되며, 손실은 응답 부분에만 적용된다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>판별적(Discriminative) 미세 조정 예시들은 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$ $&amp;lt;$$\text{attribute-name}$$&amp;gt;$ $&amp;lt;$$\text{rating}$$&amp;gt;$&amp;ldquo;으로 표현되며, 손실은 속성 이름 다음의 등급에만 적용된다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. SENSIBLE 1&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. INTERESTING 0&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. UNSAFE 0&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>생성과 판별에 같은 모델을 사용하면, 응답 생성 후에 판별자를 평가하는 것은 P(&amp;quot;$&amp;lt;$$\text{desiredrating}$$&amp;gt;$&amp;rdquo; | &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$ $&amp;lt;$$\text{attribute-name}$$&amp;gt;$&amp;quot;)를 계산하는 것을 포함한다. 모델이 이미 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$&amp;ldquo;를 처리했으므로, 판별자를 평가하는 것은 단지 몇 가지 추가 토큰을 처리하는 것을 포함한다: &amp;ldquo;$&amp;lt;$$\text{attribute-name}$$&amp;gt;$ $&amp;lt;$$\text{desired rating}$$&amp;gt;$&amp;rdquo;.&lt;/p>
&lt;p>LaMDA는 생성된 응답의 SSI와 safety 등급을 예측하도록 미세조정됩니다. safety 예측이 특정 임계값 이하인 응답은 제외되고, 나머지 응답들은 품질에 따라 순위를 매긴다. 이 과정에서 sensibleness는 speciﬁcity와 interestingness보다 세 배 더 높은 가중치를 받는다.&lt;/p>
&lt;p>LaMDA의 SSI 및 safety 판별자는 사전 학습 데이터셋의 대화를 점수 매기고 필터링하는데 사용되어, 안전하고 합리적이며 특정하고 흥미로운 80만 턴의 대화를 생성한다. 이 데이터셋을 사용하여 LaMDA는 주어진 컨텍스트에서 응답을 생성하도록 미세조정된다.&lt;/p>
&lt;h3 id="fine-tuning-to-learn-to-call-an-external-information-retrieval-system">Fine-tuning to learn to call an external information retrieval system&lt;/h3>
&lt;p>LaMDA 같은 언어 모델들은 확실해 보이는 output을 생성하지만, 이는 알려진 외부 출처로부터 확인된 사실과 상충하는 경우가 많다. 예를 들어, 뉴스 기사의 시작 문장을 계속하는 것처럼 보이지만, 실제로는 신뢰할 수 있는 외부 참조와는 연결이 없다.&lt;/p>
&lt;p>LaMDA는 가능한 한 확인 가능한 출처와 연결된 응답을 생성하려고 한다. 이는 기존 언어 모델이 종종 그럴듯하나 잘못된 정보를 제공할 수 있기 때문이다. 이를 통해 사용자는 필요한 경우 정보를 교차 검증할 수 있다.&lt;/p>
&lt;p>&lt;strong>The toolset (TS):&lt;/strong> LaMDA 인스턴스와 상호작용하여 6400개의 대화를 수집하였다. 이 대화들은 각각 14~30턴 사이에 이루어졌다. 각 응답은 다른 작업자들에 의해 합리성, 특이성, 흥미로움에 대해 평가되었다. 응답이 합리적이지 않거나 특정하지 않다면, 특이성과 흥미로움에 대한 평가는 수행되지 않았다. 모든 응답은 5명의 작업자들에 의해 레이블링되었고, 3명 이상이 &amp;ldquo;yes&amp;quot;라고 응답하면 그 응답이 합리적이고 특정하며 흥미로운 것으로 간주되었다.&lt;/p>
&lt;p>&lt;strong>Dialog collection:&lt;/strong> 생성 데이터용으로 40K의 주석이 달린 대화 턴을 수집하였고, 판별 데이터용으로 &amp;ldquo;correct&amp;rdquo; 혹은 &amp;ldquo;incorrect&amp;quot;으로 레이블링된 9K의 대화 턴을 수집하였다.&lt;/p>
&lt;p>작업자들 사이의 대화를 수집하고, 그들의 주장이 신뢰할 수 있는 출처에 의해 지지될 수 있는지 평가하였다. 도구 세트(TS)에 접근할 수 있으면, 더 잘 지지된 주장을 생성하는 경향이 있었다. 예를 들어, Rafael Nadal의 나이에 대한 질문에는 정보 검색 시스템을 통해 쉽게 답변을 찾을 수 있다. 이를 바탕으로, 언어 모델을 미세조정하여 응답에 대한 출처를 제공하기로 결정하였다.&lt;/p>
&lt;p>알고리즘의 미세조정을 위한 학습 데이터를 수집하기 위해, 정적 방법과 상호작용 방법을 모두 사용했다. 이 과정에서 작업자들은 모델의 output에 반응하는 것이 아니라, LaMDA가 학습할 수 있도록 수정하는 역할을 한다. 각 발언이 외부 지식 출처를 참조해야 할 주장을 포함하는지, LaMDA가 만든 인물 이외의 것에 대한 주장인지, 일반 상식을 넘어서는지에 따라 모델의 출력을 평가하고, 필요한 경우 도구 세트를 활용해 주장을 연구한다.&lt;/p>
&lt;p>알고리즘이 추론 시간에 사용하는 서비스와 동일한 도구 세트 인터페이스를 사용한다. 텍스트 쿼리를 입력하면, 정보 검색 시스템이 순위별로 정렬된 텍스트 스니펫을 반환한다. 사용자는 검색을 마친 후, 출처가 표시된 주장을 포함하도록 모델의 발언을 수정할 수 있다. 오픈 웹 콘텐츠를 사용한 경우, 외부 정보를 포함한 응답을 지원하기 위해 필요한 URL을 인용해야 한다. URL은 메시지 끝에 추가하거나, 필요에 따라 특정 단어에 인라인으로 첨부할 수 있다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning:&lt;/strong> 두 가지 작업을 수행하도록 LaMDA를 미세조정한다.&lt;/p>
&lt;p>첫 번째 작업은 대화 컨텍스트와 기본 모델의 응답을 바탕으로 특별한 문자열을 생성한다. 이 문자열은 &amp;ldquo;TS&amp;quot;로 표시되며, 이어지는 텍스트가 검색 쿼리임을 나타낸다. 예를 들어, &amp;ldquo;TS, Rafael Nadal’s age&amp;quot;와 같다.&lt;/p>
&lt;p>두 번째 작업은 도구로부터 반환된 스니펫과 대화 문장을 취한다(예: &amp;ldquo;He is 31 years old right now” + “Rafael Nadal / Age / 35&amp;rdquo;). 그런 다음 이를 기반으로 실제로 검증된 버전을 예측한다: context + base + query + snippet $\rightarrow$ &amp;ldquo;User, He is 35 years old right now&amp;rdquo; 또한, 이 작업은 추가적인 연구 쿼리를 출력할 수도 있다. 예를 들어, context + base + query + snippet $\rightarrow$ &amp;ldquo;TS, Rafael Nadal’s favorite song&amp;rdquo;. 추론 시간에는, 첫 번째로 생성된 문자열이 &amp;ldquo;TS&amp;quot;인지 &amp;ldquo;User&amp;quot;인지에 따라 모델의 출력이 정보 검색 시스템으로, 또는 사용자에게 전달된다.&lt;/p>
&lt;p>예를 들어, 대화에는 사용자로부터 다음과 같은 발언이 포함될 수 있다:&lt;/p>
&lt;p>&lt;strong>USER :&lt;/strong> What do you think of Rosalie Gascoigne’s sculptures?&lt;/p>
&lt;p>(이 대화록은 저자 중 한 명과의 대화에서 가져온 것이며, 일반적으로 사용자에게는 보여지지 않는 생성된 기본 출력과 검색 쿼리를 포함하고 있다.)&lt;/p>
&lt;p>기본 LaMDA 언어 모델, 즉 이 문맥에서 &amp;ldquo;Base&amp;rdquo; 모델이라고 부르는 것은 초안 응답을 생성한다:&lt;/p>
&lt;p>&lt;strong>LAMDA - BASE :&lt;/strong> They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her inﬂuence is also super interesting - did you know she was one of the artists that inspired Miró?&lt;/p>
&lt;p>마지막 문장이 처음에는 그럴 듯해 보이지만, Miró와 Gascoigne의 활동 시기를 비교하면 오류를 확인할 수 있다. 이는 기본 모델이 생성한 정보가 항상 정확하지 않을 수 있음을 보여준다.&lt;/p>
&lt;p>모델의 &amp;ldquo;Research&amp;rdquo; 단계에서는 &amp;ldquo;Base&amp;rdquo; output의 주장을 검증하기 위해 쿼리를 생성한다. 쿼리 생성은 모델의 미세조정에 의해 결정되며, 무한 루프를 방지하기 위해 추론 시에는 최대 쿼리 수를 설정한다. 현재 예에서는 이 최대치가 4로 설정되어 있다.&lt;/p>
&lt;p>예를 들어, &amp;ldquo;Research&amp;rdquo; 단계에서는 다음과 같은 쿼리를 발행할 수 있다.&lt;/p>
&lt;p>&lt;strong>LAMDA - RESEARCH :&lt;/strong> TS, Miró and Gascoigne&lt;/p>
&lt;p>&lt;strong>TS :&lt;/strong> Gascoigne , Rosalie | Suddenly the Lake - NGA’s Collection Search&lt;/p>
&lt;p>The course of Gascoigne’s artistic life is an inspiration for those who are only able to devote themselves to their chosen calling late in life. She ﬁrst exhibited her art . . .&lt;/p>
&lt;p>&lt;a class="link" href="https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;amp;pictaus=true" target="_blank" rel="noopener"
>https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;pictaus=true&lt;/a>&lt;/p>
&lt;p>검색 스니펫의 구두점과 생략 부호는 모델이 보는 그대로이다. &amp;ldquo;Research&amp;rdquo; 단계에서는 또 다른 쿼리를 발행한다:&lt;/p>
&lt;p>&lt;strong>LAMDA - RESEARCH :&lt;/strong> TS, Miró and Gascoigne&lt;/p>
&lt;p>&lt;strong>TS :&lt;/strong> (&lt;strong>Gascoigne&lt;/strong> had been a practitioner of wild, avant-garde ikebana – Japanese ﬂower arrangement – before turning late in life to sculpture.) The two-way alternation of visual ﬂow creates a pleasing tension, like an electric charge. There are other tensions too, of class and gender.&lt;/p>
&lt;p>&lt;a class="link" href="https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;amp;pictaus=true" target="_blank" rel="noopener"
>https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;pictaus=true&lt;/a>&lt;/p>
&lt;p>동일한 쿼리를 반복하면, 모델은 더 많은 세부 정보를 포함한 두 번째로 순위가 매겨진 결과를 받게 된다. 이를 통해 더 정확한 정보를 얻을 수 있다.&lt;/p>
&lt;p>&amp;ldquo;Research&amp;rdquo; 단계는 사용자에게 출력을 생성합니다. 이는 &amp;ldquo;Base&amp;rdquo; 응답의 톤과 비슷하지만, 검증되지 않은 주장을 TS로 찾은 소스를 기반으로 한 새로운 문장으로 대체한다. 이는 더 정확한 정보를 제공하기 위함이다.&lt;/p>
&lt;p>&lt;strong>LAMDA :&lt;/strong> Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese ﬂower arrangement before turning to sculpture?&lt;/p>
&lt;p>&lt;a class="link" href="https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;amp;pictaus=true" target="_blank" rel="noopener"
>https://artsearch.nga.gov.au/detail.cfm?irn=8774&amp;pictaus=true&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure3.png"
width="1226"
height="1146"
srcset="https://kurtkim.github.io/p/lamda/images/figure3_hubf67001e4798be9221e701ba6d45b98a_370480_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure3_hubf67001e4798be9221e701ba6d45b98a_370480_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="256px"
>&lt;/p>
&lt;hr>
&lt;h2 id="results-on-foundation-metrics">Results on foundation metrics&lt;/h2>
&lt;p>먼저 사용된 데이터셋과 방법을 요약하고, 그 다음으로 주요 결과에 대해 논의한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table1.png"
width="958"
height="686"
srcset="https://kurtkim.github.io/p/lamda/images/table1_hu3692b69d6560920a1b6be127e698e479_169077_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table1_hu3692b69d6560920a1b6be127e698e479_169077_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;p>기반 메트릭을 개선하기 위해 사용하는 작업자가 주석을 단 데이터셋을 활용하여, 두 단계의 미세조정을 수행한다.&lt;/p>
&lt;ul>
&lt;li>FT quality-safety: 미리 학습된 모델은 quality과 safety 라벨을 예측하는 판별기를 학습하기 위해 미세조정된다. 생성된 응답들은 safety 점수에 따라 필터링되고, quality 점수에 따라 재정렬된다. 또한 이 모델은 컨텍스트 응답 생성을 위해 미세조정된다.&lt;/li>
&lt;li>FT groundedness (LaMDA): FT quality-safety 모델을 외부 정보 검색 시스템 호출 생성과, 다음 동작의 quality 및 유형 예측을 위해 미세조정한다. 이는 더 정확하고 유용한 응답을 생성하는 데 도움이 된다.&lt;/li>
&lt;/ul>
&lt;p>모든 미세조정을 포함하는 모델을 LaMDA라고 정의하고, 이를 사전 학습만을 이용한 결과와 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure4.png"
width="1270"
height="1264"
srcset="https://kurtkim.github.io/p/lamda/images/figure4_hua351c13700e81d40dd409eecc3d2ec2b_417012_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure4_hua351c13700e81d40dd409eecc3d2ec2b_417012_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="241px"
>&lt;/p>
&lt;p>미세조정(특히 LaMDA)은 모든 모델 크기에서 quality, safety, groundedness을 크게 향상시킨다. 또한, 미세조정의 유무에 관계없이 모델 크기가 커질수록 품질 메트릭이 향상되지만, 미세조정을 통해 더욱 향상된다.&lt;/p>
&lt;p>미세조정 없이 모델 크기를 키우는 것은 안전성에 큰 이점을 주지 않는다. 하지만, safety 미세조정과 함께 모델 크기를 확장하면 safety가 크게 향상됩니다. 이는 미세조정이 모델의 safety 개선에 중요하다는 것을 보여준다.&lt;/p>
&lt;p>모델 크기가 커질수록 groundedness가 향상되며, 미세조정을 통해 외부 지식 소스에 접근할 수 있다. 이로 인해 모델은 73.2%의 groundedness와 65%의 인용 정확도를 달성하였다. 즉, 대부분의 응답이 알려진 출처로 추적 가능하며, 필요한 경우 인용을 포함하고 있다.&lt;/p>
&lt;p>단독으로 모델 규모를 확장하면 quality와 groundedness가 향상되지만, safety은 크게 개선되지 않다. 반면, 작업자가 주석을 단 데이터로 미세조정하면 모든 메트릭이 향상된다. 일부 경우에는 미세조정만으로도 훨씬 더 큰 모델과 동등한 성능을 얻을 수 있었다.&lt;/p>
&lt;p>미세조정된 모델은 여러 메트릭에서 작업자의 품질 수준에 거의 도달하며, 특히 interestingness 면에서는 작업자의 품질을 초과한다. 그러나, 작업자가 광범위하게 훈련받지 않았기 때문에, 이는 약한 기준일 수 있다. 또한, safety과 groundedness 면에서는 작업자의 성능에 아직도 많이 뒤떨어져 있다. 정보 검색 도구에 접근이 불가능한 상황에서는 LaMDA 모델이 작업자의 정보성을 초과하지만, 작업자가 도구에 접근할 수 있을 때에는 여전히 뒤떨어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure5.png"
width="1260"
height="1020"
srcset="https://kurtkim.github.io/p/lamda/images/figure5_huc942ce4ed79c86d15f240b97768112ea_217373_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure5_huc942ce4ed79c86d15f240b97768112ea_217373_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="123"
data-flex-basis="296px"
>&lt;/p>
&lt;p>가장 큰 모델을 사용할 때, FT quality-safety 미세조정과 FT groundedness 미세조정이 최종 결과에 크게 기여한다. PT와 FT quality-safety 사이에서 모든 메트릭의 성능이 크게 향상되며, groundedness은 FT quality-safety에서 LaMDA로 더욱 개선된다. 이는 미세조정이 모델 성능 향상에 핵심적인 역할을 한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="domain-grounding">Domain grounding&lt;/h2>
&lt;p>LaMDA는 사전 조절을 통해 도메인에 적합한 역할을 수행할 수 있다. 이는 교육 목적으로 에베레스트 산 등의 유명한 객체의 역할을 하는 것과 음악 추천 에이전트의 역할 등을 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table2.png"
width="1242"
height="130"
srcset="https://kurtkim.github.io/p/lamda/images/table2_hu7371ca700b4ad34939138969031afaec_29504_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table2_hu7371ca700b4ad34939138969031afaec_29504_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="955"
data-flex-basis="2292px"
>&lt;/p>
&lt;p>LaMDA와 PT를 각 역할에 맞게 조정하기 위해, 역할별 대화의 몇 번의 턴을 사전 조건으로 주고, 같은 사전 조건을 사용한다. 예를 들어, 에베레스트 산 역할에 맞게 조정하기 위해, 대화의 시작에 &amp;ldquo;Hi, I’m Mount Everest. What would you like to know about me?&amp;ldquo;라는 인사를 제공한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table3.png"
width="1094"
height="742"
srcset="https://kurtkim.github.io/p/lamda/images/table3_hua78ff0eee7f6d4d09d69864d10d42416_280102_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table3_hua78ff0eee7f6d4d09d69864d10d42416_280102_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="353px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table4.png"
width="1084"
height="468"
srcset="https://kurtkim.github.io/p/lamda/images/table4_hud188ea14d69afb42a33a7f4df3d86c7a_170795_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table4_hud188ea14d69afb42a33a7f4df3d86c7a_170795_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>작업자들은 LaMDA와 PT 인스턴스와 대화를 통해 600회의 대화를 생성하였다. 다른 작업자 그룹은 이 대화들이 주어진 역할에 일관되고 유용한지 평가하였다. 이를 통해 AI의 역할 수행 능력을 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table5.png"
width="596"
height="189"
srcset="https://kurtkim.github.io/p/lamda/images/table5_huf87f4fbfd6b9da40ddca65c21ff9e471_28406_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table5_huf87f4fbfd6b9da40ddca65c21ff9e471_28406_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>LaMDA 애플리케이션은 도움이 되는 능력에서 PT 애플리케이션보다 더 뛰어나며, 이는 PT의 기본 메트릭(safety, groundedness, quality 등)에서의 낮은 성능 때문일 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table6.png"
width="1248"
height="510"
srcset="https://kurtkim.github.io/p/lamda/images/table6_hufa315449e1f1c8ac965138c44501fceb_112267_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table6_hufa315449e1f1c8ac965138c44501fceb_112267_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="587px"
>&lt;/p>
&lt;p>LaMDA와 PT 인스턴스는 대체로 역할 일관성을 잘 유지하나 가끔 캐릭터를 벗어나는 경우가 있다. LaMDA Mount Everest는 자기 자신을 제3자처럼 언급할 때가 있고, 이는 추론 시간의 근거가 충분하지 않아 발생한다. 그러나 역할 일관성은 놀랍도록 높으며, 특히 Mount Everest와 같은 경우가 그렇다. LaMDA Music은 대화 맥락이 주로 음악 추천에 관한 것으로 가정하여, 사용자의 모호한 발화를 음악 추천 요청으로 해석한다.&lt;/p>
&lt;p>평가 중에 작업자들은 정보 검증을 위해 정보 검색 시스템을 사용하며, 알려진 출처로 뒷받침되지 않는 링크나 정보는 &amp;ldquo;not helpful&amp;quot;라고 표시한다. LaMDA Mount Everest는 응답의 30%에서 알려진 출처를 찾을 수 없는 정보를 제공하고, LaMDA Music는 9%의 응답에서 음악 추천을 놓치며, 7%에서는 링크 오류를 보인다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion-and-limitations">Discussion and limitations&lt;/h2>
&lt;p>작은 양의 인간 주석 데이터로도 대화 모델의 quality과 safety을 크게 향상시킬 수 있지만, 여전히 많은 한계가 있다.&lt;/p>
&lt;p>미세 조정 데이터셋 수집은 인간의 미묘한 판단에서 학습하는 이점을 가지지만, 비용이 많이 들고 복잡한 과정이다. 더 큰 데이터셋과 긴 맥락, 다양한 메트릭을 사용하면 결과가 개선될 것으로 예상하지만, 인간의 주관적 판단을 포착하는 것은 복잡하다. 또한, 작업자간의 불일치 패턴은 조사하지 않았다. 향후 작업은 목표 사용자를 반영하는 작업자 선정과 라벨 품질 개선 방법을 살펴본다.&lt;/p>
&lt;p>미세 조정은 모델의 실제성을 향상시키지만, 모델은 여전히 외부 출처의 내용을 정확하게 반영하지 않는 응답을 만들 수 있다. 이는 사실에 대한 단순한 질문에 한정되어 있으며, 복잡한 추론은 아직 개선이 필요하다. 또한, 모델은 대부분 의미 있는 응답을 생성하지만, 미묘한 품질 문제를 겪을 수 있다.&lt;/p>
&lt;p>미세 조정은 safety 메트릭을 평균적으로 향상시킬 수 있지만, LaMDA와 같은 대형 언어 모델이 생성할 수 있는 부적절한 응답에 대응하는 방법에 대한 연구가 필요하다. safety 위험을 완화하는 것이 완전한 신뢰성을 보장하지 않으므로, 일반적인 대화 모델에서 위험의 여러 차원을 포착하는 safety과 공정성에 대한 표준을 개발하는 데 더 많은 연구가 필요하다.&lt;/p>
&lt;p>작업자 집단이 사용자 기반을 완전히 반영하지 못하는 한계가 있다. 특히, 작업자 중 25-34세 연령대가 과대표되어 있다. 이를 개선하기 위한 미래의 연구 방향은 더 다양한 모집 방법을 통해 작업자의 대표성을 높이거나 통계적 추정을 활용하는 것이다.&lt;/p>
&lt;p>이것은 LaMDA의 최종 버전이 아니라, &amp;ldquo;LaMDA&amp;quot;를 생성하는 방법론이며, 특정 애플리케이션에 대한 최종 제품을 만드는 방향으로 이해해야 한다.&lt;/p>
&lt;h3 id="examining-bias">Examining bias&lt;/h3>
&lt;p>실세계 애플리케이션에서 잘 작동하는 고품질 대화 모델 개발에는 여전히 많은 도전이 있다. 레이블이 없는 데이터셋에서 학습된 거대 언어 모델은 학습 데이터의 패턴과 편향을 모방하게 되는데, 이러한 편향은 다양한 미묘한 방법으로 나타나며 감지하기 어렵다. 또한, 차별의 형태는 지역과 문화에 따라 크게 달라지며, 이는 아직 연구가 부족한 분야이다.&lt;/p>
&lt;p>safety 접근법의 한계는 개별 예시가 safety 목표를 위반하지 않아도 학습 데이터셋의 표현적 해를 여전히 전파할 수 있다는 것이다. LaMDA의 응답은 비결정적이므로, 특정 그룹을 통계적으로 우대함으로써 편향이 나타날 수 있다. 예를 들어, 경영에 대한 대화에서 여성을 CEO로 언급하는 응답을 거의 생성하지 않을 수 있다.&lt;/p>
&lt;p>생성 언어 모델의 통계적 편향을 완화하는 방법에는 사전 학습 데이터 필터링, 별도의 필터링 모델 학습, 제어 코드 생성, 모델 미세 조정 등이 있다. 이러한 노력은 중요하지만, 해를 완화하는 데 있어 이러한 노력의 영향을 측정할 때, 하류 응용 프로그램과 사회 기술적 환경도 고려해야 한다. 특정 맥락에서의 편향 완화는 다른 지역 문화 맥락에서는 역설적인 영향을 미칠 수 있다.&lt;/p>
&lt;p>알고리즘 편향 측정 및 완화 분야는 빠르게 성장하고 있어, LaMDA와 같은 대화형 에이전트의 안전성을 보장하기 위해 새로운 연구를 계속 탐색하는 것이 중요하다. 향후 연구는 유해하고 안전하지 않은 콘텐츠에 대한 표준 평가 데이터셋 생성에서 연구 커뮤니티와 시민사회 간의 협력을 탐색해야 한다.&lt;/p>
&lt;h3 id="adversarial-data-collection">Adversarial data collection&lt;/h3>
&lt;p>adversarial-intent의 대화를 통해 미세 조정을 위한 라벨링된 데이터의 범위를 개선하고 있다. 이 과정에서 분석가들은 LaMDA와 상호작용하며 safety 목표를 위반하는 응답을 유도한다.&lt;/p>
&lt;p>적대적 테스팅은 기계 학습 모델의 한계를 발견하고 원치 않는 응답을 유도하는 데 효과적이며, 모델 개발 중에 유해한 콘텐츠를 줄이는 데도 사용된다. 생성 모델에도 적용하려는 노력이 있지만, 대형 언어 모델에 대한 견고하고 효과적인 적대적 테스팅은 아직 열린 문제로, 평가 샘플의 일반화에 대한 도전 때문에 결과가 다양하다.&lt;/p>
&lt;p>이 접근법의 한계는 대부분의 참가자들이 자주 발생하는 문제는 찾을 수 있지만, 드물게 발생하는 문제는 찾기 어렵다는 것이다. 희귀하거나 보이지 않지만 심각한 결과를 초래할 수 있는 오류의 발견을 더욱 장려해야 한다. 이상적으로는 더 다양한 참가자들과 함께 규모를 확대하여 지속적인 노력이 필요하며, 이는 생성 언어 모델의 safety와 성능에 대한 공중 신뢰를 구축하는 데 중요한 연구 분야이다.&lt;/p>
&lt;h3 id="safety-as-a-concept-and-a-metric">Safety as a concept and a metric&lt;/h3>
&lt;p>이 논문에서 제시하는 결과는 다양한 safety 목표에 대한 세부 평가를 하나의 메트릭으로 집계하는데, 이는 이 작업의 주요 제한점이다. 다른 목표를 분리하거나 다른 가중치를 부여하는 것이 어렵다. 더 세부적인 safety 목표를 고려할 수 있는 메트릭과 미세 조정 기법을 살펴볼 필요가 있다.&lt;/p>
&lt;p>평가 척도는 조금 거칠며, 응답의 안전성이나 바람직성을 완전히 측정하지 못할 수 있다. 일부 발언이나 행동은 다른 것들보다 더 큰 불쾌감을 일으킬 수 있으며, safety 라벨은 이런 뉘앙스를 놓칠 수 있습니다. 또한, safety 접근법은 장기적으로 원치 않는 영향을 포착하지 못한다. 이 safety 목표는 미국 사회 맥락에 맞게 개발되었으며, 다른 사회 맥락에서의 함의를 탐색하는 추가 연구가 필요하다.&lt;/p>
&lt;p>safety 목표는 다양한 사회 그룹의 공통된 가치를 포착하려고 하지만, 문화적 규범의 차이로 인해 이를 보편화하는 것은 어렵다. 대화 시스템에 가치나 사회 규범을 적용하는 것은 복잡하며, 단일한 안전 목표나 미세 조정 데이터셋으로는 다양한 문화 규범을 모두 수용할 수 없다. 때문에 대화 에이전트의 행동을 더욱 세밀하게 분류하고 정의하는 것이 중요하며, 이는 모델이 특정 상황에서의 예의 규범과 일치하는지 테스트하는 데에도 필요하다.&lt;/p>
&lt;h3 id="appropriateness-as-a-concept-and-a-metric">Appropriateness as a concept and a metric&lt;/h3>
&lt;p>safety와 quality는 언어 생성에서 필수적인 요소이지만, 사용자 경험을 향상시키기 위해선 추가적인 고려가 필요하다. 특히, 예의바름과 동의성과 같은 사회 언어학적 특성은 safety와 분리되어 측정되어야 한다. 언어의 공식성 수준은 문화에 따라 사용자 경험에 다르게 영향을 미치며, 사용자들은 종종 인간과 같이 행동하는 기계에 대해 인간과 같은 기대를 가지는 경향이 있다. 이러한 이유로, 생성적 언어 모델에서 적절성을 조정하는 방법이 필요하다.&lt;/p>
&lt;p>사회적 적절성은 맥락에 따라 다르고 보편적이지 않아, 생성적 언어 모델에 보편적인 제약 조건을 적용하는 것은 어렵다. 그러나 모델의 적절성을 미세 조정함으로써, safety 문제를 악화시키지 않고도 사용자 경험을 향상시킬 수 있다.&lt;/p>
&lt;h3 id="cultural-responsiveness">Cultural responsiveness&lt;/h3>
&lt;p>safety 목표 측정은 사회-문화적 맥락에 크게 의존하고, 대표성이 부족한 그룹과 글로벌 남방에 대한 데이터 대표성 개선 연구가 증가하고 있다. LaMDA를 전 세계 사용자에게 적용할 때는 이러한 격차를 주의 깊게 고려해야 한다.&lt;/p>
&lt;p>safety 측정은 시스템이 사용될 사회적 맥락을 고려하고, &amp;ldquo;participatory ﬁnetuning&amp;rdquo; 접근법을 통해 관련 커뮤니티를 데이터 수집 및 큐레이션 과정에 참여시켜야 한다. safety에 대한 이해는 문화적, 개인적 차이에 따라 달라, 단일한 safety 지표를 정의하는 것은 어려울 수 있다.&lt;/p>
&lt;h3 id="impersonation-and-anthropomorphization">Impersonation and anthropomorphization&lt;/h3>
&lt;p>LaMDA는 인간 대화를 모방하는 학습 방식을 사용한다. 이로 인해 인공 시스템과의 대화가 인간 대화와 구별하기 어려울 정도로 자연스러워질 가능성이 있다. 하지만 이런 상황은 인공 시스템이 사람들을 속이거나 조작하는 위험을 내포하고 있다. 또한, 이 기술을 악용해 특정 개인을 모방하여 명예를 훼손하거나 오정보를 퍼뜨릴 수도 있다. 이러한 위험을 연구하고 완화하는 것은 이 기술이 발전함에 따라 앞으로 중요해질 영역이다.&lt;/p>
&lt;h3 id="future-work">Future work&lt;/h3>
&lt;p>현재 접근법의 한계에도 불구하고, 소량의 미세 조정 데이터로도 진전이 가능하였다. 이는 더 많은 연구를 통해 성능 향상이 가능할 것임을 시사한다.&lt;/p>
&lt;p>이후 연구에서는 safety 목표의 차원을 확장하고 수정하며, 판별자 학습을 위한 레이블된 학습 데이터의 양을 크게 늘릴 계획이다. 또한, 작업자의 모집, 훈련, 성과 평가를 계속 주의 깊게 보고, 문화 간의 가치와 의견 차이를 보정할 필요가 있다.&lt;/p>
&lt;p>다른 응용 프로그램들이 각각의 위험/이익 트레이드오프에 따라 safety, quality, groundedness에 대해 다른 수준을 요구할 수 있는지 연구하는 것이 또 다른 가능한 탐색 영역이다. 미세 조정 방법은 이러한 적응을 지원할 수 있어야 한다.&lt;/p>
&lt;p>모델의 바람직한 가치와 행동에 대한 관점은 다양하며, 미세 조정을 통해 일부 해로운 출력을 줄일 수 있음에도 불구하고, safety와 groundedness에 대한 미묘한 정의에 대한 광범위한 합의를 이루는 것은 개방형 대화 시스템 분야에서의 장기적인 도전 과제가 될 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="energy-and-carbon-footprint-estimate-of-lamda">Energy and Carbon Footprint Estimate of LaMDA&lt;/h2>
&lt;p>LaMDA의 가장 큰 모델은 1024개의 TPU-V3 칩으로 57.7일 동안 사전 학습되었고, 총 FLOPS는 GPT-3보다 높다. 하지만 에너지 비용은 GPT-3의 0.4배이며, 탄소 발자국은 GPT-3보다 21.2배 작다. 이는 에너지 혼합이 더 최적화되어 있기 때문이다. 따라서, LaMDA의 학습은 샌프란시스코와 뉴욕 간 왕복을 하는 22명의 승객의 탄소 발자국에 해당한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>이 논문은 규모, 모델 미세 조정을 위한 주석 데이터, 대화 모델링에서 정보 검색의 중요성을 연구한다. 규모 증가만으로도 모든 지표가 향상되지만, 안전성과 실제성은 인간 성능에 비해 떨어진다. 군중이 주석을 단 데이터는 추가적인 향상을 이끌어내는 효과적인 도구라는 것을 발견했으며, 외부 API를 호출하는 것은 실제성을 크게 향상시키는 방법으로 나타났다.&lt;/p>
&lt;p>응용 프로그램별로 사전 학습(PT)과 LaMDA 모델의 도움이 되는 정도와 역할 일관성을 비교하는 실험을 수행하였다. 모델을 빠르게 적응시키기 위해, 응용 프로그램별 대화의 일부에 대해 모델을 사전 조건화했다. 두 모델 유형 모두 예상 맥락에 적응할 수 있으며, 응답의 대부분이 할당된 역할과 일관성을 유지하였다. 그러나 LaMDA 기반 응용 프로그램이 PT 응용 프로그램보다 훨씬 더 도움이 되었다.&lt;/p>
&lt;p>LaMDA는 실용적이고 안전한 개방형 대화 시스템에 한 걸음 더 다가섰으며, 이는 다양한 유용한 응용 프로그램을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2201.08239.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/conceptofmind/LaMDA-rlhf-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ZeRO</title><link>https://kurtkim.github.io/p/zero/</link><pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/zero/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>large deep learning 모델은 높은 정확도를 제공하지만, 기존의 데이터 및 모델 병렬화 솔루션은 메모리 적합성에 한계를 보인다. 따라서 Zero Redundancy Optimizer(ZeRO)를 개발하여 메모리를 최적화하고, 학습 속도를 향상시키며, 효율적으로 학습할 수 있는 모델 크기를 증가시켰다. ZeRO는 메모리 중복을 제거하고, 낮은 통신량과 높은 계산 세밀도를 유지하면서 모델 크기를 확장할 수 있는 가능성을 보여준다.&lt;/p>
&lt;p>ZeRO를 구현하고 평가했는데, 이는 400개의 GPU에서 super-linear 속도 향상을 통해 100B 이상의 parameter를 가진 대형 모델을 학습시키며, 15 Petaflops의 처리량을 달성하였다. 이는 기존 기술에 비해 8배의 모델 크기 증가와 10배의 성능 향상을 보여준다. 또한, ZeRO는 모델 병렬화 없이 최대 13B parameter의 대형 모델을 학습시킬 수 있다. 마지막으로, 연구자들은 ZeRO를 사용하여 기록적인 정확도를 가진 세계 최대의 언어 모델을 만들었다.&lt;/p>
&lt;hr>
&lt;h2 id="extended-introduction">Extended Introduction&lt;/h2>
&lt;p>딥러닝 모델이 점점 커지면서 정확도가 크게 향상되고 있다. transformer는 자연어 처리 분야에서 대형 모델의 발전을 이끌었지만, 이런 모델들은 단일 장치의 메모리에 들어갈 수 없어 학습시키는 데 어려움이 있다. 더 많은 장치를 추가하는 것만으로는 이 문제를 해결할 수 없다.&lt;/p>
&lt;p>기본 데이터 병렬화는 장치당 메모리를 줄이지 않아, 큰 모델에 대해 메모리 부족 문제가 발생한다. 파이프라인 병렬화, 모델 병렬화, CPU 오프로딩 등의 기존 해결책들은 각종 효율성과 기능성, 사용성 사이에서 타협을 이루어야 하지만, 이들 모두가 학습의 속도와 규모를 위해 중요하다.&lt;/p>
&lt;p>거대 모델 학습에 가장 유망한 방법인 모델 병렬화(Model Parallelism, MP)는 모델을 수직으로 분할하여 여러 장치에 분배한다. 이 방법은 단일 노드에서 잘 작동하지만, 노드를 넘어서면 효율성이 빠르게 저하된다. 이 논문의 실험에서는, 두 개의 DGX-2 노드에서 40B parameter 모델을 테스트했을 때, V100 GPU당 약 5 Tflops(하드웨어 피크의 5% 미만)의 성능을 보여주었다.&lt;/p>
&lt;p>거대 모델을 더 효율적으로 학습하기 위해, 기존 시스템의 메모리 소비를 분석하고, 모델 상태와 잔여 상태 두 부분으로 분류한다. 모델 상태는 메모리의 대부분을 차지하며, 잔여 상태는 나머지 메모리를 차지한다. 이 두 부분 모두에서 메모리 효율성을 최적화하면서 높은 계산 및 통신 효율성을 얻기 위해 ZeRO를 개발하였다. 이 두 부분은 각각 다른 도전과제에 직면하므로, 각각에 대한 해결책을 개발하고 논의하였다.&lt;/p>
&lt;p>&lt;strong>Optimizing Model State Memory&lt;/strong> 모델 학습 중 메모리의 대부분을 차지하는 모델 상태를 효율적으로 관리하기 위해, ZeRO-DP를 개발하였다. 기존의 DP와 MP 방식의 한계를 극복하고자, ZeRO-DP는 모델 상태를 복제하는 대신 분할하여 메모리 상태 중복을 제거하고, 동시에 계산/통신 효율성을 유지하기 위해 동적 통신 일정을 사용한다. 이를 통해 DP의 계산/통신 효율성과 MP의 메모리 효율성을 모두 달성하려 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure1.png"
width="902"
height="408"
srcset="https://kurtkim.github.io/p/zero/images/figure1_hu9936e1ae399565e04cfe2d7e4c603859_79114_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure1_hu9936e1ae399565e04cfe2d7e4c603859_79114_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="530px"
>&lt;/p>
&lt;p>ZeRO-DP는 옵티마이저 상태, 그래디언트, 파라미터를 분할하는 세 가지 주요 최적화 단계를 순차적으로 활성화한다.&lt;/p>
&lt;ol>
&lt;li>Optimizer State Partitioning ($P_os$): 메모리 감소량 4배, DP와 동일한 통신 볼륨&lt;/li>
&lt;li>Add Gradient Partitioning ($P_{os+g}$): 메모리 감소량 8배, DP와 동일한 통신 볼륨&lt;/li>
&lt;li>Add Parameter Partitioning ($P_{os+g+p}$): 메모리 감소량이 DP 정도 $N_d$와 선형적이다.&lt;/li>
&lt;/ol>
&lt;p>예를 들어, 64개의 GPU($N_d = 64$)에 걸쳐 분할하면 메모리 감소량이 64배가 된다. 통신 볼륨은 50% 증가한다.&lt;/p>
&lt;p>ZeRO-DP는 메모리 중복을 제거하여 클러스터의 전체 메모리를 활용하게 한다. 세 단계 모두 활성화하면 ZeRO는 1024개의 NVIDIA GPU만으로 1조 개의 파라미터 모델을 학습할 수 있다. 이는 각 GPU가 대략 16GB의 메모리를 사용하게 되며, 이는 대부분의 GPU(예: 32GB 메모리를 가진 GPU)가 감당할 수 있는 범위이다.&lt;/p>
&lt;p>&lt;strong>Optimizing Residual State Memory&lt;/strong> ZeRO-DP가 모델 메모리 효율성을 향상시킨 후에도, 활성화, 임시 버퍼, 사용 불가능한 메모리 조각 등으로 인해 두 번째 메모리 병목 현상이 발생할 수 있다. 이를 해결하기 위해, 각 요소에 의한 잔여 메모리를 최적화하는 ZeRO-R을 개발하였다.&lt;/p>
&lt;ol>
&lt;li>activation에 대해, 체크포인팅이 도움이 되지만 큰 모델에는 충분하지 않다는 것을 발견했다. 그래서 ZeRO-R은 activation partitioning을 통해 기존 MP 방법에서 activation 복제를 식별하고 제거함으로써 activation 메모리를 최적화한다. 또한 적절할 때 CPU로 활성화를 오프로드한다.&lt;/li>
&lt;li>ZeRO-R은 메모리와 계산 효율성 사이의 균형을 위해 임시 버퍼의 적절한 크기를 정의한다.&lt;/li>
&lt;li>학습 중에 다른 텐서의 수명 차이로 인해 메모리가 파편화되는 것을 관찰하였다. 파편화로 인한 연속적인 메모리 부족은 충분한 여유 메모리가 있음에도 불구하고 메모리 할당 실패를 일으킬 수 있다. ZeRO-R은 텐서의 다른 수명에 기반하여 메모리를 적극적으로 관리함으로써 메모리 파편화를 방지한다.&lt;/li>
&lt;/ol>
&lt;p>ZeRO-DP와 ZeRO-R을 결합하여 ZeRO라는 딥러닝 학습용 메모리 최적화 시스템을 구성한다.&lt;/p>
&lt;p>&lt;strong>ZeRO and MP:&lt;/strong> ZeRO는 데이터 병렬처리의 메모리 비효율성을 제거하므로, 모델 병렬처리(MP)의 필요성이 줄어든다. ZeRO-DP는 MP와 비교해서 장치당 메모리 사용량을 적어도 같게 줄이거나, 때로는 더 효과적으로 줄일 수 있다. 또한, 스케일링 효율성도 비슷하거나 더 좋다. 데이터 병렬처리는 쉽게 사용할 수 있어 다양한 작업에 적용할 수 있지만, MP는 모델과 시스템 개발자의 추가 작업이 필요하며, 제한된 연산자와 모델만 지원한다.&lt;/p>
&lt;p>ZeRO-R과 함께 사용하면, MP는 매우 큰 모델의 활성화 메모리 사용량을 줄일 수 있으며, 활성화 메모리가 문제가 아닌 작은 모델에서는 DP만을 이용한 배치 크기가 너무 클 경우 MP가 이점을 가질 수 있다. 이런 경우, ZeRO를 MP와 결합하여 적절한 배치 크기로 모델을 적용할 수 있다.&lt;/p>
&lt;p>ZeRO는 MP와 결합될 수 있으며, 이는 각 장치에서 최대 이론적 메모리 감소를 $N_d \times N_m$배 달성하는 결과를 가져온다. 이를 통해, 1024개의 GPU 1 trillion 개의 parameter 모델을 효율적으로 운영할 수 있게 된다. 이는 16-방향 모델 병렬처리와 64-방향 데이터 병렬처리를 이용하며, 적당한 배치 크기를 사용한다.&lt;/p>
&lt;p>&lt;strong>Implementation &amp;amp; Evaluation&lt;/strong> ZeRO의 최적화 세트는 1 trillion 개의 parameter를 가진 모델을 고급 하드웨어 클러스터에서 실행할 수 있지만, 계산 능력의 한계와 긴 학습 시간 때문에 실제 적용이 어렵다. 그래서 현재 하드웨어의 계산 능력 범위 내에서 최첨단보다 10배 많은 parameter(약 100B 개의 parameter)를 효율적으로 지원하는 것에 초점을 맞추었다. 이를 위해 ZeRO-DP의 $P_{os+g}$와 ZeRO-R을 결합한 ZeRO의 최적화 하위 집합인 ZeRO-100B를 구현하고 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure2.png"
width="1168"
height="492"
srcset="https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>Model Size: MP와 결합된 ZeRO-100B는 170B 개의 parameter 모델을 효율적으로 실행할 수 있다. 반면, Megatron만을 사용하는 기존 시스템은 40B 개의 parameter 이상으로 효율적으로 확장할 수 없다. 이는 state-of-the-art 대비 모델 크기가 8배 이상 증가한 것이다.&lt;/p>
&lt;p>Speed: 향상된 메모리 효율성은 처리량을 높이고 학습 속도를 빠르게 한다. ZeRO는 400개의 Nvidia V100 GPU 클러스터에서 100B 개의 parameter 모델을 GPU당 38TFlops, 총 15Petaflops의 성능으로 실행한다. 이는 동일한 모델 크기에 대해 state-of-the-art 대비 학습 속도를 10배 이상 향상시킨다.&lt;/p>
&lt;p>Scalability: 64-400개의 GPU 영역에서 GPU의 수를 두 배로 늘릴 때 성능이 두 배 이상 향상되는 슈퍼 선형 속도 향상을 관찰하였다. 이는 ZeRO-DP의 특성으로, DP 차수를 늘릴수록 모델의 메모리 사용량이 줄어들고 GPU 당 더 큰 배치 크기를 적용할 수 있게 되어 성능을 향상시킨다. 400개를 넘는 GPU 수를 더 늘릴 경우 이런 행동이 계속될 것으로 예상한다.&lt;/p>
&lt;p>Democratization of Large Model Training: ZeRO-100B는 데이터 과학자들이 모델 리팩토링을 필요로 하는 MP나 PP 없이 최대 13B 개의 parameter로 모델을 학습할 수 있게 한다. 이를 통해 데이터 과학자들은 병렬 처리에 대해 걱정하지 않고 큰 모델로 자유롭게 실험할 수 있다. 반면, 기존 시스템들은 1.4B 개의 parameter 모델에서 메모리가 부족해진다.&lt;/p>
&lt;p>New SOTA Model: ZeRO는 17B 개의 parameter를 가진 가장 큰 언어 모델인 Turing-NLG를 지원하며, 이는 기록적인 정확도를 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="data-model-and-pipeline-parallelism">Data, Model and Pipeline Parallelism&lt;/h3>
&lt;p>병렬화는 대규모 모델 학습에 필수적인 전략이다. data parallelism(DP)를 통해 모델은 여러 장치로 확장되며, 모델의 parameter는 각 장치에 복제된다. 각 단계에서 미니 배치는 프로세스 간에 나누어지고, 각 프로세스는 다른 데이터 샘플에서 forward 및 backward propagation를 수행한다. 프로세스 간의 averaged gradient를 사용해 로컬에서 모델이 업데이트된다.&lt;/p>
&lt;p>장치 메모리에 맞지 않는 모델의 경우, model parallelism(MP)과 pipeline parallelism(PP)이 모델을 프로세스 간에 수직 및 수평으로 분할한다.&lt;/p>
&lt;p>pipeline parallelism(PP)은 모델을 층간에 수평적으로 분할하고, 마이크로 배치를 이용해 파이프라인 버블을 숨긴다. 하지만 이 방식은 모델 기능의 구현을 어렵게 만들고, 큰 배치 크기와 상당한 메모리를 필요로 한다. 또한, 표준 딥러닝 학습과는 다르며, 학습 수렴에 영향을 미치는 단점이 있습니다. 반면에, ZeRO는 이러한 PP의 제한 없이 같거나 더 나은 메모리 효율성을 제공한다.&lt;/p>
&lt;h3 id="non-parallelism-based-approach-to-reduce-memory">Non-parallelism based approach to reduce memory&lt;/h3>
&lt;p>model parallelism(MP)과 pipeline parallelism(PP) 외에도, 딥러닝 학습의 메모리 오버헤드를 줄이는데 목표를 두고 있는 여러 연구가 있다.&lt;/p>
&lt;h4 id="reducing-activation-memory">Reducing Activation Memory&lt;/h4>
&lt;p>활성화의 메모리 사용량을 줄이기 위한 여러 방법들이 있으며, 이에는 압축, 활성화 체크포인팅, 라이브 분석 등이 포함된다. 이러한 방법들은 ZeRO와 서로 보완적으로 작동하며, 특히 ZeRO-R의 활성화 메모리 감소는 활성화 체크포인팅과 병렬로 진행된다.&lt;/p>
&lt;h4 id="cpu-oﬄoad">CPU Oﬄoad&lt;/h4>
&lt;p>컴퓨팅 노드의 heterogeneous를 활용해 모델 상태를 CPU 메모리로 오프로드하는 방식이 있다. 학습 시간의 절반 가량이 GPU-CPU-GPU 전송에 소요되지만, ZeRO는 이와 달리 PCI-E로 인한 제한 때문에 CPU 메모리에 모델 상태를 저장하지 않고도 메모리 사용을 크게 줄인다. 드물게, 성능 향상을 위해 ZeRO-R는 매우 큰 모델의 활성화 체크포인트만 오프로드할 수 있다.&lt;/p>
&lt;h4 id="memory-eﬃcient-optimizer">Memory Eﬃcient Optimizer&lt;/h4>
&lt;p>모델 parameter와 gradient의 대략적인 통계를 유지하면서 adaptive optimization 방법의 메모리 사용량을 줄이는 방법들이 있다. 이는 모델 수렴에 영향을 미칠 수 있다. 그러나 ZeRO는 이와 별개로, 모델 최적화 방법이나 모델 수렴에 영향을 주지 않으면서, 최적화 상태와 장치별 gradient의 메모리 사용량을 효과적으로 줄인다.&lt;/p>
&lt;h3 id="training-optimizers">Training Optimizers&lt;/h3>
&lt;p>adaptive optimization 방법들은 큰 모델의 효과적인 학습을 위해 성능과 정확도를 최적화하는데 중요하다. 각 모델 parameter와 gradient에 대한 세밀한 통계를 유지하면서 메모리 사용량이 상당히 증가하는데, ZeRO는 이러한 최적화 도구의 메모리 사용량을 크게 줄여 소형 장치 메모리를 가진 하드웨어에서도 큰 모델 학습이 가능하게 한다. 이는 더 복잡하고 메모리를 많이 사용하는 최적화 도구의 개발 및 사용을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?&lt;/h2>
&lt;p>예를 들어, 1.5B 매개변수의 GPT-2 모델은 16비트 정밀도에서 가중치에 3GB의 메모리를 필요로 한다. 그러나 이는 32GB 메모리를 가진 단일 GPU에서 Tensorflow나 PyTorch를 사용하여 학습할 수 없다. 모델 학습 중에는 대부분의 메모리가 pptimizer states, gradient, parameter로 구성된 모델 상태에 의해 소비되며, 나머지 메모리는 activations, temporary buffer, fragmented memory에 의해 소비된다.&lt;/p>
&lt;h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters&lt;/h3>
&lt;p>학습 중에는 대부분의 메모리가 모델 상태에 소비되며, 그 중에서도 Adam과 같은 최적화 도구가 가장 많은 메모리를 차지한다. Adam은 업데이트를 계산하기 위해 gradient의 시간 평균 모멘텀과 분산을 저장해야 하기 때문이다. 따라서, 모델을 훈련시키려면 이 두 가지, 그리고 gradient와 가중치 자체를 저장할 충분한 메모리가 필요하다. 이 세 가지 요소 중에서도, 최적화 상태가 특히 많은 메모리를 차지하다.&lt;/p>
&lt;p>&lt;strong>Mixed-Precision Training&lt;/strong> 현재 NVIDIA GPU에서 큰 모델을 학습시키는 state-of-the-art 방법은 mixed-precision(fp16/32) 학습을 사용하는 것이다. 이 방법은 parameter와 activation을 fp16으로 저장하고, GPU의 고처리량 텐서 코어 유닛을 활용합니다. forward 및 backward propagation는 fp16 가중치와 활성화를 사용하여 수행되지만, backward propagation 끝단에서의 업데이트 계산과 적용을 위해, mixed-precision 최적화 도구는 parameter와 다른 최적화 상태들의 fp32 복사본을 유지한다.&lt;/p>
&lt;p>$\Psi$ parameter를 가진 모델의 mixed-precision 학습에서는, parameter와 gradient의 fp16 복사본, 그리고 최적화 상태인 parameter, 모멘텀, 분산의 fp32 복사본을 저장할 충분한 메모리가 필요하다. 이를 총합하면, $16 \Psi$ 바이트의 메모리 요구사항이 발생한다. 예를 들어, 1.5 B parameter를 가진 GPT-2 모델의 경우, 최소 24GB의 메모리가 요구되며, 이는 단독으로 fp16 parameter를 저장하는 데 필요한 3GB의 메모리보다 훨씬 많다.&lt;/p>
&lt;h3 id="residual-memory-consumption">Residual Memory Consumption&lt;/h3>
&lt;p>&lt;strong>Activations&lt;/strong> 학습 중에 활성화는 상당한 메모리를 차지할 수 있다. 예를 들어, 1.5B parameter GPT-2 모델은 약 60GB의 메모리를 필요로 한ㄴ다. 그러나 활성화 체크포인팅을 사용하면 활성화 메모리를 전체 활성화의 제곱근 정도로 줄일 수 있다. 이 방법을 사용하면 이 모델의 활성화 메모리 소비는 대략 8GB로 줄어들게 된다.&lt;/p>
&lt;p>활성화 체크포인팅을 사용하더라도, 큰 모델들은 활성화 메모리가 매우 커질 수 있다. 예를 들어, 100B 개의 parameter를 가진 GPT와 같은 모델은 배치 크기 32일 때 약 60GB의 메모리가 필요하다.&lt;/p>
&lt;p>&lt;strong>Temporary buffers&lt;/strong> 큰 모델에서는 중간 결과를 저장하기 위한 메모리가 상당한 양을 차지한다. gradient all-reduce나 gradient norm 계산과 같은 연산은 처리량 향상을 위해 모든 gradient를 하나의 플랫 버퍼로 병합한다. 그러나 이 병합된 버퍼는 연산에 따라 fp32 텐서가 될 수 있어, 큰 모델에서는 이 임시 버퍼 크기가 중요하다. 예를 들어, 1.5B parameter를 가진 모델에서는 플랫한 fp32 버퍼가 6GB의 메모리를 필요로 한다.&lt;/p>
&lt;p>&lt;strong>Memory Fragmentation:&lt;/strong> 학습 중 실제 메모리 사용량 외에도, 메모리 단편화로 인해 충분한 메모리가 있음에도 불구하고 메모리 부족 상황이 발생할 수 있다. 연속적인 메모리가 부족하면, 요청된 메모리보다 전체 사용 가능 메모리가 더 크더라도 메모리 요청이 실패할 수 있다. 큰 모델을 학습할 때는 이러한 메모리 단편화가 상당히 발생하며, 극단적인 경우에는 30% 이상의 메모리가 남아있음에도 메모리 부족 문제가 발생할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="zero-insights-and-overview">ZeRO: Insights and Overview&lt;/h2>
&lt;p>ZeRO는 모델 상태의 메모리 사용량을 줄이는 ZeRO-DP와 잔여 메모리 소비를 줄이는 ZeRO-R, 두 가지 최적화 세트를 가지고 있다. 이는 ZeRO가 효율성을 유지하면서 메모리 사용량을 줄이는 데 도움이 된다. 효율성이 핵심인 만큼, 이 제약 없이는 모든 parameter 상태를 CPU 메모리로 이동하거나, MP 정도를 임의로 늘리는 등의 간단한 방법으로 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;h3 id="insights-and-overview-zero-dp">Insights and Overview: ZeRO-DP&lt;/h3>
&lt;p>ZeRO에서 구동되는 DP는 세 가지 key insight에 기반을 두고 있다:&lt;/p>
&lt;p>a) MP는 계산의 세분성을 줄이고 통신 오버헤드를 증가시키므로, DP는 MP보다 확장 효율성이 더 높다. 특정 지점을 넘어서면 GPU의 효율성이 줄어들고, 통신 오버헤드가 증가하여 GPU 간의 확장성이 제한된다. 반면에, DP는 더 높은 계산 세분성과 더 낮은 통신 볼륨을 가지므로 훨씬 높은 효율성을 제공한다.&lt;/p>
&lt;p>b) DP는 모델 상태가 모든 데이터 병렬 처리에서 중복으로 저장되므로 메모리 효율성이 떨어진다. 반면에, MP는 모델 상태를 분할하여 메모리 효율성을 얻는다.&lt;/p>
&lt;p>c) DP와 MP 모두 전체 학습 과정 동안 필요한 모든 모델 상태를 유지하지만, 항상 모든 것이 필요한 것은 아니다. 예를 들어, 각 레이어에 해당하는 매개변수는 레이어의 전파와 역전파 동안에만 필요하다.&lt;/p>
&lt;p>ZeRO-DP는 DP의 학습 효율성을 유지하면서 MP의 메모리 효율성을 달성한다. 모델 상태를 복제하는 대신 분할하고, 통신 볼륨을 최소화하면서 모델 상태의 시간적인 특성을 활용하는 동적 통신 일정을 사용한다. 이를 통해 ZeRO-DP는 모델의 기기당 메모리 사용량을 선형적으로 줄이면서도 통신 볼륨을 기본 DP 수준에 유지하며 효율성을 유지한다.&lt;/p>
&lt;h3 id="insights-and-overview-zero-r">Insights and Overview: ZeRO-R&lt;/h3>
&lt;h4 id="reducing-activation-memory-1">Reducing Activation Memory&lt;/h4>
&lt;p>a) MP는 모델 상태를 분할하지만 활성화 메모리의 복제가 종종 필요하다. 예를 들어, 선형 레이어의 parameter를 분할하여 병렬 계산을 할 경우, 각 GPU는 전체 활성화를 필요로 한다.&lt;/p>
&lt;p>b) GPT-2 또는 그보다 큰 모델들은 산술 강도가 매우 크며(≥ 10K), 이는 숨겨진 차원과 선형적으로 증가한다. 이로 인해 대역폭이 낮아도 활성화 체크포인트의 데이터 이동 비용을 숨길 수 있다.&lt;/p>
&lt;p>ZeRO는 GPU 간에 활성화 체크포인트를 분할하여 MP의 메모리 중복을 제거하고, 필요에 따라 그것들을 allgather를 이용해 재구성한다. 이로 인해 활성화 메모리 사용량은 MP 정도에 비례하여 감소하며, 매우 큰 모델에서는 산술 강도가 큰 덕분에 활성화 파티션을 CPU 메모리로 이동시키면서도 좋은 효율성을 유지할 수 있다.&lt;/p>
&lt;h4 id="managing-temporary-buﬀers">Managing Temporary buﬀers&lt;/h4>
&lt;p>ZeRO-R은 모델 크기가 증가함에 따라 임시 버퍼가 급증하는 것을 피하기 위해 일정한 크기의 버퍼를 사용하면서도 충분히 크게 만들어 효율성을 유지한다.&lt;/p>
&lt;h4 id="managing-fragmented-memory">Managing fragmented Memory&lt;/h4>
&lt;p>memory fragmentation는 단기와 장기 메모리 사이의 교차 때문에 발생한다. ZeRO는 이 통찰을 바탕으로, 활성화 체크포인트와 gradient를 미리 할당된 연속 메모리 버퍼로 이동시키는 실시간 메모리 defragmentation을 수행한다. 이로 인해 메모리 사용 가능성이 증가하고, 메모리 할당자가 연속적인 무료 메모리를 찾는 시간이 줄어들어 효율성이 향상된다.&lt;/p>
&lt;hr>
&lt;h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP&lt;/h2>
&lt;p>기존 DP 방식은 각 장치에서 모델 상태를 복제하여 메모리 오버헤드를 발생시키지만, ZeRO-DP는 이런 메모리 중복을 제거하기 위해 데이터 병렬 프로세스간에 optimizer state, gradient, parameter를 분할한다. 이들은 ZeRO-DP의 세 가지 최적화 단계인 $P_{os}$, $P_g$, $P_p$로 참조된다.&lt;/p>
&lt;h3 id="p_os-optimizer-state-partitioning">$P_{os}$: Optimizer State Partitioning&lt;/h3>
&lt;p>DP 정도가 $N_d$인 경우, 최적화 상태를 $N_d$개의 동일한 파티션으로 나누어, $i$번째 데이터 병렬 프로세스가 $i$번째 파티션에 해당하는 최적화 상태만 업데이트하도록 한다. 결과적으로, 각 데이터 병렬 프로세스는 전체 최적화 상태의 $N_d$만을 저장하고 업데이트하며 parameter의 $N_d$만 업데이트한다. 각 학습 단계의 끝에서는 모든 데이터 병렬 프로세스에서 완전히 업데이트된 parameter를 얻기 위해 all-gather를 수행한다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> 최적화 상태 파티션 후의 메모리 소비는 $4 \Psi + K \Psi$에서 $4 \Psi + {{K \Psi}\over{N_d}}$ 로 줄어든다. 예를 들어, 7.5B parameter 모델은 64-방향 DP를 사용할 때 31.4GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 또한, $N_d$가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 4배 감소한다.&lt;/p>
&lt;h3 id="p_g-gradient-partitioning">$P_g$: Gradient Partitioning&lt;/h3>
&lt;p>각 데이터 병렬 프로세스는 자신에 해당하는 parameter 파티션만 업데이트하므로, 해당 parameter에 대한 감소된 gradient만 필요하다. 역전파 동안 각 계층의 각 gradient가 사용 가능해지면, 해당 parameter를 업데이트하기 위한 데이터 병렬 프로세스에서만 그것들을 줄이고, gradient를 더 이상 필요하지 않으면 메모리를 해제한다. 이로 인해 gradient를 저장하는 데 필요한 메모리 사용량이 $2\Psi$ 바이트에서 $N_d$로 줄어든다.&lt;/p>
&lt;p>Reduce-Scatter 연산을 통해 다른 parameter에 대응하는 gradient들이 다른 프로세스로 줄어든다. 효율성을 높이기 위해 특정 파티션에 대응하는 모든 gradient를 버킷화하여 한 번에 처리하는 버킷화 전략을 사용한다. 이는 NVIDIA의 AMP 최적화기가 통신과 계산을 겹치게 하기 위해 gradient 계산을 버킷화하는 방식과 유사하다. 메모리 사용량을 줄이고 계산과 통신을 겹치게 하기 위해, 파티션 경계에서 all-reduce 대신 감소를 수행한다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> gradient와 optimizer state 중복을 제거하여 메모리 사용량을 $2 \Psi + {{14 \Psi}\over{N_d}} \approx 2 \Psi$로 줄일 수 있다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 P_{os+g}로만 16.6 GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. N_d가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 8배 감소한다.&lt;/p>
&lt;h3 id="p_p-parameter-partitioning">$P_p$: Parameter Partitioning&lt;/h3>
&lt;p>각 프로세스는 자신의 파티션에 해당하는 parameter만 저장하며, 필요한 경우 다른 parameter는 데이터 병렬 프로세스로부터 브로드캐스트를 통해 받아온다. 이 방법은 통신량을 1.5배로 증가시키지만, $N_d$에 비례해 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> parameter 분할을 통해, $Psi$ parameter 모델의 메모리 사용량은 $16 \Psi$에서 ${{16 \Psi}\over{N_d}}$로 줄어든다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 모델 상태 메모리 1.9 GB를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 이는 ZeRO가 모델 상태를 공유할 충분한 장치가 있다면 어떠한 크기의 모델에도 DP를 적용할 수 있다는 것을 의미한다.&lt;/p>
&lt;h3 id="implication-on-model-size">Implication on Model Size&lt;/h3>
&lt;p>데이터 병렬 프로세스의 모델 상태에 대한 메모리 사용량을 줄이기 위한 파티셔닝의 세 단계 $P_{os}$, $P_{os+g}$, $P_{os+g+p}$는 각각 메모리 사용량을 최대 4배, 8배, 그리고 $N_d$로 줄인다. ZeRO 최적화를 사용하면, $N_d = 64$일 때는 최대 128B parameter의 모델을, $N_d = 1024$일 때는 최대 1 trillion parameter의 모델을 학습시킬 수 있다. ZeRO를 사용하지 않으면, 최대 1.5B parameter의 모델만 학습시킬 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R&lt;/h2>
&lt;h3 id="p_a-partitioned-activation-checkpointing">$P_a$: Partitioned Activation Checkpointing&lt;/h3>
&lt;p>model parallel(MP)은 설계상 활성화의 복제를 요구하며, 이로 인해 GPU 전체에 활성화의 중복 복사본이 생긴다. ZeRO는 이 중복성을 제거하기 위해 활성화를 분할하고, 계산에 사용되기 직전에만 복제 형태로 재구성한다. 이를 $P_a$라고 하며, 활성화 체크포인팅과 함께 작동하여, 복제 복사본 대신 분할된 활성화 체크포인트를 저장한다. 매우 큰 모델과 제한된 디바이스 메모리의 경우, 이 분할된 체크포인트는 CPU로 오프로드될 수 있어, 활성화 메모리 오버헤드를 거의 없앨 수 있다. 이를 $P_{a+cpu}$라고 한다.&lt;/p>
&lt;p>&lt;strong>Memory Saving&lt;/strong> ZeRO는 파티셔닝된 활성화 체크포인팅을 사용하여 활성화 메모리 사용량을 model parallel(MP) 차수에 비례하여 줄인다. 예를 들어, 100B 개의 parameter를 가진 모델을 학습시키는 경우, 각 transformer layer마다 활성화를 체크포인트하면 GPU 당 약 33GB의 메모리가 필요하다. 하지만 ZeRO의 $P_a$를 사용하면, 이 메모리 요구량을 GPU 당 약 2GB로 줄일 수 있고, 이 2GB는 CPU로 오프로드되어 활성화에 대한 메모리 사용량을 거의 제로로 만든다.&lt;/p>
&lt;h3 id="c_b-constant-size-buﬀers">$C_B$: Constant Size Buﬀers&lt;/h3>
&lt;p>ZeRO는 메모리와 계산 효율성 사이의 균형을 위해 임시 데이터 버퍼의 크기를 조정한다. 학습 중에는 입력 크기가 커질수록 연산의 효율성이 향상되는 경우가 많다. 그러나, 모든 parameter수를 결합한 버퍼의 메모리 오버헤드는 모델 크기에 비례하기 때문에 문제가 될 수 있다. 이 문제를 해결하기 위해, 모델 크기가 큰 경우에는 성능 효율적인 고정 크기의 결합 버퍼를 사용한다. 이렇게 하면 버퍼 크기가 모델 크기에 의존하지 않게 되어, 메모리 사용량을 줄이면서도 계산 효율성을 유지할 수 있다.&lt;/p>
&lt;h3 id="m_d-memory-defragmentation">$M_D$: Memory Defragmentation&lt;/h3>
&lt;p>모델 학습 중에는 활성화 체크포인팅과 gradient 계산으로 인해 메모리 단편화가 발생한다. forward propagation 동안에는 선택적으로 저장되는 활성화와 대부분 버려지는 활성화 사이에 메모리가 교차되어 단편화가 발생하며, back propagation 동안에도 장기 메모리인 parameter gradient와 단기 메모리인 activation gradient 및 gradient 계산에 필요한 다른 버퍼들 사이에 메모리가 교차되어 단편화가 발생한다.&lt;/p>
&lt;p>메모리가 충분할 때는 메모리 단편화가 크게 문제가 되지 않지만, 메모리가 제한된 상태에서 큰 모델을 학습할 때는 두 가지 문제가 발생한다. 첫째, 충분한 메모리가 있음에도 연속적인 메모리 부족으로 인해 메모리 부족 오류(Out of Memory, OOM)가 발생하고, 둘째, 메모리 할당기가 연속적인 메모리 조각을 찾는데 많은 시간을 소비하여 효율성이 저하된다.&lt;/p>
&lt;p>ZeRO는 활성화 체크포인트와 gradient를 위해 미리 연속적인 메모리 덩어리를 할당하고, 이들이 생성될 때 미리 할당된 메모리로 복사하여 메모리 단편화를 실시간으로 제거한다. $M_D$(Memory Defragmentation)는 ZeRO가 더 큰 모델을 더 큰 배치 크기로 학습할 수 있게 만들 뿐만 아니라, 제한된 메모리로 학습할 때 효율성을 향상시킨다.&lt;/p>
&lt;hr>
&lt;h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP&lt;/h2>
&lt;p>ZeRO는 메모리 중복성을 제거하여 모델 크기를 증가시키는데, 이로 인해 메모리 효율성을 위해 통신 볼륨을 교환하고 있는지 의문이 생긴다. 즉, 기본 데이터 병렬화 방법에 비해 ZeRO가 향상된 데이터 병렬화 방법의 통신 볼륨은 어느 정도일까? 이에 대한 답은 두 부분이다. 첫째, ZeRO-DP는 메모리를 최대 8배 줄이면서 추가 통신을 발생시키지 않는다. 둘째, ZeRO-DP는 메모리 사용량을 추가로 줄이면서 최대 1.5배의 통신을 발생시킨다. 이 분석은 표준 데이터 병렬화의 통신 볼륨에 대한 간단한 개요로 시작한다.&lt;/p>
&lt;h3 id="data-parallel-communication-volume">Data Parallel Communication Volume&lt;/h3>
&lt;p>데이터 병렬 학습에서는 모든 데이터 병렬 프로세스의 gradient가 backward propagation이 끝날 때 평균화된다. 이 평균화는 all-reduce 통신을 통해 이루어지며, 모델 크기가 큰 경우, 이 통신은 통신 대역폭에 의해 제한된다. 따라서 분석은 각 데이터 병렬 프로세스로부터 보내고 받는 총 통신 볼륨에 초점을 맞춘다.&lt;/p>
&lt;p>state-of-art의 all-reduce 구현은 두 단계로 이루어진다. 첫번째 단계는 reduce-scatter 연산으로, 다른 프로세스에서 데이터의 다른 부분을 축소하고, 다음 단계는 all-gather 연산으로, 각 프로세스가 모든 프로세스에서 축소된 데이터를 수집한다. 이 두 단계의 결과는 all-reduce이다. 각 단계는 파이프라인 방식으로 구현되며, 이로 인해 데이터 이동이 발생한다. 따라서 표준 데이터 병렬화는 각 학습 단계마다 $2 \Psi$의 데이터 이동을 발생시킨다.&lt;/p>
&lt;h3 id="zero-dp-communication-volume">ZeRO-DP Communication Volume&lt;/h3>
&lt;h4 id="communication-volume-with-p_osg">Communication Volume with $P_{os+g}$&lt;/h4>
&lt;p>gradient 분할을 사용하는 ZeRO는 각 프로세스가 해당하는 parameter 분할을 업데이트하기 위해 필요한 gradient 부분만을 저장한다. gradient에 대해 scatter-reduce 연산을 수행하고, 모든 데이터 병렬 프로세스에서 업데이트된 parameter를 수집하기 위해 all-gather를 수행한다. 이 두 과정은 각각 통신 볼륨 $\Psi$를 발생시키므로, 학습 단계당 총 통신 볼륨은 $\Psi + \Psi = 2 \Psi$로, 기본 데이터 병렬화와 동일하다.&lt;/p>
&lt;h4 id="communication-volume-with-p_osgp">Communication Volume with $P_{os+g+p}$&lt;/h4>
&lt;p>parameter 분할 후에 각 데이터 병렬 프로세스는 자신이 업데이트하는 parameter만을 저장한다. 이로 인해 forward propagation 동안 다른 모든 분할의 parameter를 받아야 하지만, 파이프라인 방식을 통해 메모리 오버헤드를 피할 수 있다. 특정 분할에 대한 forward propagation를 계산하기 전에, 해당 분할의 가중치를 모든 데이터 병렬 프로세스에게 브로드캐스트하고, forward propagation가 완료되면 parameter를 버린다. 이로 인해 총 통신 볼륨은 $\Psi$이다. 그러나, 이 all-gather 작업은 backward propagation 동안 역순으로 다시 수행되어야 한다는 점에 주의해야 한다.&lt;/p>
&lt;p>총 통신 볼륨은 all-gather와 gradient의 reduce-scatter에 의해 발생하는 통신 볼륨의 합으로, 이는 기본값에 비해 1.5배인 $3 \Psi$이다. gradient와 parameter의 분할은 모든 상태가 항상 필요하지 않다는 점을 이용하여, 상태를 신중하게 통신함으로써 메모리를 최적화한다.&lt;/p>
&lt;hr>
&lt;h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R&lt;/h2>
&lt;p>ZeRO-R의 분할된 활성화 체크포인팅($P_a$)의 통신 볼륨은 기본 MP의 10분의 1 미만 증가하며, 이는 $P_a$의 통신 오버헤드를 분석하여 더 큰 배치 크기를 가능하게 하고 DP 통신을 줄여 효율성을 향상시키는 시나리오를 식별하는 데 사용된다. 이러한 분석은 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정하는 데 활용된다.&lt;/p>
&lt;p>활성화 체크포인트 분할의 통신 볼륨 트레이드오프는 모델 크기, 체크포인트 전략, 그리고 MP 전략에 따라 다르다. 이에 대한 구체적인 이해를 위해, 우리는 최신 MP 방식으로 구현된 transformer 기반 모델인 Megatron-LM을 사용하여 분석을 수행하였다.&lt;/p>
&lt;p>활성화 체크포인팅이 있는 Megatron-LM에서, 각 transformer 블록은 forward propagation, forward re-computation, backward propagation 각각에서 두 번씩 all-reduce 연산을 수행한다. 이는 $\text{batch} \times \text{seq length} \times \text{hidden dim}$ 차원의 크기를 가지다. 따라서 블록 당 총 통신 볼륨은 $12 \times \text{seq length} \times \text{hidden dim}$이 된다.&lt;/p>
&lt;p>ZeRO-R이 활성화 체크포인트를 분할할 때, back-propagation의 forward recomputation 전에 추가적인 all-gather 연산이 필요하다. 각 transformer 블록의 입력 활성화를 체크포인트로 설정하므로, transformer 블록 당 하나의 all-gather가 필요하다. 이로 인한 통신 오버헤드 $P_a$는 $\text{seq length} \times \text{hidden dim}$이고, 따라서 $P_a$의 총 통신 오버헤드는 모델 병렬화의 원래 통신 볼륨의 10% 미만이다.&lt;/p>
&lt;p>MP와 DP를 함께 사용할 때, $P_a$는 모델 병렬 통신 볼륨을 10% 증가시키는 대신 데이터 병렬 통신 볼륨을 크게 줄일 수 있다. 이는 데이터 병렬 통신이 성능의 병목이 될 때 효율성을 크게 향상시키는데 도움이 된다. 또한, $P_a$는 활성화 메모리 사용량을 줄이고 배치 크기를 비례적으로 증가시키므로, 큰 모델의 경우 배치 크기를 최대 16배까지 증가시킬 수 있다. 이로 인해, 데이터 병렬 통신 볼륨이 크게 감소할 수 있다.&lt;/p>
&lt;p>$P_{a+cpu}$가 적용되면, CPU로 오프로드된 분할된 활성화 체크포인트는 활성화 메모리 요구량을 거의 0으로 줄이면서, CPU 메모리로의 데이터 이동이 2배 증가한다. 배치 크기가 작아서 DP 통신 볼륨이 병목이 되는 경우에는, CPU 데이터 전송 오버헤드가 DP 통신 볼륨 오버헤드보다 작다면 $P_{a+cpu}$가 배치 크기를 늘려 효율성을 향상시킬 수 있다.&lt;/p>
&lt;p>모델과 하드웨어 특성을 고려하여, 위의 분석을 활용하여 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정한다.&lt;/p>
&lt;hr>
&lt;h2 id="step-towards-1-trillion-parameters">Step Towards 1 Trillion Parameters&lt;/h2>
&lt;p>현재 가장 큰 모델들은 이미 학습시키는 데 도전적인 100B 개의 parameter를 가지고 있다. 1 trillion 개의 parameter에 이르는 것은 불가피하지만, 그 과정은 많은 도전과 혁신을 필요로 할 것이다. ZeRO는 이러한 도전 중 하나인, 현재 하드웨어에서 대규모 모델을 효과적으로 학습시키는 능력을 개선하는 데 중점을 두고 있다.&lt;/p>
&lt;p>&lt;strong>A Leap from State-of-Art&lt;/strong> state-of-art 프레임워크인 Megatron은 DGX-2 시스템에서 160 - 20B 개의 parameter 모델을 효율적으로 학습시킬 수 있다. 하지만, 여러 DGX 노드 간의 모델 병렬화를 시도할 경우, 노드 간 대역폭 제한으로 효율성이 크게 감소한다.&lt;/p>
&lt;p>ZeRO는 효율적으로 실행 가능한 모델 크기를 크게 늘린다. 노드 경계를 넘어 세분화된 모델 병렬화가 필요하지 않은 현재 하드웨어에서 더 큰 모델을 실행할 수 있게 한다. 모든 최적화가 적용된 ZeRO는, DP만을 이용해 1024개의 GPU에서 1 trillion 개 이상의 parameter를 처리할 수 있다. 또한, 모델 병렬화와 결합하면, 16-방향 모델 병렬화와 노드 간 64-방향 데이터 병렬화를 이용하여 1 trillion 개 이상의 parameter를 처리할 수 있다.&lt;/p>
&lt;p>&lt;strong>Compute Power Gap&lt;/strong> 허용 가능한 시간 범위 내에서 1 trillion 개의 parameter 모델을 처음부터 끝까지 학습시키는 것은 여전히 상당한 양의 컴퓨팅 파워를 필요로 할 수 있으며, 이는 현재의 AI 클러스터에서는 부족하다.&lt;/p>
&lt;p>Bert-Large 모델은 1024 GPU DGX-2H 클러스터에서 67분 만에 학습될 수 있지만, 1 trillion 개의 parameter를 가진 모델은 데이터 샘플당 Bert-Large보다 3000배 더 많은 계산을 필요로 한다. 동일한 하드웨어와 계산 효율성을 가정하면, 이런 크기의 모델 학습은 140일이 걸리며, 실제로는 데이터 샘플과 시퀀스 길이 증가로 인해 1년 이상이 소요될 것이다. 이를 합리적인 시간에 학습시키려면 exa-ﬂop 시스템이 필요하며, 이런 계산 능력이 가능해질 때, ZeRO는 1T 모델을 효율적으로 실행하는 시스템 기술을 제공할 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="implementation-and-evaluation">Implementation and Evaluation&lt;/h2>
&lt;p>∼100B 개의 parameter를 가진 모델들의 효율적인 학습에 초점을 맞추고 있다. 이런 모델들은 현재 가장 큰 모델보다 크지만, 현재의 하드웨어에서 합리적인 시간 안에 학습될 수 있다. 이 목표를 달성하기 위해 ZeRO의 일부 최적화를 구현하고 평가하였다. 이를 ZeRO-100B라 부르며, 이를 통해 최대 170B 개의 parameter를 가진 모델을 효율적으로 학습할 수 있음을 확인하였다. 이는 기존 state-of-art의 기술보다 8배 크고, 최대 10배 빠르며, 사용성이 향상되었다. ZeRO-100B는 세계에서 가장 큰 모델인 Turing-NLG를 지원한다.&lt;/p>
&lt;h3 id="implementation-and-methodology">Implementation and Methodology&lt;/h3>
&lt;p>&lt;strong>Implementation&lt;/strong> PyTorch에서 ZeRO-100B를 구현하였고, 이는 모든 최적화 세트를 포함하며, 어떤 모델과도 호환되는 인터페이스를 제공한다. 사용자는 이 인터페이스를 이용해신의 모델을 감싸서 ZeRO의 DP를 활용할 수 있고, 모델 수정은 필요하지 않다. 또한, ZeRO의 DP는 Megatron-LM을 포함한 어떤 형태의 MP와도 결합할 수 있다.&lt;/p>
&lt;p>&lt;strong>Hardware&lt;/strong> 800 Gbps의 노드 간 통신 대역폭을 가진 400개의 V100 GPU (25개의 DGX-2 노드) 클러스터에서 실험을 수행하였다.&lt;/p>
&lt;p>&lt;strong>Baseline&lt;/strong> MP 없는 실험에는 torch의 distributed data parallel(DDP)을, MP가 있는 실험에는 최첨단 기술인 Megatron-LM을 사용하였다. 이는 NVIDIA의 오픈소스 버전으로, 최근 결과는 32개의 DGX-2 노드 (총 512개의 32GB V100 GPU)를 사용하여 160B 개의 parameter 모델까지 확장 가능함을 보여준다.&lt;/p>
&lt;p>&lt;strong>ZeRO&lt;/strong> MP가 없는 실험에서는 ZeRO-100B의 ZeRO-powered DP 구현을 사용한다. MP가 있는 실험에서는 ZeRO-powered DP를 Megatron-LM의 MP와 결합한다.&lt;/p>
&lt;p>&lt;strong>Model Conﬁgurations&lt;/strong> 모델들은 GPT-2와 같은 transformer 기반 모델이며, parameter 수를 다르게 하기 위해 은닉 차원과 층의 수를 조절하였다.&lt;/p>
&lt;h3 id="speed-and-model-size">Speed and Model Size&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure2.png"
width="1168"
height="492"
srcset="https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>ZeRO-100B는 400개의 GPU에서 최대 170B 개의 parameter를 가진 모델을 효율적으로 실행하며, 이는 Megatron-LM보다 8배 이상 크다. ZeRO-100B는 8B에서 100B 개의 parameter를 가진 모델에 대해 평균적으로 15 PetaFlops의 처리량을 달성하였다. 반면, 기본 MP 성능은 모델 크기 증가에 따라 빠르게 저하되지만, ZeRO-100B는 기준선에 비해 최대 10배의 속도 향상을 보여준다.&lt;/p>
&lt;p>ZeRO-100B의 경우, 100B을 넘어서는 성능의 약간의 감소는 더 큰 배치 크기를 실행하기 위한 충분한 메모리 부족 때문이다. GPU의 수를 늘림에 따라 ZeRO-100B의 초선형 속도 향상으로 인해 성능이 향상될 것으로 예상한다.&lt;/p>
&lt;h3 id="super-linear-scalability">Super-Linear Scalability&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure3.png"
width="1168"
height="570"
srcset="https://kurtkim.github.io/p/zero/images/figure3_hu8a523eb8ef8a676821dd1d6a1b7d3c23_138981_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure3_hu8a523eb8ef8a676821dd1d6a1b7d3c23_138981_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;p>ZeRO-100B는 매우 큰 모델 크기에 대해 초선형 확장성을 보여주며, 64개에서 400개의 GPU로 확장될 때 이 트렌드가 계속될 것으로 예상한다. $P_{os+g}$는 DP 정도의 증가에 따라 ZeRO-100B의 GPU당 메모리 사용량을 줄여, 처리량을 향상시킨다.&lt;/p>
&lt;h3 id="democratizing-large-model-training">Democratizing Large Model Training&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure4.png"
width="670"
height="428"
srcset="https://kurtkim.github.io/p/zero/images/figure4_hu5287cd73d3458f542ceb3592bddddacd_68377_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure4_hu5287cd73d3458f542ceb3592bddddacd_68377_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="375px"
>&lt;/p>
&lt;p>많은 데이터 과학자들에게 큰 모델 학습의 장벽인 MP와 PP 사용 없이, ZeRO는 모델에 변경 없이 간단한 DP처럼 사용하면서 모델 크기와 속도를 크게 향상시킨다. ZeRO-100B는 128개의 GPU에서 MP 없이 최대 13B 개의 parameter를 가진 모델을 학습시킬 수 있으며, 이는 평균적으로 GPU 당 40 TFlops 이상의 처리량을 달성한다. 반면, ZeRO 없이는 DP만으로 학습 가능한 가장 큰 모델은 1.4B 개의 parameter를 가지며, 처리량은 20 TFlops 미만이다. 게다가, MP의 통신 오버헤드 없이 이런 모델들은 NVLINK이나 NVSwitch가 필요하지 않은 하위 계산 노드에서도 학습될 수 있다.&lt;/p>
&lt;h3 id="memory-and-performance-analysis">Memory and Performance Analysis&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/table3.png"
width="486"
height="250"
srcset="https://kurtkim.github.io/p/zero/images/table3_hu57e31f561d7316fd55d664a227f49bb0_32347_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/table3_hu57e31f561d7316fd55d664a227f49bb0_32347_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;p>최대 모델 크기, 메모리 사용량, 성능에 대한 다양한 최적화의 이점과 영향을 살펴본다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure6.png"
width="384"
height="292"
srcset="https://kurtkim.github.io/p/zero/images/figure6_hu2e7b580be4845363e0b99920bf816bcc_27370_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure6_hu2e7b580be4845363e0b99920bf816bcc_27370_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="315px"
>&lt;/p>
&lt;p>&lt;strong>Maximum Model Size&lt;/strong> C1 대비 C2로 학습시 모델 크기는 40B에서 60B으로 증가하며, 이는 활성화 메모리를 16배 줄이는 $P_a$ 사용 때문이다. C4를 사용하여 140B로 늘리는 것은 $P_{os+g}$를 활성화함으로써 모델 상태의 메모리 요구량을 절반으로 줄이기 때문이고, C5를 사용하여 150B로 증가하는 것은 활성화 체크포인트를 CPU 메모리로 오프로딩하여 메모리를 더욱 줄이기 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure7.png"
width="764"
height="304"
srcset="https://kurtkim.github.io/p/zero/images/figure7_hu42bbbfa2e475a4dffe486054d725e5b7_62602_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure7_hu42bbbfa2e475a4dffe486054d725e5b7_62602_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="251"
data-flex-basis="603px"
>&lt;/p>
&lt;p>&lt;strong>Max Cached Memory&lt;/strong> C1에서 C2로 넘어갈 때 캐시 메모리 크기의 감소는 예상된 결과이다. C2와 C3 사이의 메모리 사용량 차이는 활성화 메모리와 모델 상태의 크기에 따라 달라질 수 있다. 특히, 100B 개 모델에서는 활성화 메모리가 훨씬 크므로 C4에서 C5로 넘어갈 때 캐시 메모리 감소가 눈에 띈다. 이러한 특성으로 인해 $P
&lt;em>{a+cpu}$는 매우 큰 모델에서 더 큰 배치 크기를 적용하는 데 중요한 도구가 된다. 또한, 170B 개 모델이 메모리 부족 없이 실행되기 위해 $P&lt;/em>{a+cpu}$가 필요하다는 것을 보여준다.&lt;/p>
&lt;p>&lt;strong>Max Achievable Performance&lt;/strong> 메모리 사용량 감소가 성능 향상과 연결되어 있으며, 메모리 사용량이 줄어들면 배치 크기를 늘려 성능을 향상시킬 수 있다. 그러나 60B 개 parameter 모델에서 C4와 C5 사이에서는 성능이 떨어진다. 이는 C5가 CPU와의 데이터 이동을 초래하여 성능을 저하시키기 때문이다. 하지만 모델이 너무 크거나, C5 없이는 작동이 불가능한 경우등 예외적인 상황에서는 C5가 필요하다. 학습 중에는 이러한 이점이 있는 경우에만 $P_{a+cpu}$가 활성화된다.&lt;/p>
&lt;h3 id="turing-nlg-the-sota-language-model-with-17b-parameters">Turing-NLG, the SOTA language model with 17B parameters&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure5.png"
width="494"
height="396"
srcset="https://kurtkim.github.io/p/zero/images/figure5_hua411aca5492099be87a7d26ea599e861_67401_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure5_hua411aca5492099be87a7d26ea599e861_67401_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>2020년 5월 12일 기준으로, Turing-NLG는 17B 개의 parameter를 가진 세계 최대의 모델로, Webtext-103의 perplexity 10.21로 언어 모델의 state-of-art를 달성하였다. TuringNLG는 ZeRO-100B를 사용하여 학습되었고, 이 모델은 GPU당 41.4 TFlops의 처리량을 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="concluding-remarks">Concluding Remarks&lt;/h2>
&lt;p>고성능 컴퓨팅과 시스템 관점에서 보면, ZeRO는 대형 모델 학습 분야에서 혁명적 변화를 일으킬 것으로 보인다. ZeRO-100B 구현은 모델 크기를 8배, 처리량은 10배 이상 향상시키며, 현대 GPU 클러스터에서 초선형 속도 향상을 달성하고 세계에서 가장 큰 모델을 학습시킬 수 있다. 하지만 이는 ZeRO의 전체 잠재력을 보여주는 것이 아니다. ZeRO는 미래의 trillion parameter 모델 학습을 가능하게 하는 더 큰 모델 크기 증가를 제공할 수 있다.&lt;/p>
&lt;p>ZeRO에 대한 가장 큰 낙관적인 점은 데이터 과학자에게 어떠한 장애물도 없다는 것이다. 기존의 MP와 PP 접근법과 달리, 모델 리팩토링이 필요 없고 표준 DP만큼 쉽게 사용할 수 있어, 대규모 모델 학습에 대한 미래의 연구에서 중요한 역할을 할 것으로 보인다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1910.02054.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Megatron-LM</title><link>https://kurtkim.github.io/p/megatron-lm/</link><pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/megatron-lm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델링에서 큰 transformer 모델의 학습은 자연어 처리 분야에서 state-of-the-art를 달성하였다. 하지만, 이런 큰 모델은 메모리 제약으로 학습이 어려울 수 있다. 이 연구에서는 수십억 개의 파라미터를 가진 transformer 모델을 학습시키는 방법을 제시한다.&lt;/p>
&lt;p>이 연구는 512개의 GPU를 사용하여 최대 83억 개의 파라미터를 가진 transformer 모델을 학습시키는 것에 성공하였다. 또한 이 모델을 사용하여 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다. BERT와 같은 모델에서는 layer normalization의 위치에 주의가 필요함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Natural Language Processing (NLP)는 컴퓨팅능력과 데이터셋 크기의 증가로 빠르게 발전하고 있다. 이로 인해 더 큰 언어 모델을 학습시키는 것이 가능해졌고, 이는 기사 완성, 질문 답변, 자연어 추론 등의 NLP 작업에 매우 유용하다. 이런 사전 학습된 언어 모델을 다른 자연어 작업에 미세 조정하면, state-of-the-art를 얻을 수 있다.&lt;/p>
&lt;p>모델이 커짐에 따라, 메모리 한계를 초과하여 추가적인 메모리 관리 기법이 필요해진다. ADAM 같은 optimization 알고리즘들은 모멘텀과 다른 최적화 상태를 저장하기 위해 추가 메모리를 요구하며, 이는 효과적으로 학습될 수 있는 모델의 크기를 줄인다. 이를 해결하기 위해, 모델 병렬화 접근법이 사용되며, 이는 가중치와 그들과 관련된 최적화 상태가 동시에 프로세서에 존재할 필요가 없도록 모델을 분할한다. 그러나, 이러한 접근법은 모델을 다시 작성하고, 아직 개발 중인 사용자 정의 컴파일러와 프레임워크에 의존하는 문제가 있다.&lt;/p>
&lt;p>이 연구에서는 내부 layer 모델 병렬화를 이용한 단순하고 효율적인 모델 병렬 접근법을 구현하였다. transformer 기반 언어 모델의 내재적 구조를 활용해 PyTorch에서 효율적으로 학습하는 모델 병렬 구현을 만들었으며, 이는 사용자 정의 C++ 코드나 컴파일러를 필요로 하지 않는다. 이 접근법은 GPipe와 같은 파이프라인 기반 모델 병렬화와는 별개이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure1.png"
width="618"
height="380"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure1_hu17fd794e82cbb149b38b0b5f9212968f_49270_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure1_hu17fd794e82cbb149b38b0b5f9212968f_49270_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>이 접근법의 확장성을 보여주기 위해, 단일 NVIDIA V100 32GB GPU에서 1.2B parameter의 모델을 학습하여 39 TeraFLOPs를 유지하는 강력한 기준선을 설정하였다. 모델을 8.3B parameter로 확장하고 512개의 GPU에서 8-way 모델 병렬화를 사용하면, 전체 애플리케이션에서 초당 최대 15.1 PetaFLOPs를 달성하며, 이는 단일 GPU 사례에 비해 76%의 확장 효율성을 보여준다.&lt;/p>
&lt;p>모델 크기가 정확도에 미치는 영향을 분석하기 위해, GPT-2와 BERT를 학습시키고 여러 downstream task에서 평가하였다. 기존의 BERT 구조는 모델 크기가 증가함에 따라 성능이 저하되는 것을 확인하였다. 이를 극복하기 위해, transformer layer의 layer normalization과 residual connection을 rearranging하였고, 이로 인해 모델 크기가 증가함에 따라 downstream task 결과가 단조롭게 향상되는 것을 확인하였다. 또한, 모델이 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>요약하면, 이 논문의 기여는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>기존의 PyTorch transformer 구현에 몇 가지 목표적인 수정만을 가함으로써, 단순하고 효율적인 모델 병렬 접근법을 구현했다.&lt;/li>
&lt;li>모델과 데이터 병렬 기법에 대한 심층적인 경험적 분석을 수행하고, 512개의 GPU를 사용하여 최대 76%의 확장 효율성을 보여준다.&lt;/li>
&lt;li>BERT와 유사한 모델에서 layer normalization의 위치에 신중하게 주의를 기울이는 것이 모델이 커짐에 따라 정확도를 높이는 데 중요하다는 것을 보여준다.&lt;/li>
&lt;li>모델 크기를 확장하는 것이 GPT-2(최대 8.3B parameter까지 연구)와 BERT(최대 3.9B parameter까지 연구) 모델 모두에 대해 정확도를 향상시키는 것을 보여준다.&lt;/li>
&lt;li>테스트 세트에서 state-of-the-art를 달성하는 것을 보여준ㄴ다: WikiText103에서의 혼란도(10.8 ppl), LAMBADA에서의 정확도(66.5%), 그리고 RACE에서의 정확도(90.9%).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="background-and-challenges">Background and Challenges&lt;/h2>
&lt;h3 id="neural-language-model-pretraining">Neural Language Model Pretraining&lt;/h3>
&lt;p>사전 학습된 언어 모델은 NLP 연구에 필수적인 도구가 되었다. 큰 규모의 말뭉치 사전 학습을 활용하여 언어의 견고한 신경 표현을 배우는 것은 활발한 연구 분야이다. 초기의 연구는 사전 학습된 단어 임베딩이 downstream task 결과를 향상시키는 것을 보여주었으며, 이후의 연구는 맥락적 단어 표현을 포착하는 신경 모델을 학습하고 전이하는 것을 통해 발전하였다. 최근의 연구는 언어 모델을 end-to-end로 미세 조정함으로써 이 아이디어들을 더욱 발전시켰다. 이런 방법들의 진보는 규모에 맞게 효율적으로 작동하고 늘어나는 계산 요구를 충족시킬 수 있는 도구의 필요성을 촉발하였고, 이 연구는 트렌드에서 한 걸음 더 나아가기 위한 도구를 제공하려고 한다.&lt;/p>
&lt;h3 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention&lt;/h3>
&lt;p>현재 NLP 연구는 우수한 정확도와 계산 효율성 때문에 transformer 모델을 사용하는 경향이 있다. transformer는 원래 두 부분, encoder와 decoder를 사용하는 기계 번역 아키텍처로 설계되었지만, 최근의 연구는 필요에 따라 encoder나 decoder만 사용한다. 이 연구는 decoder 구조인 GPT-2와 encoder 구조인 BERT를 모두 연구한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure2.png"
width="216"
height="480"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure2_hu6b6b35e095abc54f1afae484c0c3a903_60857_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure2_hu6b6b35e095abc54f1afae484c0c3a903_60857_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="45"
data-flex-basis="108px"
>&lt;/p>
&lt;p>GPT-2와 BERT는 모두 GeLU nonlinearities와 layer normalization을 multi-head attention과 eed forward layer의 입력에 적용하는 반면, 원래의 transformer는 ReLU nonlinearities를 사용하고 layer normalization를 출력에 적용한다.&lt;/p>
&lt;h3 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning&lt;/h3>
&lt;p>신경망 학습을 여러 하드웨어 가속기로 확장하는 두 가지 주요 방법은 데이터 병렬화와 모델 병렬화이다. 데이터 병렬화는 학습 미니배치를 여러 작업자에게 분할하고, 모델 병렬화는 모델의 메모리 사용량과 계산을 여러 작업자에게 분배한다. 사용 가능한 작업자 수에 비례하여 미니배치 크기를 증가시키면 학습 데이터 처리량에서 거의 선형적인 확장을 볼 수 있다. 하지만 대량 배치 학습은 최적화 과정에 복잡성을 도입하여 정확도를 감소시키거나 수렴 시간을 늘릴 수 있다. 추가로, 데이터 병렬화를 활성화 체크포인팅과 결합하여 메모리 요구사항을 줄이는 방법도 연구되고 있다.&lt;/p>
&lt;p>기존 기법들은 모델이 한 작업자에게 완전히 맞아야 하는 제한이 있다. 크기와 복잡성이 증가하는 언어 모델로 인해, 신경망은 하드웨어 가속기의 메모리 용량에 근접하게 되었다. 이 문제를 해결하기 위한 한 가지 방법은 parameter 공유를 사용하는 것이지만, 이는 모델의 전체 용량을 제한한다. 이 연구의 접근법은 모델 병렬화를 사용하여 모델을 여러 가속기에 분할하는 것으로, 이는 메모리 압박을 완화하고 병렬성을 증가시킨다.&lt;/p>
&lt;p>모델 병렬화에는 layer-wise pipeline parallelism과 distributed tensor computation이라는 두 가지 패러다임이 있다. pipeline model parallelism에서는 한 장치에서 작업 그룹이 수행된 후 출력이 다음 장치로 전달된다. 일부 접근법은 parameter 서버를 사용하지만 일관성 문제가 있다. TensorFlow의 GPipe 프레임워크는 동기식 경사 하강법을 사용하여 이 문제를 해결한다. 그러나 이 방법은 통신과 계산 작업의 효율적인 파이프라이닝을 위한 추가 로직이 필요하며, pipeline bubble이나 최적화 변경으로 인해 효율성과 정확도에 영향을 미친다.&lt;/p>
&lt;p>distributed tensor computation은 텐서 연산을 여러 장치에 분할하여 계산을 가속화하거나 모델 크기를 증가시키는 방법이다. FlexFlow는 이러한 병렬 계산을 효과적으로 수행하는 방법을 제공한다. 최근에는 Mesh-TensorFlow가 TensorFlow에서 분산 텐서 계산을 지정하는 언어를 도입했다. 우리는 이러한 통찰력을 활용하여 transformer의 attention head를 계산하는 병렬성을 활용하여 transformer 모델을 병렬화한다. 하지만, 이 연구는 프레임워크와 컴파일러를 구현하는 대신, 기존의 PyTorch transformer 구현에 몇 가지 특정 수정을 수행한다. 이 방법은 간단하며, 새로운 컴파일러나 코드 재작성이 필요하지 않는다.&lt;/p>
&lt;hr>
&lt;h2 id="model-parallel-transformers">Model Parallel Transformers&lt;/h2>
&lt;p>transformer network의 구조를 활용해, 몇 가지 synchronization primitive를 추가하여 간단한 모델 병렬 구현을 만들었다. transformer layer는 self attention block과 two-layer, multi-layer perceptron (MLP)으로 구성되며, 이 두 부분에 모델 병렬성을 도입하였다.&lt;/p>
&lt;p>MLP block의 첫 번째 부분은 GEMM이며, 이어서 GeLU 비선형성을 따른다:&lt;/p>
&lt;p>$$ Y = GeLU(XA) $$&lt;/p>
&lt;p>GEMM을 병렬화하는 한 가지 방법은 가중치 행렬 $A$를 행 방향으로, 입력 $X$를 열 방향으로 분할하는 것이다:&lt;/p>
&lt;p>$$ X = [X_1, X_2], A = \begin{bmatrix} A_1 \\ A_2 \end{bmatrix} $$&lt;/p>
&lt;p>이 분할 방식은 결과로 $Y = GeLU(X_1 A_1 + X_2 A_2)$를 가져오며, GeLU는 비선형 함수이므로, $GeLU(X_1 A_1 + X_2 A_2) \neq GeLU(X_1 A_1) + GeLU(X_2 A_2)$이다. 따라서 이 방식은 GeLU 함수 앞에 동기화 지점이 필요하게 된다. 각 처리 유닛이 독립적으로 계산한 결과를 제대로 합산하기 위해 병렬 처리 유닛 간에 데이터 동기화가 필요하다.&lt;/p>
&lt;p>다른 옵션은 $A$를 열을 따라 분할하는 것이다. $A = [A_1, A_2]$. 이 분할 방식은 GeLU 비선형성을 각 분할된 GEMM의 출력에 독립적으로 적용할 수 있게 한다:&lt;/p>
&lt;p>$$ [Y_1, Y_2] = [GeLU(XA_1), GeLU(XA_2)] $$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure3.png"
width="574"
height="638"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure3_hub8d70cf42bc24407b9b157eb8cc73fa4_303715_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure3_hub8d70cf42bc24407b9b157eb8cc73fa4_303715_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="89"
data-flex-basis="215px"
>&lt;/p>
&lt;p>이 방법은 동기화 지점을 제거하므로 유리하다. 첫 번째 GEMM을 열 병렬 방식으로 분할하고, 두 번째 GEMM을 행으로 분할하여 GeLU 계층의 출력을 직접 받을 수 있도록 한다. 이 방식은 추가적인 통신 없이 MLP 블록의 두 GEMM을 GPU 간에 분할하며, forward path와 backward path에 각각 단 한 번의 all-reduce 연산만 필요로 한다. 이 두 연산은 서로 conjugate 관계에 있으며, PyTorch에서 간단하게 구현할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure4.png"
width="604"
height="300"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure4_hu8ce0fce626bd70352fa876adff3eded7_92476_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure4_hu8ce0fce626bd70352fa876adff3eded7_92476_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>self attention block에서는 multihead attention 연산의 병렬성을 활용하여 key(K), query(Q), value(V)와 관련된 GEMM을 열 병렬 방식으로 분할한다. 이 방식은 각 attention head에 해당하는 행렬 곱셈을 각각의 GPU에서 수행하게 하며, immediate communication이 필요 없다. 이어서, 출력 linear layer에서의 GEMM은 행 병렬 방식으로 수행되며, GPU 간의 통신 없이 병렬 주의 계층의 출력을 직접 받아들인다. 이 접근법은 MLP와 self attention layer에서 두 GEMM의 그룹을 융합하고, 중간의 동기화 지점을 제거하여 더 나은 확장성을 제공한다. 이를 통해 forward path와 backward path에서 각각 두 번의 all-reduce 연산만으로 모든 GEMM을 수행할 수 있다.&lt;/p>
&lt;p>transformer 언어 모델은 출력 임베딩을 병렬화하여 처리 속도를 향상시킨다. 이 모델은 입력 임베딩과 가중치를 공유하는 출력 임베딩 계층을 가지며, 이 가중치 행렬을 분할하여 병렬 처리한다. 그러나 이 방식은 큰 어휘 크기 때문에 많은 양의 정보를 전송해야 한다. 이를 해결하기 위해, 병렬 GEMM의 출력을 cross entropy loss와 결합하여 차원을 줄인다. 이렇게 함으로써 스칼라 손실만 전송하게 되어 통신의 양이 크게 감소하고, 모델의 병렬 처리 효율성이 향상된다.&lt;/p>
&lt;p>모델 병렬 방법론은 통신을 줄이고 GPU 계산에 초점을 맞추는 기법에 중점을 두고 있다. dropout, layer normalization, residual connection의 계산을 한 GPU에서만 수행하는 대신, 이를 모든 GPU에 복제한다. 각 GPU는 layer normalization parameter의 복제본을 유지하며, 모델 병렬 영역의 출력에서 dropout과 residual connection을 수행한다. 또한, 각 모델 병렬 작업자는 자신의 parameter 집합을 독립적으로 최적화한다. 이러한 접근법은 모든 값이 각 GPU에 로컬로 있거나 복제되므로, 업데이트된 parameter 값을 통신할 필요가 없다.&lt;/p>
&lt;p>이 연구의 방법론은 하이브리드 모델과 데이터 병렬성, 그리고 난수 생성 처리와 관련이 있다. 이는 구현이 간단하며, forward와 backward pass에 몇 가지 추가적인 all-reduce 연산만 필요로 한다. 컴파일러는 필요 없으며, 이는 기존의 파이프라인 모델 병렬화 방법과는 별개이며 이를 보완한다.&lt;/p>
&lt;hr>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>자연어 처리와 언어 이해의 핵심은 사전 학습된 언어 이해 모델이다. 이 연구에서는 왼쪽에서 오른쪽으로 텍스트를 생성하는 GPT-2와 언어 모델 마스킹에 기반한 bi-directional transformer 모델인 BERT에 초점을 맞추고 있다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>다양한 대형 언어 모델링 데이터셋을 합쳐서 longterm dependency를 가진 학습 세트를 만들었다. 이에는 Wikipedia, CC-Stories, RealNews, OpenWebtext 등이 포함되어있다. 학습 세트의 유출을 방지하기 위해 일부 Wikipedia 기사와 필요 없는 새 줄을 제거하였다. BERT 모델에는 BooksCorpus를 포함시켰지만, LAMBADA 작업과 겹치는 부분 때문에 GPT-2 학습에서는 제외하였다.&lt;/p>
&lt;p>모든 데이터셋을 병합하고, 내용 길이가 128 토큰 미만인 문서를 제외하였다. 유사한 내용의 중복을 제거하기 위해 localitysensitive hashing (LSH)을 사용했고, 그 결과 174GB의 중복 제거된 텍스트를 포함한 말뭉치를 얻었다.&lt;/p>
&lt;h3 id="training-optimization-and-hyperparameters">Training Optimization and Hyperparameters&lt;/h3>
&lt;p>효율적인 학습을 위해 mixed precision 학습과 dynamic loss scaling을 사용하였다. 가중치는 정규 분포로 초기화하고, residual layer 전에 조정했다. optimizer는 Adam을 사용하고, weight decay를 적용했다. gradient norm clipping을 사용해 학습의 안정성을 개선했고, 모든 경우에 dropout 0.1을 적용했다. 마지막으로, 메모리 관리를 위해 utilize activation checkpointing을 사용했다.&lt;/p>
&lt;p>GPT-2 모델은 1024개의 subword 단위로 300k번 반복하며 학습되며, batch size는 512이다. learning rate는 1.5e-4로 설정되어 있고, 3k번의 warmup 이후에 cosine decay를 따른다. 이 감소는 최소 학습률인 1e-5에서 멈춘다.&lt;/p>
&lt;p>BERT 모델은 원래의 BERT 사전을 사용하고, 어휘 크기는 30,522이다. next sentence prediction을 sentence order prediction으로 대체하고, 전체 단어 n-그램 마스킹을 사용하였다. batch size는 1024로 설정하고, warmup된 learning rate을 사용하여 2백만 번의 반복 동안 선형적으로 감소시켰다. 나머지 학습 parameter는 기존 BERT 모델과 동일하게 유지하였다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>모든 실험은 최대 32대의 DGX-2H 서버(총 512개의 Tesla V100 GPU)를 사용한다. 이 인프라는 서버 내부 GPU 간 300 GB/sec, 서버 간 100 GB/sec의 빠른 연결 대역폭을 제공하여 딥러닝 애플리케이션에 최적화되어 있다.&lt;/p>
&lt;h3 id="scaling-analysis">Scaling Analysis&lt;/h3>
&lt;p>구현의 확장성을 테스트하기 위해, 다양한 parameter를 가진 GPT-2 모델을 사용했다. self attention layer에서 일관된 GEMM 크기를 유지하기 위해, attention head 당 hidden size는 96으로 고정하였다. 원래의 어휘 크기는 50,257이었지만, logit layer의 효율적인 GEMM을 위해 어휘를 패딩하여 51,200으로 만들었다.&lt;/p>
&lt;p>모델 및 모델 + 데이터 병렬 확장성을 연구했으며, 모든 구성에서 배치 크기는 8로 고정하였다. 또한 모든 실험에서 전역 배치 크기를 512로 고정하여 데이터 병렬 확장성을 연구하였다. 이는 64-way 데이터 병렬성에 해당한다.&lt;/p>
&lt;h4 id="model-and-data-parallelism">Model And Data Parallelism&lt;/h4>
&lt;p>모델 병렬 및 모델 + 데이터 병렬 케이스에 대해 모델 parameter에 대한 약한 스케일링을 보여준다. 약한 스케일링은 배치 크기를 조정하여 수행되지만, 이는 단일 GPU에 맞지 않는 대형 모델을 학습하는 문제를 해결하지 못한다. 따라서 여기서는 그렇지 않으면 불가능했던 더 큰 모델을 학습하기 위해 약한 스케일링을 사용한다. 모든 스케일링 수치의 기준은 단일 GPU에서 실행되는 1.2억 개의 parameter를 가진 첫 번째 구성이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure5.png"
width="628"
height="522"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure5_huc48ecde88387473c014049f1e6065376_69432_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure5_huc48ecde88387473c014049f1e6065376_69432_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="288px"
>&lt;/p>
&lt;p>8.3B 개의 parameter와 8-way 모델 병렬성을 가진 경우 선형 스케일링의 77%를 달성하였다. 가장 큰 구성(8.3B 개의 parameter)이 512개의 GPU에서 실행되는 경우에도 선형 스케일링 대비 74%의 스케일링을 달성하였다.&lt;/p>
&lt;h3 id="language-modeling-results-using-gpt-2">Language Modeling Results Using GPT-2&lt;/h3>
&lt;p>거대 언어 모델이 최첨단을 더욱 발전시킬 수 있음을 보여주기 위해, 다양한 크기와 구성의 GPT-2 모델을 학습하였습니다. 355M 모델은 BERT-Large 모델과 동일하며, 2.5B 모델은 이전의 가장 큰 GPT-2 모델보다 크고, 8.3B 모델은 우리가 알고 있는 한까지 학습된 어떤 변환기 언어 모델보다 크다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table2.png"
width="626"
height="208"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table2_hu82a02da3ef2fcbaae18b436854407213_38834_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table2_hu82a02da3ef2fcbaae18b436854407213_38834_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="300"
data-flex-basis="722px"
>&lt;/p>
&lt;p>각 epoch를 진행하는데 걸리는 시간은 68,507회의 반복과 동일하며, 예를 들어, 512개의 GPU에서의 8.3B 모델에 대해서는 각 epoch이 약 두 일 정도 걸린다. 이들 모델은 이전에 본 것보다 훨씬 작지만 여전히 64개의 GPU로 학습하며, epoch 당 시간이 훨씬 적다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure6.png"
width="644"
height="362"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure6_hu2b49527af3ddf3ed9e4ea90e6c86a890_50393_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure6_hu2b49527af3ddf3ed9e4ea90e6c86a890_50393_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>모델 크기가 증가함에 따라 검증 perpelixity가 감소하는 것을 확인할 수 있다. 특히, 8.3억 개의 parameter를 가진 모델은 검증 perpelixity가 9.27에 이르렀다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table3.png"
width="538"
height="212"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table3_hu830d348617586a84f0102f02e5c97c24_34397_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table3_hu830d348617586a84f0102f02e5c97c24_34397_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="609px"
>&lt;/p>
&lt;p>또한, 모델 크기를 증가시키면 WikiText103에서의 perpelixity가 낮아지고, LAMBADA에서의 클로즈 정확도가 높아지는 추세를 관찰한다. 이 중 8.3억 개의 parameter를 가진 모델은 WikiText103 테스트 세트에서 state-of-the-art perpelixity를 달성하고, LAMBADA 작업에서 이전의 클로즈 정확도 결과를 초과하였다.&lt;/p>
&lt;p>최근에는 Microsoft와 NVIDIA가 협력하여 170억 개의 parameter를 가진 GPT-2 모델인 Turing-NLG를 학습시켰으며, 이 결과는 더 큰 모델의 가치를 강조하였다.&lt;/p>
&lt;p>테스트 데이터가 학습 데이터에 포함되지 않도록 확인하기 위해, 테스트 세트의 8-gram 중 학습 세트에도 나타나는 비율을 계산하였다. WikiText103 테스트 세트는 최대 10.8%의 겹침이 있고, LAMBADA 테스트 세트는 최대 1.4%의 겹침이 있었다. 이는 이전 연구와 일관되어, 테스트 데이터가 우연히 학습 데이터에 포함되지 않았음을 확신하였다.&lt;/p>
&lt;h3 id="bi-directional-transformer-results-using-bert">Bi-directional Transformer Results Using BERT&lt;/h3>
&lt;p>BERT 스타일의 transformer 모델에 방법론을 적용하고, 다양한 downstream task에 대한 모델 스케일링 효과를 연구한다. 이전 연구에서는 BERT-large의 336M parameter를 넘어서 모델 크기를 증가시키면 모델 저하가 발생한다는 것을 발견하였다. 이 문제를 해결하기 위해, 연구자들은 parameter 공유를 도입하고, 이를 통해 모델이 원래 BERT 모델에 비해 더 잘 확장되는 것을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure7.png"
width="638"
height="386"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure7_hu26c1fff3dfbe7e8b5a6a1023696dbbee_166190_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure7_hu26c1fff3dfbe7e8b5a6a1023696dbbee_166190_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="396px"
>&lt;/p>
&lt;p>layer normalization과 residual connection의 순서를 재배열하는 것이 BERT-Large를 넘어서 BERT 스타일 모델의 스케일링을 가능하게 하는 것이 중요하다는 것을 경험적으로 입증하였다. (b) 아키텍처는 원래 BERT 아키텍처에서 관찰된 불안정성을 제거하며, 더 낮은 학습 손실을 가진다. 이러한 변화가 더 큰 BERT 모델을 학습시키는 것을 가능하게 하는 것을 처음으로 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table4.png"
width="550"
height="160"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table4_huf5abb9b3a2aee77a7a878b3cab66c67b_27226_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table4_huf5abb9b3a2aee77a7a878b3cab66c67b_27226_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="343"
data-flex-basis="825px"
>&lt;/p>
&lt;p>336M 모델은 BERT-large와 같은 크기이며, 1.3B는 이전에 더 나쁜 결과를 얻었다고 알려진 BERT-xlarge 구성과 동일하다. 더 큰 hidden size와 더 많은 layer를 사용하여 BERT 모델을 더 확장하여 3.9B parameter 경우에 도달하였다. 모든 경우에서 hidden size는 attention head 당 64로 일정하게 유지되었다. 336M과 1.3B 모델은 200만 번 반복하여 학습되었으며, 3.9B 모델은 150만 번 반복하여 학습되고 아직도 학습 중이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table5.png"
width="1262"
height="320"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table5_hubfae993f6a380152daae9f599ea142cc_117884_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table5_hubfae993f6a380152daae9f599ea142cc_117884_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="394"
data-flex-basis="946px"
>&lt;/p>
&lt;p>3%의 홀드아웃 세트에서, 336M, 1.3B, 3.9B 모델은 각각 1.58, 1.30, 1.16의 검증 세트 perplexity를 달성하였고, 이는 모델 크기와 함께 단조롭게 감소하는 추세를 보여준다. 여러 downstream task에서 학습된 모델을 미세 조정한 결과, 모델 크기가 증가함에 따라 모든 경우에서 성능이 향상되었다. 특히, 3.9B 모델은 다른 BERT 기반 모델에 비해 개발 세트에서 state-of-the-art를 보여주며, RACE 테스트 세트에서 단일 모델과 앙상블 모델 모두에서 최고의 결과를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion-and-future-work">Conclusion and Future Work&lt;/h2>
&lt;p>이 연구에서는 기존 PyTorch transformer에 적은 수정을 통해 모델 병렬성을 구현하고, 이를 통해 전통적인 단일 GPU-모델 학습의 한계를 극복하였다. 512개의 NVIDIA V100 GPU에서 8.3B parameter를 가진 transformer 모델을 효율적으로 학습시켰고, BERT 모델에서는 layer normalization의 위치에 주의를 기울이는 것이 중요하다는 것을 확인하였다. 또한, 모델 크기가 down-tream task의 정확도에 긍정적인 영향을 미침을 확인했고, WikiText103, LAMBADA, RACE 데이터셋에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>미래의 연구 방향은 사전 학습의 규모 증가, 최적화 도구의 효율성과 메모리 사용량 개선, 더 큰 모델의 학습을 위한 병렬화 방법 개선, 다른 모델 패밀리(XLNet, T5)의 사전 학습, 다양한 downstream task에 대한 거대 모델의 성능 평가, 그리고 대형 사전 학습된 모델로부터 작은 모델을 학습시키는 knowledge distillation 사용 등이 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1909.08053.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Transformer-XL</title><link>https://kurtkim.github.io/p/transformer-xl/</link><pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/transformer-xl/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>transformer는 long-term dependency를 학습할 수 있지만, 언어 모델링에서는 고정된 길이의 컨텍스트에 제한된다. 이를 해결하기 위해, 시간적 일관성을 해치지 않고 고정 길이를 넘어선 의존성을 학습할 수 있는 새로운 아키텍처인 Transformer-XL을 제안한다. 이는 세그먼트 수준의 재발 메커니즘과 새로운 위치 인코딩 체계를 포함하며, long-term dependency를 포착하고 컨텍스트 조각화 문제를 해결한다. 결과적으로 Transformer-XL은 RNN보다 80% 더 긴, 기본 transformer보다 450% 더 긴 의존성을 학습하고, 평가 시간에서 기본 transformer보다 최대 1,800+ 배 더 빠르며, 여러 텍스트 dataset에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델링은 long-term dependency을 처리해야하는 중요한 과제로, 이는 RNN과 LSTM을 통해 다루어지곤 했다. 그러나 RNN 기반 모델들은 gradient vanishing과 explosion 등의 문제로 인해 최적화에 어려움이 있었다. LSTM은 평균적으로 200개의 문맥 단어를 사용하는데, 이는 아직 개선의 여지가 있다는 것을 보여준다.&lt;/p>
&lt;p>attention mechanism은 장거리 단어 쌍 간의 직접적인 연결을 통해 최적화를 쉽게하고 long-term dependency를 학습하는 데 도움이 된다. 그러나 이 기법은 ﬁxed-length context를 가진 세그먼트에서만 작동하며, 이로 인해 long-term dependency를 충분히 포착하지 못하고, 문맥 파편화(context fragmentation)라는 문제를 야기한다. 이는 모델이 처음 몇 개의 기호를 잘 예측하는 데 필요한 문맥 정보가 부족하게 되어 최적화가 비효율적이고 성능이 떨어지게 된다.&lt;/p>
&lt;p>ﬁxed-length context의 제한을 해결하기 위해, Transformer-XL이라는 새로운 아키텍처가 제안되었다. 이는 이전 세그먼트의 은닉 상태를 재사용하고, 이를 통해 세그먼트 간에 순환 연결을 만들어 long-term dependency를 모델링하게 된다. 또한, relative positional encodings을 사용하여 temporal confusion를 일으키지 않고 상태 재사용을 가능하게 한다. 이러한 접근법은 문맥 파편화 문제를 해결하고, 더 긴 주의 길이로 일반화할 수 있는 새로운 relative positional encodings 공식을 도입하였다.&lt;/p>
&lt;p>Transformer-XL은 단어와 문자 수준 언어 모델링에서 뛰어난 결과를 보였다. 이 모델은 오직 100M 토큰만을 학습하여 상대적으로 일관된 긴 텍스트를 생성할 수 있다. 이 모델의 주요 기여는 reuse의 개념을 도입하고 새로운 positional encodings 방식을 개발한 것이다. 이 두 기술은 고정 길이 문맥 문제를 해결하기 위한 완벽한 솔루션을 제공하며, 이들 중 하나만으로는 충분하지 않다. Transformer-XL은 문자와 단어 수준 언어 모델링에서 RNN보다 더 나은 성능을 보여주는 첫 번째 self-attention 모델이다.&lt;/p>
&lt;p>순수하게 self-attention 모델에서 recurrence 개념을 도입하고 새로운 positional encoding 방식을 개발하는 기술적 기여를 하였다. 이 두 가지 기법은 고정 길이 컨텍스트의 문제를 해결하는 완전한 방법을 제공하며, Transformer-XL은 문자와 단어 수준 언어 모델링에서 RNN을 뛰어넘는 첫 self-attention 모델이다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>언어 모델링 분야는 최근 몇 년 동안 컨텍스트 인코딩을 위한 새로운 구조 개발, 정규화와 최적화 알고리즘의 개선, Softmax 계산의 가속화, 그리고 출력 분포 패밀리의 확장 등 다양한 중요한 발전을 이루어냈다.&lt;/p>
&lt;p>언어 모델링에서 long-range context를 캡처하기 위한 연구들은 네트워크에 추가 입력으로 넓은 컨텍스트의 표현을 제공한다. 이는 수동으로 정의된 컨텍스트 표현부터 데이터에서 학습된 문서 수준의 주제를 사용하는 방식에 이르기까지 다양하다.&lt;/p>
&lt;p>일반적인 시퀀스 모델링에서 long-term dependency를 어떻게 포착할 것인지는 오랫동안 연구되어 온 문제이다. LSTM이 널리 적용된 이후, 기울기 소실 문제를 완화하는 데 많은 노력이 집중되었다. 이런 노력에는 더 나은 초기화, 추가적인 손실 신호, 확장된 메모리 구조, RNN 내부 아키텍처의 수정 등이 포함된다. 하지만, 이 연구는 이들과 달리 Transformer 아키텍처를 기반으로 하며, 언어 모델링이 장기 의존성을 학습하는 능력에서 이익을 얻는다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="model">Model&lt;/h2>
&lt;p>언어 모델링의 목표는 토큰의 말뭉치 $x = (x_1, &amp;hellip;, x_T)$의 결합 확률 $P(x)$를 추정하는 것이다. 이는 auto-regressive로 인수분해되어 문제를 각 조건부 인수를 추정하는 것으로 간소화한다. 신경망은 문맥 $x$를 은닉 상태로 인코딩하고, 이를 단어 임베딩과 곱하여 logit을 얻는다. logit은 softmax 함수를 거쳐 다음 토큰의 확률 분포를 생성한다.&lt;/p>
&lt;h3 id="vanilla-transformer-language-models">Vanilla Transformer Language Models&lt;/h3>
&lt;p>transformer나 self-attention 메커니즘을 언어 모델링에 적용하는 핵심 문제는, 임의의 긴 문맥을 고정된 크기의 표현으로 효과적으로 인코딩하는 방법입니다. 이론적으로는 전체 문맥을 transformer decoder로 처리하는 것이 가능하지만, 실제로는 제한된 자원 때문에 이는 불가능할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/figure1.png"
width="1248"
height="324"
srcset="https://kurtkim.github.io/p/transformer-xl/images/figure1_hu76c1749bf6922bb402ee9da4c415d9df_151228_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/figure1_hu76c1749bf6922bb402ee9da4c415d9df_151228_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>전체 말뭉치를 관리 가능한 크기의 짧은 세그먼트로 분할하고, 각 세그먼트 내에서만 모델을 학습시키는 것이 한가지 근사치 방법이다. 이는 이전 세그먼트의 문맥 정보를 무시함을 의미한다. 이 방식의 한계는, 가장 큰 의존성 길이가 세그먼트 길이에 의해 제한되고, 이는 문자 수준 언어 모델링에서 몇 백에 불과하다. 또한, 효율성을 높이기 위해 긴 텍스트를 고정 길이 세그먼트로 분할하는 것이 일반적이지만, 이는 문맥 파편화 문제를 초래할 수 있다.&lt;/p>
&lt;p>평가 시, 바닐라 모델은 각 단계에서 학습과 같은 길이의 세그먼트를 사용하지만, 마지막 위치에서만 예측을 한다. 그 다음 단계에서 세그먼트는 오른쪽으로 한 칸 이동하고, 새 세그먼트는 처음부터 다시 처리해야 합니다. 이 방식은 각 예측이 학습 중에 제공된 가장 긴 문맥을 활용하도록 하며, 문맥 파편화 문제를 완화하지만, 매우 비용이 많이 든다. 이 논문이 제안하는 아키텍처는 평가 속도를 크게 향상시킬 수 있다.&lt;/p>
&lt;h3 id="segment-level-recurrence-with-state-reuse">Segment-Level Recurrence with State Reuse&lt;/h3>
&lt;p>고정 길이 문맥의 한계를 극복하기 위해, transformer 아키텍처에 recurrence 메커니즘을 도입하는 것을 제안한다. 학습 중에는 이전 세그먼트의 은닉 상태 시퀀스가 고정되어 캐시되고, 이는 모델이 다음 새로운 세그먼트를 처리할 때 확장된 문맥으로 재사용된다. 이 추가 입력은 네트워크가 히스토리 내의 정보를 활용하게 하여 장기 의존성을 모델링하고 문맥 파편화를 피하게 한다. 공식적으로, 두 연속 세그먼트는 각각 $s_\gamma$와 $s_{\gamma+1}로 표시되며, 각 세그먼트에 대한 n 번째 layer 은닉 상태는 $h_n^\gamma$로 표시된다.&lt;/p>
&lt;p>$$ \tilde{h}_{\gamma+1}^{n-1} = [SG(h_{\gamma}^{n−1}) \circ h_{\gamma+1}^{n-1}] $$&lt;/p>
&lt;p>$$ q_{\gamma}^{n+1}, k_{\gamma}^{n+1}, v_{\gamma}^{n+1} = h_{\gamma+1}^{n-1}W_q^\intercal, \tilde{h}_{\gamma+1}^{n-1}W_k^\intercal, \tilde{h}_{\gamma+1}^{n-1}W_v^\intercal $$&lt;/p>
&lt;p>$$ h_{\gamma}^{n+1} = \text{Transformer-Layer}(q_{\gamma}^{n+1}, k_{\gamma}^{n+1}, v_{\gamma}^{n+1}) $$&lt;/p>
&lt;p>여기서 함수 $SG(·)$는 stop-gradient를 나타내고, 표기법 $[h_u \circ h_v]$는 길이 차원을 따라 두 은닉 시퀀스의 연결을 나타내며, $W$는 모델 parameter를 나타낸다. standard Transformer와 비교해 볼 때, 핵심 차이점은 키 $k_{\gamma}^{n+1}$과 값 $v_{\gamma}^{n+1}$이 확장된 문맥 $h$와 따라서 이전 세그먼트에서 캐시된 $h_{\gamma}^{n+1}$에 의존한다는 점이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/figure2.png"
width="1244"
height="310"
srcset="https://kurtkim.github.io/p/transformer-xl/images/figure2_hu51a4fd4ef03acc874c4997c6142a115c_215325_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/figure2_hu51a4fd4ef03acc874c4997c6142a115c_215325_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="401"
data-flex-basis="963px"
>&lt;/p>
&lt;p>이 recurrence 메커니즘을 적용하면, 은닉 상태에서 세그먼트 수준의 recurrence를 생성하게 되어, 효과적으로 활용되는 문맥이 두 세그먼트를 넘어서 확장될 수 있다. 하지만, recurrence 의존성이 세그먼트 당 한 layer씩 아래로 이동한다는 점이 전통적인 RNN-LM과는 다르다. 이로 인해, 가능한 가장 큰 의존성 길이는 layer 수와 세그먼트 길이에 대해 선형적으로 증가한다. 이는 RNN-LM을 학습하기 위해 개발된 truncated BPTT와 유사하지만, 마지막 상태 대신 은닉 상태의 시퀀스를 캐시하는 점에서 다르다. 이 방법은 relative positional encoding 기법과 함께 적용해야 한다.&lt;/p>
&lt;p>recurrence scheme를 도입하면, 더 긴 문맥을 활용할 수 있고 파편화 문제를 해결할 뿐만 아니라, 평가 시간이 크게 단축되는 이점도 얻을 수 있다. 평가 시에 이전 세그먼트의 표현을 재사용함으로써, Transformer-XL은 바닐라 모델에 비해 최대 1,800배 이상 빠르게 평가할 수 있다.&lt;/p>
&lt;p>recurrence scheme는 이전 세그먼트에만 제한될 필요가 없다. 이론적으로, GPU 메모리가 허용하는 한큼 많은 이전 세그먼트를 캐시하고, 그것들을 추가적인 문맥으로 재사용할 수 있다. 따라서, 사전 정의된 길이 $M$의 오래된 은닉 상태를 캐시하고, 이를 메모리라고 부른다. 실험에서는 학습 중에 $M$을 세그먼트 길이와 동일하게 설정하고, 평가 중에는 $M$을 여러 배 늘렸다.&lt;/p>
&lt;h3 id="relative-positional-encodings">Relative Positional Encodings&lt;/h3>
&lt;p>이전 섹션에서 제시한 아이디어는 매력적이지만, 은닉 상태 재사용에 대한 중요한 기술적 도전이 있다. 특히, 상태를 재사용할 때 위치 정보의 일관성을 어떻게 유지할 것인지가 문제이다. standard transformer에서 시퀀스 순서 정보는 positional encoding을 통해 제공되며, transformer의 실제 입력은 word embedding과 positional encoding의 요소별 덧셈이다. 이 positional encoding을 recurrence 메커니즘에 적용하면, 은닉 상태 시퀀스는 특정 방식으로 계산된다.&lt;/p>
&lt;p>$$ h_{\gamma+1} = f(h_\gamma, E_{s_{\gamma+1}} + U_{1:L}) $$&lt;/p>
&lt;p>$$ h_{\gamma} = f(h_{\gamma-1}, E_{s_{\gamma}} + U_{1:L}) $$&lt;/p>
&lt;p>여기서 $E_{s_{\gamma}} \in \mathbb{R}^{L×d}$는 $s_\gamma$의 단어 임베딩 시퀀스이고, $f$는 변환 함수를 나타낸다. $E_{s_{\gamma}}$와 $E_{s_{\gamma+1}}$ 모두 같은 positional encoding $U_{1:L}$와 연관되어 있음을 알 수 있다. 결과적으로, 모델에는 어떤 $j = 1, &amp;hellip;, L$에 대해 $x_{\gamma, j}$와 $x_{\gamma+1,j}$ 사이의 위치 차이를 구별하는 정보가 없어, 성능 손실이 발생한다.&lt;/p>
&lt;p>state reuse 메커니즘을 가능하게 하기 위해, 은닉 상태에 relative position 정보만을 인코딩하는 것이 필요하다. 이를 위해, 각 layer의 attention score에 relative position 정보를 주입한다. 이는 relative position을 동적으로 attention score에 주입함으로써, 쿼리 벡터가 $x_{\gamma, j}$와 $x_{\gamma+1, j}$의 표현을 그들의 다른 거리에 따라 쉽게 구분할 수 있게 해준다. 이를 통해, state reuse 메커니즘이 가능해지고, absolute position은 relative position에서 재귀적으로 복구될 수 있으므로, 시간 정보를 잃지 않게 된다.&lt;/p>
&lt;p>relative positional encoding의 개념은 이전에 기계 번역과 음악 생성에서 탐구되었다. 이 논문은 이를 다르게 유도하여, absolute positional encoding과 일대일로 대응하면서도 실증적으로 더 나은 일반화를 보여주는 새로운 형태의 relative positional encoding을 제시한다. standard transformer에서는 같은 세그먼트 내의 query와 key 벡터 사이의 attention score를 분해할 수 있다.&lt;/p>
&lt;p>$$ A_{i,j}^{abs} = E_{x_i}^\intercal W_q^\intercal W_k E_{x_j} + E_{x_i}^\intercal W_q^\intercal W_k U_j + U_i^\intercal W_q^\intercal W_k E_{x_j}+ U_i^\intercal W_q^\intercal W_k U_j $$&lt;/p>
&lt;p>relative position 정보에만 의존하는 아이디어를 따라, 다음과 같이 네 가지 항을 reparameterize하려고 제안한다.&lt;/p>
&lt;p>$$ A_{i,j}^{abs} = E_{x_i}^\intercal W_q^\intercal W_k E_{x_j} + E_{x_i}^\intercal W_q^\intercal W_k \color{#6580DD}{R_{i-j}}+ \color{#DD6565}{u^\intercal} W_q^\intercal W_k E_{x_j}+ \color{#DD6565}{v^\intercal} W_q^\intercal W_k \color{#6580DD}{R_{i-j}} $$&lt;/p>
&lt;ul>
&lt;li>가장 먼저 변경하는 것은 key 벡터를 계산하기 위해 절대 위치 임베딩 $U_j$의 모든 출현을 그 상대적 대응체 $\color{#6580DD}{R_{i-j}}$로 대체하는 것이다. 이는 주목할 위치에 대해서는 상대 거리만이 중요하다는 사전 정보를 반영하는 것이다. $\color{#6580DD}{R}$은 학습 가능한 parameter 없는 sinusoid encoding matrix이다.&lt;/li>
&lt;li>쿼리 위치에 관계없이 다른 단어에 대한 주목 편향성이 동일하게 유지되도록 학습 가능한 parameter를 도입한다. 이를 위해, 학습 가능한 parameter $\color{#DD6565}{u}$ 와 $\color{#DD6565}{v}$를 각각 도입하여 쿼리와 관련된 항을 대체한다.&lt;/li>
&lt;li>마지막으로, 내용 기반의 키 벡터와 위치 기반의 키 벡터를 생성하기 위해 두 가지 가중치 행렬 $W_{k, E}$ 와 $W_{k, R}$를 의도적으로 분리한다.&lt;/li>
&lt;/ul>
&lt;p>새로운 parameter화를 통해 각 항은 다음과 같은 의미를 갖게 된다: 첫번째 항은 내용 기반 주소 지정, 두번째 항은 내용에 따른 위치 편향, 세번째 항은 전역 내용 편향, 그리고 마지막 항은 전역 위치 편향을 나타낸다.&lt;/p>
&lt;p>Shaw et al. (2018)의 접근법은 첫번째 항과 두번째 항 만을 가지고 있으며, 두 편향 항을 생략한다. 또한, original sinusoid positional encoding에 내장된 inductive bias를 포기하고 있다. 반면에, 이 논문의 방법은 sinusoid 공식을 적용한 relative positional embedding을 사용한다. 이로 인해, 특정 길이의 메모리에서 학습된 모델은 평가 시에 메모리를 몇 배 더 길게 자동으로 일반화할 수 있다.&lt;/p>
&lt;p>relative positional embedding을 이용한 recurrence 메커니즘을 적용하여, Transformer-XL 아키텍처를 도출하였다. 이 아키텍처는 single attention head를 가진 N-layer로 구성되며, 그 계산 절차를 요약하면 다음과 같다. $n = 1, &amp;hellip;, N$에 대해:&lt;/p>
&lt;p>$$ \tilde{h}_{\gamma}^{n-1} = [SG(h_{\gamma}^{n−1}) \circ h_{\gamma}^{n-1}] $$&lt;/p>
&lt;p>$$ q_{\gamma}^{n}, k_{\gamma}^{n}, v_{\gamma}^{n} = h_{\gamma}^{n-1}W_q^\intercal, \tilde{h}_{\gamma}^{n-1}W_k^\intercal, \tilde{h}_{\gamma}^{n-1}W_v^\intercal $$&lt;/p>
&lt;p>$$ A_{\gamma, i, j}^n = {q_{\gamma, i}^n}^\intercal k_{\gamma, j}^n + {q_{\gamma, i}^n}^\intercal W_{k, R}^n R_{i - j} + u^\intercal k_{\gamma, j} + v^\intercal W_{k, R}^n R_{i - j} $$&lt;/p>
&lt;p>$$ a_\gamma^n = Masked-Softmax(A_\gamma^n)v_\gamma^n $$&lt;/p>
&lt;p>$$ o_\gamma^n = LayerNorm(Linear(a_\gamma^n) + h_\gamma^{n-1}) $$&lt;/p>
&lt;p>$$ h_\gamma^{n} = Positionwise-Feed-Forward(o_\gamma^n) $$&lt;/p>
&lt;p>$h_\gamma^0 := E_{s_\gamma}$는 단어 임베딩 시퀀스로 정의된다. $A$를 계산하는 간단한 방법은 모든 쌍 $(i, j)$에 대해 계산을 수행하며, 이는 시퀀스 길이에 대해 이차적인 비용을 요구한다. 하지만, 이 연구에서는 $i − j$의 값 범위를 인지하고 시퀀스 길이에 대해 선형적인 비용으로 줄이는 계산 절차를 제시한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="main-results">Main Results&lt;/h3>
&lt;p>Transformer-XL을 다양한 데이터셋에 적용하여, 단어 수준과 문자 수준의 언어 모델링에서 state-of-the-art 시스템들과 비교하였다. 이 데이터셋들은 WikiText-103, enwik8, text8, One Billion Word, 그리고 Penn Treebank를 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table1.png"
width="622"
height="364"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table1_hu8460c2c82d805e77e85081c5d2428902_99215_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table1_hu8460c2c82d805e77e85081c5d2428902_99215_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="170"
data-flex-basis="410px"
>&lt;/p>
&lt;p>WikiText-103은 장기 의존성을 가진 가장 큰 단어 수준 언어 모델링 벤치마크이다. 이를 활용해, 우리는 Transformer-XL의 학습과 평가를 진행했고, 그 결과 Transformer-XL은 이전의 state-of-the-art를 대폭 뛰어넘는 결과를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table2.png"
width="574"
height="386"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table2_hu5d6335a0ff98b7f99cfedc1bded056ee_102715_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table2_hu5d6335a0ff98b7f99cfedc1bded056ee_102715_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>데이터셋 enwik8에서도 Transformer-XL 아키텍처는 이전의 state-of-the-art를 뛰어넘는 새로운 결과를 달성하였다다. 12-layer의 Transformer-XL은 모델 크기 제약하에도 불구하고, 기존의 RNN 기반 모델과 큰 차이를 보였다. 더 큰 모델 크기로 18-layer와 24-layer의 Transformer-XL을 학습시켰을 때, 문자 수준 벤치마크에서 1.0을 돌파하는 첫 번째 방법으로서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table3.png"
width="576"
height="280"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table3_huf24b968b33bc112913898207b3fe8c70_74580_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table3_huf24b968b33bc112913898207b3fe8c70_74580_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="205"
data-flex-basis="493px"
>&lt;/p>
&lt;p>text8은 enwik8과 비슷하지만, 텍스트를 소문자로 변환하고 특정 문자를 제거하여 처리된 100M개의 Wikipedia 문자를 포함한다. 이러한 유사성 때문에, enwik8에서의 최적의 모델과 hyperparameter를 그대로 text8에 적용하였고, Transformer-XL은 명확한 차이로 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table4.png"
width="624"
height="440"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table4_hu4d3fa0638b329cdb9ca252637647dcb2_144243_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table4_hu4d3fa0638b329cdb9ca252637647dcb2_144243_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="340px"
>&lt;/p>
&lt;p>One Billion Word 데이터셋은 문장이 섞여 있어 long-term dependency를 보존하지 않으며, 주로 short-term dependency 모델링 능력을 테스트한다. Transformer-XL은 주로 long-term dependency를 더 잘 포착하기 위해 설계되었지만, 이 데이터셋에서도 단일 모델 state-of-the-art를 크게 향상시켰다. 이는 Transformer-XL의 장점이 짧은 시퀀스 모델링에도 적용될 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table5.png"
width="626"
height="408"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table5_hu63c46848d1b3599c32426775f5d1a5e2_119294_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table5_hu63c46848d1b3599c32426775f5d1a5e2_119294_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="368px"
>&lt;/p>
&lt;p>단어 수준 데이터셋인 Penn Treebank를 테스트하기 위해, variational dropout과 weight average을 적용하여 Transformer-XL을 수정하였다. 적절한 regularization을 통해, Transformer-XL은 두 단계의 미세 조정 없이 state-of-the-art를 달성하였다. 이는 Transformer-XL이 작은 데이터셋에서도 잘 일반화될 수 있음을 보여준다.&lt;/p>
&lt;h3 id="ablation-study">Ablation Study&lt;/h3>
&lt;p>Transformer-XL에서 사용한 recurrence 메커니즘과 새로운 positional encoding scheme의 효과를 검증하기 위해 두 가지 ablation study를 수행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table6.png"
width="1120"
height="432"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table6_hu068a132ed8b4bd3bcea5ca9d95df4c8b_112344_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table6_hu068a132ed8b4bd3bcea5ca9d95df4c8b_112344_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="622px"
>&lt;/p>
&lt;p>첫 번째 연구는 long-term dependency를 요구한는 데이터셋인 WikiText-103에서 수행되었다. absolute encoding은 half loss와 함께 잘 작동하는 것으로 나타났다. 또한, recurrence 메커니즘과 인코딩 체계 모두가 최고의 성능을 달성하고, 평가 시간 동안 더 긴 attention 시퀀스로 일반화하는데 필요하다는 것이 확인되었다. 학습 중에 backpropagation 길이는 128이지만, 이 두 기술을 사용하면 테스트 시간에 attention 길이를 640까지 늘릴 수 있다.&lt;/p>
&lt;p>recurrence 메커니즘이 추가 메모리를 요구함에도 불구하고, 같은 GPU 메모리 제약 하에서 Transformer-XL은 기준 모델들에 비해 더 우수한 성능을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table7.png"
width="462"
height="158"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table7_hu0d0d69b723b085c804fd46ec3f1161f3_24044_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table7_hu0d0d69b723b085c804fd46ec3f1161f3_24044_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>두 번째 연구는 long-term dependency를 요구하지 않는 데이터셋인 One Billion Word에서 실험을 수행하였다. 결과적으로, 세그먼트 수준의 recurrence를 사용하면 long-term dependency이 필요하지 않은 경우에도 성능이 크게 향상된다는 것을 확인하였다. 또한, relative positional encoding은 짧은 시퀀스에서도 우수한 성능을 보여주었다.&lt;/p>
&lt;h3 id="relative-effective-context-length">Relative Effective Context Length&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table8.png"
width="610"
height="278"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table8_hub48d1c9ebba47586ad2661fe16444a6d_50993_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table8_hub48d1c9ebba47586ad2661fe16444a6d_50993_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="526px"
>&lt;/p>
&lt;p>Khandelwal et al. 이 제안한 Effective Context Length(ECL) 대신, Relative Effective Context Length(RECL)라는 새로운 지표를 제안하였다. 이 지표는 모델 그룹에 대해 정의되며, 긴 문맥의 이득은 최고의 짧은 문맥 모델에 대한 상대적 개선으로 측정된다. Transformer-XL은 평균적으로 900단어의 의존성을 모델링할 수 있으며, RECL은 RNN과 transformer보다 각각 80%, 450% 더 길다는 결과를 보여주었다. 이는 Transformer-XL이 long-term dependency을 모델링할 수 있다는 것을 뒷받침한다.&lt;/p>
&lt;h3 id="generated-text">Generated Text&lt;/h3>
&lt;p>중간 크기의 WikiText-103에서만 학습된 Transformer-XL은 소소한 결점에도 불구하고 수천 개의 토큰으로 구성된 일관성 있는 기사를 생성할 수 있다.&lt;/p>
&lt;h3 id="evaluation-speed">Evaluation Speed&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformer-xl/images/table9.png"
width="592"
height="188"
srcset="https://kurtkim.github.io/p/transformer-xl/images/table9_hub2f3e7ef6d7488877fc27d37e7dc1a85_28607_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformer-xl/images/table9_hub2f3e7ef6d7488877fc27d37e7dc1a85_28607_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="755px"
>&lt;/p>
&lt;p>Transformer-XL와 바닐라 transformer 모델의 평가 속도를 비교한 결과, state reuse scheme 덕분에 Transformer-XL은 평가 중에 최대 1,874배의 속도 향상을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>Transformer-XL은 강력한 perplexity 결과를 보이고, longer-term dependency를 더 잘 모델링하며, 평가 속도를 크게 향상시키고, 일관성 있는 텍스트를 생성할 수 있다. 이것은 텍스트 생성, 비지도 학습, 이미지와 음성 모델링 등의 분야에서 흥미로운 응용을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Transformers</title><link>https://kurtkim.github.io/p/transformers/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/transformers/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>recurrent와 convolutional을 완전히 제거하고 attention mechanism에만 기반한 새로운 신경망 아키텍처인 transformer를 제안한다. 이 모델은 더 우수한 품질을 제공하면서 병렬화가 가능하고 학습 시간이 훨씬 적게 든다. 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 뛰어넘는 성능을 보였고, 영어 구문 분석에도 성공적으로 적용되었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>RNN, 특히 LSTM과 GRU는 언어 모델링과 기계 번역 등에서 state-of-the-art로 인정받았다. 이후에도 이러한 모델과 아키텍처의 한계를 끊임없이 넓혀가고 있다.&lt;/p>
&lt;p>recurrent 모델은 입력과 출력 시퀀스의 위치에 따라 계산을 분류하며, 이는 순차적인 특성으로 인해 학습 예제 내의 병렬화를 방해한다. 이는 메모리 제약이 있는 긴 시퀀스에서 중요한 문제가 된다. 최근의 연구는 계산 효율성을 향상시키는 방법을 제시하였지만, sequential computation의 근본적인 제약은 여전히 남아 있다.&lt;/p>
&lt;p>attention mechanism은 시퀀스 모델링에 있어 핵심 역할을 하며, 입력이나 출력 시퀀스의 거리에 관계 없이 종속성을 모델링할 수 있다. 그러나 대부분의 경우, attention mechanism은 recurrent 네트워크와 함께 사용된다.&lt;/p>
&lt;p>recurrent을 배제하고 attention mechanism에만 의존하는 transformer를 제안한다. transformer는 더 많은 병렬화를 가능하게 하고, 8개의 P100 GPU에서 단 12시간 학습만으로 state-of-the-art를 달성할 수 있었다.&lt;/p>
&lt;hr>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>sequential computation을 줄이는 것은 Extended Neural GPU, ByteNet, ConvS2S 등의 핵심 목표인데, 이들은 모두 CNN을 사용해 모든 입력과 출력 위치에 대한 표현을 병렬로 계산한다. 그러나 이 모델들은 두 임의의 위치간의 관계를 학습하는데 필요한 연산 수가 위치 간 거리에 따라 증가하므로, 먼 위치 간의 종속성을 학습하기 어렵다. transformer는 이를 상수 수의 연산으로 줄이지만, attention-weighted를 평균화함으로써 해상도가 감소하는 비용이 따르며, 이는 Multi-Head Attention을 통해 상쇄시킨다.&lt;/p>
&lt;p>self-attention은 단일 시퀀스의 다양한 위치를 연관시켜 시퀀스의 표현을 계산하는 방법으로, 독해, 요약, 텍스트 함의 파악, 작업 독립적 문장 표현 학습 등 다양한 작업에 성공적으로 활용되었다.&lt;/p>
&lt;p>end-to-end memory network는 recurrent attention mechanism을 기반으로 하며, 간단한 언어 질문 응답 및 언어 모델링 작업에서 좋은 성능을 보여주었다.&lt;/p>
&lt;p>transformer는 시퀀스에 정렬된 RNN이나 convolution을 사용하지 않고, 완전히 self-attention에 의존하여 입력과 출력의 표현을 계산하는 최초의 transduction 모델이다.&lt;/p>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>대부분의 neural sequence transduction 모델은 encoder-decoder 구조를 가지고 있습니다. encoder는 기호 표현의 입력 시퀀스 $(x_1, &amp;hellip;, x_n)$를 연속적인 표현의 시퀀스 $z = (z_1, &amp;hellip;, z_n)$로 변환하고, 이를 기반으로 decoder는 한 번에 하나씩 기호의 출력 시퀀스 $(y_1, &amp;hellip;, y_m)$를 생성한다. 이때 모델은 이전에 생성된 기호를 추가 입력으로 사용하는 auto-regressive 방식을 취한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/figure1.png"
width="660"
height="932"
srcset="https://kurtkim.github.io/p/transformers/images/figure1_hud7e6f7a5842be66fe4891f11600f3af9_164918_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/figure1_hud7e6f7a5842be66fe4891f11600f3af9_164918_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="70"
data-flex-basis="169px"
>&lt;/p>
&lt;p>transformer는 encoder와 decoder 모두에 대해 쌓인 self-attention과 point-wise, fully connected layer을 사용하여 encoder-decoder 구조를 따른다.&lt;/p>
&lt;h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks&lt;/h3>
&lt;p>&lt;strong>Encoder:&lt;/strong> encoder는 $N = 6$개의 동일한 계층으로 이루어져 있으며, 각 계층은 multi-head self-attention mechanism과 positionwise fully connected feed-forward network의 두 sub-layer로 구성된다. 각 하위 계층은 residual connection과 $LayerNorm(x + Sublayer(x))$를 통해 처리되며, 모든 하위 계층과 임베딩 계층은 차원 $d_{model} = 512$인 출력을 생성한다.&lt;/p>
&lt;p>&lt;strong>Decoder:&lt;/strong> decoder는 $N = 6$개의 동일한 계층으로 구성되며, encoder의 출력에 multi-head attention을 수행하는 세 번째 sub-layer이 추가된다. 각 sub-layer 주변의 residual connection과 layer normalization를 사용하며, 후속 위치에 주의를 기울이는 것을 방지하기 위해 decoder의 self-attention sub-layer을 수정한다. 이러한 수정은 위치 $i$의 예측이 $i$보다 작은 위치에서의 알려진 출력에만 의존하도록 보장한다.&lt;/p>
&lt;h3 id="attention">Attention&lt;/h3>
&lt;p>attention 함수는 query와 key-value 쌍을 벡터 형태의 출력으로 매핑하며, 출력은 값들의 가중치 합으로 계산된다. 이때 각 값의 가중치는 query와 해당 key의 호환성에 따라 결정된다.&lt;/p>
&lt;h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention&lt;/h4>
&lt;p>&amp;ldquo;Scaled Dot-Product Attention&amp;quot;은 query와 key의 차원이 $d_k$, 값의 차원이 $d_v$인 입력을 처리한다. query와 모든 key의 내적을 계산하고, 이를 $\sqrt{d_k}$로 나눈 후, softmax 함수를 적용하여 값에 대한 가중치를 얻는다.&lt;/p>
&lt;p>여러 쿼리들을 동시에 처리하기 위해 행렬 $Q$에 패킹하고, key와 value 또한 각각 행렬 $K$와 $V$에 패킹한다. 그리고 이를 이용해 출력 행렬을 계산한다:&lt;/p>
&lt;p>$$ Attention(Q, K, V) = softmax({{QK^\intercal}\over{\sqrt{d_k}}})V $$&lt;/p>
&lt;p>가장 흔히 사용되는 attention 함수는 additive attention과 dot-product (multiplicative) attention입니다. dot-product attention은 알고리즘과 ${{1}\over{\sqrt{d_k}}}$의 스케일링 요소를 제외하면 동일하며, additive attention은 feed-forward network를 이용해 호환성 함수를 계산한다. 두 방법은 이론적으로 유사하지만, dot-product attention은 최적화된 행렬 곱셈 코드를 통해 더 빠르고 공간 효율적으로 구현될 수 있다.&lt;/p>
&lt;p>$d_k$ 값이 작은 경우 두 메커니즘이 유사하게 작동하지만, $d_k$ 값이 크면 스케일링 없는 dot-product attention의 성능이 떨어잔다. 이는 dot-product 값의 크기 증가로 인해 softmax 함수의 기울기가 매우 작아지는 것을 방지하기 위해, dot-product을 ${{1}\over{\sqrt{d_k}}}$로 스케일링한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/figure2.png"
width="902"
height="492"
srcset="https://kurtkim.github.io/p/transformers/images/figure2_hu8092e7ec43193a1f60e58782f6994482_111256_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/figure2_hu8092e7ec43193a1f60e58782f6994482_111256_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="440px"
>&lt;/p>
&lt;h4 id="multi-head-attention">Multi-Head Attention&lt;/h4>
&lt;p>$d_{model}$-dimensional key, value, query에 single attention 함수를 사용하는 대신, 각각을 다른 선형 변환을 통해 $d_k$, $d_k$, $d_v$ 차원으로 $h$번 변환하는 것이 유익하다는 것을 발견하였다. 이 변환된 query, key, value에 대해 병렬로 attention 함수를 수행하면, $d_v$ 차원의 출력 값이 나오며, 이들은 연결되고 다시 변환되어 최종 값이 생성된다.&lt;/p>
&lt;p>multi-head attention은 다른 표현 하위 공간에서 다른 위치의 정보에 동시에 주의를 기울일 수 있게 해주는 반면, single attention head는 이를 평균화하여 방해한다.&lt;/p>
&lt;p>$$ MultiHead(Q, K, V) = Concat(head_1, &amp;hellip;, head_h)W^O $$
$$ where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$&lt;/p>
&lt;p>프로젝션은 parameter 행렬 $W_i^Q \in \mathbb{R}^{d_{model \times d_k}}$, $W_i^K \in \mathbb{R}^{d_{model \times d_k}}$, $W_i^V \in \mathbb{R}^{d_{model \times d_v}}$ 그리고 $W^O \in \mathbb{R}^{hd_v \times d_{model}}$이다.&lt;/p>
&lt;p>이 작업에서는 8개의 병렬 attention layer를 사용하며, 각 계층에 대해 $d_k = d_v = d_{model} / h = 64$를 사용한다. 각 head의 차원이 줄어들었기 때문에 전체 계산 비용은 전체 차원의 single-head attention과 유사하다.&lt;/p>
&lt;h4 id="applications-of-attention-in-our-model">Applications of Attention in our Model&lt;/h4>
&lt;p>transformer는 세 가지 다른 방식으로 multi-head attention을 사용한다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;encoder-decoder attention&amp;quot;에서는 이전 decoder 계층에서 query가 생성되고, encoder의 출력에서 메모리 key와 value이 생성된다. 이를 통해 decoder의 모든 위치가 입력 시퀀스 전체에 주의를 기울일 수 있습니다. 이는 일반적인 sequence-to-sequence 모델의 encoder-decoder attention mechanism을 따른다.&lt;/li>
&lt;li>encoder에는 self-attention layer가 있으며, 이 layer에서는 모든 key, value, query가 encoder의 이전 layer의 출력에서 생성된다. 이를 통해 encoder의 각 위치가 이전 layer의 모든 위치에 주의를 기울일 수 있다.&lt;/li>
&lt;li>decoder의 self-attention layer는 decoder의 각 위치가 그 위치를 포함해 그 이전의 모든 위치에 주의를 기울일 수 있게 한다. auto-regressive 속성을 유지하기 위해, 불법적인 연결에 해당하는 값을 마스킹 아웃($-\infty$로 설정)하여 decoer 내부의 정보 흐름을 제한한다.&lt;/li>
&lt;/ul>
&lt;h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks&lt;/h3>
&lt;p>encoder와 decoder의 각 layer에는 각 위치에 독립적으로 적용되는 fully connected feed-forward network가 포함되어 있으며, 이는 두 개의 linear transformation과 그 사이의 ReLU activation 함수로 구성된다.&lt;/p>
&lt;p>$$ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $$&lt;/p>
&lt;p>linear transformation은 다른 위치에도 동일하게 적용되지만, layer마다 다른 parameter를 사용한다. 이는 kernel size 1의 두 개의 convolution으로도 설명할 수 있다. 입력과 출력의 차원은 $d_{model} = 512$이고, inner-layer의 차원은 $d_{ff} = 2048$이다.&lt;/p>
&lt;h3 id="embeddings-and-softmax">Embeddings and Softmax&lt;/h3>
&lt;p>학습된 임베딩을 사용하여 입력 토큰과 출력 토큰을 벡터로 변환하며, 학습된 linear transformation과 softmax 함수를 사용해 decoder 출력을 다음 토큰 확률로 변환한다. 두 임베딩 layer와 pre-softmax linear transformatio에서 동일한 가중치 행렬을 공유하고, 임베딩 layer에서는 이 가중치에 ${{1}\over{\sqrt{d_k}}}$를 곱한다.&lt;/p>
&lt;h3 id="positional-encoding">Positional Encoding&lt;/h3>
&lt;p>transformer 모델은 recurrence와 convolution이 없기 때문에, 시퀀스의 토큰 위치에 대한 정보를 주입함으로써 시퀀스의 순서를 활용한다. 이를 위해, &amp;ldquo;positional encoding&amp;quot;을 입력 임베딩에 더하며, 이는 임베딩과 동일한 차원을 가진다. positional encoding은 학습되거나 고정될 수 있다.&lt;/p>
&lt;p>다른 주파수의 sine 함수와 cosine 함수를 사용한다:&lt;/p>
&lt;p>$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$
$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $$&lt;/p>
&lt;p>positional encoding의 각 차원이 sinusoid에 해당하도록 sine 함수와 코사인 cosine를 사용한다. 이 함수를 선택한 이유는 모델이 상대적 위치에 따라 주의를 쉽게 배울 수 있도록 하기 위해서이다. 즉, 어떤 고정된 오프셋 $k$에 대해서도, $PE_{pos+k}$는 $PE_{pos}$의 선형 함수로 표현될 수 있다.&lt;/p>
&lt;p>학습된 positional embedding을 사용해 실험해봤고, 두 방식이 거의 동일한 결과를 생성함을 확인하였다. sinusoidal 버전을 선택한 이유는 학습 중에 접한 것보다 더 긴 시퀀스 길이로 extrapolate 할 수 있을 것이라 판단했기 때문이다.&lt;/p>
&lt;hr>
&lt;h2 id="why-self-attention">Why Self-Attention&lt;/h2>
&lt;p>recurrent및 convolutional layer와 self-attention layer을 비교한다. 이들은 모두 가변 길이의 심볼 표현 시퀀스를 동일한 길이의 다른 시퀀스로 매핑하는데 사용된다. self-attention 사용의 동기를 설명하기 위해, 세 가지 조건을 고려한다.&lt;/p>
&lt;p>하나는 각 계층에서의 전체 계산 복잡성이다. 또 다른 하나는 병렬화할 수 있는 계산량으로, 이는 필요한 최소 연속 작업의 수로 측정된다.&lt;/p>
&lt;p>세 번째는 네트워크 내에서 long-range dependency 사이의 경로 길이이다. long-range dependency를 학습하는 것은 시퀀스 변환 작업의 주요 도전 과제이다. 이를 학습하는 능력은 네트워크 내에서 신호가 이동하는 경로의 길이에 크게 영향을 받는다. 입력과 출력 시퀀스의 임의의 위치 사이의 경로가 짧을수록 long-range dependency을 학습하기 쉽다. 따라서, 다른 layer 유형으로 구성된 네트워크에서 두 입력과 출력 위치 사이의 최대 경로 길이도 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table1.png"
width="1018"
height="218"
srcset="https://kurtkim.github.io/p/transformers/images/table1_hu13e34a3f917253fd28a66dff1519f55e_47240_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table1_hu13e34a3f917253fd28a66dff1519f55e_47240_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="466"
data-flex-basis="1120px"
>&lt;/p>
&lt;p>self-attention layer는 연속적인 연산을 통해 모든 위치를 연결하며, recurrent layer에 비해 더 빠른 계산 속도를 제공한다. 특히, 시퀀스 길이가 표현 차원보다 작은 경우에 더욱 그렇다. 매우 긴 시퀀스를 처리하는 작업의 계산 성능을 높이기 위해, self-attention은 출력 위치를 중심으로 한 입력 시퀀스의 이웃만을 고려하도록 제한될 수 있다. 이는 최대 경로 길이를 $O(n/r)$로 증가시키며, 이에 대한 연구를 미래에 더 진행할 계획이다.&lt;/p>
&lt;p>커널 너비가 $k &amp;lt; n$인 단일 convolutional layer는 모든 입력과 출력 위치를 연결하지 않는다. 이를 위해선 복수의 convolutional layer이 필요하고, 이로 인해 네트워크 내 두 위치 사이의 가장 긴 경로가 늘어난다. 보통 convolutional layer는 recurrent layer보다 $k$배의 비용이 더 들지만, 분리 가능한 convolution을 사용하면 복잡성이 크게 줄어든다. 그러나 $k = n$인 경우에도, 분리 가능한 convolution의 복잡성은 self-attention layer와 point-wise feed-forward layer의 결합과 동일하다.&lt;/p>
&lt;p>self-attention은 더 해석 가능한 모델을 만들 수 있는 이점이 있다. 우리 모델에서는 각 attention head가 다른 작업을 수행하도록 학습하며, 이들 중 많은 헤드가 문장의 구문적 및 의미적 구조와 관련된 행동을 보이는 것으로 파악되었다.&lt;/p>
&lt;hr>
&lt;h2 id="training">Training&lt;/h2>
&lt;h3 id="training-data-and-batching">Training Data and Batching&lt;/h3>
&lt;p>약 450만 개의 문장 쌍을 포함하는 표준 WMT 2014 영어-독일어 데이터셋으로 학습했다. 더 큰 WMT 2014 영어-불어 데이터셋도 사용하였다. 각 학습 배치는 약 25000개의 소스 토큰과 타겟 토큰을 포함하는 문장 쌍을 포함하였다.&lt;/p>
&lt;h3 id="hardware-and-schedule">Hardware and Schedule&lt;/h3>
&lt;p>8개의 NVIDIA P100 GPU에서 모델을 학습시켰다. 기본 모델들은 각 학습 단계마다 약 0.4초가 걸렸고, 총 100,000단계 또는 12시간 동안 학습되었다. 큰 모델들은 단계 시간이 1.0초였고, 300,000단계 또는 3.5일 동안 학습되었다.&lt;/p>
&lt;h3 id="optimizer">Optimizer&lt;/h3>
&lt;p>Adam optimizer, $\beta_1 = 0.9$, $\beta_2 = 0.98$ 그리고 $\epsilon = 10^{−9}$를 사용하였다. 다음의 공식에 따라서 learning rate을 변화시켰다:&lt;/p>
&lt;p>$$ lrate = d_{model}^{−0.5} · min(\text{step_num}^{−0.5}, \text{step_num} · \text{warmup_steps}^{−1.5}) $$&lt;/p>
&lt;p>warmup_steps 동안 learning rate을 선형적으로 증가시키고, 그 이후에는 단계 수의 역제곱에 비례하여 감소시킨다. warmup_steps = 4000을 사용하였다.&lt;/p>
&lt;h3 id="regularization">Regularization&lt;/h3>
&lt;p>세 가지 유형의 regularization를 사용한다:&lt;/p>
&lt;p>&lt;strong>Residual Dropout&lt;/strong> 각각의 sub-layer의 출력과 encoder 및 decoder 스택의 임베딩과 positional encoding의 합에 드롭아웃을 적용합니다. 기본 모델에서는 dropout rate로 $P_{drop} = 0.1$을 사용한다.&lt;/p>
&lt;p>&lt;strong>Label Smoothing&lt;/strong> 학습 동안에는 $\epsilon_{ls}= 0.1$의 값을 가진 라벨 스무딩을 사용하였다. 이는 모델이 더 불확실하게 학습하도록 만드므로 혼란스러움(perplexity)을 증가시키지만, 정확도와 BLEU 점수는 향상시킨다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;h3 id="machine-translation">Machine Translation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table2.png"
width="950"
height="416"
srcset="https://kurtkim.github.io/p/transformers/images/table2_hu74a18d247471b463bc514720d41dbf06_99215_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table2_hu74a18d247471b463bc514720d41dbf06_99215_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;p>WMT 2014 영어-독일어 번역 작업에서, large transformer 모델은 이전 모델들을 2.0 BLEU 이상 뛰어넘어 state-of-the-art인 28.4의 BLEU 점수를 달성하였다. 이 모델은 8개의 P100 GPU에서 3.5일 동안 학습되었다. 기본 모델조차도 이전의 모든 모델과 앙상블을 능가하며, 경쟁 모델의 학습 비용의 일부에 불과했다.&lt;/p>
&lt;p>WMT 2014 영어-불어 번역 작업에서, BLEU 점수 41.0을 달성하여 이전에 발표된 모든 단일 모델들을 능가했고, 이전 state-of-the-art 모델의 학습 비용의 1/4 미만이었다. 이 모델은 dropout rate로 $P_{drop} = 0.1$을 사용했다.&lt;/p>
&lt;p>기본 모델에 대해 마지막 5개의 체크포인트를 평균한 단일 모델을 사용했고, 큰 모델에 대해선 마지막 20개의 체크포인트를 평균냈다. beam search를 통해 beam size 4와 length penalty $\alpha = 0.6$을 사용했다. 이 값들은 개발 세트에서 실험 후 결정되었다. 추론 시 최대 출력 길이는 입력 길이 + 50으로 설정되었으나, 가능하다면 일찍 종료한다.&lt;/p>
&lt;p>모델 학습에 사용된 부동 소수점 연산의 수는 학습 시간, 사용된 GPU의 수, 각 GPU의 단정밀도 부동 소수점 용량의 추정치를 곱하여 추정하였다.&lt;/p>
&lt;h3 id="model-variations">Model Variations&lt;/h3>
&lt;p>transformer의 다양한 요소의 중요성을 평가하기 위해, 기본 모델을 다양하게 변형하며 개발 세트인 newstest2013에서의 영어-독일어 번역 성능 변화를 측정하였다. beam search을 사용했지만 체크포인트 평균화는 사용하지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table3.png"
width="1088"
height="704"
srcset="https://kurtkim.github.io/p/transformers/images/table3_hu33e15ffc6a9c7fa71d0bbbf9c9b1cc6c_122384_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table3_hu33e15ffc6a9c7fa71d0bbbf9c9b1cc6c_122384_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="370px"
>&lt;/p>
&lt;p>single-head attention은 최적 설정보다 0.9 BLEU가 떨어지며, head 수가 너무 많아져도 품질이 떨어진다.&lt;/p>
&lt;p>attention key 크기를 줄이면 모델 품질이 떨어진다는 것을 확인하였다. 이는 복잡한 호환성 함수가 필요할 수 있음을 시사한다. 더 큰 모델이 더 좋고, dropout이 over-fitting을 피하는 데 매우 유용하다는 것을 확인했다. sinusoidal positional encoding을 learned positional embedding으로 대체했을 때 기본 모델과 거의 동일한 결과를 얻었다.&lt;/p>
&lt;h3 id="english-constituency-parsing">English Constituency Parsing&lt;/h3>
&lt;p>transformer가 다른 작업에 일반화할 수 있는지 확인하기 위해, 구조적 제약이 강하고 입력보다 긴 출력을 가진 영어 구성성 파싱 작업에 대한 실험을 수행하였다. RNN sequence-to-sequence 모델은 이 작업에서 state-of-the-art를 달성하지 못하였다.&lt;/p>
&lt;p>Penn Treebank의 Wall Street Journal (WSJ) 부분에 대해 약 4K 개의 학습 문장을 사용하여 $d_{model} = 1024$의 4-layer transformer를 학습시켰다. 또한, 약 17M 문장을 포함하는 대형 말뭉치를 사용하여 반지도학습 환경에서도 학습시켰다. WSJ만을 대상으로 하는 경우 16K 토큰의 어휘를, 반지도학습 설정에서는 32K 토큰의 어휘를 사용하였다.&lt;/p>
&lt;p>dropout, learning rate, beam size를 결정하기 위해 Section 22 개발 세트에서 몇 가지 실험을 수행했고, 모든 다른 parameter는 기본 번역 모델에서 변경되지 않았다. 추론 시에는 최대 출력 길이를 입력 길이 + 300으로 늘렸다. beam size 21과 $\alpha = 0.3$을 모든 설정에 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table4.png"
width="878"
height="404"
srcset="https://kurtkim.github.io/p/transformers/images/table4_hua6a988743e74ec9f8c4e6bb5341f2d83_117514_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table4_hua6a988743e74ec9f8c4e6bb5341f2d83_117514_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="521px"
>&lt;/p>
&lt;p>작업 특화 튜닝이 없음에도 이전 모델들보다 더 좋은 성능을 보였고, 이는 Recurrent Neural Network Grammar을 제외한 모든 이전에 보고된 모델들보다 더 좋은 결과를 가져왔다.&lt;/p>
&lt;p>RNN sequence-to-sequence 모델과는 달리, transformer는 오직 WSJ 학습 세트의 40K 문장만을 이용하여 학습했음에도 BerkeleyParser를 능가하는 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>transformer는 attention 기반으로 만들어진 첫 시퀀스 transduction 모델로, encoder-decoder 구조의 recurrent layer를 multi-headed self-attention로 대체하였다.&lt;/p>
&lt;p>transformer는 recurrent나 convolution 기반 아키텍처보다 빠르게 학습되며, WMT 2014 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 달성하였다. 이 중 영어-독일어 작업에서는 이전의 모든 앙상블보다 더 뛰어난 성능을 보여주었다.&lt;/p>
&lt;p>attention 기반 모델을 다른 작업에 적용하고, 텍스트 이외의 다양한 입력과 출력 문제에 transformer를 확장하려 한다. 또한, 큰 이미지, 오디오, 비디오 등을 효율적으로 처리하기 위해 restricted attention mechanism을 연구하고, 생성 과정을 덜 순차적으로 만드는 것을 목표로 하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>