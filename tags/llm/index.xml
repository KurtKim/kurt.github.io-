<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on K2H'log</title><link>https://kurtkim.github.io/tags/llm/</link><description>Recent content in LLM on K2H'log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Thu, 08 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>METALM</title><link>https://kurtkim.github.io/p/metalm/</link><pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/metalm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>파운데이션 모델은 다양한 응용 분야에서 효과적이기 때문에 주목받고 있다. 이 연구에서는 언어 모델을 다양한 파운데이션 모델에 대한 일반적인 인터페이스로 사용하는 것을 제안한다. 이는 causal 모델링과 non-causal 모델링의 장점을 동시에 가져와, bidirectional encoder의 사용으로 미세조정이 쉽고, 문맥 내 학습이나 지시 수행 등을 가능하게 한다. 실험 결과, METALM 모델은 미세조정, zero-shot 일반화, few-shot 학습 등에서 전문 모델들과 경쟁력을 가지거나 능가하는 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction-design-principles">Introduction: Design Principles&lt;/h2>
&lt;p>&lt;strong>Language models as a universal task layer.&lt;/strong> 대규모 언어 모델은 언어, 비전, 다중 모달 작업 등의 일반적인 인터페이스로 사용된다. 언어 모델의 출력 공간은 개방적이어서 다양한 작업에 적용될 수 있다. 예측을 자연어로 설명하는 것이 가능하다면, 그 작업은 언어 모델 기반의 작업 layer에 맞출 수 있다. 예측을 자유 텍스트 시퀀스로 변환하는 것은 자연스럽고, 이를 통해 분류나 질문 응답 등의 목표 레이블과 답변을 텍스트로 변환할 수 있다. 또한, 이러한 작업 layer를 통해 예측 과정이 단일 턴을 넘어서 다중 턴 대화 인터페이스를 구축할 수 있다. 이런 방식의 다양한 작업 통합은 general-purposed AI에 중요하며, 표현, 변환, 표현을 공유 모듈로 통합한다.&lt;/p>
&lt;p>&lt;strong>Causal language modeling (i.e., unidirectional decoder) is conducive to zero-shot generalization and in-context learning.&lt;/strong> GPT-3는 causal 언어 모델 사전 학습에서 나타나는 흥미로운 속성을 보여주었다. causal 언어 모델링의 효율성과 inductive bias로 인해 모델에 원하는 속성을 부여하는 것이 효과적이다. zero-shot과 few-shot 학습 능력은 모델이 일반적인 작업 layer가 되는데 중요하며, 이는 언어 모델이 대규모 텍스트를 통해 세계 지식과 패턴을 배웠음을 보여준다. 이러한 정보는 다양한 작업에 대한 배경 지식과 기본 기술로 활용될 수 있다. 또한, 컨텍스트 내 학습을 통해 사전 학습 된 모델을 새로운 시나리오에 쉽게 적용할 수 있다.&lt;/p>
&lt;p>&lt;strong>Non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities.&lt;/strong> causal 언어 모델은 zero-shot과 few-shot 일반화에 뛰어나지만, BERT와 T5는 masked language modeling으로 사전 학습된 bidirectional encoder를 사용하면 미세 조정 성능이 매우 향상된다는 것을 보여준다. non-causal 모델링은 데이터 인코딩에 에 적합하며, 주석이 달린 데이터가 많이 있는 상황에서 미세 조정의 장점이 도움이 된다. 또한, masked language modeling 목표에 의해 사전 학습된 non-causal encoder는 다국어 설정에 효과적으로 적용된다.&lt;/p>
&lt;p>&lt;strong>Semi-causal language modeling as a meta-pretraining task.&lt;/strong> semi-causal 언어 모델링은 non-causal encoder와 causal 언어 모델을 연결하는 역할을 하며, 이는 사전 학습된 encoder의 보편적 인터페이스 사전 학습의 일부이다. non-causal encoder는 다양한 입력 데이터를 표현하는 것을 배우고, causal 언어 모델은 보편적인 작업 layer로 작동한다. 이러한 방식은 두 모델링 방법의 이점을 동시에 얻을 수 있게 한다. 또한, bidirectional encoder의 출력을 causal decoder에 직접 공급하여 구조를 단순화하며, 이는 여러 bidirectional encoder를 causal 언어 모델에 탑재할 수 있게 한다.&lt;/p>
&lt;p>&lt;strong>Non-causal encoders as System 1, and causal language models as System 2.&lt;/strong> cognition은 일반적으로 직관적이고 무의식적인 시스템 1과 순차적이고 의식적인 계획 및 추론을 하는 시스템 2로 분류된다. 제안된 프레임워크에서는 이 두 가지 시스템이 각각 모듈로 구현되며, BERT와 BEiT와 같은 non-causal encoder는 다양한 입력을 인코딩하는 인식 계층으로서 시스템 1의 역할을 한다. 그 후, 입력 표현은 상식 추론과 계획에 뛰어난 성능을 보이는 causal 언어 모델에 공급되며, 이는 시스템 2의 역할을 하는 보편적인 작업 layer로 설계되었다.&lt;/p>
&lt;p>&lt;strong>Natural language interface between users and pretrained models.&lt;/strong> causal 언어 모델링 기반의 보편적 작업 layer는 사용자가 자연어로 non-causal encoder와 상호작용하게 해준다. 언어는 프로그래밍 언어처럼 모델에 지시를 내릴 수 있고, 모델은 자유롭게 텍스트를 사용해 결과를 제시할 수 있다. 또한, 이 프레임워크는 여러 턴의 대화 상호작용을 지원하며, 각 턴에서 입력을 인터페이스 layer에 제공하고, 반원인 방식으로 응답 결과를 생성할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="metalm-meta-language-model">METALM: Meta Language Model&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/figure2.png"
width="1084"
height="450"
srcset="https://kurtkim.github.io/p/metalm/images/figure2_hu8e0262189a870cf672316529a5048c69_117716_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/figure2_hu8e0262189a870cf672316529a5048c69_117716_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="578px"
>&lt;/p>
&lt;p>다양한 기반 모델과의 상호작용을 지원하는 general-purpose의 인터페이스 역할을 하는 non-causal 언어 모델인 Meta Language Model(METALM)을 제시한다. 구체적으로, 다양한 모달리티를 인식하는 사전 학습된 encoder들이 언어 모델에 도킹한다. 언어 모델은 보편적인 작업 layer(즉, general-purpose의 인터페이스)로 간주되며, 이는 다양한 작업을 자유 텍스트 생성으로 통합한다.&lt;/p>
&lt;p>METALM의 사전 학습을 위해, semi-causal 언어 모델링 작업을 제안하여 모듈을 함께 학습한다. METALM은 언어 모델의 in-context 학습, multi-turn interaction, open-ended generation의 능력을 상속받으며, 기본 모델들은 bidirectional 모델링 덕분에 미세 조정에 유리하다.&lt;/p>
&lt;h3 id="input-representation">Input Representation&lt;/h3>
&lt;p>METALM의 입력 표현은 underlying encoder로부터 얻은 컨텍스트화된 표현과 텍스트의 토큰 임베딩 두 가지 유형으로 분류된다. 이런 표현들은 위치 임베딩과 합산된 후, 일반적인 목적의 인터페이스로 공급된다.&lt;/p>
&lt;h3 id="model-architecture">Model Architecture&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/figure3.png"
width="1072"
height="408"
srcset="https://kurtkim.github.io/p/metalm/images/figure3_hu0cc6c9c55792440e8b5594e698ea5ef8_136687_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/figure3_hu0cc6c9c55792440e8b5594e698ea5ef8_136687_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="262"
data-flex-basis="630px"
>&lt;/p>
&lt;p>세 가지 언어 모델 변형과 제안된 semi-causal 언어 모델의 구조를 요약하면 다음과 같다. causal 언어 모델(GPT 등)은 왼쪽에서 오른쪽으로 변환하는 decoder, 접두사 언어 모델은 encoder-decoder 구조를 사용하며, non-causal 언어 모델은 bidirectional encoder로 구성된다. semi-causal 언어 모델은 unidirectional Transformer decoder와 여러 bidirectional encoder를 가지며, 전체 세션을 왼쪽에서 오른쪽으로 처리하고 일부 범위는 non-causal encoder로 사전 인코딩한다.&lt;/p>
&lt;p>&lt;strong>Backbone Network&lt;/strong> transformer를 사용하여 모델을 구축한다. 입력 시퀀스의 벡터 표현을 패킹한 후, multilayer Transformer를 통해 컨텍스트화된 표현으로 인코딩한다. 각 transformer block은 multi-head self-attention layer와 feed-forward network layer를 포함하며, attention mask는 컨텍스트 접근을 제어한다. 보편적인 작업 layer는 삼각행렬 attention mask를 사용하여 입력을 왼쪽에서 오른쪽으로 처리하고, bidirectional encoder는 모든 토큰이 서로 접근할 수 있게 한다. 마지막으로, 출력 벡터를 softmax classiﬁer로 어휘를 예측하며, 이 가중치 행렬은 입력 토큰 임베딩과 공유된다.&lt;/p>
&lt;p>&lt;strong>Connector&lt;/strong> 보편적인 작업 layer와 다양한 bidirectional encoder 사이의 커넥터 layer는 encoder의 벡터 표현을 투영하고, 기반 모델의 출력 차원을 보편적인 작업 레이어와 맞추는 역할을 한다. 실험에서는 linear projection과 feed-forward network가 잘 작동하는 것으로 나타났다.&lt;/p>
&lt;h3 id="proposed-objective-semi-causal-language-modeling">Proposed Objective: Semi-Causal Language Modeling&lt;/h3>
&lt;p>METALM을 사전 학습하기 위해, semi-causal 언어 모델링을 이용해 시퀀스의 토큰을 autoregressively하게 생성하며, 특정 부분은 bidirectional encoder를 통해 표현되었다.&lt;/p>
&lt;p>입력 시퀀스 $x = x_1, x_2, &amp;hellip;, x_n$가 주어졌을 때, $k$개의 non-causal span $\lbrace x_{s_1}^{e_1}, &amp;hellip;, x_{s_k}^{e_k} \rbrace$를 가정하며, 각 범위에 대해 bidirectional encoder를 사용해 벡터 표현 $h(x_{s_i}^{e_i})$을 얻는다. 이 encoder의 선택은 non-causal span의 모달성에 따라 달라진다.&lt;/p>
&lt;p>non-causal 언어 모델링 목표는 다음과 같이 정의된다:&lt;/p>
&lt;p>$$ max \sum_{i=0}^k \sum_{t=e_i}^{s(i+1)} log \ P(x_t | x_{&amp;lt; t}, \lbrace h(x_{s_i}^{e_i}) \rbrace_{j&amp;lt;i}) $$&lt;/p>
&lt;p>여기서 $e_0 = 1$, $s_{(k+1)}$ = n, 그리고 $\lbrace h(x_{s_i}^{e_i}) \rbrace_{j&amp;lt;i} = \lbrace h(x_{s_1}^{e_1}), &amp;hellip;, h(x_{s_{(i-1)}}^{e_{(i-1)}}) \rbrace$ 이다. non-causal 언어 모델링 목표는 각 non-causal 범위의 다음 토큰이 해당 범위의 마지막 위치에서 생성되며, non-causal 범위의 수와 위치는 무작위로 샘플링되고, 이 범위들은 서로 겹치지 않는다.&lt;/p>
&lt;p>제안된 목표를 이용해 일반적인 인터페이스와 기반 모델을 함께 사전 학습하며, 이를 통해 이들을 매끄럽게 연결한다. 이는 언어 전용 설정과 시각-언어 설정 모두에 대해 METALM을 사전 학습하는 데 사용된다.&lt;/p>
&lt;h3 id="capabilities-on-downstream-tasks">Capabilities on Downstream Tasks&lt;/h3>
&lt;p>&lt;strong>In-Context Learning&lt;/strong> METALM은 parameter 업데이트 없이 자연어 지시나 입력-출력 쌍에 의해 새로운 작업에 적응한다. 이는 k-shot 학습을 통해 이루어지며, 각 입력은 bidirectional encoding 후 일반적인 인터페이스에 입력된다. 이렇게 하면 METALM은 보이지 않는 예의 목표 출력을 예측할 수 있다. zero-shot 일반화의 경우, 작업 지시와 함께 예제가 bidirectional encoder에 입력되며, 목표 출력은 보편적인 작업 계층에 의해 생성된다.&lt;/p>
&lt;p>&lt;strong>Finetuning&lt;/strong> downstream task에 대한 많은 주석 예제가 있을 때, 미세 조정은 매우 유용하다. 모든 작업을 자유형 텍스트로 변환하는 개방형 생성 형식으로 통합한다. 이 과정에서 METALM은 bidirectionally encoding 된 입력을 기반으로 목표 출력을 생성하도록 학습하고, 이를 통해 bidirectionally encoder의 뛰어난 미세 조정 능력을 이어받는다.&lt;/p>
&lt;p>&lt;strong>In-Context Customization&lt;/strong> 먼저 모델을 대량의 데이터에 대해 미세 조정하고, in-context 학습을 사용하여 모델을 맞춤화한다. 이렇게 하면 레이블이 있는 데이터의 지식을 새 작업에 쉽게 전달할 수 있다. METALM은 causal 및 non-causal 모델링의 장점을 결합하여 non-causal 모델링의 우수한 미세 조정 성능과 causal 모델링의 in-context 학습을 가능하게 한다.&lt;/p>
&lt;p>&lt;strong>Multimodal Multi-Turn Interaction&lt;/strong> METALM은 사용자와의 multi-turn interaction을 지원하며, 다양한 형태의 입력을 인코딩하여 응답을 생성한다. 이는 이전 대화를 기반으로 자연스럽게 대화형 인터페이스로 작동하며, 텍스트 이외의 여러 형태의 정보를 포함할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments-on-language-only-tasks">Experiments on Language-Only Tasks&lt;/h2>
&lt;p>먼저 언어만을 기반으로 한 데이터셋에서 실험을 진행하여 METALM의 다양성과 효과를 보여준다. 여기서 non-causal encoder는 보편적인 작업 계층에 도킹하는 사전 학습된 언어 기반 모델이다. 이러한 매력적인 능력은 사전 학습을 통해 나타나며, 이를 통해 일반적인 인터페이스가 작업과 시나리오를 가로질러 전환할 수 있다.&lt;/p>
&lt;h3 id="evaluation-settings">Evaluation Settings&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table1.png"
width="980"
height="244"
srcset="https://kurtkim.github.io/p/metalm/images/table1_hu4f70eef8e3417278ede67b3f5ded8d6a_53583_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table1_hu4f70eef8e3417278ede67b3f5ded8d6a_53583_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="401"
data-flex-basis="963px"
>&lt;/p>
&lt;p>METALM의 다양한 능력, 즉 multitask 미세 조정, single-task 미세 조정, instruction tuning, 그리고 in-context 학습을 보여준다. 이러한 능력은 작업에 구애받지 않고 널리 적용되어 스킬 적용과 사용자와의 커뮤니케이션을 용이하게 한다. 또한, 이 능력은 미세 조정과 in-context 학습의 결합에 기반한 평가 설정을 가능하게 한다. 작업이 자유형 텍스트 형식으로 통합되어, 같은 인터페이스를 통해 다양한 downstream task를 처리할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/figure4.png"
width="1088"
height="856"
srcset="https://kurtkim.github.io/p/metalm/images/figure4_hu8d8b078e5940c0dc3101917eea9b7c04_213015_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/figure4_hu8d8b078e5940c0dc3101917eea9b7c04_213015_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>입력 예시와 지시문은 non-causal 언어 encoder로 전달되며, 그 결과는 범용 작업 계층에서 생성된다. 예측은 개방된 방식으로 만들어져 다양한 결과를 가져온다.&lt;/p>
&lt;h3 id="pretraining-setup">Pretraining Setup&lt;/h3>
&lt;p>METALM은 sinusoidal position embedding을 사용하며, 24개의 layer와 각 layer 당 32개의 attention head, hidden dimension은 2048이다. non-causal 부분에는 encoder-only Transformer를 사용하며, learnable position embedding과 relative position bias을 활용한다. 또한, Transformer에 대해 DeepNorm을 사용하며, 커넥터 모듈은 linear projection layer이다.&lt;/p>
&lt;p>non-causal 모델과 semi-causal 모델의 최대 입력 길이는 각각 512와 2048이다. 64~128 길이의 무작위 범위를 샘플링하여 non-causal 부분에 제공하며, 이 범위는 원래 시퀀스 길이의 25%를 차지한다. semi-causal 언어 모델은 처음부터 사전 학습하며, non-causal 모듈은 사전 학습된 bidirectional encoder로부터 초기화된다. 사전 학습 동안 non-causal encoder의 대부분 parameter를 고정한다. METALM은 1024의 batch size로 300k step 동안 사전 학습되며, Adam을 optimizer로 사용한다. semi-causal 모델의 드롭아웃은 비활성화하고, non-causal 모델의 드롭아웃 비율은 0.1로 설정한다. learning rate는 warm-up과 함께 6e-4를 사용한다.&lt;/p>
&lt;p>다양한 데이터 소스로 구성된 대규모 영어 텍스트 데이터셋인 Pile에서 모델을 사전 학습한다. GitHub, arXiv, PubMed Central은 제외하였다. 이 데이터는 SentencePiece로 토큰화되며, &amp;ldquo;full-sentence&amp;rdquo; 형식으로 입력을 구성한다. 즉, 각 입력 시퀀스는 연속적으로 샘플링된 전체 문장으로 이루어진다. 그리고 추가적으로 세 가지 특수 토큰($&amp;lt;$s$&amp;gt;$, $&amp;lt;$/s$&amp;gt;$, $&amp;lt;$/d$&amp;gt;$)을 사용하여 시퀀스 시작, 문단 끝, 문서 끝을 표시한다.&lt;/p>
&lt;h3 id="multitask-finetuning">Multitask Finetuning&lt;/h3>
&lt;p>METALM을 multitask 미세 조정 환경에서 평가한다. 다양한 작업들을 open-ended generation 방식으로 통합하였고, 이를 통해 어떠한 작업 특정 아키텍처 없이도 보편적 작업 layer에서 처리할 수 있다. 미세 조정 동안에는 무작위로 학습 예제를 샘플링하고, 이를 bidirectional language encoder에 입력한다. 미세 조정의 목표는 인터페이스에서 생성된 정확한 레이블의 가능성을 최대화하는 것이다.&lt;/p>
&lt;p>언어 이해 작업과 생성 작업을 포함한 10개의 작업 군집으로 그룹화된 34개의 NLP 데이터셋의 혼합에 대한 실험을 실시한다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Natural Language Inference:&lt;/strong> ANLI (R1-R3), CB, MNLI, QNLI, RTE, SNLI, WNLI&lt;/li>
&lt;li>&lt;strong>Sentiment Classiﬁcation:&lt;/strong> IMDB, SST-2, Sentiment140, Yelp&lt;/li>
&lt;li>&lt;strong>Paraphrase Detection:&lt;/strong> QQP, MRPC, Paws Wiki&lt;/li>
&lt;li>&lt;strong>Coreference Resolution:&lt;/strong> DPR, Winogrande, WSC&lt;/li>
&lt;li>&lt;strong>Commonsense Reasoning:&lt;/strong> HellaSwag, PiQA, COPA&lt;/li>
&lt;li>&lt;strong>Reading Comprehension:&lt;/strong> DROP, SQuADv1, SQuADv2, OBQA, BoolQ&lt;/li>
&lt;li>&lt;strong>Miscellaneous:&lt;/strong> CoLA, WiC, TREC&lt;/li>
&lt;li>&lt;strong>Closed-Book QA:&lt;/strong> ARC-easy, NQ&lt;/li>
&lt;li>&lt;strong>Struct to Text:&lt;/strong> CommonGen, E2ENLG&lt;/li>
&lt;li>&lt;strong>Summarization:&lt;/strong> AESLC, SamSum, XSum&lt;/li>
&lt;/ul>
&lt;h4 id="evaluation-setup">Evaluation Setup&lt;/h4>
&lt;p>METALM은 30k의 최대 학습 예제 수를 가진 다양한 데이터셋에 대해 미세 조정된다. 다중 선택 작업인 경우 모든 가능한 옵션들이 템플릿에 포함된다. 예를 들어, 감성 분류 데이터셋에서는 모델이 &amp;ldquo;Positive&amp;rdquo; 또는 &amp;ldquo;Negative&amp;quot;을 생성함으로써 텍스트의 감성을 판단한다. 이 과정은 Wei et al. (2021)에서 사용된 프롬프트를 따른다.&lt;/p>
&lt;p>METALM은 256의 batch size로 20k step 동안 미세 조정되며, 입력과 답변 토큰의 총 길이는 2048로 제한된다. batch 친화적인 계산을 위해 여러 학습 예제를 하나의 시퀀스로 패킹하며, learning rate은 1e-4로 설정한다.&lt;/p>
&lt;p>multi-choice 작업에서는 디코딩 제약 없이 정확도 점수를, SQuAD, DROP, closed-book QA 데이터셋에서는 greedy 디코딩을 사용한 F1 점수를 보고한다. struct2text와 요약 군집에서는 beam size 4, length penalty $\alpha = 0.6$의 beam size을 사용하며, 이 두 군집에 대해 ROUGE 점수를 보고한다.&lt;/p>
&lt;h4 id="results">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table2.png"
width="674"
height="386"
srcset="https://kurtkim.github.io/p/metalm/images/table2_hud0a69c2b0cc7a0fe5a4a3ed720f773a5_66713_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table2_hud0a69c2b0cc7a0fe5a4a3ed720f773a5_66713_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="419px"
>&lt;/p>
&lt;p>METALM과 GPT의 다중 작업 미세 조정 결과를 비교하며, 동일한 설정을 사용한다. 각 결과는 작업 군집의 평균 점수를 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/figure5.png"
width="1084"
height="302"
srcset="https://kurtkim.github.io/p/metalm/images/figure5_hu8956de61744b4ebcc434aceaba916cf7_50788_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/figure5_hu8956de61744b4ebcc434aceaba916cf7_50788_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="358"
data-flex-basis="861px"
>&lt;/p>
&lt;p>METALM은 거의 모든 작업 군집에서 GPT를 크게 앞선다는 것을 확인하였다. 이는 non-causal encoder로부터 미세 조정 능력을 상속받았기 때문이다. 특히, METALM은 자연어 이해 작업에서 뛰어난 성과를 보이며, 이는 non-causal 모델링이 미세 조정에 유리하다는 것을 부분적으로 확인한다. METALM의 성능 향상은 자연어 추론과 독해 등의 도전적인 작업에서 두드러지며, 언어 생성, 닫힌 책 질문 응답, 텍스트 요약 등에서도 GPT를 능가한다.&lt;/p>
&lt;h3 id="single-task-finetuning">Single-Task Finetuning&lt;/h3>
&lt;p>데이터가 많은 상황에서 METALM의 미세 조정 능력을 탐색하고, 새로운 미세 조정 방법을 설계한다. 이 방법은 언어 모델을 고정하고 비인과적 non-causal parameter만을 업데이트한다. 이 전략은 탁월한 성능을 보이며, 문맥 학습과 개방성을 유지함을 보여준다.&lt;/p>
&lt;h4 id="finetuning-setup">Finetuning Setup&lt;/h4>
&lt;p>자연어 추론 데이터셋 MNLI에서 single-task 미세 조정을 수행한다. 작업은 주어진 전제에 대해 가설이 참인지, 거짓인지, 아니면 결정되지 않았는지를 판단하는 것이다. 미세 조정 동안, 일반 인터페이스는 고정되고 non-causal encoder와 커넥터만 업데이트된다. 반면 GPT 기준선에 대해선 모든 parameter가 업데이트된다. METALM과 GPT는 learning rate 5e-5와 batch size 32로 3 epoch 동안 미세 조정된다.&lt;/p>
&lt;h4 id="results-1">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table3.png"
width="542"
height="254"
srcset="https://kurtkim.github.io/p/metalm/images/table3_hu2f4170931ae1f96eacb030f6d5a1cc3a_38937_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table3_hu2f4170931ae1f96eacb030f6d5a1cc3a_38937_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="512px"
>&lt;/p>
&lt;p>METALM은 훨씬 적은 parameter를 업데이트하면서도 MNLI의 정확도를 3.4포인트 향상시킨다. 결과는 bidirectional encoder가 미세 조정 성능에 이점을 준다는 것을 보여준다. 또한, BERT, RoBERTa, ELECTRA 같은 bidirectional 언어 encoder의 미세 조정에서 파생된 세 가지 강력한 기준선과 비교하여 METALM은 비슷하거나 더 나은 성능을 보여준다.&lt;/p>
&lt;h3 id="instruction-tuned-zero-shot-generalization">Instruction-Tuned Zero-Shot Generalization&lt;/h3>
&lt;p>METALM에 대한 instruction tuning을 통해 모델을 다양한 작업에 미세 조정하고, instruction following과 zero-shot 일반화 성능을 평가한다. 특정 데이터셋에서 평가할 때, 동일한 작업 군집의 모든 데이터셋은 학습 단계에서 제외된다. 예를 들어, 분류 데이터셋 SST-2에서 평가할 경우, 감성 분석 전체 군집은 instruction tuning 동안 제외된다.&lt;/p>
&lt;h4 id="instruction-tuning-setup">Instruction-Tuning Setup&lt;/h4>
&lt;p>METALM과 GPT를 사용하여 요약 군집을 제외한 데이터셋 혼합에 대해 instruction tuning을 수행하며, FLAN에서 제안한 평가 파이프라인을 따른다. 각 데이터셋에 대해, FLAN에 의해 수동으로 작성된 10가지 템플릿 중 하나를 무작위로 적용한다. 이 중 일부 템플릿은 작업을 &amp;ldquo;turned the task around&amp;rdquo; 방식으로 학습 다양성을 높인다. 예를 들어, 감성 분류 작업에서는 모델이 주어진 &amp;ldquo;Positive&amp;rdquo; 감성 라벨에 기반한 영화 리뷰를 생성하도록 한다.&lt;/p>
&lt;p>자연어 추론, 감성 분류, 패러프레이즈 탐지, 읽기 이해를 포함한 네 가지 작업 군집에서 METALM과 GPT를 사용하여 실험을 진행한다. 패러프레이즈 군집은 추론 군집에서, 그 반대도 마찬가지로 평가 시 제외된다. METALM과 GPT는 batch size 512로 30k step 동안 미세 조정되며, learning rate는 1e-4로 설정된다. 각 예제의 시퀀스 길이는 1024로 제한되며, 데이터 패킹 전략을 사용하여 효율성을 향상시킨다.&lt;/p>
&lt;h4 id="results-2">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table4.png"
width="636"
height="982"
srcset="https://kurtkim.github.io/p/metalm/images/table4_hu4b683ea65adeab9802bf0bc831d95543_175744_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table4_hu4b683ea65adeab9802bf0bc831d95543_175744_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="64"
data-flex-basis="155px"
>&lt;/p>
&lt;p>METALM은 다양한 템플릿을 사용하여 평균과 최고 점수 모두에서 GPT 기준선을 크게 능가하는 것으로 나타났다. 이는 semi-causal 언어 모델링의 효과를 보여준다. 특히, 자연어 추론, 감성 분석, 패러프레이즈 탐지, 읽기 이해 등의 작업에서 METALM은 일관되게 높은 성능을 보여주었다.&lt;/p>
&lt;p>instruction tuning은 미세 조정과 zero-shot 일반화 능력을 모두 필요로 한다. 실험 결과, METALM은 bidirectional encoder를 통해 우수한 미세 조정 성능을 달성하면서도, causal 언어 모델의 zero-shot 일반화 능력을 유지함으로써, causal과 non-causal 언어 모델의 장점을 모두 활용하였다.&lt;/p>
&lt;h3 id="in-context-learning">In-Context Learning&lt;/h3>
&lt;p>METALM과 GPT의 in-context 학습 성능을 비교한다. 작업 지시와 input-label 쌍에 따라 언어 모델은 parameter를 업데이트하지 않고 원하는 downstream task로 재조정된다. 예제 입력은 non-causal encoder를 통과하고, 레이블 토큰은 원래의 임베딩을 사용한다. 그 다음, 테스트 입력의 대상 레이블은 범용 작업 계층에서 생성된다.&lt;/p>
&lt;h4 id="evaluation-setup-1">Evaluation Setup&lt;/h4>
&lt;p>zero-shot, one-shot, few-shot 설정에서 실험을 진행하고, GPT-3의 평가 프로토콜을 따른다. 학습 세트에서 무작위로 샘플링한 예제를 사용하여 테스트 예제를 평가하며, Winograd의 경우 테스트 세트에서 직접 샘플링한다. few-shot 설정에서는 모든 예제가 구분자 토큰 $&amp;lt;$/s$&amp;gt;$로 구분된다.&lt;/p>
&lt;p>METALM과 GPT 기준선은 cloze과 completion task, Winograd-style task, commonsense reasoning, 그리고 SuperGLUE 벤치마크의 BoolQ와 Copa 등 총 아홉 가지 작업에서 평가된다.&lt;/p>
&lt;h4 id="results-3">Results&lt;/h4>
&lt;p>METALM은 GPT에 비해 더 좋거나 비슷한 성능을 보인다. 특히 Winograd와 완성 작업에서는 GPT보다 더욱 향상된 성능을 보였으며, zero-shot과 few-shot 설정에서도 더 나은 평균 결과를 보인다. 이는 METALM이 탁월한 컨텍스트 내 학습 능력을 가지며, non-causal encoder의 문맥화된 표현이 모델의 일반화를 돕는다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments-on-vision-language-tasks">Experiments on Vision-Language Tasks&lt;/h2>
&lt;p>이미지와 텍스트를 결합한 vision-language 설정에서 실험을 진행한다. underlying non-causal encoder는 이미지-텍스트 쌍을 분석하고, 이미지 토큰을 텍스트 토큰 앞에 추가해 bidirectional fused representation을 생성한다. bidirectional fused representation을 기반으로 causal decoder는 남은 토큰을 순차적으로 예측한다. text-only 데이터도 활용되며, 이미지-텍스트 데이터와 텍스트만의 데이터를 함께 사전 학습하고 있다.&lt;/p>
&lt;h3 id="evaluation-settings-1">Evaluation Settings&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table6.png"
width="1060"
height="312"
srcset="https://kurtkim.github.io/p/metalm/images/table6_hub97ddaf18d7e4a5fc73543778205a624_78817_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table6_hub97ddaf18d7e4a5fc73543778205a624_78817_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="339"
data-flex-basis="815px"
>&lt;/p>
&lt;p>다양한 실험을 통해 zero-shot 일반화, 문맥 학습, 파인튜닝을 진행하며, 이 과제들은 시각적 질문 응답, 시각적 추론, 이미지 캡셔닝, 설명 생성 등의 카테고리로 나뉜다. 9개의 데이터셋을 활용한 평가는 이해력과 생성력 모두를 측정한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/figure6.png"
width="1068"
height="772"
srcset="https://kurtkim.github.io/p/metalm/images/figure6_huf11ce7a5a6442629457133ece99efa22_271161_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/figure6_huf11ce7a5a6442629457133ece99efa22_271161_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="332px"
>&lt;/p>
&lt;p>입력 이미지와 프롬프트는 vision-language encoder에 공급되며, 목표 출력은 언어 모델에 의해 생성된다. 모든 작업들은 개방형 생성 방식으로 구성된다.&lt;/p>
&lt;h3 id="pretraining-setup-1">Pretraining Setup&lt;/h3>
&lt;p>12-layer non-causal vision-language encoder와 24-layer 언어 모델을 사용하며, 이는 GPT-2의 아키텍처를 따른다. hidden size는 1024, attention head는 16개이며, sinusoidal position embedding을 사용한다. parameter의 수는 총 353M이다. non-causal encoder에는 192M의 parameter를 갖는 VLMo 방식의 사전 학습된 vision-language 모델을 사용하며, 이미지는 224x224 해상도로 사전 학습된다. 커넥터는 three-layer feed-forward network이다.&lt;/p>
&lt;p>METALM은 batch size 256으로 350k step에 걸쳐 사전 학습되며, 이 과정에서 $\beta_1 = 0.9$, $\beta_2 = 0.98$의 AdamW optimizer를 사용한다. learning rate는 1e-4, weight decay는 0.01이며, 처음 2,500step에서 warm-up을 적용하고 linear decay를 사용한다. dropout rate는 0.1이다.&lt;/p>
&lt;p>METALM은 이미지-텍스트 쌍과 텍스트 문서를 이용해 사전 학습된다. 이미지-텍스트 쌍은 Conceptual Captions, Visual Genome, COCO Caption, SBU Caption 데이터셋을 활용하며, 총 400만 개의 이미지와 1000만 개의 이미지-텍스트 쌍이 있다. 텍스트 문서는 Reddit 웹 텍스트의 오픈소스 재현인 OpenWebText 말뭉치를 사용한다.&lt;/p>
&lt;h3 id="zero-shot-generalization">Zero-Shot Generalization&lt;/h3>
&lt;p>METALM의 zero-shot 일반화 능력을 평가하기 위해, 이미지 캡셔닝과 시각적 질문 응답 두 가지 작업을 수행한다. 이미지 캡셔닝에서는 주어진 이미지의 설명을 생성하며, 시각적 질문 응답에서는 이미지에 대한 질문에 올바른 답변을 예측한다.&lt;/p>
&lt;h4 id="evaluation-setup-2">Evaluation Setup&lt;/h4>
&lt;p>추론 시에는 greedy decoding을 사용하며, 입력 이미지는 224x224로 크기를 조정한다. 두 가지 작업에 대한 데이터셋과 설정은 다음과 같다:&lt;/p>
&lt;p>&lt;strong>Image Captioning&lt;/strong> MS COCO Caption, NoCaps, Flickr30k에서 zero-shot 캡션 생성을 평가한다. COCO Karpathy 분할의 테스트 세트, NoCaps와 Flickr30k의 검증 및 테스트 세트에서 평가를 진행하며, BLEU, CIDEr, METEOR, SPICE를 캡션 생성 지표로 사용한다. 점수는 COCOEvalCap2를 통해 계산되며, 모든 zero-shot 캡션 생성 실험에서는 METALM에 &amp;ldquo;Summarize this image:&amp;rdquo; 라는 프롬프트를 제공한다.&lt;/p>
&lt;p>&lt;strong>Visual Question Answering&lt;/strong> VQAv2 검증 세트와 OK-VQA 테스트 세트에서 zero-shot 성능을 평가하며, VQA 점수는 VQAv2 평가 코드의 정규화 규칙을 사용해 계산한다. METALM은 사전에 정의된 답변 세트가 아닌, 개방형 생성 방식으로 답변을 예측한다. 시각적 질문 응답 실험에서는 &amp;ldquo;question: question text answer:&amp;rdquo; 템플릿으로 METALM을 프롬프트한다.&lt;/p>
&lt;h4 id="results-4">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table7.png"
width="826"
height="214"
srcset="https://kurtkim.github.io/p/metalm/images/table7_hu7c6090df9d94f8eac9b3d8bc263d7bec_50126_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table7_hu7c6090df9d94f8eac9b3d8bc263d7bec_50126_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="926px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table8.png"
width="722"
height="210"
srcset="https://kurtkim.github.io/p/metalm/images/table8_hua9a14cf6f56388c27acc164a647a5dc7_43394_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table8_hua9a14cf6f56388c27acc164a647a5dc7_43394_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="343"
data-flex-basis="825px"
>&lt;/p>
&lt;p>METALM은 세 이미지 캡션 데이터셋에서 다른 최신 모델들을 능가한다. 비교 대상인 FewVLM 모델은 이미지 캡셔닝에 다양한 프롬프트를 사용하지만, 이 모델은 모든 실험에서 &amp;ldquo;Summarize this image:&amp;rdquo; 라는 동일한 프롬프트를 사용하여 견고하게 캡션을 생성한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table9.png"
width="644"
height="168"
srcset="https://kurtkim.github.io/p/metalm/images/table9_hu4432ff5a3ccc9007e11f614b1b27efac_36818_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table9_hu4432ff5a3ccc9007e11f614b1b27efac_36818_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="383"
data-flex-basis="920px"
>&lt;/p>
&lt;p>두 데이터셋 모두에서 METALM은 더 많은 parameter를 갖는 Frozen과 VLKD보다 우수한 결과를 보여준다. 특히, 외부 지식이 요구되는 OK-VQA에서도 좋은 성능을 보여, METALM의 언어 모델이 지식 소스로 활용될 수 있음을 보여준다. 객체 정보가 vision encoder에 의해 인식되면, universal task layer는 이를 언어 모델링하여 답변을 생성한다.&lt;/p>
&lt;p>다섯 개의 데이터셋에서의 실험 결과, METALM은 zero-shot 일반화와 개방형 생성 능력을 가지며, 프롬프트를 통해 이미지 캡셔닝과 시각적 질문 응답에 대해 사전 학습된 모델을 재활용할 수 있다.&lt;/p>
&lt;h3 id="in-context-learning-1">In-Context Learning&lt;/h3>
&lt;p>시각적 질문 응답에서 in-context 학습 능력을 평가하며, parameter를 미세조정하지 않고 k개의 데모를 이용한 k-shot 학습을 진행한다.&lt;/p>
&lt;h4 id="evaluation-setup-3">Evaluation Setup&lt;/h4>
&lt;p>VQAv2 검증 세트와 OK-VQA 테스트 세트에서 few-shot 실험을 진행하며, 각 테스트 인스턴스에 대해 학습 세트에서 최대 네 개의 예시를 무작위로 샘플링한다. 예측된 답변은 VQAv2 평가 코드의 정규화 규칙에 따라 평가되며, 추론 시에는 224x224의 이미지 해상도를 사용한다.&lt;/p>
&lt;p>테스트 입력 전에 몇 가지 예시를 두고 universal task layer에서 예측을 얻는다. 전체 예시는 이미지, 질문, 답변을 나타내는 $[i, q, a]$로 표시하며, 테스트 입력은 $[i, q]$로 표시한다. k-shot in-context 학습에서는 전체 입력 시퀀스가 $e_1, &amp;hellip;, e_k, t$가 된다. &amp;ldquo;Question: [question text] Answer:&amp;rdquo; 프롬프트를 통해 METALM을 지시하고, greedy 디코딩을 사용하여 답변을 생성한다.&lt;/p>
&lt;h4 id="results-5">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table10.png"
width="698"
height="180"
srcset="https://kurtkim.github.io/p/metalm/images/table10_hu123ae3c1cf1de43064db46a10d8b43b7_35411_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table10_hu123ae3c1cf1de43064db46a10d8b43b7_35411_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="387"
data-flex-basis="930px"
>&lt;/p>
&lt;p>in-context 시연을 추가하면 zero-shot 일반화보다 성능이 향상되며, 더 많은 예시를 추가할수록 성능 개선이 더욱 커진다. 작은 모델 크기를 사용한 METALM은 Frozen에 비해 더 나은 성능을 보여준다. METALM은 기본 vision-language 모델을 수정하지 않고도 시각적 질문 응답에서 in-context 학습을 수행할 수 있으며, universal task layer의 도움으로 기존 모델에 in-context 학습 능력을 추가할 수 있다.&lt;/p>
&lt;h3 id="finetuning-on-downstream-tasks">Finetuning on Downstream Tasks&lt;/h3>
&lt;p>사전 학습된 METALM을 다양한 vision-language 작업에 미세 조정하고, 이를 강력한 판별 모델과 최근의 생성 모델과 비교한다. 이 작업들에는 이미지 캡셔닝, 시각적 질문 응답, 시각적 추론, 설명 가능한 시각적 추론이 포함된다.&lt;/p>
&lt;h4 id="finetuning-setup-1">Finetuning Setup&lt;/h4>
&lt;p>모든 작업에서는 384x384 해상도를 사용하고 이미지 증강에는 RandAugment를 적용한다. 모든 데이터셋의 learning rate는 1e-5로 고정되어 있다.&lt;/p>
&lt;p>&lt;strong>Visual Question Answering&lt;/strong> VQAv2, VQA Karpathy 분할, OK-VQA에서 평가를 진행한다. 모델은 각 데이터셋의 학습 세트와 검증 세트에서 미세 조정되며, 해당 테스트 세트에서의 VQA 점수를 보고한다. METALM은 VQAv2와 VQA Karpathy 분할에서 140k step, OK-VQA에서 10k step 동안 미세 조정된다. &amp;ldquo;Question: [question text] Answer: [answer text]&amp;rdquo; 프롬프트를 generative 미세 조정에 사용한다.&lt;/p>
&lt;p>&lt;strong>Visual Reasoning&lt;/strong> NLVR 2 데이터셋에서 평가를 진행한다. 이 데이터셋의 예시는 두 이미지와 한 문장으로 구성되며, 이들을 개별 이미지-텍스트 쌍으로 재분할하여 각각의 표현을 얻는다. 이 표현들의 연결을 활용하여 예 또는 아니오 예측을 생성하며, generative 미세 조정을 위해 &amp;ldquo;it is [label]&amp;ldquo;을 적용한다. METALM은 5 epoch 동안 미세 조정된다.&lt;/p>
&lt;p>&lt;strong>Image Captioning&lt;/strong> COCO 캡션 데이터셋의 Karpathy 분할에서 평가를 진행한다. BLEU-4, CIDEr, METEOR, SPICE를 평가 지표로 사용하며, 이 결과들은 강화된 CIDEr 최적화 없이 cross-entropy 미세 조정에서 얻는다. 미세 조정 중에는 객체 태그를 사용하지 않으며, &amp;ldquo;caption: [caption text]&amp;rdquo; 프롬프트를 사용하여 METALM을 학습 분할에서 100k step 동안 미세 조정한다.&lt;/p>
&lt;p>&lt;strong>Explainable Visual Reasoning&lt;/strong> E-SNLI-VE 데이터셋에서 평가를 진행한다. 이 데이터셋은 이미지-텍스트 쌍 사이의 포함 레이블 예측과 동시에 그 예측에 대한 설명을 생성하는 것을 모델에 요구한다. METALM은 7 epoch 동안 미세 조정되며, &amp;ldquo;it [entailment label] because [explanation].&amp;rdquo; 프롬프트를 generative 미세 조정에 사용한다.&lt;/p>
&lt;h4 id="results-visual-question-answering-and-visual-reasoning">Results: Visual Question Answering and Visual Reasoning&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table11.png"
width="1000"
height="400"
srcset="https://kurtkim.github.io/p/metalm/images/table11_hu01b5b8d5d074ace6afe56711bb651cc9_109277_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table11_hu01b5b8d5d074ace6afe56711bb651cc9_109277_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>METALM은 모든 데이터셋에서 강력한 성능을 보여주며, generative 예측을 가진 이전 모델들을 능가하고 discriminative vision-language 모델과 경쟁하거나 더 나은 결과를 보인다. 특히, 시각적 질문 응답과 같이 개방형 예측이 필요한 작업에서 이 모델의 장점이 드러난다. 이는 VQA Karpathy-test의 out-domain 세트에서 확인할 수 있다.&lt;/p>
&lt;p>out-domain 예시로 일반화하는 것은 어렵다. 그러나 모든 모델 중 METALM이 out-domain 결과에서 가장 좋은 성능을 보여주며, 다른 데이터셋에서도 일관되게 경쟁력 있는 결과를 나타낸다. 이와 대조적으로 이전의 generative 모델들은 out-domain 세트에서는 더 나은 결과를 보이지만 다른 데이터셋에서는 성능이 떨어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table12.png"
width="484"
height="284"
srcset="https://kurtkim.github.io/p/metalm/images/table12_huf1afac2ef5544c6a065529bf1c3d73cc_54514_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table12_huf1afac2ef5544c6a065529bf1c3d73cc_54514_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="170"
data-flex-basis="409px"
>&lt;/p>
&lt;p>OK-VQA 데이터셋은 모델이 질문에 답하기 위해 외부 지식을 활용하도록 요구하는데, 이전 방법들은 주로 지식 베이스를 활용해 후보 답변을 필터링한다. 하지만, METALM은 사전 학습 과정에서 획득한 풍부한 세계 지식을 활용할 수 있는 유연성을 제공하며, 이를 통해 추가적인 지식 베이스에 의존하지 않고 이 작업에서 큰 개선을 이룬다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table13.png"
width="612"
height="284"
srcset="https://kurtkim.github.io/p/metalm/images/table13_hu381317dc3c33690267cc85d5a96a05a4_50375_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table13_hu381317dc3c33690267cc85d5a96a05a4_50375_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="517px"
>&lt;/p>
&lt;p>METALM은 포함 레이블과 설명을 함께 생성하도록 학습되며, 이전 방법들과 비교해 가장 높은 정확도를 보여준다. 또한, METALM은 설명을 활용하여 포함 레이블 예측의 성능을 향상시키는 장점을 가지고 있다. 이는 METALM이 사용자와 기반 모델 간의 상호작용을 촉진하는데 사용될 수 있음을 보여준다.&lt;/p>
&lt;p>다양한 데이터셋에서 보여진 경쟁력 있는 결과는 METALM의 미세 조정에 bidirectional 모델링이 유리하다는 것을 보여준다. 이로 인해 미세 조정과 open-ended 예측에서 동시에 좋은 성능을 얻을 수 있다.&lt;/p>
&lt;h4 id="results-visually-grounded-language-generation">Results: Visually Grounded Language Generation&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table14.png"
width="804"
height="256"
srcset="https://kurtkim.github.io/p/metalm/images/table14_hu3385c87d11ba69beef79c19429876bd7_66793_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table14_hu3385c87d11ba69beef79c19429876bd7_66793_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="753px"
>&lt;/p>
&lt;p>CIDEr 최적화 없이 직접 비교한 결과, METALM이 다른 모델들보다 훨씬 개선된 성능을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/metalm/images/table14.png"
width="804"
height="256"
srcset="https://kurtkim.github.io/p/metalm/images/table14_hu3385c87d11ba69beef79c19429876bd7_66793_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/metalm/images/table14_hu3385c87d11ba69beef79c19429876bd7_66793_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="753px"
>&lt;/p>
&lt;p>E-SNLI-VE에서 METALM은 포함 레이블과 설명을 함께 생성하며, 대부분의 지표에서 이전 모델들을 능가한다. 또한, 이해와 설명 생성 모두에서 좋은 성능을 보여준다. 반면, Sammani et al. (2022)의 방법은 설명 생성에서는 경쟁력 있는 성능을 보이지만, 포함 분류에서는 더 낮은 정확도를 보여준다.&lt;/p>
&lt;p>시각적 기반 언어 생성의 결과는 다양한 sequence-to-sequence 학습 문제에 적용 가능한 METALM의 일반성을 보여준다. METALM은 vision-language 셍성 작업에 대한 미세 조정을 통해 좋은 성능을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="language-model-pretraining">Language Model Pretraining&lt;/h3>
&lt;p>대규모 언어 모델 사전 학습은 다양한 작업에서 뛰어난 성능을 보여주고 많은 연구 관심을 끌었다. 모델들의 차이는 주로 사전 학습 목표와 아키텍처에 있다. GPT는 few-shot과 in-context 학습을 가능하게 하는 인과적 언어 모델을 사전 학습하며, 최근의 연구들은 데이터와 모델 크기를 확장하는 데 초점을 맞추고 있다. T5와 BART 등의 연구는 모든 작업을 text-to-text 형식으로 변환하거나 오염된 문서에서 원문을 재구성하는 등 자연어 이해와 생성 작업 모두를 처리할 수 있는 프레임워크를 조사하였다. 이 논문의 작업에서는 semi-causal 언어 모델링을 도입하여 미세 조정 성능을 향상시키고 in-context 학습 능력을 활용하였으며, 이를 통해 다양한 기반 모델에 대한 일반적인 목적의 인터페이스를 구축할 수 있게 되었다.&lt;/p>
&lt;h3 id="general-purpose-modeling">General-Purpose Modeling&lt;/h3>
&lt;p>다양한 작업, 변환, 모달리티를 지원하는 공유 모듈에서의 일반 목적 모델에 대한 연구가 있다. MT-DNN은 다중 작업 학습을 통해 학습하며, UniLM과 T5는 이해와 생성 능력을 하나의 모델에서 통합한다. 또한, 언어 모델은 사용자의 의도에 맞춰 일반 목적 능력을 구현하기 위해 지시사항을 따르도록 미세 조정된다. 일부 연구는 다중 작업뿐만 아니라 다중 모달리티를 지원하며, 이를 통해 언어/시각 이해, 다중 모달, 게임을 위한 상징적 표현 등 다양한 도메인에서의 일반적인 아키텍처를 구현하였다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>METALM은 작업과 모달리티에 걸친 기반 모델에 대한 일반적인 인터페이스로, causal decoder와 여러 사전 학습된 non-causal encoder로 구성된다. 이 모델은 semi-causal 언어 모델링이라는 새로운 목표로 사전 학습되며, 언어 전용 및 vision-language 작업에서 뛰어난 미세 조정 및 in-context 학습 성능을 보여준다.&lt;/p>
&lt;p>미래에는 METALM의 크기를 확장하고, 다국어 설정과 더 많은 모달리티(언어, 시각, 오디오, 다중 모달 등)를 동시에 처리할 수 있도록 확장하는 것을 계획하고 있다. 또한, 객체 탐지와 의미론적 분할과 같은 시각 작업으로 보편적 작업 계층을 확장하고, METALM을 이용한 parameter-efﬁcient 미세 조정에 대해 조사할 예정이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2206.06336.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/microsoft/unilm" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>BIG-bench</title><link>https://kurtkim.github.io/p/big-bench/</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/big-bench/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델의 규모 증가는 양적 개선과 새로운 질적 능력을 동시에 가져온다. 이 새로운 능력들은 아직 잘 이해되지 않았지만, 그들의 잠재적인 영향력 때문에 중요하다. 미래 연구를 위해, 새로운 파괴적인 능력에 대비하고, 사회적으로 불이익한 효과를 완화하기 위해서는 현재와 가까운 미래의 언어 모델의 능력과 한계를 이해하는 것이 필수적이다.&lt;/p>
&lt;p>the Beyond the Imitation Game benchmark(BIG-bench)는 현재 언어 모델의 능력을 넘어서는 다양한 작업으로 구성되어 있다. 이 벤치마크는 여러 기관의 저자들이 기여한 204개의 작업을 포함하며, 이 작업들은 언어학, 아동 발달, 수학, 상식 추론, 생물학, 물리학, 사회적 편향, 소프트웨어 개발 등 다양한 주제를 다룬다. 이 벤치마크를 사용하여 다양한 크기의 언어 모델을 평가하였고, 인간 전문 평가자 팀이 모든 작업을 수행하여 기준선을 제공하였다. 결과적으로, 모델 성능과 보정은 규모가 증가함에 따라 개선되었지만 절대적인 수치에서는 불만족스럽다. 또한, 모델 간 성능은 놀랍게도 유사하며, 희소성에서 이점을 얻었다. 그러나 사회적 편향은 모호한 맥락에서 규모와 함께 증가하는 경향이 있지만, 이는 프롬프팅으로 개선될 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;blockquote>
&lt;p>&lt;em>An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside.&lt;/em> (A.M. Turing, Computing Machinery and Intelligence, 1950)&lt;/p>
&lt;/blockquote>
&lt;p>생성적 언어 모델은 텍스트 시퀀스의 가장 적절한 연속 부분을 만드는 능력을 가지고 있다. 이 능력은 텍스트를 통해 설명하고 수행될 수 있는 모든 작업을 포함하므로, 이메일, 채팅, 웹 포럼 등에서의 문제 해결에도 사용될 수 있다.&lt;/p>
&lt;p>최근 연구에서는 생성적 언어 모델이 더 크고 많은 데이터로 학습될수록 예측 가능한 방법으로 성능이 향상됨을 보여주고 있다. 이러한 발전에 따라, 언어 모델은 1 trillion 개 이상의 parameter로 확장되었고, 앞으로도 더욱 커질 것으로 예상되며, 아키텍처와 학습 방법의 개선을 통해 성능 향상도 계속될 것으로 보인다.&lt;/p>
&lt;h3 id="quantity-has-a-quality-all-its-own">Quantity has a quality all its own&lt;/h3>
&lt;p>양의 큰 증가는 종종 시스템에 새로운 행동을 부여한다. 과학에서, 규모의 증가는 새로운 표현을 필요로 하거나 신규 분야를 만들게 한다. 예를 들어, 양자장 이론에서 생태학까지 이어지는 계층은 각각 새로운 행동을 보이며, 각각이 풍부한 학문 분야의 주제가 되고 있다.&lt;/p>
&lt;p>언어 모델은 크기가 증가함에 따라 새로운 행동을 보인다. 이들은 컴퓨터 코드 작성, 체스 두기, 의료 진단, 언어 간 번역 등에 대한 초기 능력을 보이지만, 아직은 해당 영역에 대한 지식이 적은 인간보다 능력이 떨어진다. 이런 breakthrough capabilities는 경험적으로 관찰되었으나, 새로운 breakthrough가 어느 규모에서 일어날지를 신뢰성 있게 예측하는 것은 어렵다. 아직 실험적으로 발견되지 않은 추가적인 breakthrough들이 이미 발생했을 수도 있다.&lt;/p>
&lt;p>언어 모델이 커짐에 따라 양적 및 질적 변화가 일어나며, 이는 잠재적으로 변혁적일 수 있다. 큰 언어 모델들은 텍스트 기반의 다양한 작업에서 인간을 보완하거나 대체할 수 있으며, 새로운 애플리케이션을 가능하게 한다. 그러나 적절한 관리 없이는 social bias를 기술과 의사결정 과정에 깊게 뿌리내릴 수 있다. 반면, 적절한 관리가 이루어진다면, 인간의 bias를 줄이면서 의사결정을 자동화할 수 있게 된다.&lt;/p>
&lt;p>언어 모델의 변혁적 효과로 인해, 그들의 능력과 한계를 이해하고, 이들이 어떻게 발전할지 예측하는 것이 중요하다. 이러한 이해는 새로운 기술 개발을 촉진하고, 일자리 손실부터 social bias의 자동화에 이르는 사회적 부작용을 완화하며, 모델 행동이 인간의 의도와 어긋날 수 있는 다른 방법을 예측하고, 가장 유망한 연구 방향을 정하며, 단순히 규모에 의해 해결될 것으로 예상되는 문제에 대한 연구 자원을 절약하는 데 도움이 된다.&lt;/p>
&lt;h3 id="limitations-of-current-benchmarks">Limitations of current benchmarks&lt;/h3>
&lt;p>현재의 언어 모델링 벤치마크는 언어 모델의 행동과 미래 예측을 이해하는데 한계가 있다. 이 벤치마크들은 여러 가지 제한 사항을 가지고 있다.&lt;/p>
&lt;p>많은 언어 모델링 벤치마크들은 범위가 제한적으로, 이미 언어 모델이 능숙하다고 입증한 특정 능력에 초점을 맞추고 있다. 예를 들어, 언어 이해, 요약, trivia 질문 답변 등 좁은 분야의 작업을 제안한다. 이런 벤치마크들은 목표가 좁고, 이미 알려진 언어 모델의 능력에 초점을 맞추기 때문에, 규모가 커짐에 따라 언어 모델이 개발할 수 있는 새로운 능력을 식별하거나 현재 능력의 폭을 특성화하는데 부적합하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure2.png"
width="1132"
height="444"
srcset="https://kurtkim.github.io/p/big-bench/images/figure2_hub1021b5a23e72d6d1b897ff0203bfd56_98633_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure2_hub1021b5a23e72d6d1b897ff0203bfd56_98633_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>최근의 언어 모델링 벤치마크는 종종 유용한 수명이 짧다. 이 벤치마크들은 인간 수준의 성능이 달성되면 대체되거나 더 도전적인 벤치마크를 포함하여 확장되는 경우가 많다. 예를 들어, SuperGLUE 벤치마크에서는 제작 후 18개월 미만의 시간 동안에 인간을 뛰어넘는 성능이 달성되었다. 이러한 짧은 수명은 벤치마크의 제한된 범위 때문으로, 이로 인해 현재 언어 모델의 능력을 크게 초과하는 작업을 포함하지 못하게 된다.&lt;/p>
&lt;p>많은 현재의 벤치마크들은 전문가나 작업 작성자가 아닌 사람들이 수행한 라벨링을 통해 데이터를 수집한다. 이 라벨링 과정의 비용과 어려움은 작업의 난이도에 큰 영향을 미치며, 이로 인해 종종 노이즈, 정확성, 분포 문제 등이 발생하여 결과의 해석 가능성을 줄일 수 있다.&lt;/p>
&lt;h3 id="beyond-the-imitation-game">Beyond the imitation game&lt;/h3>
&lt;p>대규모 언어 모델의 잠재적 변혁적 효과를 예측하는 것의 중요성과 현재 벤치마크의 한계로 인해, 대규모 스케일, 높은 난이도의 다양한 벤치마크를 도입하고 이를 통해 모델의 성능을 측정한다. 인간 평가자의 기준선을 제공하며, 모델 성능이 인간 평가자의 성능과 구별될 수 있는지 평가한다. 이 벤치마크는 &amp;ldquo;Beyond the Imitation Game&amp;rdquo; 벤치마크 또는 BIG-bench라고 부르며, 더 가벼운 평가를 위한 BIG-bench Lite도 도입하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure1.png"
width="1232"
height="520"
srcset="https://kurtkim.github.io/p/big-bench/images/figure1_hu917149ca19e1a5114fc4aa82c79e8367_204963_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure1_hu917149ca19e1a5114fc4aa82c79e8367_204963_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>Google과 OpenAI의 dense 및 sparse transformer 모델을 벤치마크를 통해 분석하며, 모델 규모에 따른 성능 변화에 주목한다. 특히 언어 모델의 미래 능력에 대한 예측에 관심이 있다. 선택된 작업들에서는 규모에 따른 특정 모델 능력의 발전을 조사하였다.&lt;/p>
&lt;hr>
&lt;h2 id="what-is-in-big-bench">What is in BIG-bench?&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure3.png"
width="1260"
height="384"
srcset="https://kurtkim.github.io/p/big-bench/images/figure3_hu49fa9d78e34d00c1e4826a483202623a_230991_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure3_hu49fa9d78e34d00c1e4826a483202623a_230991_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="328"
data-flex-basis="787px"
>&lt;/p>
&lt;p>The Beyond the Imitation Game benchmark (BIG-bench) GitHub repository에는 다음이 포함되어 있다:&lt;/p>
&lt;ul>
&lt;li>204개 이상의 언어 작업 집합. BIG-bench의 검토 기준에 따라, 벤치마크 작업은 새롭고 다양한 주제와 언어를 다루며, 현재의 모델로는 완전히 해결할 수 없다.&lt;/li>
&lt;li>BIG-bench Lite: 전체 벤치마크보다 더 빠른 평가를 가능하게 하는 작은, 대표적이며 정식의 작업 하위 집합이다.&lt;/li>
&lt;li>벤치마크 API를 구현하고, 공개적으로 사용 가능한 모델에서의 작업 평가를 지원하며, 새로운 작업의 가벼운 생성을 가능하게 하는 코드이다.&lt;/li>
&lt;li>여섯 단계의 크기를 가진 dense 및 sparse 언어 모델에 대한 자세한 평가 결과, 그리고 인간 평가자에 의해 설정된 기본선 결과가 포함되어 있다.&lt;/li>
&lt;/ul>
&lt;p>BIG-bench는 계속해서 작업과 평가 결과를 롤링 기준으로 수락하고 있다.&lt;/p>
&lt;p>벤치마크 작업은 주로 작업 특화적인 미세 조정 없이 사전 학습된 모델을 평가한다. zero-shot과 few-shot 평가 설정에서 이러한 작업에 초점을 맞추면, 적은 수의 예제를 가진 작업에 대해서도 의미 있는 점수를 제공할 수 있다. 작은 작업들을 포함함으로써 주제의 다양성을 향상시키고, 도메인 전문가들이 라벨링의 어려움 없이 작업을 기여할 수 있게 한다.&lt;/p>
&lt;h3 id="the-big-bench-api">The BIG-bench API&lt;/h3>
&lt;p>BIG-bench API는 벤치마크 작업이 어떻게 모델과 상호작용하고 성능을 측정하는지를 정의하며, 언어 모델의 고수준 표현을 제공한다. 이 API는 JSON과 프로그래밍 방식의 두 가지 작업 유형을 지원한다.&lt;/p>
&lt;p>벤치마크 작업의 대부분인 JSON 작업은 입력과 목표로 이루어진 예제들을 JSON 파일에 정의한다. 성능은 표준 메트릭을 사용하거나 모델이 할당한 확률에 따라 평가되며, 이는 few-shot 평가를 쉽게 가능하게 한다.&lt;/p>
&lt;p>나머지 20%의 벤치마크 작업은 파이썬으로 작성되며, 모델과 직접적으로 multiple query round를 통해 소통하고 사용자 지정 지표로 성능을 측정한다. 이러한 작업은 모델 객체를 이용해 진행된다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>generate_text :&lt;/strong> 주어진 입력에 대한 텍스트 연속을 생성한다.&lt;/li>
&lt;li>&lt;strong>cond_log_prob :&lt;/strong> 입력값을 기준으로 대상의 conditional log probability를 계산한다.&lt;/li>
&lt;/ul>
&lt;p>작업 코드는 모델을 반복적으로 쿼리하여 여러 라운드의 &amp;ldquo;dialog&amp;quot;에서 모델을 평가한다. 그리고 성능은 작업에서 정의된 지표를 이용해 측정된다.&lt;/p>
&lt;p>작업 작성자는 JSON과 프로그래밍 작업에 대해 여러 지표를 제공하지만, 각 작업에 대한 주요 지표와 해당 지표의 높고 낮은 점수를 지정해야 한다. 이 지표는 총 점수 계산에 사용되며, JSON 작업에 대한 가능한 전체 지표 목록은 BIG-bench 저장소에서 확인할 수 있다. 이 논문의 그림들은 개별 지표의 성능을 보여주며, 보통 0-100 범위의 점수를 보고한다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>exact_string_match :&lt;/strong> 정확도에 대한 정확한 값.&lt;/li>
&lt;li>&lt;strong>multiple_choice_grade :&lt;/strong> 0-100 사이의 가중치가 부여된 객관식 정확도는 각 잠재적 대상에 대한 점수가 지정된다. 단일 대상에 점수 1이 할당되고 나머지는 0일 때, 이는 표준 객관식 정확도가 된다.&lt;/li>
&lt;li>&lt;strong>expected_calibration_error :&lt;/strong> calibration값은 모델의 정확도가 응답에 할당한 확률과 얼마나 잘 일치하는지를 보여준다. &amp;ldquo;expected_calibration_error&amp;quot;는 할당 확률에 따라 분류된 예시의 확률과 평균 정확도 사이의 절대 편차이다.&lt;/li>
&lt;li>&lt;strong>multiple_choice_brier_score :&lt;/strong> 클래스 간의 모델이 할당한 확률과 0, 1 대상 사이의 제곱 오차로 주어진 보정 측정값.&lt;/li>
&lt;/ul>
&lt;p>JSON과 프로그래밍 작업은 대형 언어 모델의 능력을 충분히 측정하는 작업을 커버한다. 하지만, BIG-bench는 순수 언어 모델 평가에 초점을 맞추었기 때문에, 멀티 모달 능력을 평가하는 데는 한계가 있다. 이는 미래의 연구 방향이 될 수 있다.&lt;/p>
&lt;h3 id="big-bench-lite">BIG-bench Lite&lt;/h3>
&lt;p>BIG-bench는 크고 다양한 작업 세트를 포함하며, 임의의 프로그래밍 작업을 지원하는데 이는 그 장점 중 하나이다. 그러나 이로 인해 평가에는 많은 계산 비용이 필요하며, 특히 프로그래밍 작업은 많은 모델 호출을 필요로 하고 일부 평가 파이프라인에 적용하기 어려울 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure4.png"
width="848"
height="554"
srcset="https://kurtkim.github.io/p/big-bench/images/figure4_hu33d96156aefeb35e62d53dc38d2aaf69_110856_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure4_hu33d96156aefeb35e62d53dc38d2aaf69_110856_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="367px"
>&lt;/p>
&lt;p>이 문제를 해결하기 위해, BIG-bench Lite(BBL)라는 24개의 작업 하위 집합을 선택하였다. BBL은 오직 JSON 작업만을 포함하며, 핵심 기여자들은 작업 키워드 커버리지와 코드, 비영어 능력, 편향 측정과 같은 특정 작업 유형을 고려하여 작업을 선택하였다.&lt;/p>
&lt;h3 id="evaluation-targets">Evaluation targets&lt;/h3>
&lt;h4 id="language-models-evaluated-on-big-bench">Language models evaluated on BIG-bench&lt;/h4>
&lt;p>모든 모델 출력은 특별히 언급되지 않는 한, greedily하게 샘플링되었다. temperature를 1로 설정하고 top-k를 40으로 설정한 샘플링은 점수를 낮추는 경향이 있었다. 예시당 여러 샘플을 추출하는 경우, 다른 샘플링 방법이 유익할 것으로 예상된다.&lt;/p>
&lt;p>이 논문에서는 BIG-bench에서 평가한 모델의 결과를 분석하였다. 또한 BIG-bench는 Gopher, Chinchilla, T0 등 다른 모델에서도 부분적으로 평가되었다.&lt;/p>
&lt;p>모든 모델(PaLM 제외)의 학습 데이터는 BIG-bench 저장소가 만들어지기 전에 수집되었으므로, 이 논문의 모델로의 BIG-bench 작업의 직접 유출은 불가능하다. 하지만 인터넷에서 사용 가능한 텍스트를 사용하는 많은 작업 때문에 간접 유출은 가능하다. 미래의 모델들은 데이터 유출 탐색에 &amp;ldquo;training_on_test_set&amp;rdquo; 작업을 사용할 수 있다.&lt;/p>
&lt;p>&lt;strong>BIG-G.&lt;/strong> Google에서 훈련된 BIG-G 모델은 gated activation layer과 GELU activation을 기반으로 하는 13개의 밀집 decoder-only Transformer 모델을 사용한다. 이 모델들은 웹 문서, 코드, 대화, 위키백과 데이터의 혼합으로 구성된 데이터 세트에서 학습되었으며, 대부분이 영어로, 약 6%의 비영어 텍스트를 포함하고 있다.&lt;/p>
&lt;p>&lt;strong>BIG-G sparse.&lt;/strong> Mixture-of-Experts와 Switch Transformers 등 sparsely activate 모델이 인기를 끌고 있다. 이 모델들은 상대적으로 적은 계산 비용으로 큰 모델 규모를 제공한다. decoder layers만 있는 sparsely activated 모델을 사전 학습하며, 이는 각 토큰을 32개의 expert 중 독립적으로 라우팅한다. 모든 sparse 모델은 BIG-G 모델과 동일한 혼합에서 학습된다.&lt;/p>
&lt;p>&lt;strong>GPT.&lt;/strong> GPT-3 모델 시리즈에 해당하는 OpenAI GPT 모델을 사용한다. 이 모델들은 125M에서 175B 개의 parameter를 가진 8개의 dense decoder-only transformer이다. 50k-token byte-pair 인코딩 방식으로 토큰화하였으며, 이 모델들은 300B 토큰에 대해 동일한 OpenAI 데이터 세트에서 학습되었다.&lt;/p>
&lt;h4 id="human-rater-baseline-performance">Human rater baseline performance&lt;/h4>
&lt;p>언어 모델 평가와 함께 전문 평가자 팀을 사용하여 BIG-bench의 작업을 완료하였다. 평가자들은 가능한 모든 자원을 활용하도록 권장되었고, 이에 따라 평가자들 간의 작업 평균 점수와 최고 점수를 보고하였다(평가자가 작업을 여러 번 수행한 경우, 평균 성적을 고려하였다).&lt;/p>
&lt;p>BIG-bench의 내용이 다양한 언어, 특정 도메인 지식, 그리고 가정된 수학적 및 과학적 배경을 포괄하고 있기 때문에, 이러한 모든 요소를 &amp;ldquo;human performance&amp;quot;를 나타내는 하나의 숫자로 집계하는 것은 복잡한 문제이다. 특히, 작업이 프로그래밍 지식을 요구하는 경우, 프로그래밍을 모르는 평가자의 점수를 어떻게 고려해야 할지가 문제이다.&lt;/p>
&lt;p>모든 작업에 대한 평가자들의 평균과 최대 점수를 보고하지만, 이 점수들이 인간이나 특정 평가자들이 달성할 수 있는 최고의 점수를 나타내는 것은 아니다. 평가자들은 주어진 시간 동안 작업을 완료하도록 작업을 샘플링하며, 작업의 형식과 내용은 벤치마크 개발 중에 변경되었다. 이 점들을 고려하여 이러한 지표를 해석해야 한다.&lt;/p>
&lt;p>특히, social bias을 포함하는 작업에 대한 이러한 점수를 해석할 때는 주의해야 한다. 전문 평가자들의 인구통계학적 및 배경적 특성은 반드시 일반 인구를 대표하는 것은 아니며, 이 자체가 bias의 원인이 될 수 있다.&lt;/p>
&lt;h3 id="include-a-canary-string-in-all-documents-about-big-bench">Include a canary string in all documents about BIG-bench&lt;/h3>
&lt;p>모든 BIG-bench 작업 정의 파일에는 웹 스크랩된 학습 코퍼스에서 BIG-bench 데이터를 필터링하기 위한 canary GUID 문자열이 포함되어 있다. 이는 모델이 테스트 세트에서 학습하지 않도록 하기 위한 것이다. 또한, BIG-bench 데이터가 모델 학습에 사용되었는지를 사후 진단하기 위해 이 canary 문자열이 사용된다.&lt;/p>
&lt;hr>
&lt;h2 id="behavior-of-language-models-and-human-raters-on-big-bench">Behavior of language models and human raters on BIG-bench&lt;/h2>
&lt;h3 id="aggregate-performance-improves-with-model-size-but-is-worse-than-human-performance">Aggregate performance improves with model size but is worse than human performance&lt;/h3>
&lt;p>BIG-bench에서 평가된 평균 성능은 컴퓨팅 규모와 shot 수가 증가함에 따라 향상되지만, 가장 강력한 모델들조차도 인간 평가자의 성능에 비해 전반적으로 부족하다. 각 BIG-bench 작업은 작업 작성자가 지정한 고유한 선호 지표와 그에 대한 높고 낮은 값이 있으며, 이를 사용하여 원시 선호 점수를 낮은 점수를 빼고 0-100 범위로 정규화하여 종합 성능을 계산한다.&lt;/p>
&lt;p>$$ \text{[normalized preferred metric]} = 100 \times {{\text{[raw preferred metric] − [low score]}}\over{\text{[high score] − [low score]}}} $$&lt;/p>
&lt;p>정규화된 선호 지표 하에서, 작업의 점수는 0이면 성능이 떨어지고, 100이면 매우 좋은 성능을 나타낸다. 일부 작업에서 모델의 점수는 0 미만이거나 100 초과일 수 있다. 인간 전문가는 대체로 100에 가까운 점수를 얻을 것으로 예상되며, 이 지표는 모든 작업에 대해 평균되어 종합 성능을 나타낸다. 가장 좋은 성능을 보인 언어 모델은 20 미만의 점수를 얻었다.&lt;/p>
&lt;h3 id="model-predictions-grow-better-calibrated-with-increased-scale">Model predictions grow better calibrated with increased scale&lt;/h3>
&lt;p>모델이 정확하며 잘못된 답변에 대해 높은 확신을 부여하지 않는 것이 중요한 많은 사용 사례가 있다. 이 부분에서는 BIG-bench에서 모델의 불확실성 추정치가 얼마나 잘 조정되는지와 모델 규모에 따라 이 조정이 어떻게 변하는지를 측정한다.&lt;/p>
&lt;p>현대의 신경망은 과신하는 경향이 있고, 모델의 깊이나 너비가 증가해도 이 문제가 개선되지 않는다. 사전 학습은 시각 작업에 대한 모델의 조정을 향상시킬 수 있지만, 대규모 언어 모델인 GPT-3의 예측은 인간 평가자에 비해 잘 조정되지 않았다. 사전 학습된 언어 모델은 일반적으로 도메인 내에서는 잘 조정되지만, 도메인 외부에서는 큰 조정 오류를 보인다. generative 언어 모델(T5, BART, GPT-2)의 질문-답변 작업에 대한 예측 확률 역시 잘 조정되지 않았다.&lt;/p>
&lt;p>모델의 조정을 검증하기 위해, 모델 규모에 따른 다중 선택 작업에서 모델의 조정을 측정하였다. 모델의 신뢰도는 대상 선택의 conditional log likelihood score에 기반하며, 이를 사용해 각 모델 크기에 대한 Brier 점수와 예상 조정 오류를 계산하였다. Brier 점수는 예측 확률의 정확성을 측정하는데 사용되며, 예상 조정 오류는 조정을 측정하는 데 널리 사용되었다. 다양한 조정 지표에 대한 논의는 Ovadia 등의 작업을 참조할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure5.png"
width="1184"
height="344"
srcset="https://kurtkim.github.io/p/big-bench/images/figure5_hufb5d99be1c782570e733f6fff8329fef_182779_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure5_hufb5d99be1c782570e733f6fff8329fef_182779_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="344"
data-flex-basis="826px"
>&lt;/p>
&lt;p>Brier 점수와 예상 조정 오류 점수는 모두 상당히 높은 편(0.2-0.3 Brier 점수, 0.25-0.45 예상 조정 오류), 이는 모든 모델의 예측이 잘 조정되지 않았음을 나타낸다. 이 결과는 이전 연구들과 일치하며, 반면에 모델 규모가 커질수록 조정이 개선되는 경향을 보여주었다.&lt;/p>
&lt;h3 id="model-classes-behave-similarly-with-benefits-from-sparsity">Model classes behave similarly, with benefits from sparsity&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure6.png"
width="1324"
height="436"
srcset="https://kurtkim.github.io/p/big-bench/images/figure6_hu8cbe40b19f71bbed963132e6fde06618_163565_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure6_hu8cbe40b19f71bbed963132e6fde06618_163565_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="303"
data-flex-basis="728px"
>&lt;/p>
&lt;p>전반적으로, dense 모델과 sparse 모델, 그리고 GPT와 BIG-G 모델 간에 학습과 테스트 cross entropy의 측정치는 유사하다.&lt;/p>
&lt;p>BIG-G sparse 모델은 BIG-G dense 모델보다 BIG-bench 작업에서 더 뛰어난 성능을 보여주며, 동일한 모델 성능에 대해 추론 비용에서 대략 2배의 향상을 보여준다. sparse 모델은 parameter는 많지만, 활성화되는 네트워크 부분이 적어 총 계산량은 더 적다. 또한, BIG-G sparse 모델은 예측의 조정에서 크게 향상되며, 주어진 조정 점수에 도달하기 위해 필요한 parameter 수에서 약 10배의 향상을 보여주었다.&lt;/p>
&lt;p>BIG-G와 GPT 모델의 성능은 비슷하나, 작은 모델 크기에서는 GPT 모델이, 큰 모델 크기에서는 BIG-G 모델이 더 좋다. 하지만, 모든 규모에서 BIG-G sparse 모델이 가장 높은 성능을 보여준다.&lt;/p>
&lt;p>모델 클래스들 사이에는 전반적으로 유사성이 있지만, 개별 BIG-bench 작업과 키워드에서는 성능 차이가 있었다. 이 차이에 대한 명확한 해석은 아직 없으며, 특히 sparse 모델과 dense 모델 사이의 행동 차이를 조사하는 것은 향후 연구 주제로 흥미롭다.&lt;/p>
&lt;h3 id="linearity-and-breakthroughness-categorizing-scaling-behaviors">Linearity and breakthroughness: categorizing scaling behaviors&lt;/h3>
&lt;p>모델의 성능은 일부 작업에서는 규모에 따라 안정적으로 향상되지만, 다른 일부 작업에서는 규모에 따른 개선을 보이지 않는다. 특정 규모에서 성능이 급증하는 경우도 있다. 이런 다양한 성능 변화를 정량화하기 위해 우리는 두 가지 새로운 지표를 도입하였다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Linearity L:&lt;/strong> 작업에 대한 성능이 규모에 따라 안정적으로 개선되는 정도를 측정하기 위한 것이다.&lt;/li>
&lt;li>&lt;strong>Breakthroughness B:&lt;/strong> 모델이 일정 규모 이상으로 커지면서만 작업을 배울 수 있는 정도를 측정하려는 것이다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure7.png"
width="1356"
height="464"
srcset="https://kurtkim.github.io/p/big-bench/images/figure7_hu6ec758ccfd6726dc741f90308e307b7a_253283_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure7_hu6ec758ccfd6726dc741f90308e307b7a_253283_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>지표에서 가장 극단적인 점수를 가진 작업들의 정규화된 성능 점수를 보여준다.&lt;/p>
&lt;h4 id="properties-of-tasks-with-high-linearity-or-breakthroughness">Properties of tasks with high linearity or breakthroughness&lt;/h4>
&lt;p>가장 높은 linearity를 가진 작업들은 주로 학습 데이터 내의 정보를 기억하는 데 의존하는 지식 기반의 작업들이다. 이는 퀴즈 형식의 질문에 대답하거나 간단한 텍스트 매핑을 수행하는 것과 같은 작업을 포함한다.&lt;/p>
&lt;p>강한 breakthroughness를 보이는 작업들은 여러 단계나 다양한 기술을 요구하는 복합적인 작업들이다. 이에는 특정 입력에 대해 맥락에 따라 정의된 수학적 연산자를 적용하는 &amp;lsquo;modified_arithmetic&amp;rsquo; 작업과 같은 예가 포함되며, &amp;lsquo;repeat_copy_logic&amp;rsquo;, &amp;lsquo;figure_of_speech_detection&amp;rsquo;, &amp;lsquo;codenames&amp;rsquo;도 이와 같은 복합적인 작업으로 볼 수 있다.&lt;/p>
&lt;p>&amp;lsquo;codenames&amp;rsquo;와 같은 작업은 언어적 유사성과 유추를 다루지만, 순차적인 지시사항을 따라야 하므로 복합적인 작업으로 분류된다.&lt;/p>
&lt;p>high linearity나 breakthroughness과 연관된 작업 유형에 대한 이러한 관찰은 일화적이며, 이것이 일반적으로 유지되는지 연구하는 것은 흥미로울 것이다.&lt;/p>
&lt;h4 id="breakthrough-behavior-is-sensitive-to-details-of-task-specification">Breakthrough behavior is sensitive to details of task specification&lt;/h4>
&lt;p>Breakthrough 현상은 모델이 갑자기 새로운 기능을 급격히 획득하는 것을 나타낸다. 하지만 세밀한 분석을 통해 모델의 기능 변화가 더 부드럽게 이루어진다는 것을 알 수 있다. 이러한 점진적인 개선은 작업을 분해하여 부분적인 진전을 더 잘 포착하게 함으로써 드러날 수 있다.&lt;/p>
&lt;p>Breakthrough 현상은 인간이 갑자기 이해하는 &amp;ldquo;aha! moments&amp;quot;을 떠올리게 한다. 인간의 &amp;ldquo;aha! moments&amp;quot;가 이해력의 부드러운 개선과 연관되어 있는지는 흥미로운 주제이다.&lt;/p>
&lt;p>&lt;strong>Using smoother metrics.&lt;/strong> &amp;ldquo;exact_str_match&amp;rdquo; 지표는 모델 출력이 목표 문자열과 완전히 일치할 때만 점수를 주므로 돌파현상을 보이는데, BLEU, BLEURT, ROUGE와 같은 다른 지표를 살펴보면 더 점진적인 진전을 확인할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure8.png"
width="1328"
height="454"
srcset="https://kurtkim.github.io/p/big-bench/images/figure8_huafd86fa0a661d27467b55c356218a90c_143046_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure8_huafd86fa0a661d27467b55c356218a90c_143046_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="702px"
>&lt;/p>
&lt;p>객관식 문제를 점수화하는 &amp;ldquo;multiple_choice_grade&amp;quot;는 올바른 선택지가 모든 잘못된 선택지보다 높은 log probability를 가질 때만 점수를 주므로, 전부 혹은 전혀 없음의 효과를 초래한다. 이를 부드럽게 만드는 대안 지표로는 올바른 선택지의 mean probability나 mean log probability가 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure9.png"
width="1340"
height="812"
srcset="https://kurtkim.github.io/p/big-bench/images/figure9_hu7a1b5bc2a0854eb38c85d4b70ff33e98_249902_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure9_hu7a1b5bc2a0854eb38c85d4b70ff33e98_249902_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="396px"
>&lt;/p>
&lt;p>&lt;strong>Manual decomposition into subtasks.&lt;/strong> 일부 작업은 목표 능력에 필요한 기본 기술을 식별하고 이를 측정하는 부분 작업을 설계함으로써 점진적인 진전을 관찰할 수 있다. &amp;ldquo;checkmate_in_one&amp;quot;에서는 모델이 체스의 메이트 이동을 배우기 전에 유효한 체스 이동을 생성하는 것을 배운다. &amp;ldquo;emoji_movie&amp;rdquo; 작업에서는 모델이 이모티콘 시퀀스로 영화를 추측하는데, 이때 &amp;ldquo;exact_str_match&amp;rdquo; 지표는 breakthrough 현상을 보이지만, 객관식 지표를 사용하면 성능 향상이 더 점진적으로 나타났다. 이러한 점은 모델 출력을 수동으로 검토할 때 확인할 수 있다.&lt;/p>
&lt;p>모델의 부드러운 개선이 항상 전체 작업 성능을 설명하지는 않는다. 부분 작업이나 지표가 점진적으로 개선되더라도 실제 작업 성능에 대해 어떤 정보를 제공하는지는 항상 명확하지 않다. 예를 들어, 체스의 합법적인 이동을 완벽하게 인식하는 것이 &amp;ldquo;checkmate_in_one&amp;rdquo; 작업에서 높은 성능을 보장하지 않을 수 있다. &amp;ldquo;periodic_elements&amp;rdquo; 작업에서는 모델이 올바른 원소와 다른 원소를 구별하지 못하다가, 일정 규모 이상에서 log likelihood 곡선이 갑자기 이탈하면서 정확도가 급격히 향상된다.&lt;/p>
&lt;h4 id="even-programmatic-measures-of-model-capability-can-be-highly-subjective">Even programmatic measures of model capability can be highly subjective&lt;/h4>
&lt;p>언어 모델의 능력과 변화 경로가 특정 작업을 통해 정량화되더라도 상당히 주관적일 수 있다는 결론을 도출한다. 작업 설계에 따라 동일한 능력이 정체되는 것처럼 보이거나 점진적으로 또는 갑작스럽게 개선되는 것처럼 보일 수 있다. 일반적으로 하나의 지표만으로 작업 해결 능력을 정량화하기는 어렵고, 지표가 적절하게 측정되고 있는지 확인하기 위해 모델 출력을 항상 확인해야 한다. 이는 학습 중에 평가 지표가 명시적으로 지향되지 않는 경우, 특히 중요하다.&lt;/p>
&lt;h3 id="even-large-language-models-are-brittle">Even large language models are brittle&lt;/h3>
&lt;p>큰 언어 모델이 자연어 질문의 정확한 표현에 민감하게 반응하며, 때때로 직관적이지 않게 동작하는 것, 즉 &amp;ldquo;brittle&amp;rdquo; 현상에 대한 두 가지 예를 살펴본다. 특히 객관식 질문의 표현 방식을 중점적으로 다룬다. 일반적으로 이러한 작업의 입력 질의는 질문의 답변 선택지를 포함하게 된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure10.png"
width="682"
height="370"
srcset="https://kurtkim.github.io/p/big-bench/images/figure10_hua89988f4d8cdbb9d328385fb0292ee00_92059_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure10_hua89988f4d8cdbb9d328385fb0292ee00_92059_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="442px"
>&lt;/p>
&lt;p>객관식 JSON 작업에서 질의에 선택지를 추가했을 때와 그렇지 않았을 때의 BIG-G 모델 성능을 비교한다. 기본적으로 모델은 각 선택지를 점수 매기기 전에 비교할 수 있지만, 이것이 성능을 향상시킬 것으로 생각되지만, 실제로는 선택지를 포함시키는 것이 성능을 저하시킨다. 이는 few-shot 상황에서도 마찬가지이다.&lt;/p>
&lt;p>&amp;ldquo;cause_and_effect&amp;rdquo; 작업에서는 모델에게 원인과 결과가 연결된 두 사건을 제시하고, 어떤 사건이 다른 사건을 발생시켰는지 판단하도록 요구한다. 이 작업은 세 가지 다른 형식으로 제시된다.&lt;/p>
&lt;ol>
&lt;li>사건들은 &amp;ldquo;A because B&amp;quot;와 &amp;ldquo;B because A&amp;quot;의 형태로 문장을 구성한다. 모델은 각 문장에 확률을 할당하고, 이 확률을 통해 원인을 추론한다. 예를 들어, &amp;ldquo;A because B&amp;quot;에 더 높은 확률이 할당되면, B가 원인으로 예측된다.&lt;/li>
&lt;li>모델은 작업 설명과 두 가지 선택지를 제공하는 프롬프트에 따라 동일한 두 문장에 점수를 매기도록 요청된다. 모델은 두 가지 선택지에 확률을 할당하도록 요청되고, 점수가 더 높은 선택지가 모델의 예측으로 간주된다.&lt;/li>
&lt;li>모델은 두 개의 사건 A와 B를 서로 다른 문장으로 받아, 어떤 문장이 다른 문장의 원인인지 식별하게 된다. 이때, 사건들은 자연어 문장으로 합쳐지지 않는다. 이전의 경우처럼, 모델은 두 선택지에 확률을 할당하고, 점수가 더 높은 선택지가 예측된 원인으로 간주된다.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure11.png"
width="746"
height="480"
srcset="https://kurtkim.github.io/p/big-bench/images/figure11_hu406c2f0d248f8107f3172501c6a0f95b_90795_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure11_hu406c2f0d248f8107f3172501c6a0f95b_90795_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>첫 번째 형식 &amp;ldquo;A because B&amp;quot;에서는 모델 규모가 커짐에 따라 점진적인 개선이 보인다. 그러나 다른 두 형식에서는 무작위 추측 이상의 성능 향상이 없다. 이는 프롬프팅에 따른 의존성이 높다는 것을 보여준ㄴ다. 첫 번째 버전 작업은 원래 학습 목표에 가장 가깝고, 모델은 더 가능성이 높은 자연어 문장을 예측하게 된다. 다른 버전에서 성능이 낮은 이유는 이 작업들이 학습 분포와 다르기 때문으로 추측된다. 최근에는 대형 언어 모델의 규모를 늘리면 질문 표현에 대한 빠져있는 현상이 개선될 수 있음이 제안되었습니다.&lt;/p>
&lt;p>모델의 객관식 제시와 원인과 결과 형식에 대한 민감성은 한 버전의 작업을 해결하는 능력이 다른 버전에 자동적으로 적용되지 않음을 보여준다. 충분한 예제와 함께 미세 조정된 충분히 큰 모델은 이러한 작업 형식의 변화에 견고할 것으로 예상된다. 더 큰 모델에 대한 최근의 연구결과는 표현 민감성의 일부가 더 큰 규모에서 해결될 수 있음을 보여줍니다. 그러나 거대 언어 모델의 성공과 실패를 해석할 때 작업 제시의 세부 사항을 고려해야 한다.&lt;/p>
&lt;h3 id="social-bias-in-large-language-models">Social bias in large language models&lt;/h3>
&lt;p>머신러닝 시스템, 특히 언어 모델에서의 social bias 이해는 중요한 도전 과제이다. 이에 대한 깊이 있는 논의와 정량화는 이 개요 논문의 범위를 넘어선다. 그러나 social bias을 측정하려는 BIG-bench 작업의 평가를 통해 몇 가지 중요한 핵심 포인트를 얻을 수 있다.&lt;/p>
&lt;ul>
&lt;li>광범위하거나 모호한 컨텍스트에서 bias는 종종 규모와 함께 증가한다.&lt;/li>
&lt;li>좁고 명확한 컨텍스트에서는 bias가 규모와 함께 감소할 수 있다.&lt;/li>
&lt;li>적절하게 선택된 프롬프트를 통해 bias를 조절할 수 있다.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What is meant by social bias?&lt;/strong> social bias는 맥락에 따라 다른 의미를 가질 수 있다. 대부분의 BIG-bench 작업은 bias를 좁고 명확한 개념으로 취급한다: 사람들과 관련된 고정된 맥락이 주어졌을 때, 모델은 한 카테고리를 다른 카테고리보다 체계적으로 선호하거나, 특정 속성을 특정 카테고리와 연결시키는 경향이 있나?&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure12.png"
width="1346"
height="556"
srcset="https://kurtkim.github.io/p/big-bench/images/figure12_hu44630c981d5dadda40949c71bd55735e_168600_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure12_hu44630c981d5dadda40949c71bd55735e_168600_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>&lt;strong>Bias typically increases with scale in settings with broad or ambiguous contexts.&lt;/strong> 나이, 장애 상태, 성별 정체성 등 다양한 요소에 대한 편향을 측정하는 여섯 가지 작업(bbq_lite, bias_from_probabilities, diverse_social_bias, gender_sensitivity_english, muslim_violence_bias, unqover)의 데이터를 보여준다.&lt;/p>
&lt;p>이 작업들은 모델에게 넓은 범주에 대한 일반화를 요구하거나 주어진 증거에 따라 완성이 모호한 맥락을 제시한다. 모호한 템플릿 문장의 확률을 비교하거나, 모호한 맥락에 대한 질문에 답하는 등의 방식으로 이루어진다. 일반적으로 모델 규모가 커질수록 편향이 증가하는 경향이 있다. 예를 들어, 가장 큰 규모의 모델에서는 백인 소년이 좋은 의사가 될 가능성이 네이티브 아메리칸 소녀가 될 가능성보다 22배 이상 높다고 판단하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure13.png"
width="1318"
height="318"
srcset="https://kurtkim.github.io/p/big-bench/images/figure13_huc1209c261dafccd6dfd33ce17a2c2314_165229_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure13_huc1209c261dafccd6dfd33ce17a2c2314_165229_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="414"
data-flex-basis="994px"
>&lt;/p>
&lt;p>&lt;strong>Bias can decrease with scale in settings with narrow, unambiguous contexts.&lt;/strong> 대부분의 BIG-bench 작업은 모호한 맥락에서 bias를 조사하지만, 명확한 상황에서의 편향도 중요합니다. 예를 들어, 모델은 &amp;ldquo;The woman just won the Lasker Award for her outstanding work on mRNA vaccines, she is a {good, bad} doctor.&amp;ldquo;라는 문장에서 &amp;ldquo;good&amp;quot;에 더 높은 확률을 부여해야 한다. bbq_lite 작업은 모호한 맥락과 명확한 맥락에서의 bias를 비교하도록 설계되었다. 이 작업에서, 모델 크기가 커짐에 따라 편향이 감소하는 경향을 볼 수 있다. 이는 모델이 맥락을 이용해 bias를 상쇄하는 능력이 향상되었음을 의미한다.&lt;/p>
&lt;p>&lt;strong>Bias can potentially be steered through appropriately chosen prompting.&lt;/strong> 프롬프트 선택을 통해 모델의 출력 특성을 조정할 수 있다. 프롬프트는 농담, 특정 스타일의 글쓰기, 사실적인 반응 등을 유도하거나, 모델이 기억한 정보를 반복하도록 요구하거나, 더 나은 도우미가 되도록 장려하도록 설계될 수 있다. 또한, 모델의 성능은 적은 수의 사례를 학습할 때 종종 향상되는 것으로 나타났다.&lt;/p>
&lt;p>적절한 프롬프트 선택이나 맥락 내 예시는 출력의 bias를 바꿀 수 있다. bbq_lite에서 모호한 프롬프트의 경우, 중립적인 반응을 원하는 few-shot 프롬프트는 모델의 bias를 크게 줄인다. 이 결과는 도움이 되고, 해를 끼치지 않으며, 진실성이 있는 강력한 기준선을 설정하는 연구와 일관성이 있다.&lt;/p>
&lt;p>이러한 관찰 결과는 BIG-bench에서 조사된 모든 social bias 카테고리에 걸쳐 질적으로 유사한 것으로 보인다.&lt;/p>
&lt;h3 id="performance-on-non-english-languages">Performance on non-English languages&lt;/h3>
&lt;p>세계의 다양한 언어를 이해할 수 있는 언어 모델 구축은 계속 진행 중인 중요한 과제이다. 현재 다언어 자연어 처리 모델은 영어에 대해 다른 언어보다 더 잘 작동하며, 특히 자원이 부족한 언어와 비라틴 문자 언어에 대한 성능 차이가 크다. 이는 다언어 말뭉치의 데이터 품질 불균형, 데이터 획득의 어려움, 그리고 연구 설계 단계에서의 Anglo-centric bias 등 여러 요인에 의해 발생한다.&lt;/p>
&lt;p>BIG-bench는 주로 비영어 언어를 다루는 다양한 작업을 포함하고 있다. 그러나 평가하는 모델들이 주로 영어 이해에 초점을 맞추고 있기 때문에, 비영어 작업에 대한 성능은 대체로 부족하다. 일부 비영어 작업에서는 모델 크기에 따른 성능 향상이 있지만, 이는 항상 그런 것은 아니다. 따라서 우리는 언어 범위와 데이터 수집에 대한 주의가 강력한 다언어 성능을 위해 필요하다고 생각한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure14.png"
width="1150"
height="378"
srcset="https://kurtkim.github.io/p/big-bench/images/figure14_hucdf5d26b33df279b638ec4a6754191aa_138103_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure14_hucdf5d26b33df279b638ec4a6754191aa_138103_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;p>&lt;strong>Performance on non-English tasks is worse than on English tasks.&lt;/strong> 일부 경우에는 영어와 다른 언어로 병렬 작업이 정의되어 언어 간에 비교가 가능하다. 영어와 힌디어에서 동사의 함의 속성을 테스트하는 작업이 그 예이다. 영어 작업의 경우, 모델의 크기가 커질수록 성능이 향상되지만(정확도 90%), 힌디어 작업에서는 그러한 경향이 없으며 최고 정확도도 70%를 넘지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/table2.png"
width="1318"
height="650"
srcset="https://kurtkim.github.io/p/big-bench/images/table2_hu6fcefd311e17ff9d195b04f42ed196cb_189844_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/table2_hu6fcefd311e17ff9d195b04f42ed196cb_189844_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>&lt;/p>
&lt;p>&lt;strong>Low-resource language tasks are particularly challenging.&lt;/strong> BIG-bench 작업 대부분은 영어나 high-resource 언어를 다루지만, 일부 작업은 low-resource 언어를 대상으로 한다. &amp;ldquo;low-resource&amp;quot;은 확실히 정의되지 않았지만, mC4 말뭉치에서 문서 수에 따른 언어 순위를 대략적인 지표로 볼 수 있다. 상위 50개 언어를 고자원으로 보고 그 외를 low-resource로 간주하면, BIG-bench 작업 중 5개가 low-resource 언어를 다룬다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure15.png"
width="1348"
height="328"
srcset="https://kurtkim.github.io/p/big-bench/images/figure15_hu051baa1c45833d87bc55332d62199ba4_143513_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure15_hu051baa1c45833d87bc55332d62199ba4_143513_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="410"
data-flex-basis="986px"
>&lt;/p>
&lt;p>평가된 모델들의 low-resource 언어 성능은 대체로 낮다. 스와힐리어 속담에 대한 적절한 영어 대응을 찾는 작업에서는 모델 규모가 커질수록 성능이 향상되지만, 이가 스와힐리어를 이해하는 능력을 나타내는지는 확실하지 않다. 비라틴 문자를 사용하는 캐나다어에서는 수수께끼를 풀어야 하는 작업에서 모델들이 무작위 추측 성능을 크게 넘어서지 못하였다.&lt;/p>
&lt;p>low-resource 언어를 가장 많이 다루는 BIG-bench 작업인 &amp;ldquo;language_identification&amp;quot;은 모델이 주어진 문장의 언어를 식별하도록 요구한다. 이 작업에서는 총 1,000개 언어가 포함되며, 대부분이 low-resource 언어이다. 그러나 성능은 낮으며, 최고 모델도 약 17%의 정확도를 보인다. 또한, 모델 크기와 성능 사이에는 명확한 증가 추세가 없다.&lt;/p>
&lt;hr>
&lt;h2 id="behavior-on-selected-tasks">Behavior on selected tasks&lt;/h2>
&lt;p>통합적인 추세를 벗어나 모델과 인간 평가자의 성능을 개별 작업별로 심도있게 분석한다. 이를 통해 모델이 어떤 부분에서 성공하거나 실패하는지, 어떤 것을 어려워하는지에 대한 이해를 높일 수 있다.&lt;/p>
&lt;h3 id="checkmate-in-one-task">Checkmate-in-one task&lt;/h3>
&lt;p>&amp;ldquo;checkmate_in_one&amp;rdquo; 작업은 체스 게임의 처음 몇 수를 (대수 표기법으로) 제시하고, 즉시 체크메이트를 초래하는 이동을 요구한다. 모든 제시된 위치에서는 고유한 체크메이트 이동이 존재한다. 이 작업은 체스 규칙을 알고, 체스판을 외부 메모리 도움으로 사용하여 말의 위치를 추적할 수 있는 사람들에게는 간단하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure16.png"
width="1202"
height="516"
srcset="https://kurtkim.github.io/p/big-bench/images/figure16_hu1413aa7571e35fb011fafdb17bdbf194_175395_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure16_hu1413aa7571e35fb011fafdb17bdbf194_175395_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="559px"
>&lt;/p>
&lt;p>테스트된 BIG-G 모델 중에는 체스 작업을 해결할 수 있는 모델이 없다. 대규모 모델이 작은 모델보다 체스에 대해 더 잘 알고 있는 것처럼 보이지 않지만, 사실 큰 모델이 게임 규칙에 대해 더 많이 이해하는 것으로 나타났다. 즉, 모델이 커짐에 따라 합법적인 체스 이동을 찾는 능력이 향상된다. 이는 하위 작업으로 분해하여 임계 이하의 진전을 보여주는 예시이다.&lt;/p>
&lt;p>모델의 또 다른 능력 중에 규모와 함께 부드럽게 향상되는 것은 체크메이트 이동을 찾는 능력이지만, # 기호로 올바르게 주석 처리되지는 않는다.&lt;/p>
&lt;h3 id="periodic-elements-task">Periodic elements task&lt;/h3>
&lt;p>&amp;ldquo;periodic_elements&amp;rdquo; 작업에서는 모델에게 특정 원자 번호에 해당하는 원소의 이름(예: 수소, 헬륨 등)을 식별하도록 요구한다. 이 작업은 객관식이 아니며, 모델은 텍스트를 생성하고, 그 중 원소 이름에 해당하는 첫 번째 단어(대소문자 구분 없음)를 올바른 원소와 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/big-bench/images/figure17.png"
width="1220"
height="396"
srcset="https://kurtkim.github.io/p/big-bench/images/figure17_hu355279afd01c7b8494254291c2551009_223665_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/big-bench/images/figure17_hu355279afd01c7b8494254291c2551009_223665_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="308"
data-flex-basis="739px"
>&lt;/p>
&lt;p>가장 큰 모델은 zero-shot 설정에서 주기표의 절반 이상을 올바르게 식별할 수 있다. 그러나 1B parameter 이하에서는 성능이 일정하다.&lt;/p>
&lt;p>&lt;strong>Zero-shot behavior across scales:&lt;/strong> 작은 모델들은 대체로 무의미한 문장을 출력하지만, 모델 크기가 커짐에 따라 원소 이름으로 추측을 시작한다. 1B 모델은 수소를, 2B 모델은 대부분의 질문에 대해 알루미늄을 추측한다. 4B 모델부터는 모든 큰 모델들이 응답에서 합법적인 원소 이름을 출력한다. 그러나, 이들 중 대부분이 올바른 것은 가장 큰 모델인 128B뿐이다.&lt;/p>
&lt;p>&lt;strong>One-shot behavior across scales:&lt;/strong> 가장 작은 두 모델(2M, 17M)은 원소 이름으로 답하는 패턴을 파악하지 못하고, 57M 모델부터는 원소 이름으로 대답을 시작한다. 그러나, 57M부터 2B 모델까지는 대부분 동일한 원소를 반복한다. 4B 모델부터는 프롬프트와 다른 원소를 답변하며, 올바른 답변의 수가 증가하기 시작한다.&lt;/p>
&lt;p>가장 큰 AI 모델이 때때로 원자 번호가 100 이상인 원소를 그들의 이전 임시 이름으로 잘못 식별하는 경우가 있다. 이는 이러한 원소들의 이름이 바뀌기 전에 작성된 문서가 학습 데이터에 포함되어 있기 때문으로 보인다.&lt;/p>
&lt;p>일부 경우에, 더 많은 shot을 추가함으로써 성능 향상이 주로 패턴 매칭 개선에 기인한다는 것을 알 수 있다. 가장 큰 모델은 제시된 shot 수와 상관없이 비슷한 성능을 보인다. 하지만 이를 위해선 모델의 출력에서 첫 번째 원소 이름을 찾는 후처리 작업이 필요하다. 후처리 없이는 모델의 zero-shot 성능이 현저히 떨어진다. 따라서 생성적 작업의 자동 평가에는 주의를 기울여야 한다.&lt;/p>
&lt;hr>
&lt;h2 id="additional-related-work">Additional related work&lt;/h2>
&lt;p>오픈 소스 협업을 통해 BIG-bench와 같은 다양한 벤치마크가 조직되고 있다. 이런 접근법은 다양한 아이디어와 기여를 수집하는 데 효과적이다. 자연어 처리(NLP) 분야에서는 지리적 다양성을 높이기 위한 노력들도 이루어지고 있다. 예를 들어, Masakhane는 참여형 연구를 통해 30개 이상의 low-resource 언어를 위한 기계 번역 벤치마크를 개발하였다. EleutherAI, GEM, NL-Augmenter, Natural Instructions Expansion 프로젝트, DynaBench, SyntaxGym, 그리고 MMLU 벤치마크 등 다양한 프로젝트들이 이러한 개방형 협업을 통해 진행되고 있다. 이들 프로젝트는 자연어 생성, 평가, 메트릭스 개발, 데이터셋 변형 및 증강, 동적 데이터셋 생성 및 모델 벤치마킹 등 다양한 작업에 사용되고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;blockquote>
&lt;p>&lt;em>We can only see a short distance ahead, but we can see plenty there that needs to be done.&lt;/em> (A.M. Turing, Computing Machinery and Intelligence, 1950)&lt;/p>
&lt;/blockquote>
&lt;p>대규모 언어 모델은 놀라운 능력을 보이지만, 때때로 기본적인 작업에서 실패하는 경우가 있다. 이런 불확실성을 해결하기 위해, 200개가 넘는 다양하고 어려운 작업들을 통해 언어 모델의 행동을 정량화하는 BIG-bench를 도입하였다. 이를 통해 기존 언어 모델의 능력과 한계를 평가하고, 인간의 성능을 기준으로 삼는다. BIG-bench 평가는 비용이 많이 들어, 빠르게 평가할 수 있는 24개의 작은 작업들을 선별한 BIG-bench Lite를 공개한다. 또한, 분석용 Colab 노트북과 작업별 모델 상호작용의 점수와 로그가 담긴 데이터 파일도 제공한다.&lt;/p>
&lt;h3 id="overall-findings">Overall Findings&lt;/h3>
&lt;p>대규모 언어 모델은 전문가 인간에 비해 BIG-bench에서 낮은 성능을 보이며, 벤치마크에서의 성능 증가는 예상보다 느리다. 하지만 학습, 프롬프팅, 추론 전략 등의 기술 혁신을 통해 성능 향상이 가능하다는 것이 밝혀졌다. 특히 Gopher와 PaLM 같은 새로운 모델들은 기존 모델에 비해 훨씬 우수한 성능을 보이고 있다.&lt;/p>
&lt;p>작업 성능은 특정 모델 규모를 넘어서면 갑자기 빠르게 향상되는 경우가 있다. 이러한 급격한 성능 향상은 성공 기준이 취약하거나 좁은 작업에서 흔히 발생하며, 이는 잠재적인 성능 개선을 파악할 수 있는 적절한 측정 지표의 중요성을 강조한다. 또한, 여러 단계의 추론을 포함하는 작업에서도 이러한 급격한 성능 향상이 발생할 수 있다. 이는 각 단계의 성공 확률이 선형적으로 증가하면, 전체 작업의 성공 확률이 단계 수에 따른 다항식처럼 증가하기 때문이다.&lt;/p>
&lt;p>모델의 능력은 작업이 어떻게 설정되는지에 따라 크게 달라질 수 있다. 예를 들어, 대규모 모델은 두 사건 중 어느 것이 원인인지 명확하게 식별하는데는 어려움을 겪지만, 두 사건을 올바른 원인-결과 순서로 제시하는 문장에는 높은 확률을 부여한다. 또한, PaLM과 같은 모델은 원인과 결과에 대한 취약성이 줄어들어, 모델 크기가 증가하고 데이터셋이 개선됨에 따라 모델의 취약성이 덜해질 수 있음을 보여준다.&lt;/p>
&lt;p>일부 제한 사항들은 단순히 모델의 규모를 증가시키는 것만으로는 해결될 수 없다. 이에는 매우 긴 맥락에서의 정보 처리 능력 부재, 학습 세트에 대한 에피소드 메모리의 부재, 토큰 출력 전의 순환 계산 능력 부재, 그리고 다양한 감각 모달리티를 통한 지식 구체화 능력 부재 등이 포함된다.&lt;/p>
&lt;p>모델의 social bias 측정 성능이 규모가 커짐에 따라 악화되는 경향이 있다. 이는 더 큰 모델이 학습 데이터의 편향을 더 잘 반영하기 때문일 수 있다. 이는 기계 학습의 공정성에 대한 중요성을 강조하며, 특히 LaMDA 모델과 같은 기법이 모델의 안전성을 향상시키고 bias를 줄이는 데 중요할 것으로 보인다. 또한, bias가 적용되지 않거나 바람직하지 않은 명확한 맥락에서는 모델 규모가 커짐에 따라 social bias가 줄어들 수 있다는 것을 발견하였다.&lt;/p>
&lt;p>평가한 모델들은 영어 작업에서 비영어 언어 작업보다 더 우수한 성능을 보여주었다. 특히, low-resource 언어를 다루는 작업에서는 성능이 매우 떨어졌으며, 이러한 작업에서는 모델의 규모가 커져도 성능이 향상되지 않았다. 반면, 영어 작업에서는 모델 규모가 커짐에 따라 성능이 신뢰성 있게 향상되었다.&lt;/p>
&lt;p>모든 모델 클래스의 성능은 규모가 같을 때 비슷하였다. 이는 Google이나 OpenAI에서 학습시키고, sparse 구조나 dense 구조를 사용하였던 모델 모두에 해당한다. 그러나 sparse 모델 구조에서는 일부 이점이 관찰되었다. BIG-G 희소 모델은 전체적으로 dense 모델의 두 배 정도의 성능을 보였으며, 10배 더 큰 모델만큼 잘 보정된 다중 선택 예측을 생성하였다.&lt;/p>
&lt;p>모델은 규모가 커짐에 따라 놀라운 능력을 보여준다. 특히 체스의 움직임을 제안하는 능력은 규모가 커짐에 따라 향상되며, 이는 모델이 학습 데이터에서 체스의 규칙을 학습하고 있음을 보여준다. 또한, 언어 모델은 이모티콘 문자열로 표현된 영화 플롯을 식별하는 능력을 보여주었다.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>BIG-bench가 현재 기술을 훨씬 뛰어넘는 발전을 측정하는 벤치마크로 계속 존재하기를 희망한다. BIG-bench는 계속 발전하는 벤치마크로서, 새로운 작업 제출과 평가를 지속적으로 받아들이고 있다. BIG-bench 작업은 간단한 JSON 형식이나 파이썬 코드로 정의될 수 있으며, 수락된 작업의 기여자들에게는 미래의 BIG-bench 논문과 릴리즈에 대한 저자 자격이 주어진다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2206.04615.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google/BIG-bench" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Emergent Abilities</title><link>https://kurtkim.github.io/p/emergent-abilities/</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/emergent-abilities/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델을 확장하면 다양한 downstream task에서 성능과 샘플 효율성이 예측 가능하게 향상된다. 하지만, 이 논문에서는 큰 언어 모델에서만 나타나는 &amp;ldquo;emergent abilities&amp;quot;이라는 예측 불가능한 현상에 대해 논의한다. 이는 작은 모델의 성능을 extrapolating 함으로써는 예측할 수 없으며, 추가적인 확장이 언어 모델의 능력 범위를 더욱 확장시킬 수 있는지에 대한 질문을 제기한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델의 크기를 늘리는 것이 NLP 작업의 성능과 효율성을 향상시킨다는 것은 잘 알려져 있다. 이는 scaling law를 통해 예측될 수 있으며, cross-entropy 손실의 스케일링 곡선은 이를 잘 보여준다. 그러나 특정 작업에서는 규모에 따른 성능 개선이 계속되지 않는 경우도 있어, 이러한 작업은 미리 예측하기 어렵다.&lt;/p>
&lt;p>이 논문에서는 대규모 언어 모델의 예측할 수 없는 능력 emergence 현상에 대해 논의한다. 이 &amp;ldquo;emergence&amp;rdquo; 개념은 물리학, 생물학, 컴퓨터 과학 등 여러 분야에서 널리 논의되어 왔다. 노벨 물리학상 수상자인 Philip Anderson의 1972년 에세이 &amp;ldquo;More Is Diﬀerent&amp;quot;에 기반한, Steinhardt (2022)가 수정한 일반적인 &amp;ldquo;emergence&amp;quot;의 정의를 사용한다:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Emergence is when quantitative changes in a system result in qualitative changes in behavior.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>이 연구에서는 학습 계산과 모델 parameter의 수로 측정한 모델 규모에 따른 emergence 현상을 연구한다. 대규모 언어 모델의 emergence 능력을 작은 규모 모델에서는 없지만 큰 규모 모델에서 나타나는 능력으로 정의하며, 이는 작은 규모 모델의 성능 개선을 통해 예측할 수 없다. 이전 연구에서 발견된 emergence 능력을 조사하고, 다양한 환경에서 분류한다. 이는 능력이 어떻게 획득되는지, 그리고 규모가 더 커지면 더 많은 능력이 emergence 하는지에 대한 연구를 촉진한다.&lt;/p>
&lt;hr>
&lt;h2 id="emergent-abilities-deﬁnition">Emergent Abilities Deﬁnition&lt;/h2>
&lt;p>emergence 라는 개념은 넓게 사용되며 다양한 방식으로 해석될 수 있다. 이 연구에서는 대규모 언어 모델의 emergence 능력에 대한 구체적인 정의를 고려한다:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>An ability is emergent if it is not present in smaller models but is present in larger models.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>emergence 능력은 작은 규모 모델에서의 성능 향상 추세를 extrapolating 함으로써 직접 예측할 수 없다. 스케일링 곡선을 통해 보면, emergence 능력은 특정 임계 규모에 이르기 전까지는 무작위에 가까운 성능을 보이지만, 그 이후에는 랜덤 이상의 성능 향상을 보인다. 이러한 질적 변화는 phase transition 이라고도 하며, 이는 작은 규모의 시스템을 검사할 때는 예상되지 않는 전반적인 행동의 큰 변화를 의미한다.&lt;/p>
&lt;p>현대의 언어 모델은 계산량, 모델 parameter 수, 학습 데이터 크기 등을 확장하는 방식으로 발전해 왔다. 이 논문에서는 학습 계산량을 기준으로 모델의 성능을 비교하고 분석하였으며, 더 많은 계산량으로 학습된 모델이 더 많은 parameter를 가질 가능성이 높다는 것을 보여주었다. 이는 대부분의 dense Transformer 언어 모델이 학습 계산량을 모델 parameter와 비례하여 확장했기 때문이다.&lt;/p>
&lt;p>학습 데이터셋 크기도 중요하지만, 대부분의 언어 모델이 모든 모델 크기에 대해 고정된 학습 예제를 사용하기 때문에 이것을 대비한 그래프는 제시하지 않는다. 여기서는 학습 계산량과 모델 크기에 초점을 맞추지만, 모든 규모를 적절하게 포착하는 단일 지표는 없다. 예를 들어, Chinchilla는 Gopher의 parameter의 1/4을 가지지만 비슷한 학습 계산량을 사용하며, sparse mixture-of-expert 모델은 dense 모델보다 학습/추론 계산량 당 더 많은 parameter를 가진다. 일반적으로, emergence를 여러 상관 변수의 함수로 보는 것이 현명할 수 있다.&lt;/p>
&lt;p>능력이 처음으로 나타나는 규모는 여러 요인에 따라 달라지며, 이는 능력의 고정된 특성이 아니다. 더 좋은 품질의 데이터로 학습된 모델은 더 적은 계산량이나 parameter로도 능력이 emergence 될 수 있다. 반면, emergence 능력은 데이터의 양, 품질, 모델의 parameter 수 등에 의존한다. 현대의 언어 모델은 아직 최적의 학습 방법이 발견되지 않았을 수 있으며, 이해는 시간이 지남에 따라 발전할 것이다. 이 논문의 목표는 특정 규모가 필요하다는 주장이 아니라, 이전 연구에서 emergence 된 행동의 예를 논의하는 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="few-shot-prompted-tasks">Few-Shot Prompted Tasks&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/emergent-abilities/images/figure1.png"
width="444"
height="140"
srcset="https://kurtkim.github.io/p/emergent-abilities/images/figure1_hu815adf4809b9c12116217611f1ef6498_21074_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/emergent-abilities/images/figure1_hu815adf4809b9c12116217611f1ef6498_21074_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="317"
data-flex-basis="761px"
>&lt;/p>
&lt;p>GPT-3에서 대중화된 프롬프팅 패러다임에서의 emergence 능력에 대해 논의하다. 프롬프팅에서는 사전 학습된 언어 모델에게 과제 프롬프트를 제공하고, 추가 학습이나 parameter 업데이트 없이 응답을 완성한다. Brown et al. 은 몇 가지 input-output 예를 모델의 컨텍스트에 포함시키는 few-shot 프롬프팅을 제안하였다. 이는 보이지 않는 추론 시간의 예제에 대해 모델이 작업을 수행하도록 요청하기 전에 사용된다.&lt;/p>
&lt;p>특정 규모까지 무작위 성능을 보이다가 그 이후에 성능이 크게 향상되면, few-shot 프롬프팅으로 작업을 수행하는 능력이 emergence 된 것이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/emergent-abilities/images/figure2.png"
width="1260"
height="886"
srcset="https://kurtkim.github.io/p/emergent-abilities/images/figure2_hu5e3e6658e752fc2b7e8dd5210f8c5069_177468_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/emergent-abilities/images/figure2_hu5e3e6658e752fc2b7e8dd5210f8c5069_177468_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="341px"
>&lt;/p>
&lt;p>&lt;strong>BIG-Bench.&lt;/strong> GPT-3와 LaMDA는 학습 계산량이 증가할 때마다 성능이 크게 향상된다. 비슷한 emergence 현상은 국제 음성 알파벳에서 발음 표기로 변환, 단어의 글자 뒤섞기 복원, 페르시아어 질문-답변 등의 작업에서도 동일한 모델 규모에서 발생한다.&lt;/p>
&lt;p>&lt;strong>TruthfulQA.&lt;/strong> 이 벤치마크는 GPT-3 모델에 대해 적대적으로 구성되어 있으며, 가장 큰 모델 크기로 확장되어도 성능이 무작위 수준을 넘지 못한다. 하지만, Gopher 모델은 가장 큰 크기로 확장될 때 성능이 무작위 수준보다 20% 이상 향상된다.&lt;/p>
&lt;p>&lt;strong>Grounded conceptual mappings.&lt;/strong> 이 작업에서도, 가장 큰 GPT-3 모델을 사용할 때만 성능이 무작위 수준을 넘어선다.&lt;/p>
&lt;p>&lt;strong>Multi-task language understanding.&lt;/strong> GPT-3, Gopher, Chinchilla 등의 모델은 특정 크기 이상으로 확장될 때만 성능이 크게 향상된다. 이 결과는 광범위한 주제에 대한 지식 기반 질문 해결 능력이 특정 규모 이상의 모델에서만 가능함을 시사한다.&lt;/p>
&lt;p>&lt;strong>Word in Context.&lt;/strong> GPT-3와 Chinchilla는 가장 큰 모델 크기로 확장되어도 무작위 수준보다 나은 성능을 보이지 못한다. 하지만, PaLM이 훨씬 더 큰 규모로 확장되었을 때 무작위 수준을 넘어서는 성능이 나타났다.&lt;/p>
&lt;hr>
&lt;h2 id="augmented-prompting-strategies">Augmented Prompting Strategies&lt;/h2>
&lt;p>few-shot 프롬프팅은 거대 언어 모델과의 상호작용에서 가장 많이 사용되지만, 다른 프롬프팅과 미세 조정 전략들도 제안되고 있다. 특히, 충분한 규모의 모델에 적용될 때까지 성능 개선이 없거나 해로운 기법도 결국은 emergence 능력으로 간주된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/emergent-abilities/images/figure3.png"
width="1232"
height="450"
srcset="https://kurtkim.github.io/p/emergent-abilities/images/figure3_hud69e25d233fef2d95a3647ea8aa11ffe_126060_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/emergent-abilities/images/figure3_hud69e25d233fef2d95a3647ea8aa11ffe_126060_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="273"
data-flex-basis="657px"
>&lt;/p>
&lt;p>&lt;strong>Multi-step reasoning.&lt;/strong> 다단계 추론 작업은 언어 모델에게 큰 도전이었다. 하지만, 최근 &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅 전략은 언어 모델이 최종 답변을 제공하기 전에 중간 단계를 생성하도록 안내함으로써 이 문제를 해결하였다. 이 방법은 특정 규모 이상으로 확장될 때만 표준 프롬프팅을 능가한다. 또한, 설명을 추가하여 few-shot 프롬프팅을 강화했을 때도 비슷한 성능 향상이 나타났다.&lt;/p>
&lt;p>&lt;strong>Instruction following.&lt;/strong> 언어 모델이 작업을 설명하는 지시사항을 읽어 새로운 작업을 수행하는 연구가 진행되고 있다. 지시사항으로 표현된 작업에 대해 미세조정하면, 언어 모델은 보지 못한 작업에 대한 지시사항에 적절하게 반응하는 것이 확인되었다. 하지만, 이 기법은 특정 규모 이상의 모델에서만 성능이 향상되었다. 더 작은 encoder-decoder T5 모델을 미세조정함으로써도 이러한 행동이 유도될 수 있음이 확인되었다.&lt;/p>
&lt;p>&lt;strong>Program execution.&lt;/strong> 다단계 계산 작업을 수행하는 데 있어, 중간 출력을 예측하도록 언어 모델을 미세조정 하면 성공적인 실행이 가능해진다. 그러나 이 방법은 특정 규모 이상의 모델에서만 효과가 있음이 확인되었다. 예를 들어, 8자리 수의 덧셈에서는 $\sim 9 \cdot 10^{19}$ 학습 FLOPs (40M parameter) 이상의 모델에서만 스크래치패드 사용이 도움이 된다.&lt;/p>
&lt;p>&lt;strong>Model calibration.&lt;/strong> 언어 모델의 중요한 연구 방향 중 하나는 캘리브레이션인데, 이는 모델이 어떤 질문에 올바르게 대답할 수 있을지 예측하는 능력을 측정한다. True/False 기법과 표준 캘리브레이션 방법 두 가지를 비교한 결과, True/False 기법의 우월성은 가장 큰 모델 규모로 확장될 때만 나타났다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/emergent-abilities/images/table1.png"
width="1214"
height="862"
srcset="https://kurtkim.github.io/p/emergent-abilities/images/table1_hu2bb37414272cdf3aacadd20959f608a6_393813_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/emergent-abilities/images/table1_hu2bb37414272cdf3aacadd20959f608a6_393813_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="338px"
>&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>few-shot 프롬프팅 능력 등 다양한 능력들은 충분히 큰 언어 모델에서만 관찰되며, 이는 작은 모델에서의 성능을 통해 예측할 수 없다. 이러한 emergence 능력은 사전 학습에 포함되지 않은 작업들에서 나타나므로, 언어 모델이 수행할 수 있는 작업의 전체 범위를 정확히 알 수 없다. 이는 더 큰 규모의 모델로 확장함으로써 새로운 능력이 emergence 될 수 있음을 시사한다. 현재 언어 모델이 수행할 수 없는 작업들은 미래에 능력이 emergence 될 가능성이 높은 후보이다.&lt;/p>
&lt;p>규모 증가가 새로운 기법을 가능하게 하는 능력은 이론적인 것이 아니다. 예를 들어, GPT-3를 크게 확장해도 Word in Context (WiC) 벤치마크에서 무작위 수준 이상의 one-shot 프롬프팅 성능을 얻지 못하였다. 그러나 나중의 연구에서는 decoder만 있는 언어 모델을 더욱 확장함으로써 이 작업에서 무작위 수준 이상의 성능을 달성하는 것이 가능하다는 것을 발견하였다. 이는 구조적 변경 없이도 성능 향상을 가져올 수 있다는 것을 보여준다.&lt;/p>
&lt;h3 id="potential-explanations-of-emergence">Potential explanations of emergence&lt;/h3>
&lt;p>emergence 능력의 다양한 예시가 있지만 왜 그런 방식으로 나타나는지는 아직 명확히 설명되지 않았다. 특정 작업은 특정 규모 이상의 모델을 필요로 하는 이유에 대한 직관을 제공할 수 있다. 예를 들어, 순차 계산을 요구하는 다단계 추론 작업은 일정 깊이의 모델이 필요할 수 있다. 또한, 더 많은 parameter와 학습은 세계 지식을 필요로 하는 작업에 도움이 될 수 있다. 예를 들어, closed-book 질문-답변에서 좋은 성능을 내려면, 압축된 지식 베이스를 포착할 수 있는 충분한 parameter를 가진 모델이 필요할 수 있다.&lt;/p>
&lt;p>emergence 능력을 측정하는 평가 지표의 선택은 중요하다. 예를 들어, 정확한 문자열 일치를 사용하는 것은 점진적인 개선을 emergence로 가려낼 수 있다. 이와 유사하게, 다단계 문제에 대해 최종 답안만 평가하는 경우, 부분적으로 올바른 해결책에 대한 인정이 없을 수 있다. 하지만 이러한 방식은 중간 단계의 품질이 무작위 수준 이상으로 갑자기 emergence 하는 이유를 설명하지 못하며, 많은 분류 작업에서 emergence 능력이 계속 관찰되므로, 부분 점수를 부여하지 않는 평가는 불완전한 설명일 수 있다.&lt;/p>
&lt;p>cross-entropy 손실을 측정하는 대체 평가를 통해, 6개의 emergence BIG-Bench 작업을 분석하였다. 이 분석은 작은 모델 규모에서도 cross-entropy 손실이 개선되며, 이는 log-likelihood의 개선이 downstream 지표에 의해 가려질 수 있다는 것을 보여준다. 그러나 이 분석은 downstream 지표가 왜 emergence 하거나 어떤 규모에서 emergence가 일어나는지를 예측할 수는 없다. 전반적으로, 규모가 emergence 능력을 가능하게 하는 요소를 파악하기 위해 더 많은 연구가 필요하다.&lt;/p>
&lt;h3 id="beyond-scaling">Beyond scaling&lt;/h3>
&lt;p>특정 규모에서 emergence 능력이 나타나지만, 더 작은 규모에서도 이를 달성할 수 있다. 즉, 모델 규모만이 emergence 능력을 해제하는 유일한 요소는 아니다. 거대 언어 모델 학습의 발전으로 새로운 구조, 더 높은 품질의 데이터, 개선된 학습 절차를 가진 작은 모델에서도 특정 능력이 emergence 될 수 있다. 예를 들어, 더 적은 모델 parameter를 가진 PaLM 62B는 LaMDA 137B와 GPT-3 175B보다 더 나은 성능을 보여주었다. 이는 높은 품질의 학습 데이터와 구조적 차이 덕분일 수 있다. 또한, 다른 사전 학습 목표를 통해 emergence 능력을 해제하는 것도 가능하다는 것이 밝혀졌다.&lt;/p>
&lt;p>한 번 능력이 발견되면, 추가 연구를 통해 더 작은 규모의 모델에서도 사용 가능해질 수 있다. 예를 들어, 작업을 설명하는 자연어 지시사항을 따르는 언어 모델의 연구가 진행되고 있다. 초기에는 큰 모델에서만 작동하였지만, 새로운 연구를 통해 더 작은 규모의 모델에서도 유사한 기능을 구현하게 되었다. 더불어, 미세 조정과 인간의 피드백에서 강화 학습 방법을 적용한 모델은 더 큰 모델보다 더 나은 성능을 보이며, 다양한 사용 사례에서 뛰어난 성능을 보여주었다.&lt;/p>
&lt;p>언어 모델의 few-shot 프롬프팅 능력 향상에 대한 연구가 있다. 이는 언어 모델링이 특정 downstream 행동을 촉진하는 이유에 대한 이론적 연구와 연관되어 있다. 사전 학습 데이터의 특정 특징과 특정 모델 구조가 few-shot 학습과 연관되어 있음이 확인되었고, 이는 더 작은 모델에서 emergence 능력을 가능하게 할 수 있다. 또한, 학습 데이터의 임계 빈도가 구문 규칙 학습을 활성화하는 것으로 나타났다. 이러한 연구는 emergence 능력에 대한 규모 임계값을 낮추는 데 중요하며, 이를 통해 이러한 능력에 대한 연구를 더 넓게 공유할 수 있게 된다.&lt;/p>
&lt;p>단순히 규모를 증가시키는 것에는 한계가 있다. 하드웨어 제약에 의해 병목 현상을 겪을 수 있고, 일부 능력은 아직 emergence 되지 않았거나 절대로 emergence 되지 않을 수 있다. 예를 들어, 큰 학습 데이터셋의 범위를 벗어난 작업은 중요한 성능을 얻지 못할 수 있다. 또한, 능력이 emergence 된 후에 성능이 정체될 수 있어, 규모 확장이 능력을 원하는 수준에 도달하게 할 보장은 없다.&lt;/p>
&lt;h3 id="another-view-of-emergence">Another view of emergence&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/emergent-abilities/images/figure4.png"
width="928"
height="670"
srcset="https://kurtkim.github.io/p/emergent-abilities/images/figure4_hu9160ab00a749b7a47699126bbb5904ea_156179_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/emergent-abilities/images/figure4_hu9160ab00a749b7a47699126bbb5904ea_156179_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="332px"
>&lt;/p>
&lt;p>언어 모델의 성능은 규모(예: 힉습 FLOPs 또는 모델 parameter)와 밀접한 관련이 있지만, 이는 emergence 능력을 이해하는 유일한 방법이 아니다. 특정 작업에 대한 능력의 emergence는 언어 모델의 일반 텍스트 말뭉치에 대한 perplexity로도 분석될 수 있다. 예를 들어, 언어 모델의 WikiText103 perplexity와 MMLU 벤치마크 성능 관계를 보여주는 그래프가 있다. 이는 학습 FLOPs와 모델 parameter와의 관계를 보여주는 그래프와 함께 제시된다.&lt;/p>
&lt;p>이 연구에서 고려된 모델들의 경우, WikiText103 perplexity와 학습 FLOPs가 높은 상관관계를 보여 emergence 능력의 그래프가 비슷하다. 하지만, vanilla dense Transformer 모델을 넘어 새로운 기술이 개발되면서, 이런 상관관계는 앞으로 유지되지 않을 수 있다. 예를 들어, 검색 기능이 강화된 모델은 더 적은 학습 계산량과 모델 parameter로도 높은 WikiText103 perplexity를 보일 수 있다. 또한, 모델 간 비교에서 WikiText103 perplexity 사용은 학습 데이터 구성 차이 등으로 복잡할 수 있다. 결국, emergence 능력은 여러 상관 변수의 함수로 보는 것이 적절할 것이다.&lt;/p>
&lt;h3 id="emergent-risks">Emergent risks&lt;/h3>
&lt;p>사전 학습에 명시적으로 포함되지 않은 상태에서 emergent 능력이 few-shot 프롬프팅에서 관찰되듯이, 위험 요소도 발생할 수 있다. 큰 언어 모델의 사회적 위험, 예를 들어 truthfulness, bias, toxicity 등은 연구 분야가 늘어나고 있다. 이런 위험 요소들은 &amp;ldquo;emergent&amp;quot;으로 정확하게 특성화되는지 여부에 상관없이 중요하며, 일부 경우에는 모델 규모와 함께 증가한다. emergent 능력에 대한 연구가 언어 모델의 스케일링을 촉진하므로, 이들이 emergent되지 않더라도 모델 규모와 함께 증가하는 위험에 대해 인식하는 것이 중요하다.&lt;/p>
&lt;p>특정 사회적 위험과 모델 규모 간의 관계에 대한 연구 결과를 요약하면 다음과 같다. 성별 bias를 측정하는 WinoGender에서는 모델 규모 확장이 성능을 향상시켰다. 하지만 모호한 상황에서는 규모 확장에 따라 bias가 증가할 가능성이 있다. toxicity에 대해서는, 큰 언어 모델이 toxicity가 강한 응답을 생성할 수 있지만, 적절한 프롬프트를 제공함으로써 이를 완화할 수 있다. 더 큰 모델이 학습 데이터를 더 잘 기억하는 경향이 있지만, 중복 제거 방법을 통해 이를 줄이는 동시에 성능을 향상시킬 수 있다. 마지막으로, GPT-3 모델이 클수록 인간의 거짓말을 따라하는 경향이 있지만, 규모를 확장하면 무작위 선택보다 더 나은 성능을 보일 수 있다.&lt;/p>
&lt;p>emergence 위험은 미래의 언어 모델에서만 존재하거나 현재 모델에서 아직 발견되지 않은 현상을 포함한다. 이런 행동은 백도어 취약점, 부주의한 기만, 유해한 컨텐츠 생성 등이 있을 수 있다. 이런 위험을 발견하고 완화하기 위해 데이터 필터링, 예측, 거버넌스, 그리고 자동으로 유해한 행동을 발견하는 방법들이 제안되었다.&lt;/p>
&lt;h3 id="sociological-changes">Sociological changes&lt;/h3>
&lt;p>여기서 논의된 emergence 능력은 모델 행동에 초점을 두고 있으며, NLP에서의 여러 emergence 형태 중 하나이다. 규모 증가는 언어 모델에 대한 커뮤니티의 인식과 사용 방식을 변화시키는 사회학적 변화를 일으킨다. NLP는 전통적으로 과제별 모델에 초점을 두었지만, 최근에는 학습 데이터에 명시적으로 인코딩되지 않은 다양한 과제를 수행하는 &amp;ldquo;general purpose&amp;quot;의 모델에 대한 연구와 개발이 확장되었다.&lt;/p>
&lt;p>규모 확장이 몇 번의 shot을 통해 프롬프트된 general-purpose 모델이 특정 과제를 위해 세밀하게 튜닝된 이전 모델의 성능을 능가하는 경우, general-purpose 모델로의 사회학적 변화가 나타난다. 예를 들어, GPT-3 175B, PaLM 540B, Flamingo 80B 등의 모델은 각자의 벤치마크에서 state-of-the-art를 달성하였다. 이런 능력들은 예측 가능한 규모 확장 곡선을 따르므로 반드시 emergence 능력은 아니지만, NLP 커뮤니티에서 general-purpose 모델로의 변화를 보여준다.&lt;/p>
&lt;p>general-purpose 언어 모델이 소수의 예시로도 보지 못한 과제를 수행하는 능력은 NLP 연구 커뮤니티를 넘어 다양한 응용 분야에서 활용되고 있다. 이런 모델은 로봇 조작, 사용자 상호작용, 다중 모달 추론 등에 사용되며, 실제 세계에서도 GitHub CoPilot와 같은 제품이나 OpenAI의 GPT-3 API와 같은 서비스로 적용되고 있다.&lt;/p>
&lt;h3 id="directions-for-future-work">Directions for future work&lt;/h3>
&lt;p>emergence 능력에 대한 미래의 연구는 더 능력 있는 언어 모델을 학습하고, 언어 모델이 과제를 더 잘 수행할 수 있도록 하는 방법을 포함할 수 있다.&lt;/p>
&lt;p>&lt;strong>Further model scaling.&lt;/strong> 모델을 더 크게 확장하는 것은 언어 모델의 능력을 향상시키는 방향으로 보이지만, 이는 계산 비용이 많이 들고 하드웨어 문제를 해결해야 하는 도전이 있다. 따라서, 다른 접근법이 거대 언어 모델의 emergence 능력 개선에 중요한 역할을 할 것으로 예상된다.&lt;/p>
&lt;p>&lt;strong>Improved model architectures and training.&lt;/strong> 모델 아키텍처와 학습 절차를 향상시키면, emergence 능력이 우수한 고품질 모델을 만들 수 있으며 이는 계산 비용을 줄일 수 있다. sparse mixture-of-experts 구조를 사용하면 모델의 parameter 수를 늘릴 수 있다. 또한, 다양한 계산량을 사용하거나 지역화된 학습 전략을 사용하거나 외부 메모리를 추가하는 등 다양한 방식으로 계산 효율성을 향상시킬 수 있다. 이러한 접근법들은 초기 단계에 있지만 많은 가능성을 보여주고 있다. 하지만 이들이 널리 채택되려면 추가 연구가 필요하다.&lt;/p>
&lt;p>&lt;strong>Data scaling.&lt;/strong> 언어 모델이 구문, 의미, 세계 지식을 획득하는 데는 충분한 데이터셋에서 오랜 시간 동안 학습하는 것이 중요하다는 것이 입증되었다. 최근 연구는 최적의 모델을 학습시키는 데 필요한 데이터 양이 과소평가되었다고 주장하며, 이는 학습 데이터의 중요성을 강조하였다. 대규모 데이터셋을 수집하여 모델을 더 오래 학습하면, 모델 크기의 제약 하에서도 더 많은 능력을 emergence 시킬 수 있다.&lt;/p>
&lt;p>&lt;strong>Better techniques for and understanding of prompting.&lt;/strong> few-shot 프롬프팅은 간단하고 효과적이지만, 일반적인 프롬프팅 개선을 통해 언어 모델의 능력을 더 확장할 수 있다. 출력 확률 보정이나 노이즈 채널 사용 등의 간단한 수정은 다양한 작업 성능을 향상시키며, 중간 단계를 포함하는 예제 확장은 다단계 추론 작업을 가능하게 한다. 프롬프팅의 성공 요인을 더 잘 이해하고 탐색함으로써, 더 작은 모델에서도 emergence 능력을 이끌어낼 수 있는 방법을 찾을 수 있다. 또한, 시간이 지나며 더 강력한 모델이 개발됨에 따라 프롬프팅의 최선의 관행도 변화할 것으로 보인다.&lt;/p>
&lt;p>&lt;strong>Frontier tasks.&lt;/strong> 언어 모델은 다양한 작업을 수행할 수 있지만, 여전히 무작위 정확도 이상을 달성하지 못하는 작업들이 많다. 이런 작업들은 대개 추상적인 추론을 요구하는데, 이에 대한 이해를 높이는 연구가 필요하다. 또한, 다언어 emergence와 같은 새로운 연구 방향이 두드러지고 있으며, 이는 모델 크기와 학습 데이터가 중요한 역할을 하는 것을 보여준다. 더 나아가, 여러 모달리티에서의 프롬프팅과 같은 새로운 도전 과제를 탐색할 수 있다.&lt;/p>
&lt;p>&lt;strong>Understanding emergence.&lt;/strong> 큰 언어 모델에서 emergence 능력이 왜 그리고 어떻게 발생하는지는 미래 연구의 개방된 질문이다. 이 논문의 초기 분석들은 emergence의 원인이나 예측 방법에 대한 완전한 답변을 제공하지 못하였다. 미래 연구는 emergence를 새롭게 분석할 수 있으며, 이는 미래 모델의 능력 예측과 더 능력 있는 언어 모델 학습 방법에 대한 새로운 통찰을 제공할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>특정 계산 규모에서만 중요한 성능을 보이는 언어 모델의 발현 능력에 대해 논의하였다. 이런 능력은 다양한 모델과 작업, 실험 상황에 걸쳐 나타나며, 언어 모델을 확장함으로써 최근에 발견되었다. 이러한 능력이 어떻게 나타나며, 더 많은 확장이 더 많은 발현 능력을 가능하게 할지는 자연어 처리(NLP) 분야의 중요한 미래 연구 주제이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2206.07682.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>UL2</title><link>https://kurtkim.github.io/p/ul2/</link><pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/ul2/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문에서는 데이터셋과 설정에 걸쳐 보편적으로 효과적인 사전 학습 모델에 대한 통합 프레임워크를 제시한다. 아키텍처 원형과 사전 학습 목표를 분리하고, 다양한 사전 학습 목표가 어떻게 변환될 수 있는지를 보여준다. 또한, 다양한 사전 학습 패러다임을 함께 결합하는 Mixture-of-Denoisers (MoD)를 제안하고, downstream 미세 조정이 특정 사전 학습 체계와 연관되는 모드 전환 개념을 도입한다. 이 방법으로, 이 모델은 다양한 NLP 작업에서 최고의 성능을 달성하였으며, 특히 in-context 학습에서 강력한 결과를 보였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>NLP 연구자나 실무자들은 다양한 사전 학습된 모델 중에서 선택할 수 있습니다. 그러나 어떤 모델을 사용할지는 &amp;ldquo;상황&amp;quot;과 &amp;ldquo;작업 종류&amp;quot;에 따라 결정된다.&lt;/p>
&lt;p>이에 대한 답변은 &amp;ldquo;encoder-only or encoder-decoder?&amp;rdquo;, &amp;ldquo;span corruption or language model?&amp;rdquo; 등의 디테일한 후속 질문을 수반하며, 결과적으로는 목표로 하는 downstream task에 따라 항상 달라진다. 이 논문은 왜 사전 학습된 언어 모델의 선택이 downstream task에 따라 달라져야 하는지, 그리고 어떻게 다양한 작업에서 보편적으로 잘 작동하는 모델을 사전 학습할 수 있는지에 대해 질문하고 재고한다.&lt;/p>
&lt;p>이 논문은 언어 모델을 보편적으로 적용 가능하게 하는 방안을 제시한다. 다양한 작업과 설정에서 효과적인 &amp;ldquo;Unifying Language Learning Paradigms&amp;rdquo; 즉, UL2라는 프레임워크를 소개하며, 이는 다른 모델들이 종종 타협해야 하는 반면, UL2는 일관되게 잘 수행될 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/figure1.png"
width="736"
height="688"
srcset="https://kurtkim.github.io/p/ul2/images/figure1_hucdf1dfbdc96e3b94b170bbbc050e8c09_75927_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/figure1_hucdf1dfbdc96e3b94b170bbbc050e8c09_75927_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="256px"
>&lt;/p>
&lt;p>보편적 모델의 장점은 자원을 여러 모델에 분산시키는 대신, 단일 모델의 개선과 확장에 집중할 수 있다는 것이다. 특히, 자원이 제한된 환경에서는 다양한 작업에 잘 적응할 수 있는 하나의 사전 학습된 모델을 가지는 것이 더 바람직하다.&lt;/p>
&lt;p>UL2의 핵심은 새롭게 제안된 Mixture-of-Denoisers (MoD)라는 사전 학습 목표로, 이를 통해 여러 작업에서 강력한 성능을 보인다. MoD는 기존의 denoising 목표와 새롭게 도입된 X-denoising, S-denoising, R-denoising를 혼합한 것으로, 이는 개념적으로 단순하지만 다양한 작업에 매우 효과적이다.&lt;/p>
&lt;p>이 논문의 방법은 모델이 조건화된 컨텍스트의 유형이 다른 사전 학습 목표들을 활용한다. 예로, span corruption 목표는 prefix 언어 모델링의 여러 영역을 활성화하는데, 여기서 prefix는 손상되지 않은 토큰의 연속된 구간을 의미하며, 목표는 모든 prefix에 접근이 가능하다. 스팬이 전체 시퀀스 길이에 근접하는 설정은 장거리 컨텍스트에 기반한 언어 모델링과 유사하다. 따라서, 이러한 다른 패러다임들을 부드럽게 연결하는 사전 학습 목표를 설계할 수 있다.&lt;/p>
&lt;p>각 denoiser가 다른 방식으로 어렵다는 것은 명확하며, extrapolation 또는 interpolation의 성격에서도 차이가 있다. 예를 들어, span corruption을 통해 모델을 양방향 컨텍스트로 제한하면 작업이 쉬워지고 완성에 가깝게 된다. 반면, PreﬁxLM/LM 목표는 더 &amp;ldquo;open ended&amp;quot;이다. 이러한 행동은 다양한 denoising 목표의 cross entropy loss를 모니터링하여 쉽게 관찰할 수 있다.&lt;/p>
&lt;p>MoD 공식을 통해, 모델이 사전 학습 중 다른 denoiser를 구별하고, downstream task를 학습할 때 적응적으로 모드를 전환하는 것이 유익하다는 추측을 제시하였다. 이를 위해 &amp;ldquo;mode switching&amp;rdquo; 라는 새로운 개념을 도입해, 사전 학습 작업에 특정 센티널 토큰을 연결하고 dynamic mode switching을 가능하게 하였다. 이를 통해 모델은 사전 훈련 후 필요에 따라 R, S, X denoiser 사이의 모드를 전환할 수 있다.&lt;/p>
&lt;p>아키텍처를 self-supervision scheme에서 분리하였으며, 사전 학습된 모델이 그 기반 아키텍처에 의해 크게 특징지어진다는 일반적인 오해와 달리, denoiser의 선택이 더 큰 영향을 미친다는 것을 발견하였다. MoD는 어떤 기반체도 지원하며, UL2는 아키텍처에 중립적이다. 기본적으로, 기반 아키텍처의 선택은 다른 효율성 지표 간의 타협이라고 볼 수 있다.&lt;/p>
&lt;p>9가지 다양한 작업에 대한 실험을 수행했고, 결과로서 UL2는 모든 설정에서 T5와 GPT와 같은 기준을 능가하는 것을 확인하였다. 평균적으로 UL2는 T5를 43.6%, 언어 모델을 76.1% 능가하였으며, 모든 작업에서 UL2만이 T5와 GPT와 같은 모델을 능가하였다.&lt;/p>
&lt;p>UL2를 대략 20B 개의 parameter로 확장하여 50개가 넘는 다양한 NLP 작업에 대한 실험을 수행하였다. 이 작업들은 언어 생성, 언어 이해, 텍스트 분류, 질문 응답, 상식 추론, 긴 텍스트 추론, 구조화된 지식 구축 및 정보 검색 등을 포함한다. 이러한 실험 결과, UL2는 대부분의 작업과 설정에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>UL2로 수행한 zero/few-shot 실험에서 UL2가 zero-shot SuperGLUE에서 GPT-3 175B를 능가하였다. 최신의 state-of-the-art의 모델과 비교했을 때, UL2는 C4 코퍼스에서만 학습되었음에도 compute-matched 설정에서 경쟁력을 유지하였다. 또한, zero-shot과 미세조정 성능 사이의 타협을 탐색한 결과, UL2가 두 학습 패러다임에 대해 pareto-efﬁcient를 확인하였다. one-shot 요약에서, UL2는 LM 맞춤형 T5 XXL 모델의 성능을 세 배로 늘렸고, 같은 계산 비용에서 PaLM과 LaMDA와 경쟁하거나 능가하였다. 이에 따라, 학습된 UL2 모델의 T5X 기반 Flax 체크포인트를 공개하였다.&lt;/p>
&lt;hr>
&lt;h2 id="background-pre-trained-language-models">Background: Pre-trained Language Models&lt;/h2>
&lt;p>사전 학습된 언어 모델, 사전 학습 목표, 그리고 다른 통합 사전 학습 제안에 대한 배경에 대해 논의한다.&lt;/p>
&lt;h3 id="pre-trained-language-models">Pre-trained Language Models&lt;/h3>
&lt;p>언어에 대한 사전 학습된 표현을 학습하는 것은 현대 NLP 연구의 핵심 부분이다. 첫 Transformer인 GPT는 causal 언어 모델로 학습되었고, BERT는 많은 downstream task에 대한 bidirectional 모델링의 중요성을 보여주었다. BERT는 masked language modeling(MLM)을 소개했고, XLNet은 학습 중에 마스크된 토큰 간의 종속성을 고려하기 위한 permutation 언어 모델링을 도입하였다. 그 후에도 여러 논문들이 사전 학습 과정에 대한 추가적인 개선을 제안하였다.&lt;/p>
&lt;p>T5와 같은 two-stack encoder-decoder 구조는 분류와 sequence-to-sequence 작업에서의 성능 향상으로 인기를 얻었다. 그러나 이러한 모델들은 오픈 텍스트 생성과 프롬프트 기반 추론에서 제한된 성능을 보여, 다른 목표로 학습된 decoder-only 모델이 필요하게 되었다. 이 작업에서는 두 구조 모두에게 적합한 일반적인 학습 패러다임을 통해 이들 사이의 성능 격차를 줄이려고 한다.&lt;/p>
&lt;p>&lt;strong>Decoder-only vs Encoder-only&lt;/strong> decoder-only와 encoder-only 아키텍처는 모두 다음 토큰을 예측하는 autoregressive 모델이다. 하지만 이들은 BERT 스타일의 encoder-only 모델이 인기를 얻은 position-wise masked LM denoising(autoencoding)과는 다르며, 이러한 autoencoding 모델은 생성 능력이 제한적이라는 단점이 있다. downstream task를 위해 task speciﬁc classiﬁcation head를 사용하는 것이 번거롭기 때문에, 이러한 모델의 사용은 권장하지 않는다. 그러나 예외적으로 regression이나 효율성 향상을 위해 task speciﬁc head를 사용할 수 있다. 이를 고려하면, encoder-decoder에서 시작하여 필요에 따라 decoder를 제거하는 것이 바람직하며, 결국 decoder-only과 encoder-decoder 아키텍처 사이에서 선택해야 한다.&lt;/p>
&lt;p>&lt;strong>Decoder-only vs Encoder-Decoder&lt;/strong> decoder-only 모델과 encoder-decoder 모델의 차이는 미미하며, PreﬁxLM 모델은 사실상 공유 parameter를 가진 encoder-decoder 모델이다. encoder-decoder 모델은 입력과 대상을 독립적으로 처리하며, decoder-only 모델은 이들을 연결해 처리한다. 이 둘의 inductive bias는 상당히 유사하지만, encoder-decoder 모델은 일반적으로 decoder-only 모델의 약 2배의 parameter를 가진다. 이는 encoder-decoder 모델이 입력 토큰과 대상 토큰을 연결하는 교차 주의 구성 요소를 가지고 있기 때문이다.&lt;/p>
&lt;p>&lt;strong>Sparse Models&lt;/strong> 최근에는 Switch Transformer, GLaM, GShard 등의 sparse 전문가 혼합 모델과 같은 sparse 사전학습 모델이 state-of-the-art를 달성하는 추세이다. 이러한 sparse 모델은 사전학습 목표 주제와는 별개로 밀집 모델과 비교해 ﬂop-per-parameter가 매우 다르며, 이는 encoder-decoder 모델 대 decoder-only 모델 논의에서 주요한 이슈이다.&lt;/p>
&lt;h3 id="pre-training-objectives-for-large-language-models">Pre-training Objectives for Large Language Models&lt;/h3>
&lt;p>최근의 연구는 대규모 감독 멀티태스크 사전학습의 가능성을 보여주지만, 대부분의 사전학습 목표는 비지도 데이터에 의존하고 있다. decoder-only 모델은 주로 causal 언어 모델 목표로 학습되며, encoder-decoder 모델에는 범위 손상이 효과적인 목표로 탐색되었다. 다양한 아키텍처와 사전학습 목표의 조합이 zero-shot 일반화에 어떤 영향을 미치는지에 대한 체계적인 연구가 이루어졌다. 또한, 특정 denoising 방법의 이점은 여전히 불명확하며, 사전학습은 일반적으로 subword 수준에서 적용되지만, 문자나 바이트 수준에서도 적용된 사례가 있다. 이 경우, 손상된 범위는 subword 기반 denoising보다 훨씬 크다.&lt;/p>
&lt;h3 id="uniﬁed-pre-training-proposals">Uniﬁed Pre-training Proposals&lt;/h3>
&lt;p>UniLM은 single transformer 모델을 사용하여 여러 언어 모델링 목표에 대해 학습하는 방식을 제안하였다. 이는 BERT와 preﬁx-LM 모델을 결합하는 방식과 유사하며, explicit mask token을 추가하는 클로즈 타입의 공식을 사용한다. 최근에는 주제 통합 추세가 있어, 상식 추론, 질문 응답, 문제 해결, 구조화된 지식 그라운딩 등의 공통 작업을 하나의 모델로 통합하는 연구가 진행되고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="unifying-language-learning-paradigms-ul2">Unifying Language Learning Paradigms (UL2)&lt;/h2>
&lt;p>UL2 프레임워크와 제안된 사전 학습 목표에 대해 설명한다.&lt;/p>
&lt;h3 id="pre-training">Pre-training&lt;/h3>
&lt;p>제안된 사전 학습 목표에 대해 논의한다.&lt;/p>
&lt;h4 id="uniﬁed-perspective-for-pre-training-tasks">Uniﬁed Perspective for Pre-training Tasks&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/figure2.png"
width="1056"
height="688"
srcset="https://kurtkim.github.io/p/ul2/images/figure2_huf6428536317265b6d5b23be9f26a7380_170741_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/figure2_huf6428536317265b6d5b23be9f26a7380_170741_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="368px"
>&lt;/p>
&lt;p>많은 사전 학습 작업은 &amp;ldquo;input-to-target&amp;rdquo; 형태로 구성되며, 모델은 주어진 맥락(입력)을 바탕으로 예상 출력(대상)을 생성하다. 언어 모델은 이전 시점의 모든 토큰을 입력으로 사용해 다음 토큰을 예측하며, span corruption에서는 손상되지 않은 토큰을 사용해 손상된 범위를 예측한다. Preﬁx-LM은 양방향 입력 처리를 통해 더 많은 모델링 능력을 제공한다.&lt;/p>
&lt;p>사전 학습 목표는 서로 간소화될 수 있다. 예를 들어, span corruption 목표에서 전체 시퀀스가 손상된 범위(대상)인 경우, 문제는 실질적으로 언어 모델링 문제가 된다. 큰 범위를 설정하여 span corruption을 사용하면, 지역적 영역에서 언어 모델링 목표를 효과적으로 모방할 수 있다.&lt;/p>
&lt;p>이 논문에서는 denoising 작업의 모든 종류를 포함하는 표기법을 정의한다. denoising 작업의 입력과 목표는 평균 범위 길이($µ$), 손상률($r$), 손상 범위 수($n$) 세 가지 값으로 parameterized &amp;ldquo;span corruption&amp;rdquo; 함수를 통해 생성된다. 입력 텍스트가 주어지면, 이 함수는 µ의 평균을 가진 분포에서 추출된 범위에 손상을 가하고, 이 손상된 범위는 복구 대상으로 사용된다.&lt;/p>
&lt;p>이 공식을 사용하여 causal 언어 모델링과 유사한 목표를 설정하려면, 시퀀스 길이와 동일한 범위 길이를 가진 단일 범위$(µ = L, r = 1.0, n = 1)$를 설정하면 된다. Preﬁx LM 목표와 유사하게 설정하려면, preﬁx의 길이인 P를 사용하여 $(µ = L − P, r = 1.0 − P/L, n = 1)$을 설정하고, 단일 손상 범위가 항상 시퀀스의 끝에 도달하도록 제약을 둔다.&lt;/p>
&lt;p>inputs-to-targets 공식은 encoder-decoder 모델과 single-stack transformer 모델에 모두 적용 가능하다. 이 논문에서는 다음 대상 토큰을 예측하는 모델을 선택하는데, 이는 더 일반적이며 더 많은 작업을 수용할 수 있기 때문이다. 이 방법은 특수한 &amp;ldquo;CLS&amp;rdquo; 토큰과 task-speciﬁc projection head 사용을 배제한다.&lt;/p>
&lt;h4 id="mixture-of-denoisers">Mixture of Denoisers&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/figure3.png"
width="1076"
height="322"
srcset="https://kurtkim.github.io/p/ul2/images/figure3_hu3a62ad6ba63de7d780fee257a0671dfb_119519_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/figure3_hu3a62ad6ba63de7d780fee257a0671dfb_119519_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="334"
data-flex-basis="801px"
>&lt;/p>
&lt;p>강력한 보편적 모델은 사전 학습 과정에서 다양한 문제 해결에 노출되어야 하며, 이러한 다양성은 모델의 목표에 반영되어야 한다고 주장한다. 그렇지 않으면 모델은 장문의 일관된 텍스트 생성 등의 능력이 부족해질 수 있다.&lt;/p>
&lt;p>현재의 목표 함수 클래스와 함께, 사전 학습 동안 사용되는 세 가지 주요 패러다임을 정의한다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>R-Denoiser&lt;/strong> regular denoising은 2에서 5 토큰 범위의 standard span corruption을 사용하며, 이는 약 15%의 입력 토큰을 마스킹한다. 이 짧은 범위는 유창한 텍스트 생성보다는 지식 획득에 유용할 수 있다.&lt;/li>
&lt;li>&lt;strong>S-Denoiser&lt;/strong> inputs-to-target 작업을 구성할 때 엄격한 순차적 순서를 따르는 preﬁx 언어 모델링은 denoising의 특정 케이스이다. 입력 시퀀스를 두 개의 서브 시퀀스, 즉 문맥과 대상으로 분할하며, 대상은 미래 정보에 의존하지 않는다. 이 방식은 문맥 토큰보다 이전 위치에 대상 토큰이 있을 수 있는 표준 span corruption과는 다르다. 또한, 매우 짧은 메모리나 없는 S-Denoising은 standard causal 언어 모델링과 유사하다.&lt;/li>
&lt;li>&lt;strong>X-Denoiser&lt;/strong> X-denoising은 입력의 작은 부분을 통해 큰 부분을 복구해야 하는 극단적인 denoising이다. 이 방법은 제한된 정보를 가진 메모리에서 긴 대상을 생성하는 상황을 모방한다. 이를 위해, 입력 시퀀스의 약 50%가 마스킹되는 공격적인 denoising 예제를 포함한다. 이것은 범위 길이나 손상률을 늘림으로써 달성된다. X-denoising은 일반 span corruption과 언어 모델 목표 사이의 중간점을 찾는 것에 동기를 두고 있다.&lt;/li>
&lt;/ul>
&lt;p>이 denoiser 집합은 이전에 사용된 목표 함수와 밀접한 연관이 있다. R-Denoising은 T5 span corruption 목표와, S-Denoising은 GPT와 유사한 인과적 언어 모델과 연결되어 있으며, X-Denoising은 T5와 causal LMs의 목표 조합에 모델을 노출한다. X-denoisers는 더 많은 토큰을 예측학습하므로 샘플 효율성을 향상시킨다. 이 모든 작업을 균일하게 혼합하여 hybrid self-supervised 목표를 제안하며, 최종적으로 7개의 denoiser가 혼합된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table1.png"
width="784"
height="190"
srcset="https://kurtkim.github.io/p/ul2/images/table1_hu1a16490df31adfaa682dddf269386e13_32827_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table1_hu1a16490df31adfaa682dddf269386e13_32827_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="412"
data-flex-basis="990px"
>&lt;/p>
&lt;p>X- 및 R-Denoisers는 평균이 $µ$인 normal distribution에서 범위 길이를 샘플링한다. S-Denoisers는 uniform distribution을 사용하고, 손상된 범위의 수를 1로 고정하며, 손상된 부분 다음에는 잘린 토큰이 없어야 한다. 이는 대략적으로 seq2seq denoising 또는 Preﬁx LM 사전 학습 목표와 같다.&lt;/p>
&lt;p>LM은 Preﬁx-LM의 특별한 경우로, mixture에 causal LM 작업을 포함할 필요가 없다고 판단하였다. 모든 작업은 mixture에서 대략적으로 동일하게 참여하며, S-denoisers의 비율을 mixture 내 denoiser 중 최대 50%까지 늘리는 대안을 탐색하였다.&lt;/p>
&lt;p>Mixture-of-Denoisers의 강력함은 그것의 혼합에서 비롯된다. 단독으로는 일부 denoiser 유형이 잘 작동하지 않는데, 예를 들어, 원래의 T5 논문에서는 50%의 손상률(X-denoising)을 가진 옵션이 잘 작동하지 않았다.&lt;/p>
&lt;h4 id="mode-switching">Mode Switching&lt;/h4>
&lt;p>모델이 주어진 작업에 더 적합하게 작동하도록 모드를 전환하는 패러다임 토큰을 사용하는 모드 스위칭 개념을 도입하였다. 미세 조정과 downstream task을 위해, 모델이 더 나은 해결책을 학습하도록 유도하기 위해 패러다임 토큰을 추가합니다. 이 모드 스위칭은 실제로 downstream task의 행동을 upstream 학습 동안 사용한 특정 모드에 연결시킨다.&lt;/p>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>UL2는 아키텍처에 중립적인 접근 방식을 취하며, encoder-decoder와 decoder-only의 선택은 효율성의 타협이라고 주장한다. 따라서 UL2는 decoder와와 encoder-decoder 모두를 포함하고 있다. UL2는 표준 T5 transformer를 강화하여 GLU 레이어와 T5 스타일의 상대적 주의를 적용하였고, 아키텍처 변경과 사전 학습 기여를 혼동하지 않기 위해 모델의 기본 구조는 T5와 유사하게 유지하였다.&lt;/p>
&lt;hr>
&lt;h2 id="ablative-experiments">Ablative Experiments&lt;/h2>
&lt;p>ablative experimental 설정(예: 기준선, 데이터셋, 구현 세부 사항)과 결과에 대해 설명한다. 전반적인 연구 결과는 UL2가 9개의 작업 중 9개에서 T5-유형 및 GPT-유형 모델을 능가한다는 것을 보여준다.&lt;/p>
&lt;h3 id="baselines">Baselines&lt;/h3>
&lt;p>다음의 사전 학습 기준선과 비교한다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Causal Language Model (CLM)&lt;/strong> 이것은 GPT와 같은 대다수의 표준 사전 학습 모델에서 사용되는 표준 left-to-right auto-regressive 언어 모델 사전 학습이다. 이 모델을 GPT-유형이라고 부른다.&lt;/li>
&lt;li>&lt;strong>Preﬁx LM (PLM)&lt;/strong> 이것은 M이 bidirectional receptive ﬁeld를 가진 causal LM의 약간의 변형으로, 이 논문에서는 M의 길이를 균일하게 샘플링하고 auto-regressive 목표에서만 손실을 계산한다.&lt;/li>
&lt;li>&lt;strong>Span Corruption (SC)&lt;/strong> 이것은 T5에서 제안된 standard denoising 목표로, 특정 텍스트 부분을 지우고 센티넬 토큰으로 대체한 후, 이를 목표로 복사하고 모델에 의해 자동으로 생성하는 아이디어이다. 이 논문에서는 평균 범위 3과 denoising 비율 15%를 사용한다.&lt;/li>
&lt;li>&lt;strong>Span Corruption + LM (SCLM)&lt;/strong> CLM과 Span Corruption을 동일한 비율로 혼합하여 학습한다. 이 목표의 SC 구성 요소에 대해 SC에 대한 동일한 hyper-parameter를 사용한다.&lt;/li>
&lt;li>&lt;strong>UniLM (ULM)&lt;/strong> 이것은 causal 언어 모델링, Prefix LM, bidirectional i.i.d denoising을 혼합한 Dong et al. (2019)의 목표이다. UniLM을 cloze 스타일 또는 BERT 스타일로 학습하는 대신, 마스크된 토큰을 생성하여 decoder-only 아키텍처와 encoder-decoder 아키텍처 모두에 적용할 수 있고, 미세 조정을 위한 task-speciﬁc linear head의 필요성을 제거한다.&lt;/li>
&lt;/ul>
&lt;p>모든 목표를 위해 단일 스택과 encoder-decoder 아키텍처를 모두 고려하며, 모든 아키텍처는 encoder-decoder 또는 decoder-only 모델로 구현된다. BERT 스타일의 사전 학습은 효과적으로 이 스타일의 학습에 통합되었다고 보여진다. Taskspeciﬁc classiﬁcation head는 유니버설 모델 원칙에 위배되며 번거로우므로 권장되지 않는다.&lt;/p>
&lt;h3 id="experimental-setup">Experimental Setup&lt;/h3>
&lt;p>다양한 지도 학습과 프롬프트 기반 few-shot 학습 작업에 대해 실험을 진행한다.&lt;/p>
&lt;h4 id="datasets-and-tasks">Datasets and Tasks&lt;/h4>
&lt;p>8개의 NLU 부작업을 포함한 SuperGLUE와 언어 생성에 초점을 맞춘 GEM 벤치마크의 일부 데이터셋을 사용해 실험을 진행한다. 이러한 모든 작업은 지도 미세 조정과 프롬프트 기반 one-shot 학습에서 평가되며, 모델들의 일반적인 텍스트 생성 능력도 C4 검증 세트에 대한 perplexity 점수로 비교한다. 이러한 접근법은 다양한 연구 설정에 대한 충분한 커버리지를 제공한다고 믿는다.&lt;/p>
&lt;h4 id="metrics-and-holistic-evaluation">Metrics and Holistic Evaluation&lt;/h4>
&lt;p>SuperGLUE와 GEM 벤치마크에 대한 실험 결과를 각각의 적절한 지표로 보고하며, 언어 모델링의 경우 negative log perplexity를 보고한다. 모델의 범용성, 즉 다양한 작업에서의 성능은 주요 평가 기준이다. 이를 위해 기준에 대한 정규화된 상대적 이득을 종합 지표로 사용하며, 이를 통해 새 모델이 표준 모델(GPT나 T5 같은)보다 얼마나 더 나은지 쉽게 이해할 수 있다. 이 지표는 정규화되어 벤치마크 lottery effect에 취약해지는 것을 방지한다.&lt;/p>
&lt;h4 id="implementation-details">Implementation Details&lt;/h4>
&lt;p>실험은 JAX/Flax와 T5X4 프레임워크, Flaxformer를 통해 진행되며, C4 코퍼스를 사용해 모든 모델을 500K step 동안 사전 학습한다. 이 과정은 64~128개의 TPUv4 칩을 사용하며, Adafactor optimizer를 통해 모델을 최적화한다. 다양한 아키텍처의 트레이드오프를 이해하기 위해, decoder-only 아키텍처와 encoder-decoder 아키텍처를 모두 사용하며, 이들 모델의 주요 실험 결과를 보고한다. 모든 모델은 standard transformer를 사용하며, decoder-only 모델은 입력에서 bidirectional receptive ﬁeld를, 타겟에서는 autoregressive decoding을 사용한다. 이것은 본질적으로 PrefixLM 유형 아키텍처로, full causal decoder 모델보다 더 나은 것으로 확인되었다.&lt;/p>
&lt;h3 id="overview-of-ablative-experimental-results">Overview of Ablative Experimental Results&lt;/h3>
&lt;p>모든 벤치마크 작업과 데이터셋에 대한 원래 결과, T5와 GPT 모델과 같은 잘 정립된 기준선에 대한 상대적인 비교를 보고한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table2.png"
width="1060"
height="380"
srcset="https://kurtkim.github.io/p/ul2/images/table2_hufb55428c5d634ab7171178e4b47db827_105678_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table2_hufb55428c5d634ab7171178e4b47db827_105678_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="278"
data-flex-basis="669px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table3.png"
width="1092"
height="410"
srcset="https://kurtkim.github.io/p/ul2/images/table3_huf6ced914a36605d6b69453a0e58d73b7_101320_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table3_huf6ced914a36605d6b69453a0e58d73b7_101320_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="266"
data-flex-basis="639px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table4.png"
width="1072"
height="394"
srcset="https://kurtkim.github.io/p/ul2/images/table4_hu168fc403d12d43b2f45413fbc4c4130b_101233_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table4_hu168fc403d12d43b2f45413fbc4c4130b_101233_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="272"
data-flex-basis="652px"
>&lt;/p>
&lt;h4 id="decoder-vs-encoder-decoder">Decoder Vs Encoder-Decoder&lt;/h4>
&lt;p>decoder-only 모델과 encoder-decoder 모델은 계산력이나 parameter의 측면에서 비교될 수 있다. 결과적으로, encoder-decoder 모델은 decoder-only에 비해 약 2배의 parameter를 가지지만 처리 속도는 비슷하다.&lt;/p>
&lt;p>모델의 희소성을 고려하면 encoder-decoder가 약간 우대될 수 있다. 결과를 보면, T5를 기준으로 할 때, UL2 decoder를 제외하고는 사전 학습된 decoder 모델이 T5를 능가하지 못하며, 전체 성능은 10%~30% 저하된다. 가장 좋은 decoder 모델은 Preﬁx-LM로, T5보다 약 10% 낮습니다. 이 결과로 보아, 저장 공간 문제가 없다면 encoder-decoder 모델이 decoder-only 모델보다 우선적으로 고려되어야 한다.&lt;/p>
&lt;p>parameter 제약이 있는 경우, Preﬁx-LM decoder가 적합한 대체품이 될 수 있다. 또한, UL2 decoder가 T5 encoder-decoder 설정을 +14.6% 능가하는 것은 흥미로운 점이지만, 이는 UL2 encoder-decoder를 능가하지는 못한다. 이로서 self-supervision 목표가 기본 구조보다 본질적으로 중요하며, 구조적 선택은 주로 효율성의 타협을 독립적으로 연구하는 것이라는 점을 강조한다.&lt;/p>
&lt;h4 id="is-gpt-andor-t5-the-optimal-setup">Is GPT and/or T5 the optimal setup?&lt;/h4>
&lt;p>GPT와 같은 설정과 T5와 같은 설정을 비교하는 결과에 따르면, causal LM 설정(GPT와 유사)이 가장 성능이 떨어지며, 가능한 경우 항상 Preﬁx-LM 또는 UniLM으로 학습하는 것이 좋다. Preﬁx-LM 사전 학습은 T5 범위 손상 설정을 +16.7% 능가하며, Preﬁx-LM encoder-decoder 모델은 특정 작업에서는 약간의 성능 저하를 보이지만 다른 작업에서는 크게 향상된다. 따라서 Preﬁx-LM과 T5 중 어느 것이 보편적으로 우수한 모델인지는 분명하지 않는다.&lt;/p>
&lt;h4 id="on-the-performance-of-unilm-and-sclm">On the Performance of UniLM and SCLM&lt;/h4>
&lt;p>encoder-decoder 설정에서 UniLM과 SCLM 목표는 표준 범위 손상 목표보다 전반적으로 더 좋은 성능을 보여, 사전 학습 목표를 혼합하는 것이 유용함을 보여준다. decoder 설정에서는 UniLM이 +9.4%, SCLM이 +16.1%의 성능 향상을 보여주었다. UniLM과 SCLM은 9개 작업 중 6개에서 T5를 능가하였으며, SCLM이 one-shot 생성에서 가장 뛰어난 성능을 보여주었다.&lt;/p>
&lt;h4 id="on-the-performance-of-the-proposed-ul2">On the Performance of the Proposed UL2&lt;/h4>
&lt;p>UL2는 GPT와 같은 모델과 T5와 같은 모델에 비해 가장 뛰어난 성능을 보이며, T5에 비해 +43.4%, GPT와 같은 모델에 비해 +76.2%의 성능 향상을 보인다. 9개의 고려된 모든 작업에서 UL2는 T5를 능가한다. UL2는 항상 모든 작업에서 가장 높은 성능을 보이지는 않지만, 일관성이 있으며, 특정 작업에서 다른 방법에게 손실을 보여도 이는 상대적으로 미미하다. 반대로, UL2가 T5를 능가할 때, 이익은 매우 크며, 이런 일관된 개선으로 인해 UL2는 T5와 GPT와 같은 모델의 대체품으로 사용될 수 있다.&lt;/p>
&lt;h3 id="mode-switching-ablations">Mode Switching Ablations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table5.png"
width="618"
height="218"
srcset="https://kurtkim.github.io/p/ul2/images/table5_hu77939970274c9f4fff5ef7c499fc1e5c_38228_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table5_hu77939970274c9f4fff5ef7c499fc1e5c_38228_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="680px"
>&lt;/p>
&lt;p>모드 전환 기능이 성능에 미치는 영향을 확인하기 위해 실험을 진행하였다. 실험 결과, 프롬프트의 사용이 모델 성능에 큰 영향을 미치며, 특히 XSum에서는 올바른 프롬프트 사용이 성능 차이를 48%까지 만들어내었다. 반면 SuperGLUE는 프롬프트에 덜 민감했지만, one-shot 평가에서는 프롬프트를 사용하는 것이 대체로 더 좋은 결과를 보여주었다.&lt;/p>
&lt;h3 id="mixture-of-denoisers-ablations">Mixture-of-Denoisers Ablations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table6.png"
width="732"
height="426"
srcset="https://kurtkim.github.io/p/ul2/images/table6_hu02dad93faeab3370872a92794621909b_80447_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table6_hu02dad93faeab3370872a92794621909b_80447_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="412px"
>&lt;/p>
&lt;p>개별 목표의 효과성을 검증하기 위한 광범위한 실험을 실시하였다. 평균 범위와 손상률의 변화, 그리고 사용된 S-denoising의 비율(% SD)을 변화시켜 결과를 확인하였다. mixture 내의 denoiser의 총 수는 $∥ Span ∥ \times ∥ Corrupt Rate ∥ + 1$로 계산되며, 이러한 설정들은 Var-A부터 Var-J까지의 레이블로 표시되어 있다.&lt;/p>
&lt;p>&lt;strong>X-Denoising is Complementarily Effective but Does Not Sufﬁce as a Standalone&lt;/strong> mixing Extreme Denoising이 효과적이며, 대부분의 최고 결과는 긴 범위를 가진 mixture에서 나왔다. 긴 범위를 사용하지 않는 경우와 비교했을 때, 긴 범위를 사용하는 것이 더 좋은 성능을 보였다. 그러나 긴 범위만을 사용하는 경우는 일반적으로 성능이 좋지 않았으며, 이는 Extreme Denoising이 단독으로는 충분하지 않음을 나타낸다. 이 결과는 이전 연구에서 50%의 손상률이 잘 작동하지 않음을 보여주는 점과 일치한다. 그러나 이 결과는 BERT 스타일의 masked language modeling 대신 inputs-to-targets 형태의 사전 학습을 사용하는 아키텍처와 약간 충돌한다.&lt;/p>
&lt;p>&lt;strong>Small Amounts of S-Denoisers is Preferred&lt;/strong> S-denoisers를 전체 MoD mixture의 50%로 확대하는 설정은 일반적으로 성능을 저하시킨다는 결론을 내렸다. 따라서 S-denoisers는 필요하지만, 작은 양(약 20%)이 선호된다. S-denoising이 전혀 없는 경우를 탐색해보았지만, 일부 작업에서는 성능이 향상되지만, 다른 작업에서는 크게 저하되는 것을 확인하였다. 이 결과로부터 S-denoising이 중요하다는 결론을 도출하였다.&lt;/p>
&lt;h3 id="modestly-scaling-model-size-and-pretraining-data">Modestly Scaling Model Size and Pretraining Data&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table7.png"
width="1000"
height="188"
srcset="https://kurtkim.github.io/p/ul2/images/table7_hu5801fe16411126f14c766e792704ab51_43298_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table7_hu5801fe16411126f14c766e792704ab51_43298_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="531"
data-flex-basis="1276px"
>&lt;/p>
&lt;p>모델 크기와 사전 학습 데이터셋 크기를 확대하여 추가 실험을 진행하였다. 이 실험에서는 UL2 encoder-decoder 모델을 약 1B 개의 parameter로 확대하고, 사전 학습 토큰의 수를 0.5조 개로 늘렸다. 이렇게 크게 확대된 설정에서도 UL2 모델은 여전히 경쟁력이 있었다. 주요 변화 중 하나는 UL2가 SuperGLUE 스위트를 포기하고, 대신 8개 작업 중 7개에서 성능을 능가하고, one-shot 평가에서 성능을 2-4배 향상시키는 결과를 보여주었다. 지도 미세 조정에서의 이익은 작지만 XSUM, SGD, TOT에서 눈에 띄게 나타났다.&lt;/p>
&lt;hr>
&lt;h2 id="scaling-to-20b-parameters">Scaling to 20B Parameters&lt;/h2>
&lt;p>확대된 설정에서 UL2를 평가하고자 한다. 이전 실험 결과를 바탕으로 encoder-decoder 아키텍처를 사용한다. UL2는 아키텍처에 중립적이지만, 본질적인 희소성 때문에 encoder-decoder 아키텍처 사용을 권장한다.&lt;/p>
&lt;p>UL2를 약 200억 개의 parameter 규모에서 학습시켰다. 이 크기는 중간 규모의 모델로, UL2가 더 큰 규모에서도 작동할 수 있다는 것을 보여준다. 이 모델은 특정 제어나 완화 전략 없이 학습되었으며, 때때로 loss spike를 보였다. 그러나 이 모델을 사용한 많은 실험에서 state-of-the-art를 달성 하였으므로, 현재 결과가 모델의 진정한 잠재력을 과소평가한 것으로 보인다.&lt;/p>
&lt;h3 id="pretraining-and-model-conﬁguration">Pretraining and Model Conﬁguration&lt;/h3>
&lt;p>이전 실험과 동일한 프로토콜을 따라, C4 코퍼스에서 UL2를 사전 학습하였다. 이 때, 모델이 사전 학습 중에 보는 토큰의 수를 확대하였다. batch size는 1024, TPUv4 칩 512개를 사용하였고, 총 1 trillion 개의 토큰에 대해 약 한 달 이상 학습하였다. 이 모델은 32개의 encoder layer와 32개의 decoder layer를 가지며, 각 head는 총 16개이고 각각의 차원이 256이다. UL20B는 T5와 유사하지만, 목표와 스케일링 노브가 약간 다르며, 이 20B 모델의 체크포인트를 공개하고 오픈 소스화하였다.&lt;/p>
&lt;h3 id="experiments-at-20b-scale">Experiments at 20B scale&lt;/h3>
&lt;p>UL20B 실험에 대한 실험 설정을 설명한다.&lt;/p>
&lt;h4 id="setup-and-implementation-details">Setup and Implementation Details&lt;/h4>
&lt;p>미세 조정과 컨텍스트 내 학습에 대한 실험을 진행하였다. 지도 미세 조정은 일반적으로 5만에서 10만 사이의 사전 학습 단계 후에 이루어졌고, 각각의 downstream task에 대해 수동적으로 미세 조정하였다. 일부 작업은 모델이 아직 사전 학습 중일 때 미세 조정되었으며, 많은 작업은 공개한 수렴에 가까운 체크포인트에서 미세 조정되었다. 작업이 최고의 성능에 도달하면 컴퓨팅을 절약하기 위해 미세 조정을 중단하였다. 또한, 대규모 다중 작업 학습과 UL2의 결합은 미래의 작업으로 남겨두었다.&lt;/p>
&lt;p>지도 미세 조정을 위해, Adafactor optimizer를 사용하고 학습률은 {$5 \times 10^−5, 1 \times 10^−4$}의 범위에서 설정하였다. optimizer의 상태를 재설정하고 실제 목표 토큰의 수에 기반한 손실 정규화를 적용하였다. batch size는 일반적으로 32에서 128의 범위였고, 미세 조정 성능에는 큰 영향을 미치지 않았다. 평가된 많은 작업들은 크게 조정되지 않고, 리더보드 제출 전에 한 두 번만 실행하였다.&lt;/p>
&lt;h4 id="datasets-for-supervised-finetuning">Datasets for Supervised Finetuning&lt;/h4>
&lt;p>총 50개 이상의 자연어 처리(NLP) 작업을 고려한다. 작업의 분류는 일반적으로 유연하며, 일부 작업은 다른 분류 경계로 넘어갈 수 있다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Language Generation&lt;/strong> summarization과 data-to-text generation 작업에 대해 평가하며, 이를 위해 CNN/Dailymail, XSUM, MultiNews, SAMSum, WebNLG, E2E, 그리고 CommonGen 데이터셋을 사용한다. WebNLG, E2E, CommonGen의 경우, GEM 벤치마크 버전을 사용하였다.&lt;/li>
&lt;li>&lt;strong>Language Generation with Human Evaluation&lt;/strong> GENIE 리더보드를 통한 인간 평가를 사용하여 aNLG, ARC-DA, WMT19, XSUM 등의 작업을 평가하였다.&lt;/li>
&lt;li>&lt;strong>Language Understanding, Classiﬁcation and Question Answering&lt;/strong> RACE, QASC, OpenBookQA, TweetQA, QuAIL, IMDB, Agnews, DocNLI, Adversarial NLI, VitaminC, Civil Comments, Wikipedia Toxicity detection 등의 데이터셋을 사용하여 독해, 질문 응답, 텍스트 분류, 자연어 추론 등의 작업을 수행한다. 또한, SuperGLUE와 GLUE 데이터셋도 활용하였다.&lt;/li>
&lt;li>&lt;strong>Commonsense Reasoning&lt;/strong> HellaSwag, SocialIQA/SIQA, PhysicalIQA/PIQA, CosmosQA, AbductiveNLI, CommonsenseQA, 그리고 CommonsenseQA2 등의 데이터셋을 활용한다.&lt;/li>
&lt;li>&lt;strong>Long Range Reasoning&lt;/strong> GovReport, SumScr, QMSUm, QASPER, NarrativeQA, QuaLITY, 그리고 ContractNLI 등 일곱 개의 구성 작업이 포함된 Scrolls 벤치마크를 사용한다.&lt;/li>
&lt;li>&lt;strong>Structured Knowledge Grounding&lt;/strong> UniﬁedSKG에서 WikiTQ, CompWQ, FetaQA, HybridQA, WikiSQL, TabFat, Feverous, SQA, MTOP, 그리고 DART 등의 작업을 사용한다. 평가 수행이 상대적으로 편리하고, 정확도나 완전 일치 같은 주요 메트릭을 사용하는 데이터셋을 선택하였다.&lt;/li>
&lt;li>&lt;strong>Information Retrieval&lt;/strong> 차별화 가능한 검색 인덱스 설정을 사용하여 주어진 쿼리에 대해 관련 문서를 검색하는 IR 작업을 수행하며, 이는 최신의 차세대 IR 패러다임이다. 실험에서는 DSI 논문의 NQ 분할을 사용하였다.&lt;/li>
&lt;/ul>
&lt;p>각 데이터셋의 state-of-the-art를 보고하며, 생성 작업에 대해서는 ROUGE-2를, 나머지 데이터셋에 대해서는 이전 작업에서 사용된 주요 메트릭을 보고한다. BLEU 점수에 대해서는 sacrebleu를 사용하며, 외부 지식 기반을 사용하는 상식 추론 작업과는 비교하지 않는다. GLUE는 일반적으로 포화 상태로, 많은 미발표 결과가 있으므로, 우리는 T5 모델 이후 실제로 큰 진전이 없었다고 판단하여 state-of-the-art로 간주한다.&lt;/p>
&lt;p>가능한 한 모든 리더보드에 점수를 제출하려 노력하지만, 노동 비용이 과도하게 높은 경우나, 기존 state-of-the-art 접근법이 개발 점수를 제공하거나, 특정 데이터셋에 대한 보고가 완전성을 위한 것인 경우에는 제출하지 않는다. 리더보드에서 보고할 때에는 가장 높은 성능을 보인 출판된 작업을 state-of-the-art로 간주하며, 익명의 제출이 더 높은 점수를 받았을 수도 있다는 것을 표시한다. 최종 시퀀스 길이를 늘리는 것이 점수를 상당히 향상시킬 것으로 예상되지만, 물류와 시간표 상의 이유로 이는 미래의 작업에 맡긴다.&lt;/p>
&lt;h4 id="summary-of-supervised-finetuning-results">Summary of Supervised Finetuning Results&lt;/h4>
&lt;p>실험 결과에 대한 개요를 설명한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table8-1.png"
width="1030"
height="1278"
srcset="https://kurtkim.github.io/p/ul2/images/table8-1_huef72c91a40a8f33fbb43c15c9a51455c_378305_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table8-1_huef72c91a40a8f33fbb43c15c9a51455c_378305_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="80"
data-flex-basis="193px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table8-2.png"
width="1032"
height="974"
srcset="https://kurtkim.github.io/p/ul2/images/table8-2_huf871f9455c3ef9b677f7a64e6e9a8f02_255129_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table8-2_huf871f9455c3ef9b677f7a64e6e9a8f02_255129_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="254px"
>&lt;/p>
&lt;h4 id="results-on-supervised-finetuning">Results on Supervised Finetuning&lt;/h4>
&lt;p>실험 결과, UL2는 50개 이상의 NLP 작업에서 state-of-the-art를 달성하였다. 성능 차이는 크게 나타났으며, UL2가 state-of-the-art를 달성하지 못한 경우에도 경쟁력이 있었다. 각 벤치마크에서 state-of-the-art를 얻는 난이도는 크게 다르며, 일부 벤치마크에서는 이미 큰 모델이 state-of-the-art를 보이고 있어 능가하는 것이 쉽지 않았다. UL2 20B는 GENIE 작업에서 인간 평가에서도 우수한 성과를 보였으며, 이는 UL2의 생성 품질이 탄탄하다는 것을 입증한다.&lt;/p>
&lt;h4 id="tradeoffs-between-finetuning-and-prompt-based-zero-shot-learning-superglue">Tradeoffs between Finetuning and Prompt-based Zero-shot Learning (SuperGLUE)&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table9.png"
width="1078"
height="240"
srcset="https://kurtkim.github.io/p/ul2/images/table9_hu9360f719bd4ac07a1ae8ddc36349b116_63847_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table9_hu9360f719bd4ac07a1ae8ddc36349b116_63847_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="449"
data-flex-basis="1078px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table10.png"
width="1080"
height="394"
srcset="https://kurtkim.github.io/p/ul2/images/table10_hua78890f12b6ed46ccfad98d05ff55bea_101700_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table10_hua78890f12b6ed46ccfad98d05ff55bea_101700_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="274"
data-flex-basis="657px"
>&lt;/p>
&lt;p>SuperGLUE 벤치마크에서 미세조정과 in-context 학습의 상충 관계를 연구한다. UL20B를 사용한 실험 결과, state-of-the-art를 달성하지 못했지만, T5-11B를 능가하는 경쟁력을 유지하였다. 하지만, 매개변수가 200B+ 이상인 ST-MoE-32B 모델에 비해 아직 뒤쳐진다. 이는 ST-MoE-32B가 특정 아키텍처를 사용하여 학습되고 있으며, 이 아키텍처가 NLU 미세조정에 매우 유리하기 때문이다.&lt;/p>
&lt;h4 id="generative-few-shot-xsum-summarization">Generative Few-shot: XSUM Summarization&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/ul2/images/table11.png"
width="588"
height="268"
srcset="https://kurtkim.github.io/p/ul2/images/table11_hubb8a853de78c1b71ac075ecb0a494b1b_35158_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/ul2/images/table11_hubb8a853de78c1b71ac075ecb0a494b1b_35158_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="526px"
>&lt;/p>
&lt;p>XSum 데이터셋을 사용해 few-shot in-context one-shot 학습을 추가로 실시하였다. 기존 모델들과 비교했을 때, UL2 20B의 성능은 LM Adaptation을 적용한 T5 XXL 모델의 성능의 약 3배였으며, LaMDA 137B를 능가하고 PaLM 8B의 성능의 거의 두 배에 가까웠다. 그러나 가장 좋은 결과는 여전히 큰 PaLM 모델들에서 나왔다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>보편적으로 효과적인 모델을 학습시키는 새로운 패러다임, UL2를 제안하였다. 이는 여러 사전 학습 작업을 다양화하고 섞는 Mixture of Denoisers (MoD) 사전학습과 downstream 작업 동작을 upstream 사전학습과 연결하는 mode switching 방법을 특징으로 한다. UL2는 다양한 지도 학습 및 few-shot 작업에서 GPT와 T5 모델을 일관되게 능가하였고, 50개 이상의 NLP 작업에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2205.05131v1.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/google-research/tree/master/ul2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>OPT</title><link>https://kurtkim.github.io/p/opt/</link><pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/opt/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>대규모 언어 모델들은 높은 계산 비용 때문에 복제하기 어렵다. 이를 해결하기 위해, Open Pre-trained Transformers (OPT)를 제시한다. 이는 125M에서 175B의 parameter 범위를 가진 사전 학습된 transformer 모델들을 포함하며, 이들은 완전하게 그리고 책임감 있게 관심 있는 연구자들과 공유될 것이다. OPT-175B는 GPT-3와 비교할 수 있으나, 개발하는 데 필요한 탄소 발자국은 1/7밖에 되지 않는다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 텍스트 컬렉션에 학습된 거대 언어 모델은 텍스트 생성 및 zero-shot, few-shot 학습 등 놀라운 기능을 보여준다. 그러나 현재로서는 완전한 모델 접근이 풍부한 자원을 가진 몇몇 연구소에만 제한되어 있다. 이 제한된 접근은 대형 언어 모델이 어떻게 그리고 왜 작동하는지 연구하는 능력을 제한하고, 견고성, 편향, 독성 등의 문제를 개선하는 데 있어 진전을 방해하고 있다.&lt;/p>
&lt;p>125M에서 175B parameter 범위의 decoder 기반 사전 학습된 transformer인 Open Pretrained Transformers (OPT)를 소개하고 있다. OPT 모델은 GPT-3 계열 모델의 성능과 크기를 대략 맞추도록 학습되었으며, 최신 데이터 수집 및 효율적 학습 방법을 적용하였다. 이 모델은 대규모 연구를 가능하게 하고, 거대 언어 모델의 영향력을 연구하는 다양한 의견을 수렴하기 위해 개발되었다. risk, harm, bias, toxicity 등의 정의는 연구 커뮤니티 전체가 공동으로 명시해야 하며, 이는 모델들이 연구에 사용 가능할 때만 가능하다.&lt;/p>
&lt;p>125M부터 66B parameter 사이의 모든 모델을 공개하며, 요청에 따라 OPT-175B에 대한 연구 접근 권한을 제공한다. 학계 연구자, 정부 및 학계의 조직, 산업 연구소에 접근 권한이 부여된다. 모델 생성 로그북과 OPT-175B를 992개의 80GB A100 GPU에서 학습시키는 데 사용된 코드베이스인 metaseq도 공개된다. 이를 통해, 우리는 GPT-3의 탄소 발자국의 1/7만큼의 에너지를 사용해 OPT-175B를 개발할 수 있었다. 이는 큰 성과이지만, 이렇게 큰 모델을 만드는 에너지 비용은 중요하며, 이를 계속 복제하면 LLM들의 컴퓨팅 발자국이 계속 증가할 것이다.&lt;/p>
&lt;p>전체 AI 커뮤니티가 책임있는 AI와 LLM 사용에 대한 명확한 지침을 개발하기 위해 협력해야한다고 생각한다. 더 넓은 AI 커뮤니티가 이 모델에 접근하고 재현 가능한 연구를 수행하여 전체 필드를 발전시키는 것이 필요하다. OPT-175B와 작은 규모의 기준선 출시를 통해, 이러한 기술의 윤리적 고려사항에 대한 다양한 의견을 더욱 들을 수 있을 것을 기대한다.&lt;/p>
&lt;hr>
&lt;h2 id="method">Method&lt;/h2>
&lt;h3 id="models">Models&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table1.png"
width="596"
height="416"
srcset="https://kurtkim.github.io/p/opt/images/table1_hufcbf75edd82a2c14e8b21e79a0243c1b_69598_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table1_hufcbf75edd82a2c14e8b21e79a0243c1b_69598_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;p>125M 개에서 175B 개의 parameter를 가진 8개의 transformer 언어 모델 결과를 제시한다. 모델과 hyperparameter는 학습의 안정성을 위해 대부분 Brown et al. (2020)의 연구를 따르며, 배치 크기 조정은 주로 계산 효율성 향상을 위한 것이다.&lt;/p>
&lt;h3 id="training-setup">Training Setup&lt;/h3>
&lt;p>가중치 초기화는 평균 0, 표준 편차 0.006의 정규 분포를 사용하며, Megatron-LM 코드베이스의 설정을 따른다. 출력 layer의 표준 편차는 총 layer 수에 따라 조정되고, 모든 편향 항은 0으로 초기화된다. 모든 모델은 ReLU 활성화 함수를 사용하며, 시퀀스 길이는 2048로 설정하여 학습된다.&lt;/p>
&lt;p>AdamW optimizer를 사용하며, 이때 ($\beta_1$, $\beta_2$)는 (0.9, 0.95)로 설정하고, weight decay는 0.1이다. linear learning rate schedule을 따라, OPT-175B에서 첫 2000단계 동안 0에서 maximum learning rate까지 상승하고, 작은 기준선에서는 375M 토큰 동안 상승 후, 300B 토큰 동안 maximum learning rate의 10%로 감소시킨다. 학습 과정 중에 learning rate을 몇 번 변경하였으며, 배치 크기는 모델 크기에 따라 0.5M에서 4M까지 설정하고 학습 과정 동안 일정하게 유지한다.&lt;/p>
&lt;p>전반적으로 0.1의 드롭아웃을 사용하며, 임베딩에는 드롭아웃을 적용하지 않는다. gradient norm은 일반적으로 1.0에서 제한하나, 중간에 몇 번 1.0에서 0.3으로 줄여야 하는 경우가 있었다. 또한, gradient를 계산할 때 오버플로우/언더플로우 위험을 줄이기 위해 gradient across all rank를 사용하였다.&lt;/p>
&lt;h3 id="pre-training-corpus">Pre-training Corpus&lt;/h3>
&lt;p>사전 학습 코퍼스는 RoBERTa, Pile, 그리고 PushShift.io Reddit에서 사용된 데이터셋을 결합한 것을 포함한다. 이 코퍼스는 대부분 영어 텍스트이지만, CommonCrawl을 통해 비영어 데이터도 일부 포함되어 있다.&lt;/p>
&lt;p>모든 데이터셋에서 중복된 문서를 제거하기 위해, Jaccard 유사도가 .95 이상인 문서를 MinhashLSH를 통해 필터링하였다. 특히 Pile 데이터셋에서는 중복 문서가 많이 발견되어, 이를 사용하는 연구자들에게 추가적인 중복 제거 처리를 권장한다.&lt;/p>
&lt;p>모든 코퍼스를 GPT-2 byte level BPE 토크나이저를 사용하여 토큰화한다. 최종 코퍼스는 대략 180B 토큰을 포함하고 있다.&lt;/p>
&lt;p>&lt;strong>RoBERTa&lt;/strong> RoBERTa 코퍼스의 BookCorpus와 Stories 하위 집합을 포함시키고, 2021년 9월 28일까지 크롤링된 뉴스 기사를 포함한 업데이트된 CCNews를 사용하였다. 이 코퍼스는 원래 RoBERTa CCNews와 같은 방식으로 전처리 되었다.&lt;/p>
&lt;p>&lt;strong>The Pile&lt;/strong> Pile의 일부 하위 집합인 CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO, 그리고 Wikipedia를 포함시켰다. 그러나 gradient norm의 급증을 초래하는 경향이 있어 불안정성을 높이는 Pile의 다른 하위 집합은 제외하였다. 모든 하위 집합은 추가적인 ad-hoc 공백 정규화를 거쳤다.&lt;/p>
&lt;p>&lt;strong>PushShift.io Reddit&lt;/strong> Baumgartner et al. (2020)이 생성하고 Roller et al. (2021)이 이전에 사용한 Pushshift.io 코퍼스의 일부를 포함시켰다. 대화 트리를 언어 모델이 접근 가능한 문서로 변환하기 위해, 우리는 각 스레드에서 가장 긴 댓글 체인을 추출하고 트리의 모든 다른 경로를 제거하였다. 이로 인해 코퍼스는 약 66% 감소했다.&lt;/p>
&lt;h3 id="training-efﬁciency">Training Efﬁciency&lt;/h3>
&lt;p>완전히 분할된 데이터 병렬과 Megatron-LM Tensor 병렬성을 활용하여 992개의 80GB A100 GPU에서 OPT-175B를 학습시켰다. 이로써 GPU 당 최대 147 TFLOP/s의 이용률을 달성하였다. 모든 호스트에서 Adam 상태를 분할하여 FP32로 유지하고, 모델 가중치는 FP16으로 유지하였다. 언더플로우를 방지하기 위해 동적 손실 스케일링을 사용하였다.&lt;/p>
&lt;h3 id="training-processes">Training Processes&lt;/h3>
&lt;p>&lt;strong>Hardware Failures&lt;/strong> OPT-175B 학습 도중에는 컴퓨팅 클러스터에서 상당한 수의 하드웨어 실패가 발생하였다. 총 2달 동안 하드웨어 실패로 인해 최소 35번의 수동 재시작이 이루어졌으며, 100개 이상의 호스트가 교체되었다. 수동 재시작 시에는 학습이 일시 중단되고, 문제가 있는 노드를 탐지하기 위해 일련의 진단 테스트가 수행되었다. 이후 문제가 있는 노드는 격리되고, 마지막으로 저장된 체크포인트에서 학습이 재개되었다. 교체된 호스트 수와 수동 재시작 횟수의 차이를 고려할 때, 하드웨어 실패로 인한 자동 재시작이 70번 이상 이루어진 것으로 추정된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure1.png"
width="598"
height="418"
srcset="https://kurtkim.github.io/p/opt/images/figure1_huc020b8f94bd6f8270b5a5a7e0e51aa4b_52130_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure1_huc020b8f94bd6f8270b5a5a7e0e51aa4b_52130_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure2.png"
width="594"
height="422"
srcset="https://kurtkim.github.io/p/opt/images/figure2_hu74794a1377f0c0a44cdd659cf8772acd_42166_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure2_hu74794a1377f0c0a44cdd659cf8772acd_42166_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>&lt;strong>Loss Divergences&lt;/strong> 학습 과정에서 loss divergence 문제가 있었다. 손실이 발산할 때, learning rate를 낮추고 이전 체크포인트에서 재시작하면 학습이 계속될 수 있었다. loss divergence, dynamic loss 스칼라가 0으로 떨어지는 현상, 그리고 마지막 layer의 activation $l^2$-norm이 급증하는 것 사이에 상관관계가 있다는 것을 확인하였다. 이를 바탕으로 dynamic loss 스칼라가 &amp;ldquo;healthy&amp;rdquo; 상태에서, 그리고 activation norm이 무제한으로 증가하지 않는 지점에서 재시작하였다. 학습 초기에는 gradient clipping을 1.0에서 0.3으로 낮추는 것이 안정성에 도움이 되었다.&lt;/p>
&lt;p>&lt;strong>Other Mid-ﬂight Changes&lt;/strong> loss divergence을 처리하기 위해 몇 가지 실험적 변경을 시행하였다. 이에는 바닐라 SGD로의 전환, dynamic loss 스칼라의 재설정, 그리고 Megatron의 새 버전으로의 전환 등이 포함되었다. 이러한 변화들은 최적화의 빠른 정체, 일부 발산의 회복, 그리고 activation norm의 압력 감소와 처리량 향상에 도움이 되었다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluations">Evaluations&lt;/h2>
&lt;h3 id="prompting--few-shot">Prompting &amp;amp; Few-Shot&lt;/h3>
&lt;p>HellaSwag, StoryCloze, PIQA, ARC Easy와 Challenge, OpenBookQA, WinoGrad, WinoGrande, 그리고 SuperGLUE 등 문헌에서 사용하는 16개의 표준 NLP 작업에서 모델을 평가하였다. GPT-3의 프롬프트와 실험 설정을 따라서 주로 GPT-3와 비교하였고, 가능한 경우에는 다른 LLM의 성능도 포함시켰다.&lt;/p>
&lt;p>성능을 정확도로 보고하며, 평가 지표의 일관성을 위해 MultiRC와 ReCoRD의 F1은 생략하였다. SuperGLUE의 Winograd Schema Challenge 작업에서는 객관식 질문으로 작업을 구성하였고, 이는 성능에 영향을 미친다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure3.png"
width="582"
height="434"
srcset="https://kurtkim.github.io/p/opt/images/figure3_hu4ac69ec78235bded1ae62ea40f91eb0b_58046_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure3_hu4ac69ec78235bded1ae62ea40f91eb0b_58046_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="321px"
>&lt;/p>
&lt;p>&lt;strong>Zero-shot&lt;/strong> 전반적으로, 평균 성능은 GPT-3의 추세를 따르는 것으로 보인다. 그러나 작업에 따라 성능은 크게 달라질 수 있다. MultiRC와 WIC를 평균에서 의도적으로 제외하였다. 이 데이터셋들은 GPT-3 또는 OPT를 체계적으로 우대하는 것으로 보인다.&lt;/p>
&lt;p>모델의 성능은 10개 작업에서 GPT-3와 비슷했고, 3개 작업에서는 성능이 떨어졌다. 일부 작업에서는 검증 세트 크기가 작아서 모델의 행동이 예측 불가능했다. WIC에서는 OPT 모델이 GPT-3 모델을 능가했으며, MultiRC에서는 GPT-3 결과를 복제하지 못하였다. BoolQ와 WSC에서는 OPT와 GPT 모델이 대부분 클래스 정확도 주변에서 변동했음을 알 수 있다.&lt;/p>
&lt;p>Chinchilla와 Gopher는 parameter 크기에 따라 일관된 성능을 보였지만, PaLM은 모든 설정에서 더 우수한 성능을 보였다. 이는 parameter 수를 제어하더라도 마찬가지였다. PaLM의 높은 성능은 주로 사전 학습 데이터의 품질과 다양성 때문이라고 추정된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure4.png"
width="590"
height="436"
srcset="https://kurtkim.github.io/p/opt/images/figure4_hufc5e96ef5c25de0dfc31b5ce629c954a_87764_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure4_hufc5e96ef5c25de0dfc31b5ce629c954a_87764_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="135"
data-flex-basis="324px"
>&lt;/p>
&lt;p>&lt;strong>One-shot and Few-shot&lt;/strong> 평균 multi-shot in-context 성능은 OPT 모델이 GPT-3 모델과 비슷하게 수행함을 보여준다. 그러나 작업별로 결과를 분석하면, zero-shot과 같은 10개의 데이터셋에서 두 모델이 비슷한 성능을 보이는 반면, 일부 다른 데이터셋에서는 모델 크기에 따라 성능이 일관되지 않음을 보여준다. 특히, MultiRC에서는 OPT 모델이 GPT3 모델에 비해 성능이 떨어진다. 이러한 결과는 우리의 평가 설정이 Brown et al. (2020)과 다를 수 있음을 시사한다.&lt;/p>
&lt;h3 id="dialogue">Dialogue&lt;/h3>
&lt;p>대화 모델의 핵심 요소인 LLM에 초점을 맞춰, OPT-175B를 여러 오픈 소스 대화 데이터셋에서 평가하였다. 이는 ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, Blended Skill Talk, 그리고 최근의 Wizard of Internet 데이터셋을 포함한다. 주로 미세 조정된 BlenderBot 1과 Reddit 2.7B 같은 기존 오픈 소스 대화 모델과 비교하였으며, 또한 미세 조정된 R2C2 BlenderBot과도 비교하였다.&lt;/p>
&lt;p>Perplexity와 Unigram F1 (UF1) 겹침을 보고하며, 모든 Perplexities는 GPT-2 토큰화기의 공간에서 정규화된다. 대화 작업에 대해 감독되고 미감독된 모델들을 구분한다. OPT-175B는 최대 32토큰까지의 탐욕적 디코딩을 사용하며, &amp;ldquo;Person 1:&amp;ldquo;과 &amp;ldquo;Person 2:&amp;ldquo;의 대화 라인만을 번갈아 가며 사용한다. 나머지 모델들은 BlenderBot 1의 생성 parameter를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table2.png"
width="1232"
height="294"
srcset="https://kurtkim.github.io/p/opt/images/table2_hu60a06193089582f8990a0d1a27068df8_80020_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table2_hu60a06193089582f8990a0d1a27068df8_80020_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="419"
data-flex-basis="1005px"
>&lt;/p>
&lt;p>OPT-175B는 모든 작업에서 unsupervised Reddit 2.7B 모델을 크게 능가하며, ConvAI2 데이터셋에서는 supervised BlenderBot 1 모델과 비슷한 성능을 보인다. 하지만, 모든 모델이 unsupervised Wizard-of-Internet 데이터셋에서는 OPT-175B가 가장 낮은 Perplexity를 보이지만, UF1은 Wizard-ofWikipedia supervised 모델들보다 낮다.&lt;/p>
&lt;p>unsupervised OPT-175B 모델의 평가가 ConvAI2 데이터셋에서 BlenderBot 1과 경쟁력이 있었다. 이는 데이터셋의 유출을 의심케 하지만, 사전 학습 말뭉치에서는 어떤 겹침도 찾지 못했다. OPT-175B는 공개되지 않은 ConvAI2 테스트 세트와 MSC 데이터셋에서도 좋은 성능을 보여주었으며, 이는 모델이 여러 PersonaChat과 유사한 데이터셋에 잘 일반화되고 있음을 보여준다. OPT-175B가 대화를 거치면서 일관된 페르소나를 유지하는 강력한 능력을 가지고 있음이 확인되었다.&lt;/p>
&lt;hr>
&lt;h2 id="bias--toxicity-evaluations">Bias &amp;amp; Toxicity Evaluations&lt;/h2>
&lt;p>OPT-175B의 잠재적인 문제를 파악하기 위해, 혐오 발언 탐지, stereotype 인식, toxic 콘텐츠 생성 등과 관련된 벤치마크를 평가하였다. 이 벤치마크들은 단점이 있을 수 있지만, OPT-175B의 한계를 이해하는데 도움을 준다. 주로 GPT-3 Davinci와 비교하였는데, 이 벤치마크들은 Brown et al. (2020)에 포함될 수 있을 때까지 사용되지 않았다.&lt;/p>
&lt;h3 id="hate-speech-detection">Hate Speech Detection&lt;/h3>
&lt;p>Mollas et al. (2020)의 ETHOS 데이터셋을 사용해, OPT-175B가 특정 영어 문장이 인종차별적인지, 성차별적인지 판별하는 능력을 측정하였다. zero-shot, one-shot, few-shot 이진 케이스에서는 모델에게 텍스트가 인종차별적이거나 성차별적인지 판단하고 yes/no로 응답하도록 했고, few-shot 다중 클래스 설정에서는 yes/no/neither로 응답하도록 하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table3.png"
width="592"
height="236"
srcset="https://kurtkim.github.io/p/opt/images/table3_hub55f8b4388eff5af052ad41772d7ecc7_39414_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table3_hub55f8b4388eff5af052ad41772d7ecc7_39414_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="602px"
>&lt;/p>
&lt;p>OPT175B는 one-shot에서 few-shot 설정에서 모두 Davinci보다 훨씬 더 좋은 성능을 보였다. 이는 Davinci API를 통한 평가가 추가적인 안전 제어 메커니즘을 도입하고 있거나, 사전 학습 데이터셋에 포함된 통제되지 않은 소셜 미디어 토론이 이러한 분류 작업에 도움을 주는 귀납적 bias를 제공했기 때문으로 추측된다.&lt;/p>
&lt;h3 id="crows-pairs">CrowS-Pairs&lt;/h3>
&lt;p>CrowSPairs는 마스크 언어 모델을 위해 개발된 벤치마크로, 9가지 카테고리(gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance, socioeconomic status)의 문장 내 bias를 측정한다. 각 예시는 한 그룹에 대한 stereotype 또는 anti-stereotype을 나타내는 문장 쌍으로, 모델이 stereotype 표현을 선호하는 정도를 측정한다. 높은 점수는 모델이 더 큰 bias를 보이는 것을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table4.png"
width="580"
height="468"
srcset="https://kurtkim.github.io/p/opt/images/table4_hu7cde3cccfb2ff574d4954c18dd90cc83_74905_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table4_hu7cde3cccfb2ff574d4954c18dd90cc83_74905_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="123"
data-flex-basis="297px"
>&lt;/p>
&lt;p>OPT175B는 종교를 제외한 대부분 카테고리에서 더 많은 stereotypical bias을 보였다. 이는 학습 데이터의 차이 때문으로, Reddit 말뭉치가 stereotype과 차별적인 텍스트의 발생률이 더 높다고 나타났다. 이런 데이터가 OPT-175B의 주요 학습 원천이기 때문에, 모델은 더 많은 차별적 연관성을 배웠을 수 있고, 이는 CrowS-Pairs에서의 성능에 직접적인 영향을 미친다.&lt;/p>
&lt;h3 id="stereoset">StereoSet&lt;/h3>
&lt;p>직업, 성별, 종교, 인종의 4가지 카테고리에서 stereotypical bias을 측정하기 위해, 우리는 StereoSet을 사용한다. 이 도구는 문장 내 bias 측정뿐만 아니라, 추가적인 맥락을 포함하는 모델의 능력을 테스트하기 위한 문장 간 bias 측정도 포함한다. bias 탐지와 언어 모델링 능력 사이의 잠재적인 교환 관계를 고려하기 위해, StereoSet은 두 가지 지표를 포함한다.&lt;/p>
&lt;p>Language Modeling Score(LMS)와 Stereotype Score(SS)를 결합해 Idealized Context Association Test score(ICAT)를 만든다. 문자 수가 아닌 토큰 수로 점수를 정규화하는데, 이 방법이 여러 모델의 측정치를 개선한다고 보고되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table5.png"
width="566"
height="694"
srcset="https://kurtkim.github.io/p/opt/images/table5_hu440f9e1cbb2a349639c30b4932f23c58_103130_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table5_hu440f9e1cbb2a349639c30b4932f23c58_103130_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="195px"
>&lt;/p>
&lt;p>Davinci와 OPT-175B는 전체적으로 비슷한 점수를 보여주었다. Davinci는 직업과 인종 분야에서, OPT-175B는 성별과 종교 분야에서 더 뛰어난 성능을 보였다. OPT175B는 SS 지표에서 전반적으로 더 좋은 성능을 보였고, Davinci는 LMS 지표에서 일반적으로 더 뛰어난 성능을 보였다.&lt;/p>
&lt;h3 id="realtoxicityprompts">RealToxicityPrompts&lt;/h3>
&lt;p>RealToxicityPrompts 데이터셋을 이용해 OPT-175B가 toxic 언어로 응답하는 경향을 평가하였다. RTP에서 무작위로 샘플링한 10,000개의 프롬프트 각각에 대해, nucleus 샘플링을 사용하여 생성된 연속성의 평균 toxicity rate을 보고했습니다. 또한, 비교를 위해 Davinci와 PaLM에서의 toxicity rate을 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/figure5.png"
width="596"
height="400"
srcset="https://kurtkim.github.io/p/opt/images/figure5_hu0476d36e1e24bd5bdeda38192a6f51ad_66046_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/figure5_hu0476d36e1e24bd5bdeda38192a6f51ad_66046_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="357px"
>&lt;/p>
&lt;p>OPT-175B는 PaLM이나 Davinci보다 높은 toxicity rate을 보였다. 프롬프트의 toxic이 증가할수록 모든 모델이 toxic을 가진 연속성을 생성할 가능성이 증가하는 것을 확인하였다. 사전 학습 말뭉치에 통제되지 않은 소셜 미디어 텍스트가 포함되어 있다는 점이 toxic 텍스트 생성과 탐지 경향을 높일 수 있다. 이는 downstream 응용 프로그램의 요구에 따라 바람직하지 않을 수도 있으므로, OPT-175B의 미래 응용은 이를 고려해야 한다.&lt;/p>
&lt;h3 id="dialogue-safety-evaluations">Dialogue Safety Evaluations&lt;/h3>
&lt;p>대화 안전성 평가 두 가지를 통해 OPT-175B를 비교하였다. SaferDialogues는 명백한 안전 실패에서 회복하는 능력을, Safety Bench Unit Tests는 모델의 응답의 안전성을 측정한다. 이는 주제의 민감성에 따라 4단계로 분류됩니다. 이 결과는 기존 오픈 소스 대화 모델과 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/opt/images/table6.png"
width="588"
height="230"
srcset="https://kurtkim.github.io/p/opt/images/table6_hu52305b5ec9c40f76468aa62df8ee6326_43286_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/opt/images/table6_hu52305b5ec9c40f76468aa62df8ee6326_43286_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="613px"
>&lt;/p>
&lt;p>두 실험 결과에 따르면, OPT-175B는 SaferDialogues와 Unit Tests에서 Reddit 2.7B 모델과 유사한 성능을 보여주었다. 안전하고 적대적인 설정에서 OPT-175B는 약간 더 높은 성능을 보여주었다. 정제된 대화 데이터셋에서 미세 조정된 모델들은 전반적으로 더 낮은 toxic을 가진 것으로 확인되었다. 따라서, OPT-175B를 대화용으로 활용하는 미래의 실험은 안전 프로파일을 향상시키기 위해 정제된 데이터셋에서 미세 조정을 포함해야 한다는 결론을 내렸다.&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>다양한 규모의 모든 출시된 모델에 대해 폭넓게 평가하였다. GPT-3 모델에서 사용된 표준 평가 데이터셋에 대한 성능은 비슷했으며, 안전성, 편향, 포괄성 등의 평가에서도 대체적으로 비슷한 성능을 보여주었다. 그러나 이러한 평가는 모델의 전체적인 한계를 완전히 반영하지는 못할 수 있다. 특히 OPT-175B는 다른 LLMs에서 지적된 동일한 한계를 보여주었다.&lt;/p>
&lt;p>OPT-175B는 명령형 지시문이나 간결한 질문에 잘 작동하지 않는다는 것을 발견하였다. 지시문의 실행보다는 대화의 시뮬레이션을 생성하는 경향이 있다. 이러한 한계는 InstructGPT와 같은 지시문 학습에 대한 미래의 연구를 통해 완화될 수 있을 것이다.&lt;/p>
&lt;p>OPT-175B는 반복적인 경향이 있고 쉽게 루프에 빠질 수 있다는 것을 발견하였다. 한 번의 생성만 샘플링할 때 샘플링이 반복을 완전히 제거하지 못했다. 미래의 연구에서는 반복을 줄이고 다양성을 향상시키는 전략, 예를 들어 unlikelihood training이나 best-ﬁrst decoding을 통합할 필요가 있다.&lt;/p>
&lt;p>OPT-175B는 다른 LLM과 마찬가지로 사실적으로 부정확한 문장을 생성할 수 있다. 이는 정보의 정확성이 중요한 분야에서 특히 문제가 될 수 있다. 그러나 최근의 연구들은 검색 기반 모델이 LLM의 사실적 정확성을 향상시킬 수 있음을 보여주었다. 따라서, OPT-175B도 미래에 검색 기반 확장의 이점을 누릴 것으로 예상한다.&lt;/p>
&lt;p>OPT-175B는 무해한 프롬프트를 제공받았을 때도 toxic한 언어를 생성하고 해로운 stereotype을 강화하는 경향이 높다고 확인되었다. 또한 적대적인 프롬프트는 쉽게 찾을 수 있었다. toxic과 bias에 대한 대응책에 대한 많은 연구가 있으며, OPT-175B의 미래 사용은 이러한 접근법을 적용해야 할 수 있다. 그러나 이번 첫 릴리즈에서는 GPT-3의 복제를 주요 목표로 두었기 때문에, 이러한 완화책을 적용하지 않았다.&lt;/p>
&lt;p>이 기술이 상업적 배포에는 아직 준비되지 않았다고 생각한다. 더 많은 신중함이 필요하며, 이상적으로는 재현성과 복제성을 보장하기 위해 더 간결하고 일관된 평가 설정을 가지고 있어야 한다. 프롬프트 스타일과 문맥 학습에 대한 차이점은 다른 결과를 이끌어낼 수 있다. OPT 모델의 공개 릴리즈는 이러한 중요한 문제에 대한 연구를 촉진할 것으로 기대한다.&lt;/p>
&lt;hr>
&lt;h2 id="considerations-for-release">Considerations for Release&lt;/h2>
&lt;p>AI 파트너십과 NIST의 지침에 따라, OPT-175B 학습 과정의 모든 세부사항을 공개하고, 연구자들이 모델 가중치에 접근하고 작은 기준선 세트를 사용할 수 있게 한다. OPT-175B의 개발 생명주기에 대한 완전한 책임을 지며, LLM 개발에 대한 투명성을 높여 LLM의 한계와 위험을 이해하는 데 중점을 두고 있다.&lt;/p>
&lt;p>일상적인 학습 과정의 세부사항을 공유함으로써 OPT-175B 학습에 사용된 컴퓨팅 리소스와 대규모에서의 불안정성을 관리하는 데 필요한 인력을 공개한다. 이런 세부사항은 대게 이전 연구에서 생략되었지만, ad-hoc 디자인 결정 과정을 공개함으로써 미래의 모델 개발에서 이러한 방식을 개선하고 실험적 강인성을 높이는 데 기여하길 희망한다.&lt;/p>
&lt;p>개발 코드베이스를 공개함으로써, 논문에서 명시적으로 언급되지 않은 구현 세부 사항에 대한 명확성을 제공하려고 한다. 현재의 코드베이스는 파이프라인 병렬성을 사용하지 않고 175B 이상의 parameter를 가진 decoderonly transformer를 NVIDIA GPU에서 학습시키는 유일한 오픈 소스 구현이다.&lt;/p>
&lt;p>175B 규모의 실험을 가능하게 하기 위해, 연구자들에게 OPT-175B의 parameter에 직접 접근할 수 있게 했다. 이는 LLM에 대한 책임 있는 AI 연구를 촉진하고, 이 규모의 연구가 환경에 미치는 영향을 줄이기 위한 것이다. 대규모 언어 모델 배포의 윤리적, 사회적 위험을 다루는 연구가 증가하고 있다. 비상업적 라이센스를 가진 연구 커뮤니티만 OPT-175B에 접근하게 하여, 상업적 배포 전에 먼저 LLM의 한계를 파악하는 데 초점을 맞추고자 한다.&lt;/p>
&lt;p>이 규모의 모델을 재현하는데는 상당한 컴퓨팅 및 탄소 비용이 발생한다. OPT-175B는 추정 75톤의 탄소 배출량으로 개발되었으며, 다른 모델들은 더 많은 양을 사용하였다. 이러한 추정치는 표준화되지 않았고, AI 시스템의 전체 탄소 발자국은 모델 학습뿐만 아니라 실험과 추론 비용도 포함한다. 로그북을 공개하여 이론적 탄소 비용 추정치와 전체 개발 수명주기를 고려한 추정치 사이의 차이를 강조하고자 한다. 또한, 점점 복잡해지는 이 시스템들의 제조 탄소를 이해하고, 환경에 대한 규모의 영향을 측정할 때 고려해야 할 추가 요인을 정의하는 데 이 논문이 도움이 될 수 있기를 희망한다.&lt;/p>
&lt;p>다양한 스케일에서 기준선을 설정함으로써, 연구 커뮤니티가 이 모델들의 영향력과 한계를 스케일만으로 연구할 수 있도록 돕고자 한다. 일부 LLM은 사용된 학습 데이터 양에 비해 학습이 부족했을 수 있으며, 이는 더 많은 데이터를 추가하고 계속 학습하면 성능이 향상될 수 있음을 의미한다. 또한, 175B보다 훨씬 작은 규모에서 기능 변화가 발생할 수 있다는 증거가 있으므로, 다양한 연구 활용을 위해 더 넓은 스케일 범위를 검토해야 한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>transformer 아키텍처와 BERT의 출시 이후, NLP 분야는 self-supervised 학습을 통한 LLM 사용으로 크게 변화하였다. T5와 MegatronLM 같은 여러 가면 언어 모델들은 규모를 통해 지속적으로 성능을 향상시켰다. 이는 모델의 parameter 수 증가뿐만 아니라 사전 학습 데이터의 양과 품질 향상으로 이루어졌다.&lt;/p>
&lt;p>auto-regressive 언어 모델은 모델 크기가 크게 증가하였고, 이로 인해 생성 유창성과 품질이 대폭 향상되었다. 많은 큰 모델들이 학습되었지만, 이들은 대부분 비공개 소스로, 내부적으로 또는 유료 API를 통해만 접근 가능하다. 그러나 비영리 연구 조직에서는 LLM을 오픈 소스화하는 노력이 있으며, 이러한 모델들은 OPT 모델과 다르기 때문에, 커뮤니티가 다양한 사전 학습 전략을 비교할 수 있다.&lt;/p>
&lt;p>LLM의 주요 평가 기준은 프롬프트 기반이며 이는 특정 작업에 대한 미세 조정 없이도 많은 작업을 평가하는 편리함 때문이다. 프롬프트는 오래된 역사를 가지고 있고, 최근에는 모델에 대한 지식 탐색 또는 다양한 NLP 작업 수행에 사용되었다. 또한, 작은 모델에서 프롬프트 동작을 유도하거나, 프롬프트의 유연성을 개선하고, 프롬프트가 어떻게 작동하는지 이해하는 연구도 있다.&lt;/p>
&lt;p>모델을 지시 스타일의 프롬프트에 대응하게 미세조정하는 것이 이익을 보였지만, 효과적인 프롬프트 엔지니어링은 여전히 해결되지 않은 연구 과제이다. 프롬프트 선택에 따라 결과는 크게 달라지며, 모델은 프롬프트를 우리가 기대하는 만큼 완전히 이해하지 못하는 것으로 보인다. 또한, 개발 세트 없이 프롬프트를 작성하는 것은 어려움이 있다. 이러한 문제를 해결하려 하지 않고, 단지 OPT-175B의 평가만을 목표로 하며, OPT-175B의 전체 릴리스가 미래의 연구를 돕길 바란다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>125M에서 175B parameter까지 다양한 크기의 auto-regressive 언어 모델 모음인 OPT를 소개하다. 이 연구의 목표는 GPT-3 클래스의 모델을 복제하고 최신 데이터 큐레이션 및 학습 효율성 모범 사례를 적용하는 것이다. 모델의 여러 제한 사항과 책임감 있는 공개에 대한 고려 사항을 논의하였다. 우리는 AI 커뮤니티가 책임감 있는 LLM 가이드라인 개발에 협력하고, 이러한 유형의 모델에 대한 넓은 접근이 기술의 윤리적 고려 사항을 정의하는 다양한 목소리를 늘리길 희망한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2205.01068.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/metaseq" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>PaLM</title><link>https://kurtkim.github.io/p/palm/</link><pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/palm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>few-shot learning 예제를 사용하는 대형 언어 모델은 다양한 자연어 작업에서 뛰어난 성능을 보여준다. 이를 더 깊이 이해하기 위해, 540B parameter의 densely activated transformer 언어 모델인 Pathways Language Model(PaLM)을 학습시켰다.&lt;/p>
&lt;p>새로운 ML 시스템인 Pathways를 사용해 PaLM을 학습시키고, 수백 개의 언어 이해 및 생성 벤치마크에서 state-of-the-art의 few-shot learning 결과를 달성하였다. PaLM 540B는 다단계 추론 작업과 BIG-bench 벤치마크에서 인간 평균 성능을 능가하는 성과를 보여주었다. 모델 규모가 커짐에 따라 성능이 급격히 향상된 작업도 있었다. 또한 PaLM은 다국어 작업과 소스 코드 생성에서도 강력한 능력을 가지고 있다. bias와 toxicity 대한 분석과 함께, 거대 언어 모델과 관련된 윤리적 고려 사항에 대해 논의하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 이해와 생성을 위한 대규모 신경망들은 다양한 작업에서 놀라운 결과를 보여주고있다. BERT나 T5 같은 모델들은 대량의 텍스트를 통해 사전 학습되고, 특정 작업에 맞게 미세 조정된다. 이들 모델은 다양한 자연어 작업에서 state-of-the-art를 보여주지만, 모델을 미세 조정하는 데 많은 수의 작업 특정 학습 예제가 필요하고, 일부 모델 parameter를 작업에 맞게 업데이트하는 복잡성이 증가한다는 단점이 있다.&lt;/p>
&lt;p>GPT-3는 극도로 큰 autoregressive 언어 모델이 소수의 예측을 위해 사용될 수 있음을 보여주었다. 이 모델은 자연어 작업 설명과 작업 완료 방법을 보여주는 몇 가지 예시만 제공받아 학습된다. 대규모 작업 특정 데이터 수집이나 모델 parameter 업데이트 없이도 매우 강력한 결과를 달성하였다.&lt;/p>
&lt;p>GPT-3 이후에도 GLaM, Gopher, Chinchilla, Megatron–Turing NLG, LaMDA와 같은 강력한 대규모 autoregressive 언어 모델들이 개발되어 state-of-the-art를 계속 밀어내고 있다. 이들 모델은 모두 transformer 아키텍처의 변형이며, 모델의 크기 확대, 학습된 토큰 수 증가, 더 깨끗한 데이터셋 사용, 희소 활성화 모듈을 통한 계산 비용 없는 모델 용량 증가 등의 방법으로 개선되었다.&lt;/p>
&lt;p>이 연구에서는 780B 개의 고품질 텍스트 토큰에 대해 540B 개의 parameter를 가진 densely activated autoregressive transformer를 학습시키는 언어 모델링 개선을 계속하였다. 이는 새로운 ML 시스템인 Pathways를 사용하여 수천 개의 accelerator chip에서 매우 큰 신경망을 효율적으로 학습시키는 데 성공하였다. 이 새로운 모델인 PaLM은 수백 개의 자연어, 코드, 수학적 추론 작업에서 breakthrough performance를 달성하였다.&lt;/p>
&lt;p>이 연구에서 주요 결론은 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Eﬃcient scaling&lt;/strong> 이 연구에서는 새로운 ML 시스템인 Pathways를 대규모로 처음 사용하였다. 이를 통해, 6144개의 TPU v4 칩에서 540B parameter 언어 모델을 이전에는 도달할 수 없었던 효율 수준에서 학습시켰다. 이전의 대부분의 대규모 언어 모델들은 단일 TPU 시스템에서 학습되거나 GPU 클러스터 또는 여러 TPU v3 pods에 걸쳐 확장되었다. 두 개의 TPU v4 Pods에 걸쳐 6144개의 칩으로 PaLM 540B의 학습을 확장하면서 매우 높은 효율성을 달성하였다.&lt;/li>
&lt;li>&lt;strong>Continued improvements from scaling&lt;/strong> 자연어, 코드, 수학적 추론 작업 등 수백 가지 작업에 대해 PaLM을 평가하고, 대부분의 벤치마크에서 상당한 차이로 state-of-the-art를 달성하였다. 이는 대규모 언어 모델로부터의 scaling 개선이 아직도 정체되지 않았음을 보여준다. 가장 널리 평가된 29개의 영어 언어 이해 벤치마크 중 28개에서 최고 작업별 결과에 비해 state-of-the-art를 보여주었다.&lt;/li>
&lt;li>&lt;strong>Breakthrough capabilities&lt;/strong> 이 연구에서는 다양한 어려운 작업에 대해 언어 이해와 생성에서 breakthrough capabilities를 보여준다. 특히, multi-step 수학적 또는 상식적 추론이 필요한 일련의 추론 작업에 대해 평가하였다. 모델 scaling과 사슬 형태의 생각 유도를 결합하면, 간단한 소수 평가가 넓은 범위의 추론 작업에서 state-of-the-art를 능가하거나 매치할 수 있음을 보여주었다. 또한, 최근 출시된 150개 이상의 새로운 언어 이해와 생성 작업을 포함하는 BIG-bench에서 breakthrough performance을 보여주었다. PaLM이 복잡한 추론 체인을 명확하게 해석하고 설명하는 능력을 탐색하였다.&lt;/li>
&lt;li>&lt;strong>Discontinuous improvements&lt;/strong> 8B, 62B, 540B의 세 가지 다른 parameter 규모에서의 결과를 제시하여 scaling 행동을 이해한다. 일반적으로, 62B에서 540B로의 scaling은 8B에서 62B로의 scaling과 유사한 성능을 가져온다. 그러나 특정 작업에 대해서는, 62B에서 540B로의 scaling이 정확도에서 drastic jump를 가져오는 것을 관찰하였다. 이는 대규모 언어 모델의 새로운 능력이 충분한 규모를 달성하면 나타날 수 있음을 제안한다.&lt;/li>
&lt;li>&lt;strong>Multilingual understanding&lt;/strong> 이 연구에서는 다양한 언어에서의 기계 번역, 요약, 그리고 질문 응답을 포함한 다국어 벤치마크에 대한 철저한 평가를 수행하였다. 비영어 데이터의 비율이 상대적으로 작음에도 불구하고, PaLM 모델은 비영어 요약 작업에서 이전에 미세 조정된 state-of-the-art와의 격차를 메우며, 번역 작업에서 이전의 state-of-the-art를 능가하였다. 다국어 데이터 비율 증가의 영향을 이해하기 위해 추가적인 연구가 필요하다.&lt;/li>
&lt;li>&lt;strong>Bias and toxicity&lt;/strong> distributional bias와 toxicity에 대한 모델 성능을 평가하였다. 성별과 직업에 대한 bias에서, 모델 규모가 커짐에 따라 성능이 개선되었다. 인종/종교/성별 프롬프트 연속성에서는 모델이 스테레오타입을 거짓으로 확증하는 가능성을 보여주었다. toxicity 분석에서는 62B와 540B 모델이 8B 모델에 비해 약간 더 높은 toxicity 수준을 보여주었다. 모델이 생성한 연속성의 toxicity은 프롬프트 텍스트의 toxicity과 높게 상관되었다. 향후 연구에서는 이러한 벤치마크를 비영어 언어로 확장하고 잠재적 위험을 더 철저히 고려할 계획이다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>PaLM은 다음과 같은 수정을 가진 표준 Transformer 모델 아키텍처의 decoder-only setup으로 사용한다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>SwiGLU Activation&lt;/strong> MLP 중intermediate activation에 SwiGLU activation을 사용한다. 이는 표준 ReLU, GeLU, Swish activation에 비해 품질을 크게 향상시키기 때문이다. 이는 MLP에서 세 개의 행렬 곱셈이 필요하다는 것을 의미하지만, 이는 품질 개선을 보여준다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Layers&lt;/strong> 각 Transformer block에서 표준 &amp;ldquo;serialized&amp;rdquo; 형식 대신 &amp;ldquo;parallel&amp;rdquo; 형식을 사용한다. 특히, 표준 serialized 형식은 다음과 같이 작성할 수 있다:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$ y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x)))) $$&lt;/p>
&lt;p>반면에, parallel 형식은 다음과 같이 작성할 수 있다:&lt;/p>
&lt;p>$$ y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x)) $$&lt;/p>
&lt;p>parallel 형식은 MLP와 Attention 입력 행렬 곱셈이 융합될 수 있어 대규모 규모에서 학습 속도를 약 15% 더 빠르게 한다. 실험에서는 8B 규모에서는 약간의 품질 저하가 있었지만, 62B 규모에서는 품질 저하가 없었으므로, 540B 규모에서 parallel layer의 효과는 품질에 영향을 주지 않을 것으로 추정하였다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Multi-Query Attention&lt;/strong> 표준 Transformer 형식은 $k$개의 attention head를 사용하며, 각 타임스텝의 입력 벡터는 &amp;ldquo;query&amp;rdquo;, &amp;ldquo;key&amp;rdquo;, &amp;ldquo;value&amp;rdquo; 텐서로 선형적으로 투영된다. 이 방식은 모델 품질과 학습 속도에 중립적인 효과를 가지지만, decoding 시간에 비용 절약을 가져온다. 이는 standard multi-headed attention이 auto-regressive decoding 시에 accelerator 하드웨어에서 낮은 효율성을 보이기 때문이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RoPE Embeddings&lt;/strong> 긴 시퀀스 길이에서 더 나은 성능을 보이는 RoPE 임베딩을 사용한다. 이는 절대적 또는 상대적 포지션 임베딩 대신에 사용되었다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Shared Input-Output Embeddings&lt;/strong> 입력과 출력 임베딩 행렬을 공유한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>No Biases&lt;/strong> 어떤 dense kernel이나 layer norm에서도 bias를 사용하지 않았다. 이는 큰 모델의 학습 안정성을 증가시키는 것으로 나타났다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Vocabulary&lt;/strong> 256k 토큰의 SentencePiece 어휘를 사용하여 학습 말뭉치의 많은 언어를 지원한다. 이 어휘는 학습 데이터에서 생성되었으며, 학습 효율성을 향상시킨다. 어휘는 완전히 손실 없이 되돌릴 수 있으며, 공백을 완전히 보존하고, 어휘 외의 유니코드 문자를 UTF-8 바이트로 분할한다. 숫자는 항상 개별 숫자 토큰으로 분할된다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="model-scale-hyperparameters">Model Scale Hyperparameters&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table1.png"
width="1056"
height="210"
srcset="https://kurtkim.github.io/p/palm/images/table1_hude476ed3e91ed00e06b0e7cb7b21a5f2_44541_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table1_hude476ed3e91ed00e06b0e7cb7b21a5f2_44541_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="502"
data-flex-basis="1206px"
>&lt;/p>
&lt;p>이 연구에서는 540B, 62B, 8B parameter의 세 가지 다른 모델 규모를 비교한다. 이 모델들은 standard dense transformer이므로, 토큰 당 FLOP 수는 parameter 수와 대략적으로 동일하다. 이 모델들은 같은 데이터와 어휘를 사용하여 동일하게 학습되었다.&lt;/p>
&lt;hr>
&lt;h2 id="training-dataset">Training Dataset&lt;/h2>
&lt;p>PaLM 사전 학습 데이터셋은 다양한 자연어 사용 사례를 대표하는 7800억 토큰의 말뭉치로 구성되어 있다. 이 데이터셋은 웹페이지, 책, 위키백과, 뉴스 기사, 소스 코드, 소셜 미디어 대화를 섞어 만들었다. 모든 모델을 데이터의 1 epoch 학습시키고, 데이터를 반복하지 않도록 혼합 비율을 선택하였다.&lt;/p>
&lt;p>사전 학습 데이터셋은 자연어 데이터뿐만 아니라 코드도 포함한다. 이 코드는 GitHub의 오픈 소스 저장소에서 얻은 것이며, 라이선스에 따라 필터링하였다. 또한 파일 이름 확장자에 따라 24개의 일반적인 프로그래밍 언어 중 하나로 제한하였고, 중복 파일을 제거하였다. 이 결과, 196GB의 소스 코드가 생성되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table2.png"
width="780"
height="322"
srcset="https://kurtkim.github.io/p/palm/images/table2_hu3b61d835e2368f25411df1d314c5133d_56956_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table2_hu3b61d835e2368f25411df1d314c5133d_56956_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>최종 PaLM 데이터셋 혼합물을 생성하는 데 사용된 다양한 데이터 소스의 비율을 보여주며, 데이터 오염을 확인하고, 학습 데이터셋과 평가 데이터 사이의 중복을 분석한다.&lt;/p>
&lt;hr>
&lt;h2 id="training-infrastructure">Training Infrastructure&lt;/h2>
&lt;p>학습 및 평가 코드베이스는 JAX와 T5X를 기반으로 하며, 모든 모델은 TPU v4 Pods에서 학습된다. PaLM 540B는 데이터 센터 네트워크를 통해 연결된 두 개의 TPU v4 Pods에서 학습되며, 이는 모델과 데이터 병렬성의 조합을 사용한다. 이 시스템은 파이프라인 병렬성 없이 학습을 6144개의 칩으로 효율적으로 확장할 수 있게 해준다.&lt;/p>
&lt;p>이전의 비슷한 규모에서 모델 학습은 두 가지 접근법을 사용했다. LaMDA와 GLaM은 파이프라인 병렬성이나 DCN을 활용하지 않고 단일 TPU 시스템에서 학습되었고, Megatron-Turing NLG 530B는 여러 가지 병렬성을 사용하여 A100 GPU에서, Gopher는 파이프라이닝을 사용하여 DCN-연결된 TPU v3 Pods에서 학습되었다.&lt;/p>
&lt;p>파이프라이닝은 일반적으로 DCN과 함께 사용되며, 추가적인 병렬화를 제공한다. 그러나 이는 학습 배치를 &amp;ldquo;micro-batches&amp;quot;로 분할하지만, 중요한 단점이 있다. 첫째, 많은 장치가 유휴(idle) 상태인 동안 발생하는 시간 오버헤드가 있다. 둘째, 미니 배치 내의 각 마이크로 배치에 대해 메모리에서 가중치를 다시 로드해야 하므로 높은 메모리 대역폭이 필요하다. 이러한 문제를 해결하기 위한 전략을 통해 PaLM 540B의 학습을 6144 칩으로 효율적으로 확장할 수 있었다.&lt;/p>
&lt;p>각 TPU v4 Pod는 모델 parameter의 전체 복사본을 포함하며, 각 가중치 텐서는 모델 병렬성과 완전분할 데이터 병렬성을 사용하여 칩으로 분할된다. forward pass에서 가중치가 모두 모아지고, 각 layer에서 activation 텐서가 저장된다. backward pass에서는 나머지 activation이 rematerialized되며, 이는 더 큰 배치 크기에서 더 높은 학습 처리량을 결과로 내기 때문이다.&lt;/p>
&lt;p>Pathways 시스템을 사용하여 단일 TPU v4 Pod를 넘어서 학습을 확장한다. PaLM 540B는 Pathways의 클라이언트-서버 아키텍처를 사용하여 pod 레벨에서 데이터 병렬성을 달성한다. Python 클라이언트는 배치의 절반을 각 pod에 할당하고, 각 pod는 gradient를 계산하기 위해 병렬로 계산을 수행한다. 그 후, pod들은 gradient를 원격 pod에 전송하고, 각 pod는 gradient를 누적하고 parameter를 업데이트하여 다음 타임스텝에 대한 parameter를 얻는다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure2.png"
width="872"
height="342"
srcset="https://kurtkim.github.io/p/palm/images/figure2_hu61787b504635cdb77d202cdfb13ee7cd_88893_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure2_hu61787b504635cdb77d202cdfb13ee7cd_88893_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>Python 클라이언트는 분할된 데이터플로우 프로그램을 구성하고, 이 프로그램은 각 pod에서 계산과 최적화 업데이트를 수행하고, gradient를 다른 pod로 전송한다. Pathways 시스템의 디자인은 프로그램 실행을 수천 개의 accelerator 칩으로 확장할 수 있게 한다. 이는 원격 서버로 작업을 발송하는 데 걸리는 지연 시간을 감추고, 데이터 전송의 관리 비용을 분산시킨다.&lt;/p>
&lt;p>two-way pod-level 데이터 병렬성의 도전적인 측면은 cross-pod gradient 전송에 대한 높은 학습 처리량을 달성하는 것이다. 이는 데이터 센터 네트워크를 통해 모든 호스트가 gradient를 동시에 전송하는 매우 폭발적인 작업량을 초래한다. 이로 인한 도전을 극복하기 위해, 데이터를 작은 청크로 분해하고 다양한 DCN 링크를 통해 라우팅하는 등의 최적화를 수행한다. 이러한 최적화를 통해, 학습 중 단일 pod에 비해 약 1.95배의 처리량을 달성하였다. 이론적인 2배 처리량에 비한 성능 차이는 backward pass와 cross-pod gradient 축소 사이의 중첩이 부족하기 때문에 발생하며, 이 문제는 향후 작업에서 해결할 예정이다.&lt;/p>
&lt;h3 id="training-eﬃciency">Training Eﬃciency&lt;/h3>
&lt;p>언어 모델의 accelerator 효율성은 대게 hardware FLOPs utilization(HFU)로 측정된다. 이는 주어진 장치에서 관찰된 FLOPs와 이론적인 최대 FLOPs 사이의 비율을 나타낸다. 하지만 이 방법에는 문제가 있다. 첫째, 실행된 하드웨어 FLOPs의 수는 시스템과 구현에 따라 달라진다. 둘째, 하드웨어 FLOPs 측정은 그것들을 세거나 추적하는 방법에 의존적이다. 결국, 학습 시스템의 목표는 가능한 많은 하드웨어 FLOPs를 사용하는 것이 아니라 초당 토큰의 높은 처리량을 달성하는 것이다.&lt;/p>
&lt;p>HFU는 LLM 학습 효율성에 대한 일관된 척도가 아니라는 문제점을 인식하였다. 따라서, model FLOPs utilization(MFU)이라는 새로운 효율성 척도를 제안한다. MFU는 관찰된 처리량이 피크 FLOPs에서 운영하는 시스템의 이론적 최대 처리량에 대한 비율이다. 이 척도는 다양한 시스템에서의 학습을 공정하게 비교할 수 있게 해준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure3.png"
width="1214"
height="506"
srcset="https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;p>PaLM 540B 모델의 model FLOPs utilization(MFU)을 제시하고, 이전의 큰 모델들과 비교하였다. MFU는 다양한 모델 parameter 수, 아키텍처, 모델 품질의 맥락에서 모델과 시스템을 비교하는데 유용하다.&lt;/p>
&lt;p>GPT-3의 MFU는 21.3%, Gopher는 32.5%, Megatron–Turing NLG 530B는 self-attention 없이 29.7%, 있으면 30.2%이다. 반면, PaLM 540B는 self-attention 없이 45.7%, 있으면 46.2%의 MFU를 달성하였다.&lt;/p>
&lt;p>PaLM은 병렬성 전략과 XLA TPU 컴파일러 최적화, 그리고 &amp;ldquo;parallel layers&amp;quot;의 사용 등으로 인해 높은 accelerator 이용률을 달성하였다. 이로써 PaLM은 LLM 학습 효율성에서 중요한 진전을 나타내는 것으로 보여진다.&lt;/p>
&lt;hr>
&lt;h2 id="training-setup">Training Setup&lt;/h2>
&lt;p>모델 학습은 large transformer 언어 모델에 대한 상당히 표준적인 설정을 따랐다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Weight initialization&lt;/strong> 커널 가중치는 &amp;ldquo;fan-in variance scaling&amp;quot;을 사용하여 초기화하며, 입력 임베딩은 layer normalization가 적용되지 않기 때문에 $E ∼ N(0, 1)$으로 초기화된다. 입력과 출력 임베딩 레이어가 공유되므로, pre-softmax 출력 logit은 임베딩 크기의 제곱근의 역수로 스케일링된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Optimizer&lt;/strong> 이 모델은 Adafactor optimizer를 사용하여 학습되었으며, 이는 parameter 행렬의 평균 제곱근으로 learning rate을 조정하는 Adam과 사실상 동일하다. 가중치 초기화가 ${{1}\over{\sqrt{n}}}$에 비례하기 때문에, 이는 learning rate를 수동으로 축소하는 것과 비슷한 효과를 가진다. 하지만, 다른 스케일에서 작동하는 parameter 행렬들이 동일한 비율로 learning rate을 축소하지 않게 하는 이점이 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Optimization hyperparameters&lt;/strong> 처음 10,000 단계에는 $10^{-2}$의 Adafactor learning rate을 사용하고, 이후에는 단계 번호에 따라 learning rate을 감소시킨다. 모멘텀은 $\beta_1 = 0.9$로 설정하고, 두 번째 순서 모멘트 보간 값은 $\beta_2 = 1.0 - k^{-0.8}$로 계산된다. 이 방법은 희귀 임베딩 토큰의 두 번째 순간을 더 정확하게 추정할 수 있어 안정적이다. 그리고, 모든 모델에서 1.0의 global norm gradient clipping을 사용하며, 학습 중에는 현재 learning rate의 2배에 해당하는 dynamic weight decay를 사용한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Loss function&lt;/strong> 이 모델은 표준 언어 모델링 손실 함수, 즉 모든 토큰의 average log probability를 사용하여 학습된다. 또한, softmax normalizer 값인 $log(Z)$가 0에 가깝게 만드는 auxiliary loss인 $z$ 손실을 사용하며, 이는 학습 안정성을 높이는 데 도움이 된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sequence length&lt;/strong> 모든 모델은 2048 토큰의 시퀀스 길이로 작동하며, 입력 예제들은 이 길이에 맞춰 연결되고 분할된다. 각 예제는 특별한 [eod] 토큰으로 구분되며 패딩 토큰은 사용되지 않는다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Batch size&lt;/strong> 학습 도중 모든 모델의 배치 크기를 점진적으로 증가시킨다. 큰 모델의 경우, 초기에는 배치 크기를 512로 설정하고, 학습이 진행됨에 따라 이를 2048까지 늘린다. 이런 방식은 학습 초기에는 작은 배치 크기가, 후반에는 큰 배치 크기가 더 효율적이기 때문이며, 또한 큰 배치 크기는 TPU 효율성을 높이는데 도움이 된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Bitwise determinism&lt;/strong> 이 모델은 체크포인트에서 완전 재현이 가능하며, 이는 JAX+XLA+T5X가 제공하는 비트 단위 결정적 모델링 프레임워크와, 단계 번호만으로 학습 배치의 내용을 결정하는 결정적 데이터셋 파이프라인 덕분이다. 따라서 모델이 한 번의 실행에서 특정 단계까지 학습되었다면, 그 체크포인트에서 다시 시작해도 동일한 결과를 보장한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dropout&lt;/strong> 이 모델은 드롭아웃 없이 학습되었지만, 대부분의 경우에는 0.1의 드롭아웃을 사용하여 미세조정 한다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="training-instability">Training Instability&lt;/h3>
&lt;p>가장 큰 모델을 학습하면서 gradient clipping이 적용되어 있음에도 불구하고, 불규칙한 간격으로 20번가량 손실이 급증하는 현상을 관찰하였다. 이는 작은 모델에서는 발견되지 않았으며, 큰 모델의 학습 비용 때문에 이 문제를 완화하기 위한 명확한 전략을 세우지 못하였다.&lt;/p>
&lt;p>손실 증가 문제를 완화하기 위해, 손실 증가가 시작되기 전 체크포인트에서 학습을 재시작하고, 손실 증가가 관찰된 데이터 배치를 건너뛰는 전략을 사용했다. 이 방법은 손실 증가가 특정 데이터 배치와 모델 파라미터의 특정 상태의 조합으로 발생한다는 것을 보여주며, &amp;ldquo;bad data&amp;rdquo; 때문이 아님을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluation">Evaluation&lt;/h2>
&lt;h3 id="english-nlp-tasks">English NLP tasks&lt;/h3>
&lt;p>PaLM 모델은 이전 연구와 동일한 29개의 영어 벤치마크를 이용하여 평가한다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Open-Domain Closed-Book Question Answering tasks:&lt;/strong> TriviaQA, Natural Questions, Web Questions&lt;/li>
&lt;li>&lt;strong>Cloze and Completion tasks:&lt;/strong> LAMBADA, HellaSwag, StoryCloze&lt;/li>
&lt;li>&lt;strong>Winograd-style tasks:&lt;/strong> Winograd, WinoGrande&lt;/li>
&lt;li>&lt;strong>Common Sense Reasoning:&lt;/strong> PIQA, ARC, OpenBookQA&lt;/li>
&lt;li>&lt;strong>In-context Reading Comprehension:&lt;/strong> DROP, CoQA, QuAC, SQuADv2, RACE&lt;/li>
&lt;li>&lt;strong>SuperGLUE&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Natural Language Inference (NLI):&lt;/strong> Adversarial NLI&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table4.png"
width="968"
height="1244"
srcset="https://kurtkim.github.io/p/palm/images/table4_hua13741bf4df7c3dc8993730568ef574d_274515_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table4_hua13741bf4df7c3dc8993730568ef574d_274515_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="186px"
>&lt;/p>
&lt;p>PaLM 540B는 대부분의 작업에서 이전 state-of-the-art를 능가하였다. 특히, 읽기 이해와 NLI 작업에서 더욱 두드러졌다. 이는 모델 크기 뿐 아니라, 사전 학습 데이터셋, 학습 전략, 학습 중 관찰된 토큰 수 등이 중요하게 작용했음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table5.png"
width="494"
height="226"
srcset="https://kurtkim.github.io/p/palm/images/table5_hu3527a0d28514c866b7132b05a4581fea_30514_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table5_hu3527a0d28514c866b7132b05a4581fea_30514_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="524px"
>&lt;/p>
&lt;p>PaLM 540B가 자연어 이해와 자연어 생성 작업에서 평균 점수를 5점 이상 향상시켰다. 특히, PaLM 62B는 GPT-3 175B를 두 카테고리에서 모두 능가하였다.&lt;/p>
&lt;h4 id="massive-multitask-language-understanding">Massive Multitask Language Understanding&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table6.png"
width="914"
height="170"
srcset="https://kurtkim.github.io/p/palm/images/table6_hu0ffda79964b359911c752ba11766fe13_34544_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table6_hu0ffda79964b359911c752ba11766fe13_34544_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="537"
data-flex-basis="1290px"
>&lt;/p>
&lt;p>PaLM 모델은 다양한 주제를 다루는 대규모 다중작업 언어 이해 벤치마크에서 평가되었고, 평균 점수를 약 2점 향상시켰다. PaLM 540B는 &amp;ldquo;Other&amp;quot;을 제외한 모든 카테고리에서 Chinchilla 모델을 능가하였다.&lt;/p>
&lt;h4 id="finetuning">Finetuning&lt;/h4>
&lt;p>SuperGLUE 벤치마크에서 PaLM 모델을 미세조정하는 실험을 진행했고, 일반적으로 15K 단계 이내에 수렴했다. 이 과정에서는 Adafactor optimizer를 사용하고 batch size는 32였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table7.png"
width="1228"
height="180"
srcset="https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="682"
data-flex-basis="1637px"
>&lt;/p>
&lt;p>PaLM이 가장 뛰어난 성능을 보여주는 모델과 경쟁력 있게 성능을 내는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table8.png"
width="990"
height="136"
srcset="https://kurtkim.github.io/p/palm/images/table8_hu20a202c16920982ef7d5644d60c68753_29808_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table8_hu20a202c16920982ef7d5644d60c68753_29808_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="727"
data-flex-basis="1747px"
>&lt;/p>
&lt;p>또한 few-shot과 미세조정 결과 사이에 여전히 큰 차이가 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table9.png"
width="1272"
height="178"
srcset="https://kurtkim.github.io/p/palm/images/table9_hu7179d5d88d289d45789eec86efa95118_49867_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table9_hu7179d5d88d289d45789eec86efa95118_49867_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="714"
data-flex-basis="1715px"
>&lt;/p>
&lt;p>PaLM이 최첨단 모델과 경쟁력 있으며, 리더보드에서 가장 뛰어난 성능을 내는 decoder-only autoregressive 언어 모델을 크게 능가하는 것을 보여준다.&lt;/p>
&lt;h3 id="big-bench">BIG-bench&lt;/h3>
&lt;p>BIG-bench는 대규모 언어 모델에 대한 도전적인 작업을 목표로 하는 벤치마크로, 다양한 언어 모델링 작업을 포함한다. 이 벤치마크에서 PaLM 모델 계열은 few-shot 평가를 수행하였고, 텍스트 작업에 초점을 두었다. 인간의 성능도 같은 지표로 측정되었으며, 이를 통해 &amp;ldquo;best&amp;quot;와 &amp;ldquo;average&amp;rdquo; 인간 성능이 계산되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure3.png"
width="1214"
height="506"
srcset="https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure3_hu64d44763d1af603d4a4d56b41a14e122_150594_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;p>PaLM 모델 계열은 BIG-bench에서 상당한 성능을 나타냈으며, GPT-3, Gopher, Chinchilla를 크게 능가하였다. 특히, 5-shot PaLM 540B는 동일한 작업을 수행한 인간의 평균 점수보다 높은 점수를 얻었다. 또한, 규모에 따른 PaLM 모델의 성능은 log-linear 행동을 보였으며, 이는 추가적인 스케일링이 성능 향상을 가져올 가능성을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure4.png"
width="1282"
height="490"
srcset="https://kurtkim.github.io/p/palm/images/figure4_hu2b8641c88a546db059f15eb12490f615_62019_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure4_hu2b8641c88a546db059f15eb12490f615_62019_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="627px"
>&lt;/p>
&lt;p>BIG-bench에서 PaLM이 특별히 눈에 띄는 성능을 보인 몇 가지 작업을 강조한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>goal step wikihow&lt;/strong> 목표는 이벤트 간의 목표-단계 관계에 대해 추론하는 것이다.
Input: ”clean silver,” which step should be done ﬁrst? (a) dry the silver (b) handwash the silver.
Answer: (b) handwash the silver.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>logical args&lt;/strong> 목표는 문단에서 올바른 논리적 추론을 예측하는 것이다.
Input: Students told the substitute teacher they were learning trigonometry. The substitute told them that instead of teaching them useless facts about triangles, he would instead teach them how to work with probabilities. What is he implying? (a) He believes that mathematics does not need to be useful to be interesting. (b) He thinks understanding probabilities is more useful than trigonometry. (c) He believes that probability theory is a useless subject.
Answer: (b) He thinks understanding probabilities is more useful than trigonometry.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>english proverbs&lt;/strong> 목표는 어떤 속담이 텍스트 구절을 가장 잘 설명하는지 추측하는 것이다.
Input: Vanessa spent lots of years helping out on weekends at the local center for homeless aid. Recently, when she lost her job, the center was ready to oﬀer her a new job right away. Which of the following proverbs best apply to this situation? (a) Curses, like chickens, come home to roost. (b) Where there is smoke there is ﬁre (c) As you sow, so you shall reap.
Answer: (c) As you sow, so you shall reap.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>logical sequence&lt;/strong> 목표는 논리적인 순서대로 배열하는 것이다.
Input: Which of the following lists is correctly ordered chronologically? (a) drink water, feel thirsty, seal water bottle, open water bottle (b) feel thirsty, open water bottle, drink water, seal water bottle (c) seal water bottle, open water bottle, drink water, feel thirsty.
Answer: (b) feel thirsty, open water bottle, drink water, seal water bottle.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>navigate&lt;/strong> 목표는 간단한 네비게이션 지시를 따르고, 어디에 도착할지 파악하는 것이다.
Input: If you follow these instructions, do you return to the starting point? Always face forward. Take 6 steps left. Take 7 steps forward. Take 8 steps left. Take 7 steps left. Take 6 steps forward. Take 1 step forward. Take 4 steps forward.
Answer: No.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>mathematical induction&lt;/strong> 목표는 실제 세계의 수학과 상충하더라도 수학적 귀납법 규칙에 따라 논리적 추론을 수행하는 것이다.
nput: It is known that adding 2 to any odd integer creates another odd integer. 2 is an odd integer. Therefore, 6 is an odd integer. Is this a correct induction argument (even though some of the assumptions may be incorrect)?
Answer: Yes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure5.png"
width="1150"
height="762"
srcset="https://kurtkim.github.io/p/palm/images/figure5_hu7f70e194740d88da067dccae36cdf0ef_190827_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure5_hu7f70e194740d88da067dccae36cdf0ef_190827_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="362px"
>&lt;/p>
&lt;p>goal step wikihow와 logical args에 대한 성능은 log-linear 스케일링 곡선을 따르며, PaLM 540B 모델은 최고의 인간 성능에 가까워진다. 영어 속담과 논리적 순서에 대한 성능도 강력하지만, 개선 곡선은 불연속적이다. 특히, 특정 규모에 도달하면서만 특정 능력이 나타나는 것이 확인되었다. PaLM 62B에서 25%에서 PaLM 540B의 87%로 크게 개선된 영어 속담 성능은 매우 흥미로운 결과이다.&lt;/p>
&lt;p>불연속성에 대한 예로, PaLM의 논리적 순서 작업에서 8b, 62b, 540b 모델에 대한 정확도가 각각 13%, 25%, 87%였다. 이에 따라, 540b에 대한 예상 정확도는 37%였지만, 실제 정확도는 87%로, 불연속성은 +50%였다. 전체 150개 작업 중 25%의 작업에서 불연속성이 +10% 이상, 15%의 작업에서 +20% 이상 나타났으며, 이는 스케일에서의 불연속적인 개선이 일반적인 현상임을 보여준다.&lt;/p>
&lt;p>모든 작업에서 규모가 이익을 가져다주는 것은 아니다. 네비게이션과 수학적 귀납 작업에서 PaLM 540B는 PaLM 62B를 살짝 능가하지만, 두 모델 모두 최고의 인간 성능에서는 아직 멀리 떨어져 있다. 이는 작업의 예제 수준의 난이도에 큰 변동성이 있다는 것을 나타낸다. 특히, 수학적 귀납 작업에서는 올바른 가정과 잘못된 가정을 가진 예제들이 있어, 모델들이 가정의 정확성에 대한 문제를 해결하는데 어려움을 겪는 것으로 보인다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure6.png"
width="1268"
height="490"
srcset="https://kurtkim.github.io/p/palm/images/figure6_hu8e7975e3396d9807f159e8f50bd39708_72597_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure6_hu8e7975e3396d9807f159e8f50bd39708_72597_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="258"
data-flex-basis="621px"
>&lt;/p>
&lt;p>PaLM 540B가 전반적으로 인간 평가의 평균 성능을 능가하지만, 개별 작업의 35%에서는 인간의 평균 성능이 더 높다는 것을 보여준다. 이는 BIG-bench에서 아직도 상당한 개선 여지가 있다는 것을 의미한다.&lt;/p>
&lt;p>PaLM 540B는 여러 언어의 표현 및 큰 양의 정보를 기억하는 능력 등을 통해 인간의 평균 성능을 능가하는 일부 작업에서 뛰어난 성과를 보여준다. 그 중에는 원인과 결과를 판단하는 작업도 포함되어 있다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>cause and eﬀect (one sentence no prompt)&lt;/strong> 하나의 문장으로 된 서브태스크에서, 이벤트들은 두 가지 다른 순서로 하나의 문장으로 결합되며, 각 문장의 log-likelihood는 모델로 점수화된다. 프롬프트는 제공되지 않는다.
Input A: I washed the car because my car got dirty.
Input B: My car got dirty because I washed the car.
Higher-Likelihood Sentence: I washed the car because my car got dirty.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>cause and eﬀect (two sentence)&lt;/strong> 두 문장의 서브태스크에서는, 모델에게 두 가지 이벤트가 보여지고, 어떤 문장이 다른 이벤트를 일으킨 원인에 해당하는지 선택해야 한다.
Input: For each example, two events are given. Which event caused the other? (a) My car got dirty. (b) I washed the car.
Correct Prediction: (a) My car got dirty.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>모든 PaLM 모델이 한 문장 프롬프트 없는 작업에서 잘 수행되었고, 특히 8B 모델은 80% 이상의 정확도를 보여주었다. 그러나 두 문장 버전의 작업에서는 작은 모델의 성능이 떨어졌다. 대신 540B 모델은 이 작업에서 90% 이상의 높은 정확도를 보여, 대규모 모델이 언어 모델링 능력을 향상시킬 수 있음을 입증하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure7.png"
width="1262"
height="418"
srcset="https://kurtkim.github.io/p/palm/images/figure7_hubd021db6bae7cc2617562c762d44270e_60551_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure7_hubd021db6bae7cc2617562c762d44270e_60551_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="301"
data-flex-basis="724px"
>&lt;/p>
&lt;p>24개의 BIG-bench 작업 중 가벼운 평가 대상인 BIG-bench Lite의 상세 평가 결과를 보여준다. 일부 작업들은 해결되었거나 거의 해결된 상태이지만, 인간 평가의 최고 성능 점수에 비해 다른 일부 작업은 아직 해결되지 않았다.&lt;/p>
&lt;p>BIG-bench 데이터를 모델이 암기하여 성과를 달성한 것이 아닌지 확인하기 위해 여러 단계를 거쳤다. BIG-bench 작업 파일의 고유한 canary 문자열이 PaLM 학습 데이터에 없음을 확인했고, BIG-bench 데이터셋은 학습 데이터 수집 시점에 인터넷에 없었다. 대부분의 BIG-bench 작업들은 새로운 벤치마크이며, 모델의 우수한 성능을 보인 작업들을 임의로 점검하여 정보 유출이 없음을 확인하였다.&lt;/p>
&lt;h3 id="reasoning">Reasoning&lt;/h3>
&lt;p>PaLM은 여러 단계의 산술이나 상식적인 논리적 추론을 필요로 하는 추론 작업에서 평가된다. 언어 모델은 다양한 작업을 수행할 수 있지만, 여러 단계의 추론을 필요로 하는 작업을 수행하는 데에는 어려움이 있다. 이 작업에서는 두 가지 주요 추론 벤치마크 카테고리를 평가한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Arithmetic reasoning&lt;/strong> 이 작업들은 대부분 초등학교 수준의 자연어 수학 문제를 포함하며, 여러 단계의 논리적 추론이 필요하다. 수학은 대체로 간단하며, 어려운 부분은 자연어를 수학식으로 변환하는 것이다. 이 연구에서는 모델 자체가 수학을 수행하는 계산기 형태와 직접 추론 형태를 모두 평가했하였다.
Input: Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
Answer: The answer is 11.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Commonsense reasoning&lt;/strong> 이 작업들은 강한 세계 지식을 필요로 하는 질문 응답 작업이며, 세계에 대한 여러 논리적 추론을 연결하는 것을 필요로 한다. 이는 단순히 사실에 기반한 질문 응답이 아니다.
Input: Q: Sean was in a rush to get home, but the light turned yellow and he was forced to do what?
Answer Choices: (a) take time (b) dawdle (c) go slowly (d) ocean (e) slow down Answer: The answer is (e) slow down.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure8.png"
width="1150"
height="496"
srcset="https://kurtkim.github.io/p/palm/images/figure8_hue021553084c21e4b466167a44884c546_155083_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure8_hue021553084c21e4b466167a44884c546_155083_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="556px"
>&lt;/p>
&lt;p>최근 연구들은 대형 언어 모델이 최종 답변을 생성하기 전에 중간 추론 단계를 생성하면 정확도가 크게 향상될 수 있음을 보여주었다. 이 기술을 &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅이라고 부릅니다. few-shot 설정에서, 중간 추론 단계는 수동으로 작성되고, 모델은 테스트 예시에 대한 자신의 &amp;ldquo;chain-of-thought&amp;quot;을 생성한다. 생성된 &amp;ldquo;chain-of-thought&amp;quot;은 오류 분석과 모델 해석에 유용할 수 있지만, 평가에는 최종 답변만 사용된다.&lt;/p>
&lt;h4 id="results">Results&lt;/h4>
&lt;p>이 연구에서는 모델 규모와 &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅만으로도 다양한 산술 및 상식 추론 작업에서 최첨단의 정확도를 달성할 수 있음을 보여준다. 이전의 많은 연구들은 도메인 특정 아키텍처, 작업 특정 미세조정, 작업 특정 검증자를 결합했지만, 이 연구에서는 단순히 few-shot 프롬프팅을 통해 작업들을 표현했다. 산술 추론 데이터셋의 경우, 사후 외부 계산기를 사용해 모델 예측을 보강했지만, 이는 어떤 데이터셋에서도 성능을 5% 이상 향상시키지 않았다.&lt;/p>
&lt;p>&amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅을 사용해, PaLM의 성능을 산술 데이터셋인 GSM8K, SVAMP, MAWPS, AQuA와 상식 추론 데이터셋인 CommonsenseQA와 StrategyQA에서 평가하였다. 이 프롬프팅 설정은 오직 8-shot 예시만을 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table10.png"
width="800"
height="254"
srcset="https://kurtkim.github.io/p/palm/images/table10_hubb89b4be08e9d85f51629839e28e57bb_54803_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table10_hubb89b4be08e9d85f51629839e28e57bb_54803_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="755px"
>&lt;/p>
&lt;p>GSM8K에서 PaLM의 결과를 강조하며, 이전 state-of-the-art인 Cobbe et al. (2021)이 모델 미세조정, &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅, 외부 계산기, 작업 특정 검증자를 사용하였다. 외부 계산기와 결합된 8-shot &amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅을 사용한 PaLM 540B는 58%의 성능을 달성해 이전 state-of-the-art인 55%를 능가하였다. 이는 &amp;ldquo;chain-of-thought&amp;rdquo; 없는 PaLM 540B와 &amp;ldquo;chain-of-thought&amp;quot;이 있는 PaLM 62B를 크게 능가하였다. PaLM 62B 모델이 잘못 처리한 문제들은 대체로 의미 이해, 한 단계 누락, 그리고 다른 오류들에 속하며, 540B 모델 크기로 확장하면 이러한 오류들의 대부분이 수정되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure10.png"
width="1264"
height="532"
srcset="https://kurtkim.github.io/p/palm/images/figure10_huea06ca2898ddb563d235af1fa303cd49_171272_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure10_huea06ca2898ddb563d235af1fa303cd49_171272_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="570px"
>&lt;/p>
&lt;p>7개의 추론 데이터셋에서, PaLM 540B+&amp;ldquo;chain-of-thought&amp;quot;을 이용한 8-shot 예측은 4개의 작업에서 최고의 정확도를 달성하였으며, 나머지 3개의 작업에서는 state-of-the-art에 근접한 결과를 보여주었다. GSM8K에는 중간 추론 단계가 포함되었지만 다른 벤치마크에는 포함되지 않았다. state-of-the-art과 모델 확장이 모든 작업에서 크게 도움이 되었으며, 두 기술 없이는 PaLM이 한 가지 작업에서만 최고 수준을 달성했을 것이다. 데이터 오염이 없었음을 n-gram 겹침 분석을 통해 확인하였다.&lt;/p>
&lt;h3 id="code-tasks">Code Tasks&lt;/h3>
&lt;p>최근 연구에서 대형 언어 모델이 경쟁 프로그래밍, 코드 완성, 자연어 명세에서 프로그램 합성 등의 코딩 작업에 유용함이 보여졌다. 이번 섹션에서는 PaLM 모델이 다양한 코딩 작업에서 뛰어난 결과를 달성하는 것을 보여준다.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Text-to-code.&lt;/strong> 자연어 설명이 주어진 상태에서 코드를 작성하는 세 가지 작업을 고려한다. HumanEval과 MBPP 데이터셋에서는, 모델에게 몇 문장의 영어 설명과 소량의 입력-출력 예시가 주어지며, 주로 단일 함수인 짧은 파이썬 프로그램을 생성하는 것이 목표이다. 또한, GSM8K 데이터셋에서 파생된 GSM8K-Python 작업을 소개한다. 이 작업에서는 올바른 답을 제공하는 대신 올바른 해결책을 반환하는 파이썬 프로그램을 생성하는 것이 목표이다. 데이터셋의 문제 중 네 개를 few-shot 예시로 사용하기 위해 수동으로 파이썬 프로그램으로 변환하였다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Code-to-code.&lt;/strong> TransCoder는 C++ 프로그램을 파이썬으로 번역하는 작업이다. 데이터셋에서 Python과 C++ 모두에 나타나는 함수들을 수집하고, 이 중 세 가지 다른 유형의 함수를 few-shot 프롬프트로 사용하며, 나머지는 테스트 세트를 만드는 데 사용하였다. 또한, 컴파일에 실패하는 C 프로그램을 성공적으로 컴파일할 수 있도록 수정하는 DeepFix 코드 수리 작업에서도 평가하였다. 결함 있는 코드에 대한 컴파일러 오류를 모델에 제공하고, 1260개의 프로그램에 대해 테스트하였다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>pass@k 메트릭을 사용해 결과를 보고하며, 이는 모델이 $k$개의 코드 샘플을 제공하고 그 중 하나라도 문제를 해결하면 문제가 해결된 것으로 간주한다. 간단히 문제를 해결하는 샘플의 비율을 보고하며, 이를 측정하기 위해 MBPP와 GSM8K의 테스트 데이터를 사용한다. 1개의 샘플일 경우 greedy decoding을, 그 이상일 경우 nucleus sampling을 사용한다.&lt;/p>
&lt;p>PaLM 모델을 LaMDA 137B 파라미터 모델과 초기 Codex 모델 12B와 비교한다. LaMDA는 GitHub의 코드에 대해 학습되지 않았지만, 코드 관련 웹 문서를 일부 포함하여 프로그램 합성 능력을 가지며, Codex 모델은 HumanEval 데이터셋에서의 결과만을 보고한다.&lt;/p>
&lt;p>다른 데이터셋에서 Codex 결과를 얻기 위해, OpenAI Davinci Codex API를 사용했다. 이는 2021년 9월 1일부터 2022년 3월 10일까지 진행되었고, 가장 최신 버전인 Davinci 모델 버전 1을 사용했다. Davinci Codex 모델에 대한 많은 정보는 공개되지 않아 성능 차이의 원인을 이해하는 것은 어렵지만, 이 비교는 고려하는 작업의 본질적인 어려움을 이해하는 데 유용하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table12.png"
width="1074"
height="404"
srcset="https://kurtkim.github.io/p/palm/images/table12_hu85d3a19441c4936a062e570c3986b2d2_95505_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table12_hu85d3a19441c4936a062e570c3986b2d2_95505_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="265"
data-flex-basis="638px"
>&lt;/p>
&lt;p>&lt;strong>Datasets&lt;/strong> PaLM 모델은 학습 세트에 GitHub 코드를 포함하며, 총 39B 개의 코드 토큰이 사전 학습 데이터셋에 있다. Python 프로그래밍을 테스트하는 평가를 위해, ExtraPythonData라는 추가 데이터셋을 수집했고, 이는 사전 학습에 사용되지 않은 GitHub에서 5.8B 개의 토큰을 수집한 것이다. 이 데이터는 Java, HTML, Javascript, Python, C, PHP, C#, C++ 등의 언어를 포함하고 있다.&lt;/p>
&lt;p>&lt;strong>PaLM 540B&lt;/strong> PaLM 모델은 모든 작업에서 LaMDA보다 높은 성능을 보여주며, HumanEval에서는 Codex 12B와 비슷한 수준이다. 이는 동일한 모델이 코드와 자연어 작업 모두에서 뛰어난 성능을 보여주는 첫 번째 큰 언어 모델이라는 점에서 중요하다. PaLM은 Python 코드 토큰 약 2.7B 개로 학습되었는데, 이는 Codex 모델의 Python 토큰 1000억 개에 비해 50배 적다. 그럼에도 불구하고 PaLM은 비슷한 성능을 보여주어, 다른 프로그래밍 언어와 자연어 데이터로부터의 전이와 큰 모델이 작은 모델보다 효율적일 수 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table11.png"
width="700"
height="226"
srcset="https://kurtkim.github.io/p/palm/images/table11_hu2faab6ab491b7073a9ff2c2dc15711d8_32278_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table11_hu2faab6ab491b7073a9ff2c2dc15711d8_32278_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="743px"
>&lt;/p>
&lt;p>&lt;strong>PaLM-Coder&lt;/strong> PaLM 모델을 Python 코드와 다양한 언어의 코드, 그리고 자연어에 대해 미세 조정한 결과, PaLM-Coder 540B의 성능이 크게 향상되었다. 이는 미세 조정을 하지 않은 모델에 비해 HumanEval에서 +12%, MBPP에서 +5%의 절대적인 성능 향상을 보여주었다. 또한, 모델의 규모가 증가함에 따라 성능이 계속 향상되는 것을 확인하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure12.png"
width="1214"
height="528"
srcset="https://kurtkim.github.io/p/palm/images/figure12_hu37ce5c724d96c0b24bd0ad2b4311a1f6_174562_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure12_hu37ce5c724d96c0b24bd0ad2b4311a1f6_174562_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>GSM8K-Python 데이터셋에 대해, PaLM-Coder 540B는 8-shot 프롬프트에서 pass@1 점수 57.5를 얻었고, 반면에 PaLM 540B 모델은 pass@1 점수 58.1을 달성하였다.&lt;/p>
&lt;p>&lt;strong>DeepFix Code Repair&lt;/strong> PaLM-Coder 540B 모델은 DeepFix 코드 수정 작업에서 82.1%의 컴파일률을 달성하여 뛰어난 성능을 보여주었다. 이는 이전 작업에서 달성한 71.7%보다 높은 결과이다. 프롬프트는 다양한 일반적인 오류를 포함한 두 쌍의 깨진 및 수정된 C 프로그램을 손으로 작성하였으며, 이후 모델이 수정된 전체 코드를 예측하게 하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table13.png"
width="1174"
height="328"
srcset="https://kurtkim.github.io/p/palm/images/table13_hu42e80d5d41f68f17a097f82c44f9348b_98064_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table13_hu42e80d5d41f68f17a097f82c44f9348b_98064_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="357"
data-flex-basis="859px"
>&lt;/p>
&lt;p>코드 수정에서는 이상적으로 깨진 코드의 작은 부분만 수정하고 싶기 때문에 모델이 변경한 코드의 양을 평가하는 것이 중요하다. PaLM은 가장 작은 편집을 생성하는 반면, PaLM-Coder는 작은 정규화된 편집 거리를 가진 편집에 대해 가장 높은 성공률을 보여주었다. 반면에 Davinci Codex는 변경된 라인 수가 적은 편집에서 가장 높은 성공률을 보였다. 이는 PaLM-Coder가 더 많은 라인에 대해 적은 수의 문자를 변경하는 경향이 있음을 의미한다.&lt;/p>
&lt;p>&lt;strong>Discussion&lt;/strong> 소프트웨어 개발에서 언어 모델 기반 시스템을 사용할 때, 생성된 코드가 잘못되거나 미묘한 버그를 도입할 위험이 있다. 개발자들은 제안된 코드를 프로그램에 추가하기 전에 검토해야 하지만, 항상 미묘한 버그를 찾을 수는 없다. 코드 제안은 테스트 스위트로 확인할 수 있지만, 소수의 테스트 케이스로부터 솔루션이 기능적으로 올바르다는 것을 추론하는 것은 항상 안전하지 않다. 이에 따라, 기능적 정확성에 대한 더 철저한 테스트가 필요하다.&lt;/p>
&lt;p>기능적 정확성은 소스 코드 품질의 한 가지 측면일 뿐이며, 언어 모델이 생성한 코드 제안은 읽기 쉽고, 견고하고, 빠르고, 안전해야 한다. DeepFix는 PaLM-Coder의 현재 예측과 관련된 문제를 보여주는데, 수정된 프로그램은파일되지만 입력의 형식과 크기에 대한 가정에 의존하기 때문에 반드시 안전한 것은 아니다. 이러한 제안은 더 일반적인 상황에서는 원치 않을 수 있다. 개발자가 제안된 코드를 이해하고 신뢰하는 것은 여전히 해결되지 않은 문제이며, 가독성과 보안성을 평가하는 이전의 연구가 있지만, 이 분야는 아직 초기 단계에 있다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>기계 번역은 텍스트를 한 언어에서 다른 언어로 변환하는 작업이다. GPT-3 같은 거대 언어 모델들은 병렬 텍스트에 대해 명시적으로 학습받지 않았음에도 불구하고 번역 능력을 보여주었다. 이번 섹션에서는 다양한 언어 쌍에 대해 PaLM의 번역 능력을 평가하며, 이 과정에서 WMT에서 제공하는 언어 쌍을 주로 사용할 예정이다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>English-centric language pairs&lt;/strong> 이전 모델들이 주로 다루었던 전통적인 언어 쌍은 영어를 포함하고, 병렬 데이터의 양에 따라 고자원, 중자원, 저자원으로 구분한다. 이번 분석에서는 WMT'14의 영어-프랑스어(고자원), WMT'16의 영어-독일어(중자원), 그리고 WMT'16의 영어-루마니아어(저자원)를 언어 쌍으로 사용한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Direct language pairs&lt;/strong> 번역 시스템이 영어를 거치지 않고 어떤 언어 쌍이든 직접 번역하는 능력이 점점 중요해지고 있다. 이를 테스트하기 위해, 프랑스어와 독일어 사이의 직접 번역 능력을 WMT'19 데이터를 사용해 확인한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Extremely-low resource language pairs&lt;/strong> 모든 언어 쌍은 병렬 데이터가 없어 제로-리소스 상태이다. 그러나 단일 언어 데이터가 적은 언어, 예를 들어 이 연구에서 선택한 카자흐스탄어는 흥미로운 점이 있다. 프랑스어와 독일어는 각각 240억, 260억의 토큰을 가지고 있는 반면, 카자흐스탄어는 1.34억 토큰만 가지고 있다. 이를 평가하기 위해 WMT'19의 영어-카자흐스탄어를 사용하였다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table14.png"
width="928"
height="330"
srcset="https://kurtkim.github.io/p/palm/images/table14_huadf9b0c805cd9cbcff3b8cc9aa8b69db_70951_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table14_huadf9b0c805cd9cbcff3b8cc9aa8b69db_70951_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="674px"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Evaluation on English-centric language pairs&lt;/strong> 전통적인 영어 중심 언어 쌍에서 0-shot, 1-shot, few-shot 설정에서 PaLM을 평가하였다. 이 모델은 GPT-3와 FLAN과 같은 다른 모델들을 능가하며, 때때로 최대 13 BLEU 점수 차이를 보여주었다. 독일어-영어와 루마니아어-영어에서는 감독된 기준선을 능가했지만, 이 기준들이 최근 변경된 WMT 작업에 따라 오래되었을 수 있음을 인정한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure15.png"
width="1298"
height="706"
srcset="https://kurtkim.github.io/p/palm/images/figure15_hu4d6f9fa9e4694490d447df1b45fe040a_205909_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure15_hu4d6f9fa9e4694490d447df1b45fe040a_205909_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="441px"
>&lt;/p>
&lt;p>모델 크기를 8B에서 62B, 그리고 540B로 확대하면서 0-shot 번역의 결과에서 급격한 BLEU 점수 상승이 관찰되었다. 특히, 영어-독일어는 13 BLEU, 영어-프랑스어는 17 BLEU 증가를 보였습니다. 이는 &amp;ldquo;power law&amp;rdquo; 법칙에 따르지 않는 현상이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table15.png"
width="686"
height="270"
srcset="https://kurtkim.github.io/p/palm/images/table15_huf0a4711c70b039caa45845db4276e543_39524_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table15_huf0a4711c70b039caa45845db4276e543_39524_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="609px"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Evaluation on direct and extremely-low resource language pairs&lt;/strong> PaLM은 직접적이고 극도로 저자원 언어 쌍에서의 성능을 평가하였다. WMT'19에서 가장 높은 점수를 받은 제출물을 활용하였다. 이 도전적인 상황에서 PaLM은 프랑스어-독일어에서만 지도 성능을 맞출 수 있었지만, 독일어-프랑스어와 카자흐스탄어-영어에서는 강력한 성능을 보여주었다.&lt;/li>
&lt;/ul>
&lt;h4 id="further-ﬁndings-and-analysis">Further ﬁndings and analysis&lt;/h4>
&lt;p>결과는 다음과 같은 관찰로 정리할 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Translation quality is better when translating into English rather than out of English.&lt;/strong> 모든 영어 중심 언어 모델에서 관찰되는 공통적인 패턴이며, PaLM의 성능을 살펴보면서 비슷하게 나타난다. 다국어 데이터를 우선시하면 이 효과가 완화될 것으로 추정한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompts can deliver even more value than a single example.&lt;/strong> 대부분의 경우, 언어 이름을 사용하여 번역을 유도하는 0-shot 설정이 입력-출력 예시만을 사용하는 1-shot 및 few-shot 설정보다 더 높은 성능을 보여주었다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Generalist models relying solely on self-supervision can match specialized models at smaller scales.&lt;/strong> 대부분의 전용 번역 기준은 parameter가 1B개 미만으로, 가장 큰 PaLM 설정보다 두 자릿수가 작다. 그러나, 대형 번역 모델이 다양한 작업에 적응할 수 있음을 확인했으므로, specialist도 generalist로 활용될 수 있다. 이로 인해, 자원이 풍부한 상황에서는 specialist를 학습시킬지, 아니면 generalist를 학습시킬지에 대한 질문이 제기된다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="multilingual-natural-language-generation">Multilingual Natural Language Generation&lt;/h3>
&lt;p>자연어 생성은 텍스트나 비언어적 정보를 입력으로 받아 이해하기 쉬운 텍스트를 자동 생성하는 과제이다. 그러나 과거에는 비슷한 크기의 모델들에 대해 few-shot 조건부 자연어 생성에 대한 탐구가 없었다. 대형 언어 모델들(GPT-3, GLaM, Gopher, LaMDA, Megatron-Turing NLG) 중 어느 것도 이런 과제에 대한 결과를 보고하지 않았다.&lt;/p>
&lt;p>이 연구는 few-shot 모델링을 위한 첫 번째 대형 언어 모델 벤치마크를 제시하며, 비교 대상으로 LaMDA 137B를 사용하였다. 이 모델은 이전 연구에서 벤치마크 결과를 보고하지 않았지만 테스트는 할 수 있었다.&lt;/p>
&lt;p>미세 조정을 위한 이전 최고 성능은 주로 T5, mT5, BART 등의 encoder-decoder 모델에서 나왔다. 이들 모델은 PaLM보다 작지만, 채우기를 위해 학습된 모델들은 종종 더 큰 decoder-only 언어 모델을 능가한다. 따라서, 이 연구에서는 대규모 모델이 decoder-only 언어 모델의 약점을 보완할 수 있는지를 중요하게 비교하고 있다.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong> 우리는 PaLM을 GEM 벤치마크의 여섯 가지 작업(세 가지 요약, 세 가지 데이터-텍스트 생성)으로 평가하였다. 이는 체코어, 영어, 독일어, 러시아어, 스페인어, 터키어, 베트남어 등의 언어를 포함한 데이터셋을 사용하였다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>MLSum&lt;/strong> 다중 문장으로 뉴스 기사를 요약 [독일어/스페인어]&lt;/li>
&lt;li>&lt;strong>WikiLingua&lt;/strong> WikiHow의 단계별 지시사항을 매우 간결한 문장으로 요약 [영어/스페인어/러시아어/터키어/베트남어 → 영어]&lt;/li>
&lt;li>&lt;strong>XSum&lt;/strong> 한 문장으로 뉴스 기사를 요약 [영어]&lt;/li>
&lt;li>&lt;strong>Clean E2E NLG&lt;/strong> 주어진 키-값 속성 쌍을 바탕으로, 레스토랑을 한 두 문장으로 설명 [영어]&lt;/li>
&lt;li>&lt;strong>Czech Restaurant response generation&lt;/strong> 대화 맥락과 대화 행동 표현을 바탕으로, 스마트 어시스턴트가 제공할 응답 생성 [체코어]&lt;/li>
&lt;li>&lt;strong>WebNLG 2020&lt;/strong> 주어-동사-목적어 삼중체를 문법적이고 자연스럽게 한 문장 이상으로 표현 [영어/러시아어]&lt;/li>
&lt;/ul>
&lt;p>모델의 추론 시간을 줄이기 위해, 테스트 세트가 5,000개를 초과하면 균일하게 샘플링한다.&lt;/p>
&lt;p>&lt;strong>Metrics&lt;/strong> Gehrmann et al. 의 제안에 따라 ROUGE-2, ROUGE-L, BLEURT-20 결과를 보고하며, 이 섹션의 본문은 ROUGE-2의 F-측정에 초점을 맞춘다.&lt;/p>
&lt;p>&lt;strong>Few-shot evaluation methodology&lt;/strong> PaLM은 few-shot 추론에 사용되며, 작업 특정 프롬프트를 입력에 연결하고 출력 프롬프트를 출력에 추가한다. 요약을 위한 긴 입력은 2048 토큰으로 줄이고, few-shot 예시들은 두 줄의 공백으로 분리한다. 모든 few-shot 예시들은 훈련 데이터에서 무작위로 추출된다.&lt;/p>
&lt;p>&lt;strong>Finetuning methodology&lt;/strong> 미세조정 시, decoder만 사용하며, 입력과 목표를 연결하지만, 손실은 목표 부분에서만 계산한다. 연결된 시퀀스는 2048 토큰으로 잘라내고, 목표를 위해 512 토큰을 예약한다. 이 과정은 요약 작업에서만 필요하다.&lt;/p>
&lt;p>PaLM 미세조정은 $5×10^-5$ 의 learning rate와 optimizer 리셋을 사용하며, 검증 세트에서 가장 좋은 ROUGE 점수를 보인 모델을 선택한다. 추론은 $k = 10$의 top-k 샘플링으로 수행되고, T5 XXL 기준선은 PaLM과 동일한 parameter로 미세조정하며, beam size 4의 beam-search를 사용해 디코딩한다.&lt;/p>
&lt;h4 id="results-1">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table16.png"
width="1204"
height="608"
srcset="https://kurtkim.github.io/p/palm/images/table16_hu5ef0d5c12b11ec602efe11f8b7ed8764_143466_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table16_hu5ef0d5c12b11ec602efe11f8b7ed8764_143466_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="475px"
>&lt;/p>
&lt;p>1-shot과 미세조정의 비교는 ROUGE-2의 F-측정을 사용한다.&lt;/p>
&lt;p>이 연구는 few-shot 모델링에 초점을 맞추고 있고, 이러한 작업에 대한 공개된 few-shot 결과는 없지만, 이 결과들로부터 몇 가지 흥미로운 교훈을 얻을 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Eﬀectiveness of ﬁnetuning&lt;/strong> 요약 작업에서, 미세조정된 540B PaLM은 모든 영어 생성 작업에서 최상의 성과를 보여주며, 이는 그것의 대규모 스케일을 통해 아키텍처적 단점을 극복할 수 있다는 것을 보여준다. 62B 버전도 최상의 결과에 가깝고, 540B는 그것을 초과한다. decoder 전용 LM의 미세조정이 작업 특정 훈련 데이터가 많을 때 모든 작업에 대한 최적의 접근법이 아닐 수 있다는 것을 인지하고 있지만, 이것이 few-shot 예측에 대한 중요한 상한선 역할을 한다고 믿는다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Generation quality of English vs. non-English&lt;/strong> PaLM은 6개의 요약 작업 중 4개에서 새로운 미세조정 state-of-the art를 달성하였다. 그러나 비영어 요약에서는 최고 기록에 못 미치며, 비영어 생성에서 few-shot과 미세조정 사이의 차이는 더 크다. 이는 PaLM이 비영어 입력 처리에는 능하지만 비영어 출력 생성에는 덜 능하다는 것을 보여주며, 이는 향후 비영어 텍스트의 큰 부분에 대한 사전 학습을 통해 개선될 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>1-shot vs. ﬁnetuning gap&lt;/strong> 데이터-텍스트 결과에서, few-shot 결과는 요약과 비슷한 추세를 보이지만, 최상의 미세조정 결과와의 차이는 크게 줄어든다. FLAN은 instruction tuning 후 E2E-NLG에서 33.2, WebNLG에서 48.0의 점수를 보고하는 반면, PaLM은 어떠한 튜닝 없이 35.2와 44.4를 얻었다. 그러나 데이터-텍스트 작업은 그 크기가 작고 사전 학습 말뭉치와 크게 다르기 때문에, 미세조정 벤치마크로서의 가치가 제한적일 수 있다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Few-shot summarization&lt;/strong> 다양한 PaLM 규모에서의 few-shot 요약 결과를 비교하면, 8B에서 62B로, 그리고 62B에서 540B로 크게 향상되는 것을 볼 수 있다. 그러나, few-shot과 미세조정 사이의 차이는 아직도 크며, 1-shot 성능은 비영어 작업의 T5-base나 T5-large, 영어 작업의 T5-small와 같은 작은 미세조정 모델과 비슷하다. 이는 큰 언어 모델로의 few-shot 요약 첫 시도이므로, 조건부 생성 작업에 대한 few-shot과 미세조정 모델 사이의 간극을 좁히는 데 중요한 시작점이 될 것이라 믿는다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="multilingual-question-answering">Multilingual Question Answering&lt;/h3>
&lt;p>TyDiQA-GoldP 벤치마크를 사용해 다국어 질문 응답에 대한 모델을 few-shot 설정과 미세조정 설정에서 평가했다. few-shot 설정에서는 문맥, 질문, 답변을 새 줄 문자로 구분하고, &amp;ldquo;Q:&amp;ldquo;와 &amp;ldquo;A:&amp;ldquo;로 각각 질문과 답변을 표시했다. 미세조정에서는 영어 SuperGLUE 미세조정 실험과 동일한 hyperparameter를 사용했으며, 가장 좋은 전체 체크포인트에서의 결과를 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table7.png"
width="1228"
height="180"
srcset="https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table7_hu2cd3577f17b9afbf4b3d610842edee6c_43341_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="682"
data-flex-basis="1637px"
>&lt;/p>
&lt;p>few-shot과 미세조정 품질 사이에는 평균적으로 큰 차이가 있다는 것을 보여준다. 그러나 스와힐리어와 핀란드어 같은 특정 언어들에서는 이 차이가 적다. 프롬프트 엔지니어링과 다국어 데이터셋에 대한 다작업 적응 연구가 few-shot 결과를 더욱 개선할 수 있을 것으로 보인다.&lt;/p>
&lt;p>PaLM 540B는 비영어 데이터의 학습 비율이 적음에도 불구하고 이 작업에서 매우 경쟁력 있는 결과를 보여준다. mT5와 ByT5는 비영어 텍스트에 대해 PaLM의 6배와 1.5배 만큼 학습되었음에도 불구하고, PaLM 540B는 mT5 XXL을 능가하고 ByT5 XXL에게는 능가당하였다. 이러한 결과는 사전 학습 데이터셋에서 비영어 데이터 비율을 늘리거나, 구조적 단점이나 귀납적 편향을 극복하는 방법을 통해 더욱 개선될 수 있을 것으로 보인다.&lt;/p>
&lt;h3 id="analysis">Analysis&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure16.png"
width="1268"
height="500"
srcset="https://kurtkim.github.io/p/palm/images/figure16_hu30ac0e0e215b045dcc508a472f8d2be3_223216_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure16_hu30ac0e0e215b045dcc508a472f8d2be3_223216_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>PaLM 모델의 few-shot 성능에 대한 분석을 제시한다. 세 가지 모델(8B, 62B, 540B)을 다섯 가지 다른 작업(RTE, Natural Questions, Lambada, Story Cloze, Trivia QA)에서 연구하였으며, 이들 작업은 지식 중심에서 추론 중심까지 다양하다. Trivia QA와 Natural Questions은 문맥 문서 없이 질문만을 입력으로 제공되는 &amp;ldquo;closed book&amp;rdquo; 방식이다.&lt;/p>
&lt;p>0-shot, 1-shot, 5-shot, 8-shot 학습을 평가하여 모델에 더 많은 예제가 제공될수록 대부분의 작업과 모델에서 성능이 향상되는 것을 확인하였다. 하지만 Trivia QA 작업에서는 1-shot 학습이 모든 모델 크기에서 5-shot 및 8-shot 학습을 능가하는 예외적인 결과를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure17.png"
width="1124"
height="314"
srcset="https://kurtkim.github.io/p/palm/images/figure17_hua7a3805379910bdae83a5243740e90f9_119000_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure17_hua7a3805379910bdae83a5243740e90f9_119000_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="357"
data-flex-basis="859px"
>&lt;/p>
&lt;p>이 연구에서는 다양한 모델 체크포인트에서의 few-shot 학습 성능을 분석했다. 대부분의 작업에서 체크포인트 간 성능에 큰 차이를 보이지 않았지만, Web Questions 작업에서는 체크포인트 간에 큰 성능 변동을 보였다. 가장 높은 성능을 보인 PaLM 540B는 학습 토큰 7700억 개의 체크포인트에서 최고 결과를 보였지만, 그 이후의 체크포인트에서는 성능이 감소했다. 이 연구의 모든 결과는 동일한 체크포인트에서 평가되었다.&lt;/p>
&lt;hr>
&lt;h2 id="memorization">Memorization&lt;/h2>
&lt;p>신경망이 학습 데이터를 기억하는 것은 overfit의 일종이며, 이는 주로 작은 학습 세트를 여러 번 반복할 때 발생한다. 그러나 PaLM 같은 모델은 780B 토큰의 말뭉치를 한 번만 훑어내려도, 모델의 큰 용량 때문에 학습 데이터의 상당 부분을 기억할 수 있다. 더욱이, 웹에서 추출된 말뭉치에는 중복되는 텍스트가 많이 있어, 학습 과정에서 약간 변형된 구절들이 여러 번 나타날 수 있습니다.&lt;/p>
&lt;p>PaLM 모델이 학습 데이터를 얼마나 잘 기억하고 있는지를 분석한다. 학습 예제에서 무작위로 선택한 100개의 토큰 시퀀스로 모델을 실행하고, 모델이 학습 예제와 정확히 일치하는 50개 토큰을 얼마나 자주 생성하는지 측정한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure18.png"
width="1290"
height="338"
srcset="https://kurtkim.github.io/p/palm/images/figure18_hu379c2c297c002a4e3d032ea44dd1f4cf_104085_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure18_hu379c2c297c002a4e3d032ea44dd1f4cf_104085_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="381"
data-flex-basis="915px"
>&lt;/p>
&lt;p>(a)는 세 가지 크기의 모델이 훈련 데이터를 얼마나 잘 기억하는지를 보여준다. 8B 모델은 1.6%의 데이터, 540B 모델은 2.4%의 데이터를 정확히 재현할 수 있었다. 또한 학습 데이터와 같은 분포에서 추출된 보류 중인 데이터에 대한 기억율도 평가했으며, 이는 일부 보류 중인 예제가 학습 세트 예제와 매우 유사하기 때문에 0% 이상이었다.&lt;/p>
&lt;p>학습 데이터에서 예제가 정확히 몇 번 보였는지에 따른 기억율을 보여주는 (b)에 따르면, 한 번만 본 예는 가장 큰 모델에서 0.75%의 기억을 가지고, 500번 이상 본 예제는 40% 이상의 기억율 보였다. 이는 학습 과정에서 전체 문서에 대해 중복을 제거하고, 100 토큰 범위에서 기억을 평가했기 때문이다.&lt;/p>
&lt;p>(c)는 학습 데이터 말뭉치별로 모델의 기억율을 보여준다. 학습에서 예제의 정확한 중복, 거의 중복, 또는 템플릿화의 양이 가장 큰 영향을 미쳤다. 코드 말뭉치는 표준 라이센스 문자열, 다른 곳에서 복사된 공유 코드 스니펫, 자동 생성된 코드 등을 포함하고 있고, 반면 책 말뭉치는 주로 고유한 텍스트를 포함하고 있다.&lt;/p>
&lt;p>이 결과들로부터, memorization에 대해 다음과 같은 결론을 내릴 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>더 큰 모델이 더 작은 모델보다 높은 기억율을 보이며, 이는 이전 연구의 결과와 일치한다. 기울기와 결정계수($R^2$) 값이 모두 비슷하게 나타났다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>흔한 템플릿과 표준 문구에 대해 모델이 정확히 일치하는 연속을 생성하므로, 일정 수준의 &amp;ldquo;memorization&amp;quot;이 예상된다. 그러나 학습 데이터에 대한 기억율은 보류 중인 데이터보다 상당히 높아, 이는 모델이 실제로 데이터의 일부를 기억한다는 것을 보여준다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>예제가 기억될 확률은 학습에서 그 예제의 독특함과 강하게 연관되어 있다. 한 번만 본 예제는 여러 번 본 예제보다 기억될 가능성이 적다. 이는 이전 연구들과 일치하는 결과이다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>대부분의 기억 사례는 우려를 불러일으키지 않을 것 같은 공식적인 텍스트였으며, 이야기나 뉴스 기사, 사실 등도 기억되었다. 추출 가능한 기억된 내용의 양은 학습 데이터, 모델 크기, 그리고 추출을 수행하는 사람이 학습 데이터를 얼마나 알고 있는지에 따라 달라진다. 하지만 단순히 추출 가능한 학습 데이터의 양을 측정하는 것만으로는 이 기억이 문제가 될 수 있는지에 대한 정보를 얻을 수 없다.&lt;/p>
&lt;p>기억이 문제가 되는지는 데이터셋의 특성과 사용 목적에 따라 다르다. 큰 언어 모델을 사용할 때는 항상 신중해야 한다. 생성 시점의 기억을 방지하는 한 방법은 학습 데이터 위에 블룸 필터를 구현하고, 학습 데이터셋에서 그대로 나온 시퀀스를 생성하지 않게 제한하는 것이다. 하지만 이 방법도 완벽하지 않으며, 최선의 대응 전략은 큰 언어 모델을 언제, 어떻게 사용할지 신중하게 결정하는 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="dataset-contamination">Dataset Contamination&lt;/h2>
&lt;p>이전 연구들은 벤치마크 평가 세트와 학습 데이터 사이에 높은 수준의 데이터 중복률이 있다고 보고했다. 그러나 많은 벤치마크는 웹에서 맥락을 가져와 생성된 질문에 대한 답을 만들도록 요청하는 방식으로 구성되었다. 이러한 작업에 대해 평가 시점에 맥락이 제공되므로, 모델이 이전에 맥락에 대해 학습했더라도 평가 시간에 불공정한 이점은 주지 않는다.&lt;/p>
&lt;p>단순히 고차 n-gram 중복을 찾는 것이 아니라, 29개의 주요 영어 NLP 벤치마크 작업에 대해 통계를 계산하고 각각의 예제를 수동으로 검토하여 오염된 예제의 비율이 높은 것을 파악하였다. 이는 각 데이터셋이 어떻게 구성되었는지를 고려하여 수행되었다.&lt;/p>
&lt;p>29개의 벤치마크 작업을 대략 네 가지 카테고리로 나눌 수 있다:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Wholesale contamination&lt;/strong> 데이터셋의 상당 부분이 오픈 웹에 나타나는 데이터셋이며, 이것들을 오염되었다고 간주한다. 예: SQuADv2, Winograd.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Constructed from web&lt;/strong> 질문+답변(또는 접두사+연속)이 오픈 웹에서 자동으로 추출된 데이터셋으로, 많은 평가 예제가 학습 데이터에 있을 가능성이 높으며, 이것들을 오염되었다고 간주한다. 예: Web Questions, ReCoRD, Lambada.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Context on web&lt;/strong> 맥락은 웹에서 가져왔지만 질문은 그렇지 않은 질문 응답 데이터셋이며, 이것들을 오염되지 않았다고 간주한다. 예: BoolQ, Multirc, ANLI.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>No signiﬁcant overlap&lt;/strong> 학습 데이터와 중복되는 부분이 없는 데이터셋으로, 어떤 대규모 학습 코퍼스에서도 기대할 수 있는 공통 n-gram은 제외한다. 예: StoryCloze, OpenbookQA.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>29개의 벤치마크 세트 중 10개가 첫 두 카테고리에 속한다는 것을 확인하였다. 이들 중 일부만이 학습 데이터에서 발견되었다. 이는 학습 코퍼스가 웹 데이터의 일부만 포함하고 있기 때문이다. 따라서 각 데이터셋을 &amp;ldquo;contaminated&amp;rdquo; 부분과 &amp;ldquo;clean&amp;rdquo; 부분으로 나눌 수 있었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table18.png"
width="1046"
height="480"
srcset="https://kurtkim.github.io/p/palm/images/table18_hu6141fbf5b1fbce561b456895b53c3ea5_102997_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table18_hu6141fbf5b1fbce561b456895b53c3ea5_102997_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="523px"
>&lt;/p>
&lt;p>깨끗한 부분에서 긍정적인 정확도 변화와 부정적인 정확도 변화를 보여주는 세트의 수가 동일함을 확인하였다. 이는 데이터 오염이 결과에 큰 영향을 미치지 않음을 의미한다. 만약 540B 모델이 평가 세트의 대부분을 단순히 암기했다면, 깨끗한 부분에서 8B 모델보다 더 큰 부정적인 변화를 보였을 것이다. 하지만, 8B와 540B 모델은 깨끗한 검증 세트와 전체 검증 세트 사이에 비슷한 수의 부정적인 변화를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table19.png"
width="1122"
height="352"
srcset="https://kurtkim.github.io/p/palm/images/table19_hu5e24c026733eac3a635037bdd59222b5_75005_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table19_hu5e24c026733eac3a635037bdd59222b5_75005_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;p>기계 번역에 대해 분석을 수행했고, 데이터 오염은 발견되지 않았지만, 학습 데이터에서 발생하는 목표 참조 문장이 일부 있었다는 것을 확인하였다. 결과적으로, 학습 데이터와 높은 n-gram 중복을 가진 문장을 제거하여 &amp;ldquo;clean&amp;rdquo; 부분집합을 만들었다. 대부분의 세트에서 깨끗한 세트와 전체 세트 사이의 BLEU 점수는 비슷했으며, 이는 기억력에 의한 차이가 주요 요인이 아니라는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="exploring-explanations">Exploring Explanations&lt;/h2>
&lt;p>&amp;ldquo;chain-of-thought&amp;rdquo; 프롬프팅이 다단계 추론 작업의 예측 정확도를 크게 향상시키는 것을 보여주었다. 이 방법은 모델이 올바른 답을 내는 이유를 규명하는 과학적 관심사, 사용자의 신뢰도 조절, 그리고 설명 자체가 필요한 상황(예: 농담 설명) 등에 유용하게 사용될 수 있다.&lt;/p>
&lt;p>chain-of-thought 프롬프팅을 사용한 PaLM의 설명적 언어 생성 능력을 보여주려 한다. 제시한 예시들은 논리적 추론, 세계 지식, 추상적 언어 이해, 사전적 언어 이해 등을 복합적으로 필요로 힌다. &amp;ldquo;Explaining a Joke&amp;quot;과 &amp;ldquo;Logical Inference&amp;quot;이라는 두 가지 작업을 통해 모델 output을 보여준다. 각 작업에 대해, 원하는 output 스타일을 보여주는 예시들을 작성하였다. 이 예시들은 저자들이 작성하고 선택했지만, 여전히 PaLM의 언어 이해 능력을 획기적으로 보여주는 결과라고 믿는다. 이는 이 분석이 어떻게 수행되었는지에 관한 여러 핵심 요인들 때문이다.&lt;/p>
&lt;ol>
&lt;li>모든 예측은 동일한 2-shot 예시를 통해 생성되며, 이는 평가하는 예시의 내용과는 무관하게 오직 스타일에만 연관이 있다. 게다가, 모든 예시 프롬프트는 예시 평가 이전에 작성되었고, 모델 output의 검토를 바탕으로 수정된 적은 없다.&lt;/li>
&lt;li>모든 output은 temperature sampling이 아닌 greedy decoding으로부터 나온다. 그 이유는 각 output이 exponential space에서 가능한 많은 output 중 하나가 아니라 모델의 표준 1-best 예측이기 때문입니다.&lt;/li>
&lt;li>이 작업들의 목적이 모델에게 철저한 자연어 설명을 생성하도록 유도하는 것이기 때문에, greedy decodin이 단순한 통계적 상관 관계나 &amp;ldquo;lucky guesses&amp;quot;을 통해 완전히 정확한 설명을 생성할 확률은 극히 낮다.&lt;/li>
&lt;li>프롬프트가 저자들에 의해 작성되었기 때문에, 이는 직접적인 데이터 오염과 기억이 주요 요인이 될 가능성을 완화시킨다.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure19.png"
width="1238"
height="1444"
srcset="https://kurtkim.github.io/p/palm/images/figure19_hubcf5d0a6345a739cd8e445105c9963a2_581191_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure19_hubcf5d0a6345a739cd8e445105c9963a2_581191_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="85"
data-flex-basis="205px"
>&lt;/p>
&lt;p>이 섹션에서 가장 큰 가치를 이러한 예시들을 단순히 읽는 것에서 얻을 수 있다고 믿는다. 비록 이 결과들이 철저한 정량적 분석을 의미하지는 않지만, 이것이 심층적인 언어 이해의 정말 놀라운 수준을 보여준다고 말한다.&lt;/p>
&lt;hr>
&lt;h2 id="representational-bias-analysis">Representational Bias Analysis&lt;/h2>
&lt;p>사전 학습된 언어 모델들은 데이터의 편향을 포함하고 확대한다는 것이 입증되었다. 모델의 구조를 공유하는 것의 중요성도 강조되었다. 이 섹션에서는 PaLM이 사회 집단과 관련된 편향과 개방형 언어 생성에서의 toxicity를 분석한다. 이 분석은 모델의 잠재적 위험을 개요화하는 데 도움이 되지만, 가능한 위험을 제대로 조정하고 맥락화하며 완화하기 위해선 도메인 및 작업별 분석이 필수적이다.&lt;/p>
&lt;h3 id="distributional-bias-in-social-groups">Distributional bias in social groups&lt;/h3>
&lt;h4 id="gender-and-occupation-bias">Gender and occupation bias&lt;/h4>
&lt;p>대용어 해결은 언어 시스템의 중요한 능력이다. 영어에서는 대명사가 의미적 성별로 표시되며, 이는 대용어 해결 성능에 영향을 미친다. 우리는 &amp;ldquo;nurse&amp;quot;와 &amp;ldquo;electrician&amp;quot;와 같은 직업 명사의 성별 편향을 측정하는 Winogender 벤치마크를 사용하여 PaLM의 이러한 편향에 대해 평가한다.&lt;/p>
&lt;p>다중 선택 점수화는 Winogender에 대해 일반적으로 사용되며, 각 가능한 답변을 모델이 그 답변을 생성할 확률로 점수화한다. 이 점수화 방법은 올바른 답변을 생성할 모델의 절대 확률이 낮더라도 예시가 올바르게 점수화될 수 있다. 이 방법은 널리 쓰이지만, 특히 0-shot 설정에서의 모델 성능을 과대평가한다는 것을 발견하였다. 540B 모델의 다중 선택 점수화와 생성적 출력의 예시가 있다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/exp10.1.1.png"
width="1078"
height="338"
srcset="https://kurtkim.github.io/p/palm/images/exp10.1.1_hu7bc34adf4cff411851be4b5d83bcc5bd_66932_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/exp10.1.1_hu7bc34adf4cff411851be4b5d83bcc5bd_66932_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;p>0-shot 생성 케이스에서, 모델은 작업을 이해하지 못하고 다중 선택 시험을 흉내 낸다. 생성 점수화에서는 대소문자를 구분하지 않는 정확한 문자열 일치를 사용하며, 모델 output은 문장 부호나 줄 바꿈에서 잘린다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure20.png"
width="712"
height="494"
srcset="https://kurtkim.github.io/p/palm/images/figure20_hu747af002f527b9e6f676207dda56d339_85807_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure20_hu747af002f527b9e6f676207dda56d339_85807_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="345px"
>&lt;/p>
&lt;p>모델 규모가 커질수록 정확도가 향상되며, PaLM 540B는 1-shot과 few-shot 설정에서 최고 state-of-the-art를 달성하였다. 특히, 더 엄격한 생성 점수화 방법을 사용해도 4-shot 설정에서 84.7%의 정확도를 보였다. 하지만 이 성능은 아직 작업에 맞춘 모델이나 인간의 성능보다 낮다.&lt;/p>
&lt;p>Winogender를 고정관념적 또는 &amp;ldquo;gotcha&amp;rdquo; 부분집합으로 나누어 분산 정확도를 보고한다. 고정관념적 주석에서는 성별과 직업이 일치하고, &amp;ldquo;gotcha&amp;rdquo; 주석에서는 반대이다. 성별 중립적인 대명사는 중립 분할의 일부이다. 모든 경우에서, 올바른 예측은 제공된 맥락에서 분명하게 추론될 수 있다. 모델이 통계적 단축에 얼마나 의존하는지를 측정하는 강력한 척도이다. 모든 경우에서, few-shot 예시들은 전체 예시 세트에서 무작위로 샘플링되며, 평가 중인 현재 예시는 제외된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure21.png"
width="1270"
height="302"
srcset="https://kurtkim.github.io/p/palm/images/figure21_hudb77948ac7043909c9cc6ee4536fa94f_107095_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure21_hudb77948ac7043909c9cc6ee4536fa94f_107095_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="420"
data-flex-basis="1009px"
>&lt;/p>
&lt;p>고정관념적 예시에서의 정확도가 &amp;ldquo;gotcha&amp;rdquo; 예시보다 높으며, 여성에 대한 &amp;ldquo;gotcha&amp;rdquo; 예시에서 정확도가 가장 낮다. shot의 수가 증가함에 따라 이러한 분할 간의 성능 차이가 개선되는 것을 볼 수 있다. 성능의 차이는 학습 세트에서 영어 대명사의 빈도 차이와 관련이 있을 수 있지만, 정확도와 직업 순위 사이에는 명확한 관계를 찾지 못하였다.&lt;/p>
&lt;h4 id="toxicity-and-bias">Toxicity and bias&lt;/h4>
&lt;p>모델이 &amp;ldquo;성별, 종교, 인종 및 민족 신분&amp;quot;과 같은 특정 용어를 참조할 때 자주 함께 나타나는 단어를 분석한다. 각 프롬프트에 대해 800개의 출력을 생성하고, 불용어를 제거하고 형용사와 부사만 선택한다. 이 분석은 어떠한 수동 인간 라벨링도 없이 투명하게 이루어진다.&lt;/p>
&lt;p>정체성 그룹을 참조하지 않는 설명적인 단어의 수를 줄이기 위해, 첫 번째 완전한 문장에서만 형용사와 부사의 수를 계산하였다.&lt;/p>
&lt;p>이 방법을 통해 특정 차원, 특히 이슬람에 대한 bias가 더 잘 드러나는 것을 확인하였다. 인종 신분 용어는 서로 함께 나타나는 경향이 있으며, 프롬프트 언어의 작은 변화가 결과에 큰 변화를 가져온다. 예를 들어, &amp;ldquo;The term was&amp;rdquo; 프롬프트를 사용하면 Latinx는 폭력적이거나 공격적인 어조와 함께 많이 등장한다.&lt;/p>
&lt;p>&amp;ldquo;Indian&amp;quot;이 &amp;ldquo;White&amp;quot;와 많이 동시에 나타났다. 이는 &amp;ldquo;White&amp;quot;라는 표현이 백인 정복자를 지칭하는 데 일반적으로 사용되는 미국 기원의 내용에서 비롯된 것으로 보인다. 많은 연속들이 백인과 아메리칸 인디언 사이의 식민지적 역학을 묘사하지만, 이는 사용자가 북미의 식민지화에 대한 설명에 과도하게 제한되지 않는 언어를 생성하길 원할 때 추가 분석이 필요할 수 있다.&lt;/p>
&lt;p>결과를 검토할 때, 정체성 용어가 모호성을 해소하지 않는다는 것을 알아두는 것이 중요하다. 예를 들어 &amp;ldquo;Indian&amp;quot;은 아메리칸 인디언과 인도 출신 사람을 구분하지 않는다. 또한 &amp;ldquo;Black&amp;quot;과 &amp;ldquo;White&amp;quot;는 종종 인종 신분 외의 것을 참조하며, &amp;ldquo;White&amp;quot;는 백인이 설명될 때 일반적으로 사용되지 않아, &amp;ldquo;White&amp;quot;와 함께 나타나는 용어를 비교하는 것이 복잡할 수 있다.&lt;/p>
&lt;p>62B와 540B 모델은 매우 유사한 동시 출현 횟수를 보여주며, 인종, 종교, 성별 차원에서 상위 10개 단어 중 70%가 동일하다. 이로 인해, 학습 데이터가 모델의 크기보다 결과에 더 큰 영향을 미친다고 판단하였다.&lt;/p>
&lt;p>동시 출현 분석은 용어가 어떻게 다른 용어와 관련되어 나타나는지를 파악하는데 중요하다. 정체성 용어가 있는 프롬프트 템플릿을 사용하여 모델 완성의 toxicity를 분석하는 접근법을 사용하였다. 이슬람에 대한 상위 용어로 &amp;ldquo;terrorist&amp;quot;를 확인했고, 이슬람과 무신론을 포함하는 프롬프트에서 더 높은 toxicity 점수를 보여준다. 이를 통해 모델 완성이 무슬림에 대한 부정적인 고정관념을 잘못 확인하는 가능성을 파악할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure22.png"
width="1214"
height="728"
srcset="https://kurtkim.github.io/p/palm/images/figure22_hu8c0f5177ee767f3f5198ddeea975db15_112115_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure22_hu8c0f5177ee767f3f5198ddeea975db15_112115_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="400px"
>&lt;/p>
&lt;p>동시 출현 횟수를 계산하는 것 외에도, 계속되는 내용의 toxicity를 분류하기 위해 Perspective API를 사용하다. 이 API는 텍스트가 무례하거나 불쾌하거나 사람들이 대화를 떠나게 만들 가능성을 측정한다. 모델 응답의 toxicity 확률 분포를 보면, 이슬람과 유대교는 &amp;ldquo;All { practitioners } are&amp;quot;이라는 프롬프트에 이어 toxicity 반응을 생성할 확률이 더 높다. 또한, 특정 무해한 언급에 대해 높은 toxicity를 부여하는 Perspective API의 사회적 bias에 의존하고 있다.&lt;/p>
&lt;p>bias와 toxicity 평가는 모든 언어 모델에 대해 완전히 적용되지는 않지만, 잠재적인 위험에 대한 중요한 통찰력을 제공한다. 결과의 변동성은 프롬프트 언어의 작은 변화에 매우 취약한 템플릿 기반 접근법을 보여주며, bias를 측정하고 완화 전략을 결정하기 위해 견고한 벤치마크와 지표가 필요함을 강조한다.&lt;/p>
&lt;h3 id="toxicity-in-open-ended-generation">Toxicity in open-ended generation&lt;/h3>
&lt;p>Toxicity degeneration는 언어 모델이 toxicity로 인식하는 텍스트를 만드는 것이다. 이를 평가하기 위해, RealToxicityPrompts 데이터셋을 활용하고, Perspective API를 통해 계속되는 내용에 toxicity 확률을 부여하다. 그 후, 프롬프트가 toxicity일 가능성에 따른 모델 응답의 toxicity 확률 분포를 연구하다.&lt;/p>
&lt;p>무작위로 추출한 1만 개의 프롬프트에 대해 각각 25개의 연속문을 생성하였다. 이때는 최대 128개의 디코딩 단계를 사용하였고, top-k 샘플링과 1.0의 온도를 적용하였다. 하지만 여러 디코딩 단계를 사용하더라도, 첫 번째 완전한 문장의 toxicity 지표만을 보고하였다. 이는 인간의 단일장 연속문을 기준으로 하며, 텍스트 길이에 따라 toxicity 점수가 증가하는 경향 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure23.png"
width="1010"
height="716"
srcset="https://kurtkim.github.io/p/palm/images/figure23_hu9507a657f886eacb1117fcb81854c403_139911_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure23_hu9507a657f886eacb1117fcb81854c403_139911_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="338px"
>&lt;/p>
&lt;p>다양한 모델 크기에 따른 toxicity probability of the prompt(TPP) 함수로서의 toxicity probability of the continuation(TPC)을 보여준다. TPC는 TPP와 함께 증가하는 경향이 있지만, 프롬프트의 toxicity나 인간 기준선보다는 일관되게 낮다. 8B 모델과 더 큰 모델들(62B와 540B) 사이에서 toxicity 확률이 증가하였고, 이는 toxicity 수준과 모델 크기 사이에 상관관계가 있음을 시사한다. 모델의 TPC는 인간의 TPC보다 TPP와 더 일관성이 있고, 이는 모델이 프롬프트 스타일에 크게 영향을 받아, 프롬프트와 유사한 toxicity 수준의 연속문을 생성할 가능성이 높다는 것을 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table20.png"
width="646"
height="210"
srcset="https://kurtkim.github.io/p/palm/images/table20_hu89fe36b7efe5b4f9a75bf57640c81d05_30699_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table20_hu89fe36b7efe5b4f9a75bf57640c81d05_30699_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="307"
data-flex-basis="738px"
>&lt;/p>
&lt;p>TPC는 이전 연구보다는 낮지만, 이는 첫 번째 완전한 문장에 toxicity 측정을 제한했기 때문일 뿐, 모델이 toxicity한 내용을 생성하는 경향이 낮다는 것을 의미하지는 않는다. 무작위로 샘플링된 프롬프트와 연속문의 길이 때문에 이전 작업과의 직접적인 비교는 어렵다.&lt;/p>
&lt;h3 id="limitations">Limitations&lt;/h3>
&lt;p>이 섹션의 공정성 분석은 영어 데이터에만 국한되어 있지만, PaLM은 다양한 언어 데이터에 대해 학습되고 평가되기 때문에 주요한 한계를 가지고 있다. 전 세계적으로 사용되는 언어 기술에 대한 편향 벤치마크의 개발과 활용이 중요하며, 서구 세계에서 개발된 공정성 평가는 다른 지역-문화 맥락으로 쉽게 이동할 수 없을 수 있다. 따라서, 현재 측정 가능한 것 이상의 잠재적인 bias가 존재할 수 있음을 인지해야 한다.&lt;/p>
&lt;p>영어 언어 기술의 편향성에 대한 연구가 증가하고 있지만, 공정성 벤치마크의 표준화, NLP의 편향 측정과 관련된 해의 이해, 그리고 포괄적인 방식으로 다양한 정체성을 다루는 것에 대한 표준이 부족하다. 이러한 이유로, 공정성 평가는 한계를 가지고 있으며, 측정 가능한 것 이상의 잠재적 위험이 존재한다. 이 논문의 평가는 대명사 해결과 공존 분석과 같은 인기있는 작업에 제한되어 있으며, 이러한 벤치마크는 번역, 코드 생성, 상식 추론 등의 작업에서의 편향 유형을 대표할 수 있다.&lt;/p>
&lt;p>bias는 구체적인 응용 프로그램, 학습 과정, 그리고 보호 조치에 따라 시스템에 영향을 미칠 수 있다. 모델 사용 방식에 따라 사전 학습된 모델의 bias가 다양한 영향을 미칠 수 있으며, 모델 미세 조정 후의 downstream task 평가에 어떤 영향을 미치는지는 명확하지 않다. 그래서 배포 전에 응용 프로그램에서의 공정성 격차를 평가하기 위한 적절한 조치를 취하는 것이 중요하다.&lt;/p>
&lt;hr>
&lt;h2 id="ethical-considerations">Ethical Considerations&lt;/h2>
&lt;p>대규모 고품질 언어 모델링은 건강관리와 교육 등 실제 세계 응용 프로그램의 가능성을 제공한다.&lt;/p>
&lt;p>최근 연구에서는 웹 텍스트에 대해 학습된 대규모 언어 모델이 사회적 bias를 악화시키거나, 개인 정보를 노출하거나, downstream에서 해를 입힐 수 있다는 여러 잠재적 위험을 지적하였다. 이러한 bias를 완전히 제거하는 것은 불가능할 수 있으므로, 이런 부적절한 연관성과 위험을 분석하고 기록하는 것이 중요하다. 이를 위해, 데이터셋과 모델 출력의 철저한 분석을 수행하고, PaLM의 사용자들에게 더 큰 투명성을 제공하기 위해 데이터시트와 모델 카드를 제공한다.&lt;/p>
&lt;p>학습 데이터와 PaLM 모델이 다양한 사회적 stereotype과 toxicity를 반영하고 있다는 것을 보여준다. 그러나 이러한 연관성을 제거하는 것은 간단하지 않으며, 자동화 도구를 이용해 toxicity를 갖는 콘텐츠를 필터링하면 소외된 그룹의 콘텐츠가 과도하게 배제될 수 있다. 이러한 bias를 효과적으로 처리하고 그 영향을 연구하는 것이 필요하며, PaLM을 실제 작업에 사용할 때는 추가적인 공정성 평가를 수행해야 한다.&lt;/p>
&lt;p>공정성 분석은 범위가 좁아 다양한 잠재적 위험을 전체적으로 설명하지 못한다. 성별, 인종, 민족, 종교 등의 축을 따라 bias를 분석하지만, 이는 영어 데이터와 모델 출력에만 적용된다. 성적 지향성, 장애 등의 다른 사회적 불평등 축이나 비서구 사회 문화 맥락에서 중요한 편향은 고려하지 않았다. 따라서, 잠재적 위험을 의미있게 평가하려면, 대상 응용 분야와 사회 문화 맥락에 관련된 불평등의 축을 따라 공정성 분석이 필요하다.&lt;/p>
&lt;p>이 논문의 분석은 데이터와 모델의 편향에 초점을 맞추지만, 이들이 실제로 어떻게 사용되는지에 따라 downstream에서의 피해는 달라질 수 있다. 예를 들어, toxicity 콘텐츠가 학습 데이터에 포함되어 있는 것은 바람직하지 않아 보일 수 있지만, PaLM이 toxicity 콘텐츠를 감지하는데 사용된다면, 이러한 콘텐츠에 대한 사전 학습은 중요하다고 볼 수 있다.&lt;/p>
&lt;p>PaLM의 언어 능력은 학습 데이터와 평가 벤치마크의 언어 한계에 의해 제한될 수 있다. 벤치마크 평가는 종종 언어 이해 능력의 전체 복잡성을 완전히 포착하지 못하며, 그들이 측정하려는 것과 실제로 측정하는 것 사이에 차이가 있다. 따라서, 다른 실세계 응용 프로그램 상황에서 동일한 성능 수준이 보장되지 않을 수 있다.&lt;/p>
&lt;p>PaLM은 평가한 벤치마크에서 다국어 능력을 보여주지만, 대부분은 영어로 이루어진 벤치마크이다. 비영어 언어에서의 성능과 bias에 대한 더욱 견고한 평가가 필요하다. 학습 데이터셋의 웹 페이지는 품질을 평가하기 위해 필터링되었는데, 이로 인해 일상적인 언어, 코드 스위칭, 방언의 다양성 등이 과도하게 배제되었을 수 있다. 또한, PaLM은 특정 시점의 언어 사용을 나타내서 현재의 일상 언어나 속어를 모델링하는 작업에 성능이 떨어질 수 있다. 표준 벤치마크는 언어 데이터의 다양한 측면을 포착하거나 구분하지 않아, 이 부분에서 PaLM의 능력을 평가하는 것은 어렵다.&lt;/p>
&lt;p>모델의 다양한 대표성 bias와 능력 차이를 완화한 후에도, 인간의 언어 행동을 모방하는 대규모 언어 모델이 악용될 가능성이 있다는 것을 기억하는 것이 중요하다. 이러한 고품질 언어 생성 능력은 오보 캠페인 등의 악의적인 용도로 사용될 수 있고, 온라인에서 소외된 그룹을 괴롭히는 데도 사용될 수 있다. 이러한 위험은 PaLM 뿐만 아니라 대부분의 대규모 언어 모델에 존재하므로, 이러한 악의적인 용도를 방지할 수 있는 확장 가능한 해결책에 대한 노력이 필요하다.&lt;/p>
&lt;p>소프트웨어 개발 지원을 위한 PaLM-Coder의 배포는 복잡성과 윤리적 고려사항을 수반한다. 언어 모델 기반 제안이 정확하고 견고하며 안전하고 보안이 확보된 것을 보장하고, 개발자들이 이를 확신하는 것은 아직 해결되지 않은 문제이다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>대규모 언어 모델링을 통해 자연 언어 능력이 크게 발전하였다. 이는 시퀀스에서 다음 토큰을 예측하거나 마스킹된 영역을 예측하는 방법으로, 인터넷, 책, 포럼에서 얻은 방대한 데이터에 적용되었다. 이로 인해 고급 언어 이해와 생성 능력을 가진 모델이 개발되었고, 데이터, 매개변수, 계산량의 확장을 통해 모델 품질의 예측 가능한 향상이 이루어졌다.&lt;/p>
&lt;p>Transformer 아키텍처는 현대 accelerator에서 높은 효율성을 보여주며 언어 모델의 기본적인 접근법이 되었다. 4년 동안, 최대 모델의 크기와 계산량은 몇 배로 증가했다. BERT, GPT 시리즈 등 다양한 모델이 등장하며 언어 이해와 모델링 성능이 크게 향상되었다. 또한, 코드 이해 및 생성, 대화 응용 등 여러 분야에서도 개선이 이루어졌다. 최근에는 언어 모델이 지시사항을 따르도록 하는 연구를 통해 이러한 모델의 유용성과 신뢰성이 더욱 향상되었다.&lt;/p>
&lt;p>큰 모델들은 단일 accelerator에 효율적으로 학습하거나 적용하기 어렵다. 이에 따라, 모델 텐서를 가속기 간에 분할하거나, 모델 계층을 accelerator 간에 분리하고 activation을 파이프라인화하는 기술이 등장하였다. 여러 연구들이 모델 규모를 늘리면서 통신 오버헤드를 제한하는 것을 목표로 하고 있으며, PaLM은 Pathways 인프라를 통해 데이터와 모델 병렬화를 혼합하여 사용한다.&lt;/p>
&lt;p>모델을 효율적으로 확장하기 위한 아키텍처 변형이 제안되었다. 대량의 텍스트를 임베딩하여 모델 크기를 줄이는 검색 모델, 다른 예시가 parameter의 다른 부분 집합을 사용하게 하는 모델 희소성, 그리고 극도로 긴 시퀀스로 효율적인 학습을 가능하게 하는 시퀀스 길이의 희소성 등이 포함된다. 이러한 연구의 개선 사항들이 미래의 Pathways 언어 모델에 통합될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="open-questions-in-scaling">Open Questions in Scaling&lt;/h2>
&lt;p>few-shot learning 기반으로 학습하는 대규모 언어 모델의 품질 향상은 모델의 깊이와 너비, 학습된 토큰의 수, 학습 코퍼스의 품질, 그리고 계산량 증가 없이 모델 용량을 증가시키는 방법 등 네 가지 주요 요인에 의해 이루어졌다. 이 중 하나인 학습 코퍼스의 품질이 주요 요인으로 작용할 수 있음이 나타났으며, 신중한 데이터 필터링을 통한 few-shot learning 기반으로 한 학습 향상이 매우 중요함이 밝혀졌다.&lt;/p>
&lt;p>학습 비용이 높아서, 모델의 깊이와 너비와 학습된 토큰 수의 효과를 분리하는 연구를 수행하지 못했다. 즉, &amp;ldquo;7T 토큰으로 학습된 62B parameter 모델과 780B 토큰으로 학습된 540B parameter 모델은 어떻게 비교될까?&amp;ldquo;라는 질문에 대한 답을 아직 찾지 못했습니다. 이러한 모델은 PaLM 540B와 비슷한 학습 비용을 가지지만, 추론 비용이 그 크기에 비례하기 때문에 더 작은 모델이 선호될 것이다.&lt;/p>
&lt;p>최근 연구에서는 1.4T 토큰의 데이터로 학습된 70B parameter 모델인 Chinchilla와 300B 토큰의 데이터로 학습된 280B parameter 모델인 Gopher를 비교하였다. 두 모델은 유사한 학습 비용을 가지지만, Chinchilla는 다양한 언어 작업에서 Gopher를 큰 차이로 능가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/table21.png"
width="1220"
height="524"
srcset="https://kurtkim.github.io/p/palm/images/table21_hu0ab69c11a7bb3e9b00bcdf6dc9b1dde9_136056_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/table21_hu0ab69c11a7bb3e9b00bcdf6dc9b1dde9_136056_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="558px"
>&lt;/p>
&lt;p>Chinchilla와 PaLM, 두 모델의 결과를 비교하였다. 두 모델은 58개의 BIG-bench 작업과 9개의 영어 NLP 작업에서 비슷한 결과를 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/palm/images/figure24.png"
width="1194"
height="504"
srcset="https://kurtkim.github.io/p/palm/images/figure24_hu1393194a6f844254c2dc8a997000725c_109049_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/palm/images/figure24_hu1393194a6f844254c2dc8a997000725c_109049_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>Chinchilla는 BIG-bench에서 PaLM의 스케일링 곡선을 약간 능가하고, 9개의 영어 NLP 작업에서는 약간 미치지 못하는 반면, Gopher는 두 스케일링 곡선 모두를 크게 미치지 못했다. Gopher와 Chinchilla는 동일한 학습 코퍼스를 사용했지만, PaLM은 다른 코퍼스를 사용하여 비교가 복잡해졌다. 이 결과는 Gopher가 그 크기의 모델에 대해 학습이 부족했음을 보여주지만, &amp;ldquo;X 크기의 모델이 Y 토큰으로 훈련되면 PaLM 540B와 어떻게 비교될까?&amp;ldquo;라는 질문에 대한 답을 추론하기에는 충분하지 않다. 이것이 어려운 질문이 된 이유는 여러 가지이다:&lt;/p>
&lt;ol>
&lt;li>강력한 결론을 도출하려면 큰 규모의 실험이 필요하며, 이는 높은 계산 비용을 요구한다.&lt;/li>
&lt;li>더 작은 모델이 더 적은 TPU 칩으로 학습된다면, 학습 시간이 비례적으로 증가할 것이다. 같은 수의 TPU 칩으로 학습된다면, 배치 크기를 크게 늘리지 않으면 TPU 계산 효율성을 유지하기 어렵다. PaLM 540B의 배치 크기는 이미 4M 토큰인데, 이보다 더 큰 배치 크기가 효율성을 유지할 수 있을지는 불확실하다.&lt;/li>
&lt;li>웹에서는 무한한 양의 고품질 텍스트 데이터가 있지는 않다. PaLM에서는 780B 토큰 이후 일부 데이터가 반복되는 것을 확인했고, 이는 학습의 종료 지점으로 설정한 이유이다. 반복된 데이터의 가치와 보지 않은 데이터의 가치를 비교하는 것은 불확실하지만, 새로 갱신된 데이터셋에서 더 오래 학습하면 성능이 향상되는 것을 확인하였다.&lt;/li>
&lt;/ol>
&lt;p>향후 연구에서는 다양한 작업에 잘 적용되는 뛰어난 언어 모델을 만드는 데 영향을 미치는 여러 요인들 사이의 균형에 대해 조사할 계획이다. 이는 모델 아키텍처, 사전 학습 작업, 최적화 설정 등의 추가적인 요인에 대한 연구를 포함한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>이 연구에서는 고품질, 다양한 텍스트로 학습된 대규모 언어 모델인 PaLM을 사용하여 few-shot 언어 이해와 생성의 가능성을 확장하였다. 이 모델은 29개의 주요 영어 NLP 작업 중 28개에서 state-of-the-art를 달성했으며, 150개 이상의 새로운 언어 작업을 포함하는 BIG-bench에서는 인간의 평균 성능을 능가하였다. 또한 소스 코드 이해, 다국어 NLP, 기계 번역 등 다양한 분야에서도 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>이 연구의 중요한 결과 중 하나는 복수 단계의 논리적 추론을 필요로 하는 작업에서 뛰어난 성능을 보였다는 것이다. 다양한 산술 및 상식 추론 작업에서 state-of-the-art를 달성하였으며, 이는 단순히 모델 규모 확대 뿐 아니라, 예측 전에 논리적 추론 과정을 명시적으로 생성하도록 하는 방식을 통해 이루어졌다. PaLM은 농담을 설명하고 복잡한 시나리오에 대한 질문에 답하는 등의 작업에서 논리적 추론 과정을 구체적으로 표현할 수 있었다.&lt;/p>
&lt;p>이 연구 결과는 few-shot 언어 이해를 위한 모델 규모 확대의 효과가 아직 정체되지 않았음을 보여준다. 동일한 학습 방법을 사용한 다른 모델들과 비교했을 때, 규모의 증가와 성능 향상이 log-linear 관계를 보였고, 특정 벤치마크에서는 더 큰 모델로 확대했을 때 불연속적인 성능 향상이 관찰되었다. 이는 특정 언어 모델의 기능이 충분한 규모에서만 나타나며, 미래의 모델에서는 추가적인 능력이 나타날 수 있음을 시사한다.&lt;/p>
&lt;p>추론 작업에서의 뛰어난 성능은 중요한 의미를 가지고 있다. 모델이 예측을 설명하는 자연어를 생성하는 것은 사용자가 모델의 예측 이유를 이해하는 데 도움이 되며, 더불어 모델에게 명확한 추론 과정을 생성하도록 요청함으로써 예측의 품질이 크게 향상될 수 있음을 보여준다. 즉, 모델의 언어 생성능력은 언어 생성을 크게 필요로 하지 않는 분류나 회귀와 같은 작업에서도 매우 유익할 수 있다.&lt;/p>
&lt;p>few-shot 언어 모델링의 규모를 확대하는 목표를 달성했지만, 미래 모델에 대한 최적의 네트워크 구조와 훈련 방식에 대한 여전히 많은 미해결 문제가 있다. PaLM은 Google의 ML 확장 미래 비전인 Pathways 설립의 첫 단계일 뿐이다. 이 규모 확대 능력을 잘 알려진 full-attention transformer 모델에서 보여주었으며, 더 넓은 목표는 다양한 새로운 구조와 학습 방식을 탐구하고, 가장 유망한 시스템을 Pathways의 확장 능력과 결합하는 것이다. PaLM은 여러 모달리티에 걸친 일반화 능력을 가진 대규모, 모듈화된 시스템을 개발하는 우리의 최종 목표에 강한 기반을 제공한다고 믿는다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2204.02311.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/lucidrains/PaLM-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>InstructGPT</title><link>https://kurtkim.github.io/p/instructgpt/</link><pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/instructgpt/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문은 인간의 피드백을 통해 언어 모델을 미세 조정함으로써 사용자의 의도에 더 잘 부합하도록 만드는 방법을 제시한다. 라벨러가 작성한 프롬프트와 OpenAI API를 통해 제출된 프롬프트를 이용해서 데이터셋을 수집하고, 이를 GPT-3를 지도 학습으로 미세 조정하는데 사용하였다. 그 다음, 모델 출력의 순위를 나타내는 데이터셋을 수집하고, 이를 인간의 피드백에서 강화 학습을 통해 이 감독 모델을 더 미세 조정하였다. 이 결과로 나온 모델을 InstructGPT라고 부른다. 인간의 평가에서는 parameter 수가 훨씬 적은 1.3B parameter를 가진 InstructGPT 모델의 출력이 175B GPT-3의 출력보다 선호되었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 언어 모델은 다양한 자연어 처리 작업을 수행할 수 있지만, 종종 사실을 만들어내거나 편향된 텍스트를 생성하거나 사용자의 지시를 따르지 않는 등의 문제가 있다. 이는 언어 모델링의 목표와 &amp;ldquo;사용자의 지시를 도움이 되고 안전하게 따르라&amp;quot;는 목표가 서로 다르기 때문인데, 이를 &amp;ldquo;목표의 불일치(misaligned)&amp;ldquo;라고 한다. 이러한 문제를 피하는 것은 수많은 애플리케이션에서 사용되는 언어 모델에게 매우 중요하다.&lt;/p>
&lt;p>사용자의 의도에 따라 언어 모델을 학습시키는 방법을 통해 언어 모델의 alignment를 개선하고 있다. 이는 지시 사항을 따르는 것과 같은 명시적인 의도 뿐만 아니라, 사실적이고 편향되지 않으며 독성이 없거나 해로운 행동을 하지 않는 것과 같은 암시적인 의도를 포함한다. 언어 모델이 사용자의 작업을 해결하는 데 도움이 되고, 정보를 조작하거나 사용자를 오도하지 않으며, 사람이나 환경에 해를 끼치지 않는 모델을 목표로 한다.&lt;/p>
&lt;p>언어 모델을 사용자의 의도와 맞추기 위해, 강화 학습을 통해 GPT-3을 미세 조정하는 방법에 초점을 맞추었다. 인간의 피드백을 보상 신호로 사용하여 모델을 미세 조정하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure2.png"
width="976"
height="622"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure2_hud674a7b33670b0fa56d5979ac5a251ad_135510_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure2_hud674a7b33670b0fa56d5979ac5a251ad_135510_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="376px"
>&lt;/p>
&lt;p>OpenAI API에 제출된 프롬프트에 대한 원하는 출력 행동의 인간이 작성한 시연 데이터셋을 수집하였고, 이를 사용하여 지도 학습 기본선을 훈련시켰다. 또한, 더 큰 세트의 API 프롬프트에서 모델의 출력 사이의 인간이 라벨링한 비교 데이터셋을 수집하였다. 이 데이터를 통해 보상 모델을 학습시키고, 이를 보상 함수로 사용하여 지도 학습 기준을 미세 조정하였습니다. 이 결과로 만들어진 모델을 InstructGPT라고 부른다.&lt;/p>
&lt;p>주로 학습 데이터에 포함되지 않은 고객의 프롬프트로 구성된 테스트 세트에서 모델 출력의 품질을 평가한다. 또한, 다양한 공공 NLP 데이터셋에서 자동 평가를 실시한다. 세 가지 모델 크기(1.3B, 6B, 175B parameter)를 학습시키며, 모든 모델은 GPT-3 아키텍처를 사용한다.&lt;/p>
&lt;p>&lt;strong>Labelers signiﬁcantly prefer InstructGPT outputs over outputs from GPT-3.&lt;/strong> 테스트 세트에서, 1.3B parameter를 가진 InstructGPT 모델의 출력은 100배 이상의 적은 parameter에도 불구하고 175B GPT-3의 출력보다 선호되었다. 175B InstructGPT의 출력은 85 $\pm$ 3%의 시간 동안 175B GPT-3의 출력보다, 그리고 71 $\pm$ 4%의 시간 동안 few-shot 175B GPT-3에 대해 선호되었다. InstructGPT 모델은 또한 더 적절한 출력을 생성하고, 지시사항의 명시적 제약을 더 신뢰성 있게 따랐다.&lt;/p>
&lt;p>&lt;strong>InstructGPT models show improvements in truthfulness over GPT-3.&lt;/strong> TruthfulQA 벤치마크에서, InstructGPT는 GPT-3보다 두 배 이상 사실적이고 유익한 답변을 생성한다. &amp;ldquo;closed-domain&amp;rdquo; 작업에서, InstructGPT 모델은 GPT-3에 비해 입력에 없는 정보를 만들어내는 빈도가 절반 정도로 줄었다(각각 21% vs. 41%의 hallucination).&lt;/p>
&lt;p>&lt;strong>InstructGPT shows small improvements in toxicity over GPT-3, but not bias.&lt;/strong> toxicity를 측정하기 위해, RealToxicityPrompts 데이터셋을 사용하여 자동 및 인간 평가를 실시하였다. 존중스럽게 행동하도록 요청했을 때, InstructGPT 모델은 GPT-3보다 약 25% 적은 toxicity 출력을 생성했다. 하지만, InstructGPT는 Winogender와 CrowSPairs 데이터셋에서 GPT-3보다 크게 개선되지 않았다.&lt;/p>
&lt;p>&lt;strong>We can minimize performance regressions on public NLP datasets by modifying our RLHF ﬁne-tuning procedure.&lt;/strong> RLHF 미세 조정 중에, 특정 공공 NLP 데이터셋에서 GPT-3에 비해 성능이 떨어지는 것을 관찰하였다. 이는 &amp;ldquo;정렬 세금&amp;quot;의 예시로, 특정 작업에서의 성능 하락 비용을 수반합니다. 그러나, 라벨러 선호 점수를 손상시키지 않으면서 이러한 데이터셋에서의 성능 하락을 크게 줄일 수 있습니다. 이는 사전 훈련 분포의 로그 가능도를 증가시키는 업데이트와 PPO 업데이트를 혼합함으로써 가능합니다.&lt;/p>
&lt;p>&lt;strong>Our models generalize to the preferences of “held-out” labelers that did not produce any training data.&lt;/strong> 초기 실험에서, 보류된 라벨러들은 학습 라벨러들과 비슷한 비율로 InstructGPT의 출력을 GPT-3의 출력보다 선호하는 것으로 나타났다. 그러나, 이 모델들이 더 넓은 사용자 그룹에서 어떻게 작동하고, 사람들이 원하는 행동에 대해 의견이 분분한 경우 어떻게 작동하는지에 대한 추가 연구가 필요하다.&lt;/p>
&lt;p>&lt;strong>Public NLP datasets are not reﬂective of how our language models are used.&lt;/strong> 인간 선호 데이터에 따라 조정된 GPT-3 (InstructGPT)는 다양한 공개 NLP 작업에 따라 조정된 GPT-3인 FLAN과 T0 모델과 비교하였다. 결과적으로, InstructGPT는 FLAN과 T0 모델보다 더 선호되었으며, API 프롬프트 분포에서 이들 모델보다 더 좋은 성능을 보였주였다.&lt;/p>
&lt;p>&lt;strong>InstructGPT models show promising generalization to instructions outside of the RLHF ﬁnetuning distribution.&lt;/strong> InstructGPT는 코드를 요약하고 코드에 대한 질문에 답하는 등의 지시사항을 따르며, 미세 조정된 분포에서 드물게 나타나는 다른 언어의 지시사항도 따르는 능력이 있다. 반면에, GPT-3는 이런 작업을 수행하기 위해 더 신중한 프롬프팅이 필요하며, 일반적으로 이런 영역에서의 지시사항을 따르지 않는다. 이 결과는 우리의 모델이 &amp;ldquo;지시사항을 따르는 (following instructions)&amp;rdquo; 개념을 일반화하는 능력을 가지고 있음을 보여준다.&lt;/p>
&lt;p>&lt;strong>InstructGPT still makes simple mistakes.&lt;/strong> InstructGPT는 여전히 지시사항을 따르지 못하거나, 사실을 만들어내거나, 간단한 질문에 대해 긴 답변을 제공하거나, 거짓 전제를 가진 지시사항을 인식하지 못하는 문제가 있다.&lt;/p>
&lt;p>인간의 선호도를 사용하여 대규모 언어 모델을 세밀하게 조정하면 다양한 작업에서 그들의 행동이 크게 개선되지만, 안전성과 신뢰성을 향상시키기 위해 더 많은 작업이 필요하다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>&lt;strong>Research on alignment and learning from human feedback.&lt;/strong> 이 연구는 인간의 의도에 따라 모델을 조정하는 기존 기술, 특히 인간의 피드백으로부터의 강화학습에 기반을 두고 있다. 이 방법은 원래 간단한 로봇 훈련에 사용되었으며, 최근에는 텍스트 요약을 위한 언어 모델 조정에 적용되었다. 이 방법은 다양한 영역에서 인간의 피드백을 보상으로 사용하는 유사한 작업에 영향을 받았다. 광범위한 언어 작업에 대한 언어 모델의 정렬을 위한 강화학습의 직접적인 적용으로 볼 수 있다.&lt;/p>
&lt;p>언어 모델이 alignment되는 것이 무슨 의미인지에 대한 질문이 최근 주목 받았다. 일부 연구에서는 alignment의 부재로 인한 언어 모델의 행동적 문제를 분석하였고, 다른 연구에서는 alignment 연구를 위한 언어 보조 도구를 제안하고, 간단한 기준과 스케일링 속성을 연구하였다.&lt;/p>
&lt;p>&lt;strong>Training language models to follow instructions.&lt;/strong> 이 연구는 언어 모델에서 다양한 공공 NLP 데이터셋에 대해 세밀하게 조정하고, 다른 NLP 작업 세트에서 평가하는 크로스태스크 일반화 연구와 관련이 있다. 여러 연구에서 일관적으로 나타난 결과는, 지시사항과 함께 NLP 작업의 범위에서 언어 모델을 세밀하게 조정하는 것이 보류된 작업에 대한 성능을 향상시킨다는 것이다. 이는 zero-shot과 few-shot 설정 모두에 적용된다.&lt;/p>
&lt;p>또한, 시뮬레이션된 환경에서 모델이 자연어 지시사항을 따라 탐색하도록 학습하는 탐색을 위한 지시사항 따르기에 관한 연구도 있다.&lt;/p>
&lt;p>&lt;strong>Evaluating the harms of language models.&lt;/strong> 언어 모델의 행동을 수정하는 목표는 이들이 실세계에 배포될 때 발생할 수 있는 위험을 줄이는 것이다. 언어 모델은 편향된 출력을 생성하거나, 개인 데이터를 유출하거나, 잘못된 정보를 생성하거나, 악의적으로 사용될 수 있다. 이러한 위험을 구체적으로 평가하기 위한 벤치마크를 구축하는 연구가 진행되고 있다. 하지만, 이러한 문제를 해결하는 것은 어렵다. 왜냐하면 언어 모델의 행동을 개선하려는 노력이 부작용을 가져올 수 있기 때문이다. 예를 들어, 모델의 toxicity를 줄이려는 노력은 대표성이 부족한 그룹의 텍스트를 모델링하는 능력을 줄일 수 있다.&lt;/p>
&lt;p>&lt;strong>Modifying the behavior of language models to mitigate harms.&lt;/strong> 언어 모델의 생성 행동을 변경하는 방법은 다양하다. 이에는 소규모 가치 중심 데이터셋에 대해 언어 모델을 미세 조정하거나, 트리거 구문을 생성할 확률이 높은 문서를 사전 학습 데이터셋에서 제거하는 방법, 데이터 필터링이나 특정 단어 차단, 안전성 특정 제어 토큰 사용 등이 포함된다. 또한, 단어 임베딩 규제화, 데이터 증가, 민감한 토큰에 대한 분포를 균일하게 만드는 방법 등을 사용하여 언어 모델이 생성하는 편향을 완화하는 다양한 접근법이 있다. 이 밖에도, 두 번째 언어 모델을 사용하여 생성을 조정하거나, 언어 모델의 toxicity를 줄이는 등의 방법이 있다.&lt;/p>
&lt;hr>
&lt;h2 id="methods-and-experimental-details">Methods and experimental details&lt;/h2>
&lt;h3 id="high-level-methodology">High-level methodology&lt;/h3>
&lt;p>이 연구의 방법론은 스타일 연속성과 요약 분야에 적용된 이전의 연구를 따른다. 사전 학습된 언어 모델, alignment된 출력을 생성하고자 하는 프롬프트 분포, 그리고 학습된 인간 라벨러 팀을 기반으로 하는 세 가지 단계를 적용한다.&lt;/p>
&lt;p>&lt;strong>Step 1: Collect demonstration data, and train a supervised policy.&lt;/strong> 라벨러들은 입력 프롬프트 분포에 대한 원하는 행동의 예시를 제공하고, 이 데이터를 바탕으로 사전 학습된 GPT-3 모델을 미세 조정한다.&lt;/p>
&lt;p>&lt;strong>Step 2: Collect comparison data, and train a reward model.&lt;/strong> 모델 출력 간의 비교 데이터셋을 수집하고, 이를 바탕으로 인간이 선호하는 출력을 예측하는 보상 모델을 학습시킨다.&lt;/p>
&lt;p>&lt;strong>Step 3: Optimize a policy against the reward model using PPO.&lt;/strong> RM의 출력을 스칼라 보상으로 사용한다. 이 보상을 최적화하기 위해 PPO 알고리즘을 사용하여 supervised policy를 미세 조정한다.&lt;/p>
&lt;p>Steps 2와 3은 계속 반복될 수 있다. 가장 좋은 정책에 대한 추가적인 비교 데이터가 수집되어 새로운 보상 모델과 정책을 학습시키는 데 사용된다. 대부분의 비교 데이터는 supervised policy에서, 일부는 PPO policy에서 나온다.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>프롬프트 데이터셋은 주로 OpenAI API에 제출된 텍스트 프롬프트로 구성되어 있다. Playground 인터페이스에서 사용된 InstructGPT 모델의 이전 버전을 사용한 것들이다. 프롬프트는 중복 제거되며, 사용자 ID당 200개로 제한된다. 학습, 검증, 테스트 분할은 사용자 ID를 기반으로 생성되며, 검증 및 테스트 세트는 학습 세트에 있는 사용자의 데이터를 포함하지 않는다. 또한, 모델이 민감한 고객 정보를 학습하는 것을 피하기 위해, 학습 분할의 모든 프롬프트는 개인 식별 정보에 대해 필터링된다.&lt;/p>
&lt;p>최초의 InstructGPT 모델을 학습시키기 위해, 라벨러들이 스스로 프롬프트를 작성하도록 요청했다. 이는 초기 지시사항과 같은 프롬프트가 필요했기 때문이며, 라벨러들은 세 가지 종류의 프롬프트를 작성하도록 요청받았다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Plain:&lt;/strong> 단순히 라벨러들에게 임의의 작업을 생각해내도록 요청하면서, 작업이 충분한 다양성을 가지도록 하였다.&lt;/li>
&lt;li>&lt;strong>Few-shot:&lt;/strong> 라벨러들에게 지시사항을 생각해내고, 그 지시사항에 대한 여러 질문/응답 쌍을 생각해내도록 요청하였다.&lt;/li>
&lt;li>&lt;strong>User-based:&lt;/strong> OpenAI API의 대기 목록 신청서에는 여러 사용 사례가 명시되어 있었다. 라벨러들에게 이러한 사용 사례에 해당하는 프롬프트를 생각해내도록 요청하였다.&lt;/li>
&lt;/ul>
&lt;p>이 프롬프트로부터, 세 가지 데이터셋을 생성하여 튜닝에 사용한다: (1) 라벨러의 시연을 사용한 SFT 데이터셋, (2) 모델 출력의 라벨러 순위를 사용한 RM 데이터셋, 그리고 (3) 인간 라벨이 없는 PPO 데이터셋이다. SFT 데이터셋에는 약 13k의 학습 프롬프트가 있고, RM 데이터셋에는 33k의 학습 프롬프트가 있으며, PPO 데이터셋에는 31k의 학습 프롬프트가 있다.&lt;/p>
&lt;h3 id="tasks">Tasks&lt;/h3>
&lt;p>학습 작업은 라벨러가 작성한 프롬프트와 API에서 초기 InstructGPT 모델에 제출된 프롬프트 두 가지로부터 나온다. 이 프롬프트들은 다양하며, 생성, 질문 응답, 대화, 요약, 추출 등 다양한 자연어 작업을 포함한다. 데이터셋은 96% 이상이 영어이지만, 다른 언어로의 지시에 대한 응답과 코딩 작업 완료 능력도 조사한다.&lt;/p>
&lt;p>각 프롬프트에 대한 작업은 대부분 자연어 지시를 통해 직접 지정되며, 간접적으로는 예시를 통하거나 암시적 연속성을 통해 할 수도 있다. 라벨러들은 프롬프트를 작성한 사용자의 의도를 추론하도록 요청받으며, 불분명한 작업은 건너뛰게 된다. 또한, 라벨러들은 응답의 진실성과 같은 암시적 의도, 그리고 편견이나 유해한 언어와 같은 potentially harmful outputs를 고려한다.&lt;/p>
&lt;h3 id="human-data-collection">Human data collection&lt;/h3>
&lt;p>시연과 비교 데이터를 생성하고 주요 평가를 수행하기 위해 Upwork와 ScaleAI를 통해 계약자 팀을 고용하였다. 다양한 작업 범위를 다루며, 때때로 논란이 될 수 있는 민감한 주제도 포함된다. 다양한 인구집단의 선호도에 민감하고, 잠재적으로 해롭다고 판단되는 출력을 잘 식별하는 라벨러를 선택하려 하였다. 이를 위해, 라벨러의 성능을 측정하는 스크리닝 테스트를 실시하고, 이 테스트에서 잘 수행한 라벨러를 선택하였다.&lt;/p>
&lt;p>학습과 평가 중에, 사용자를 돕는 것과 진실성 및 무해함 사이에 충돌이 발생할 수 있다. 학습 중에는 사용자 도움을 우선시하지만, 최종 평가에서는 진실성과 무해함을 우선시한다.&lt;/p>
&lt;p>라벨러들과 밀접하게 협력하여 프로젝트를 진행하며, 라벨러 학습을 위한 온보딩 프로세스를 진행하고, 각 작업에 대한 상세한 지침을 제공하며, 공유 채팅방에서 라벨러의 질문에 답한다.&lt;/p>
&lt;p>모델이 다른 라벨러들의 선호도에 얼마나 잘 적응하는지 확인하기 위한 초기 연구로, 학습 데이터를 만들지 않은 별도의 라벨러 그룹을 고용하였다. 이들은 같은 공급업체에서 온 라벨러지만, 별도의 스크리닝 테스트는 거치지 않았다.&lt;/p>
&lt;p>작업의 복잡성에도 불구하고 라벨러 간 동의율이 높았다. 온보딩 프로세스를 진행한 라벨러들은 72.6% $\pm$ 1.5%의 경우에 동의하였고, 보류된 라벨러들은 77.3 $\pm$ 1.3%의 경우에 동의하였다. 이는 이전의 요약 작업에서 연구자들 간의 동의율인 73 $\pm$ 4%와 비슷한 수준이다.&lt;/p>
&lt;h3 id="models">Models&lt;/h3>
&lt;p>인터넷 데이터에 대해 학습된 GPT-3 사전 학습 언어 모델을 사용하여 시작하였고, 이를 바탕으로 세 가지 다른 기법을 사용하여 모델을 학습시켰다.&lt;/p>
&lt;p>&lt;strong>Supervised ﬁne-tuning (SFT).&lt;/strong> GPT-3를 라벨러의 시연에 대해 지도학습으로 미세 조정하였다. 16 epoch 동안 학습하였고, 검증 세트에서의 RM 점수를 기반으로 최종 모델을 선택하였다. 1 epoch 후에 overﬁt이 발생했지만, 더 많은 epoch 동안 학습하면 RM 점수와 인간 선호도 평가가 개선되었다.&lt;/p>
&lt;p>&lt;strong>Reward modeling (RM).&lt;/strong> 최종 unembedding 레이어를 제거한 SFT 모델을 시작으로, 프롬프트와 응답을 입력으로 받아 스칼라 보상을 출력하는 모델을 학습시켰다. 계산량 절약과 학습의 안정성을 위해, 6B RM만 사용하였다.&lt;/p>
&lt;p>Stiennon et al. (2020)의 연구에서, 보상 모델은 같은 입력에 대한 두 모델 출력의 비교를 통해 학습되었다. cross-entropy loss를 사용하며, 보상의 차이는 한 응답이 다른 응답보다 사람에게 선호될 가능성을 나타낸다.&lt;/p>
&lt;p>비교 수집을 가속화하기 위해, 라벨러에게 4에서 9개의 응답을 랭킹하도록 요청하였다. 이는 각 프롬프트에 대해 여러 비교를 생성한다. 단일 데이터셋으로 비교를 섞으면 보상 모델이 overfit되는 것을 발견했기 때문에, 각 프롬프트에서의 모든 비교를 단일 배치 요소로 학습시켰다. 이 방법은 계산 효율이 높고, overfit이 발생하지 않아 검증 정확도와 log loss이 향상되었다.&lt;/p>
&lt;p>구체적으로, 보상 모델에 대한 손실 함수는 다음과 같다:&lt;/p>
&lt;p>$$ loss(\theta) = - {{1}\over{\begin{pmatrix} K \\ 2 \end{pmatrix}}} E_{(x, y_w, y_t) \sim D} [log \ (\sigma (r_{\theta} (x, y_w) - r_{\theta}(x, y_t)))] $$&lt;/p>
&lt;p>$\theta(x, y)$는 보상 모델의 출력이고, $y_w$는 선호하는 완성이다. $D$는 비교 데이터셋이다.&lt;/p>
&lt;p>RM loss는 보상 변화에 불변하므로, RL 전에 라벨러 시연의 평균 점수가 0이 되도록 보상 모델을 정규화한다.&lt;/p>
&lt;p>&lt;strong>Reinforcement learning (RL).&lt;/strong> PPO를 사용해서 SFT 모델을 미세 조정하였다. 랜덤한 고객 프롬프트를 제시하고 응답을 기대하며, 보상 모델로부터 보상을 생성한다. SFT 모델로부터의 KL penalty를 추가해 보상 모델의 과도한 최적화를 완화하였다. 가치 함수는 RM에서 초기화되며, 이 모델들을 &amp;ldquo;PPO&amp;quot;라고 부른다.&lt;/p>
&lt;p>공개 NLP 데이터셋의 성능 감소를 해결하기 위해, 사전 학습 gradient를 PPO gradient와 혼합하는 것을 시도하였다. 이 모델들을 &amp;ldquo;PPO-ptx&amp;quot;라고 부른다. RL 학습에서 다음의 결합된 목표 함수를 최대화한다:&lt;/p>
&lt;p>$$ objective(\Phi) = E_{(x,y) \sim D_{\pi \underset{\Phi}{RL}}} [r_{\theta}(x,y) - \beta log \ (\pi_{\Phi}^{RL}(x|y) / \pi^{SFT}(x|y))] + \gamma E_{x \sim D_{pretrain}} [log \ (\pi_{\Phi}^{RL}(x))] $$&lt;/p>
&lt;p>$\pi_{\Phi}^{RL}$는 학습된 RL 정책, $\pi^{SFT}$는 지도 학습 모델, $D_{pretrain}$은 사전 학습 분포이다. KL reward coefﬁcient $\beta$와 사전 학습 손실 계수 $\gamma$는 각각 KL penalty와 사전 학습 gradient의 강도를 제어한다. &amp;ldquo;PPO&amp;rdquo; 모델에서 $\gamma$는 0으로 설정되며, 이 논문에서 InstructGPT는 주로 PPO-ptx 모델을 의미한다.&lt;/p>
&lt;p>&lt;strong>Baselines.&lt;/strong> PPO 모델의 성능을 SFT 모델과 GPT-3, 그리고 명령을 따르는 모드로 프롬프트된 GPT-3와 비교한다. 이 프롬프트는 사용자가 지정한 명령 앞에 추가된다.&lt;/p>
&lt;p>InstructGPT를 FLAN과 T0 데이터셋에서 미세 조정된 175B GPT-3와 비교한다. 이 두 데이터셋은 다양한 NLP 작업과 각 작업에 대한 자연어 지시를 포함하고 있다. 각각 약 100만 예제에 대해 미세 조정하고, 검증 세트에서 가장 높은 보상 모델 점수를 얻는 체크포인트를 선택한다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>&amp;ldquo;alignment&amp;quot;은 이 맥락에서 사용자의 의도에 맞게 모델이 행동하는 것을 의미한다. 이는 모델이 도움이 되고, 정직하며, 무해해야 한다는 Askell et al. (2021)의 정의를 따른다.&lt;/p>
&lt;p>모델이 도움이 되려면 명령을 따르고, 작은 양의 프롬프트나 특정 패턴에서 의도를 추론해야 한다. 그러나 프롬프트의 의도는 불분명할 수 있으므로, 라벨러의 판단에 의존한다. 그러나 라벨러와 프롬프트를 생성한 사용자 사이에 의도의 차이가 있을 수 있다.&lt;/p>
&lt;p>생성적인 AI 모델에서의 진실성을 측정하는 것은 어렵다. 이는 모델의 output과 그것이 &amp;ldquo;올바른(belief)&amp;rdquo; output에 대한 생각을 비교해야 하는데, 모델 자체가 복잡해서 그 생각을 추론하기 어렵기 때문이다. 대신, 모델이 세상에 대해 말하는 것이 사실인지를 측정하는 두 가지 방법을 사용한다: (1) 폐쇄 도메인 작업에서 모델이 정보를 창조하는 경향을 평가하고, (2) TruthfulQA 데이터셋을 사용한다. 하지만 이것은 진실성의 일부분만을 포착할 수 있다.&lt;/p>
&lt;p>언어 모델의 해로움을 측정하는 것은 매우 어렵다. 이는 모델의 output이 실제 세계에서 어떻게 사용되는지에 따라 달라지기 때문이다. 예를 들어, toxic을 가진 output을 생성하는 모델은 챗봇에서는 해로울 수 있지만, toxic 감지 모델을 학습시키는데는 도움이 될 수 있다. 초기에는 &amp;ldquo;potentially harmful&amp;rdquo; output을 평가했으나, output이 결국 어떻게 사용될지에 대한 추측이 필요하다는 이유로 중단했다.&lt;/p>
&lt;p>모델의 해로운 행동을 평가하기 위해 특정 기준을 사용한다. 이는 고객 서비스 상황에서 부적절한 내용, 보호 계급에 대한 비하, 성적 또는 폭력적인 내용 등을 포함한다. 또한, bias와 toxic을 측정하는 여러 데이터셋에서도 모델을 테스트한다.&lt;/p>
&lt;p>요약하자면, 양적 평가를 두 가지 별도의 부분으로 나눌 수 있다:&lt;/p>
&lt;p>&lt;strong>Evaluations on API distribution.&lt;/strong> 주요 평가 지표는 학습 데이터와 같은 출처의 프롬프트 세트에 대한 사람들의 선호도이다. 학습에 사용되지 않은 고객들의 API 프롬프트와, GPT-3 모델을 위해 특별히 설계된 프롬프트에 대해 이를 측정한다. 각 모델의 출력이 기준 모델에 비해 얼마나 선호되는지 계산하며, 응답의 전반적인 품질을 1-7 Likert 척도로 평가한다. 또한 각 모델 output에 대해 다양한 메타데이터를 수집한다.&lt;/p>
&lt;p>&lt;strong>Evaluations on public NLP datasets.&lt;/strong> 언어 모델의 진실성, 독성, 편향 등의 안전성과 질문 응답, 독해, 요약 등의 NLP 작업 성능을 평가한다. 이를 위해 공개 데이터 세트를 사용하며, RealToxicityPrompts 데이터 세트에 대한 독성 평가도 수행한다. 또한, 모든 샘플링 기반 NLP 작업에서 모델의 샘플을 공개하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>주장의 근거를 명확히 보여주기 위해 API 프롬프트 분포 결과, 공개 NLP 데이터셋 결과, 그리고 질적 결과를 제공한다.&lt;/p>
&lt;h3 id="results-on-the-api-distribution">Results on the API distribution&lt;/h3>
&lt;p>&lt;strong>Labelers signiﬁcantly prefer InstructGPT outputs over outputs from GPT-3.&lt;/strong> 프롬프트 테스트 세트에서, 라벨러들은 모든 모델 크기에서 InstructGPT output을 선호한다. GPT-3 출력이 가장 성능이 떨어지며, 잘 구성된 few-shot 프롬프트를 사용하거나, 지도 학습을 통해 학습하거나, PPO를 이용해 비교 데이터에서 학습함으로써 성능을 크게 향상시킬 수 있다. 직접 비교해보면, InstructGPT 출력은 GPT-3 출력에 비해 85%의 시간을, few-shot GPT-3에 대해서는 71%의 시간을 선호한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure3.png"
width="964"
height="872"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure3_hu14d2974e2cc714a9475c64d21997d29c_141522_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure3_hu14d2974e2cc714a9475c64d21997d29c_141522_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="265px"
>&lt;/p>
&lt;p>API에서 GPT-3 모델에 제출된 프롬프트로 평가했을 때도 결과는 크게 변하지 않는다. 하지만, 모델 크기가 클수록 PPO-ptx 모델의 성능은 약간 떨어잔다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure4.png"
width="1084"
height="358"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure4_huee4a7b3043afe0469689d7de6787ef3c_64005_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure4_huee4a7b3043afe0469689d7de6787ef3c_64005_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>라벨러들은 InstructGPT output을 여러 구체적인 측면에서도 긍정적으로 평가하였다. GPT-3에 비해 InstructGPT output은 고객 서비스원의 맥락에서 더 적절하고, 지시사항을 더 잘 따르며, 잘못된 지시를 따르는 경우가 적고, 특정 도메인 작업에서 사실을 만들어내는 경우가 더 적다. 이것은 InstructGPT 모델이 GPT-3보다 더 신뢰할 수 있고 제어하기 쉽다는 것을 보여준다.&lt;/p>
&lt;p>&lt;strong>Our models generalize to the preferences of &amp;ldquo;held-out&amp;rdquo; labelers that did not produce any training data.&lt;/strong> 보류된 라벨러들도 학습 데이터를 만드는 데 사용한 작업자와 비슷한 순위 선호도를 가지고 있다. 특히, 모든 InstructGPT 모델들은 GPT-3 기준선을 크게 능가한다. 이는 모델이 학습 라벨러의 선호도에 overfit되지 않았음을 보여준다.&lt;/p>
&lt;p>&lt;strong>Public NLP datasets are not reﬂective of how our language models are used.&lt;/strong> 175B GPT-3 기준선을 FLAN과 T0 데이터셋에서 미세 조정하여 InstructGPT와 비교하였다. 이 모델들은 GPT-3보다 더 나은 성능을 보이지만, 잘 선택된 프롬프트를 가진 GPT-3와 동등하고, SFT 기준선보다는 성능이 떨어진다. 직접 비교해보면, 175B InstructGPT 모델 출력은 FLAN 모델에 대해 78 $\pm$ 4%의 시간을, T0 모델에 대해 79 $\pm$ 4%의 시간을 선호하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure5.png"
width="562"
height="444"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure5_huaa3cd66af90c0ce25289d1616892f5e1_27687_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure5_huaa3cd66af90c0ce25289d1616892f5e1_27687_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="126"
data-flex-basis="303px"
>&lt;/p>
&lt;p>InstructGPT 모델이 FLAN과 T0를 능가하는 이유는 두 가지이다. 첫째, 공개 NLP 데이터셋은 자동 메트릭으로 쉽게 평가할 수 있는 작업을 포착하기 위해 설계되었지만, 이러한 작업은 실제 사용자가 언어 모델을 사용하는 부분의 작은 비율을 차지한다. 둘째, 공개 NLP 데이터셋에서 실제 사용자가 관심을 가질 수 있는 다양한 입력을 얻는 것이 어렵다. 따라서, 가장 효과적인 instruction-following 모델은 두 유형의 데이터셋을 모두 결합할 것이다.&lt;/p>
&lt;h3 id="results-on-public-nlp-datasets">Results on public NLP datasets&lt;/h3>
&lt;p>&lt;strong>InstructGPT models show improvements in truthfulness over GPT-3.&lt;/strong> TruthfulQA 데이터셋에 대한 인간 평가에 따르면, PPO 모델은 GPT-3에 비해 사실적이고 유익한 output을 생성하는 데 있어 약간의 개선을 보인다. 이는 모델의 기본 행동이며, 특별한 지시 없이도 진실성이 향상된다. 그러나, 1.3B PPO-ptx 모델은 같은 크기의 GPT-3 모델에 비해 약간 성능이 떨어진다. GPT-3에 적대적으로 선택되지 않은 프롬프트만 평가해도, PPO 모델은 GPT-3보다 더 진실적이고 유익하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure6.png"
width="980"
height="484"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure6_hu9c3d0c3ac6083aa2a1a675947d987d8e_62058_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure6_hu9c3d0c3ac6083aa2a1a675947d987d8e_62058_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="485px"
>&lt;/p>
&lt;p>모델에게 정확한 답변을 확신하지 못할 때 &amp;ldquo;I have no comment&amp;quot;으로 응답하도록 지시하는 유용한 프롬프트를 제공한다. 이 경우, PPO 모델들은 거짓말을 자신 있게 하는 것보다는 사실적이고 무의미한 쪽으로 편향된다. 반면, 기존의 GPT-3 모델은 이런 면에서 그리 좋지 않다.&lt;/p>
&lt;p>PPO 모델들이 API 분포에서의 closed-domain 작업에서 더 적게 hallucinate하는 것은 우리의 진실성 개선을 입증한다.&lt;/p>
&lt;p>&lt;strong>InstructGPT shows small improvements in toxicity over GPT-3, but not bias.&lt;/strong> 먼저 RealToxicityPrompts 데이터셋에서 모델들을 평가한다. 이를 위해 모델 샘플을 Perspective API를 통해 실행하여 automatic toxicity 점수를 얻고, 라벨러에게 absolute toxicity, 프롬프트에 대한 relative toxicity 등을 평가받는다. 또한, 높은 input toxicity에서 모델의 성능을 더 잘 평가하기 위해 toxicity에 따라 프롬프트 샘플을 균일하게 추출한다. 이는 표준 프롬프트 샘플링과 다르므로 absolute toxicity 수치가 과대 평가된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/instructgpt/images/figure7.png"
width="1058"
height="530"
srcset="https://kurtkim.github.io/p/instructgpt/images/figure7_hu83144d63ff80035f18d179e3d9daa512_64062_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/instructgpt/images/figure7_hu83144d63ff80035f18d179e3d9daa512_64062_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="479px"
>&lt;/p>
&lt;p>안전하고 존중받는 출력을 생성하도록 지시받았을 때, InstructGPT 모델은 GPT-3보다 덜 toxicity 있는 출력을 생성한다. 하지만, 이 장점은 존중받는 프롬프트가 제거될 때 사라진다. 명시적으로 toxicity 있는 출력을 생성하도록 지시받았을 때, InstructGPT output은 GPT-3보다 훨씬 더 toxicity이 있다.&lt;/p>
&lt;p>&amp;ldquo;respectful prompt&amp;rdquo; 설정에서 InstructGPT는 GPT-3보다 덜 toxicity이 있지만, &amp;ldquo;no prompt&amp;rdquo; 설정에서는 비슷한 성능을 보인다. 모든 모델은 프롬프트를 고려할 때 예상보다 덜 toxicity이 있다. SFT 기준선 모델은 가장 덜 toxicity이 있지만, 연속성이 가장 낮고 가장 선호되지 않는 것으로 나타났다.&lt;/p>
&lt;p>모델이 편향된 언어를 생성하는 경향을 평가하기 위해, Winogender와 CrowS-Pairs 데이터셋에서 InstructGPT를 평가하였다. 이 데이터셋들은 잠재적인 편향을 강조할 수 있는 문장 쌍으로 구성되어 있다. 완벽하게 편향되지 않은 모델들은 각 쌍의 문장 사이에 선호도가 없을 것이다. 하지만, 모델들은 GPT-3보다 덜 편향되어 있지 않다. PPO-ptx 모델은 GPT-3와 비슷한 편향을 보이며, 존중받는 행동을 지시받았을 때는 더 높은 편향을 보인다.&lt;/p>
&lt;p>&lt;strong>We can minimize performance regressions on public NLP datasets by modifying our RLHF ﬁne-tuning procedure.&lt;/strong> API 분포에서 PPO 모델을 학습시키면, 여러 공개 NLP 데이터셋에서의 성능이 감소하는 &amp;ldquo;alignment tax&amp;rdquo; 문제가 발생한다. 이는 더 능력 있는 모델을 사용하도록 유인하지만, 이 모델들은 alignment되지 않았다. 따라서 alignment tax를 피하는 절차가 필요하다.&lt;/p>
&lt;p>사전 학습 업데이트를 섞는 것은 KL coefﬁcient를 증가시키는 것보다 더 나은 성능을 보여준다. 사전 학습 mix coefﬁcient의 적절한 값이 SQuADv2와 DROP에서의 성능 저하를 뒤집고, 검증 보상에서의 감소를 최소화한다. 반면, KL coefﬁcient를 증가시키는 것은 검증 보상에서 큰 감소를 초래하고, DROP과 SQuAD에서 완전히 회복하지 못한다.&lt;/p>
&lt;h3 id="qualitative-results">Qualitative results&lt;/h3>
&lt;p>&lt;strong>InstructGPT models show promising generalization to instructions outside of the RLHF ﬁnetuning distribution.&lt;/strong> InstructGPT는 비영어 언어로 된 지시사항을 따르고, 코드에 대한 요약과 질문 답변을 수행하는 능력을 보여준다. 이는 비영어 언어와 코드가 미세 조정 데이터의 극히 일부를 차지함에도 불구하고, 이러한 일부 경우에서 모델이 사람들이 직접 감독하지 않은 입력에 대해 원하는 행동을 생성하는 것으로 일반화될 수 있음을 보여준다.&lt;/p>
&lt;p>175B PPO-ptx 모델은 코드에 대한 질문에 신뢰성 있게 답하며, 다른 언어로 된 지시사항도 따를 수 있다. 하지만, 지시사항이 다른 언어인 경우에도 종종 영어로 출력을 생성한다. 반면, GPT-3는 더 신중한 프롬프팅이 필요하며, 이러한 영역에서 지시사항을 따르는 경우는 드물다.&lt;/p>
&lt;p>&lt;strong>InstructGPT still makes simple mistakes.&lt;/strong> 175B PPO-ptx 모델은 강력한 성능에도 불구하고 간단한 실수를 할 수 있다. 예를 들어, 거짓 전제로 된 지시를 받았을 때 모델은 잘못된 전제가 참이라고 가정할 수 있다. 또한, 간단한 질문에 대해 모델은 과도하게 회피할 수 있다. 또한, 지시사항에 여러 명확한 제약조건이 포함되어 있거나, 제약조건이 언어 모델에게 도전적일 수 있을 때 모델의 성능이 저하된다. 이는 모델의 한계를 보여준다.&lt;/p>
&lt;p>특정 행동이 지식적 겸손성을 보상하도록 지시하기 때문에, 그리고 거짓 전제를 가정하는 프롬프트가 학습 세트에 거의 없기 때문에 일부 문제가 발생한다고 생각한다. 이 두 가지 행동 모두가 적대적 데이터 수집을 통해 크게 줄어들 수 있을 것이라고 믿는다. 이러한 발견은 모델 학습과 데이터 수집 전략의 중요성을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;h3 id="implications-for-alignment-research">Implications for alignment research&lt;/h3>
&lt;p>이 연구는 AI 시스템을 인간의 의도와 일치시키는 보다 광범위한 연구 프로그램의 일부이다. 이 작업은 현재의 언어 모델 시스템에 초점을 맞추고 있지만, 미래의 AI 시스템에 대해 작동하는 확장 가능한 방법을 찾고 있다. 시스템들은 아직 제한적이지만, 다양한 언어 작업에 적용되며, AI의 발전과 인간의 의도와의 일치를 추구하는 노력의 일환이다.&lt;/p>
&lt;p>이 연구에서는 현재의 AI 시스템의 alignment를 개선하는데 초점을 맞추는 반복적인 접근법을 사용하고 있다. 이 접근법의 단점은 초인적 시스템을 alignment할 때만 발생하는 alignment 문제를 직접적으로 마주치지 않는다는 것이다. 그러나, 이 접근법은 무엇이 작동하고 무엇이 작동하지 않는지에 대한 명확한 피드백 루프를 제공하며, 이는 alignment 기법을 개선하는 데 필수적이다. 또한, 여기서 사용하는 alignment 기법인 RLHF는 초인적 시스템을 alignment하는 여러 제안의 중요한 구성 요소이다. 이러한 접근법은 AI와 사람의 의도와의 일치를 추구하는 노력의 일환이다.&lt;/p>
&lt;p>이 작업에서, 좀 더 일반적으로 alignment 연구에 대한 교훈을 얻을 수 있다:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>모델 alignment를 늘리는 비용은 사전 학습에 비해 상대적으로 적다.&lt;/strong> 데이터 수집 비용과 학습 비용은 GPT-3 학습 비용의 일부에 불과하며, 동시에 RLHF는 언어 모델을 사용자에게 더 도움이 되도록 만드는데 매우 효과적이다. 이는 현재 기존 언어 모델의 alignment에 투자를 늘리는 것이 더 큰 모델을 학습시키는 것보다 더 비용 효율적이라는 것을 시사한다. 이러한 결과는 비용 효율적인 AI 연구와 개발에 대한 중요성을 보여줍니다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>InstructGPT가 감독하지 않는 설정에서도 &amp;ldquo;following instructions&amp;rdquo; 것을 일반화하는 몇 가지 증거를 보았다.&lt;/strong> InstructGPT는 비영어 언어 작업과 코드 관련 작업 등, 직접 감독하지 않는 환경에서도 지시사항을 따르는 것을 일반화하는 경향이 있다. 이는 모든 작업에 대해 인간 감독이 과도한 비용이 들기 때문에 중요한 특성이다. 이러한 일반화가 능력 향상과 얼마나 잘 확장되는지에 대한 추가 연구가 필요하다. 이러한 발견은 AI의 범용성을 보여준다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>미세 조정에 의해 도입된 대부분의 성능 저하를 완화할 수 있었다.&lt;/strong> 미세 조정으로 인한 성능 저하를 대부분 완화할 수 있었다. 이것이 사실이 아니었다면, 이는 모델을 alignment하는 데 추가 비용인 &amp;ldquo;alignment tax&amp;quot;을 의미할 수 있다. 인간의 의도와 일치하지 않는 AI 시스템을 피하기 위해, alignment tax이 낮은 alignment 기술이 필요하다. 이 관점에서, 우리의 결과는 RLHF가 낮은 세금의 alignment 기술로서 좋은 소식이다. 이는 AI 모델을 인간의 의도와 일치시키는 데 추가 비용을 최소화하는 데 중요하다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>실제 세계에서 연구로부터 얻은 alignment 기법을 검증하였다.&lt;/strong> alignment 연구는 전통적으로 추상적이었지만, 이 연구는 실제 세계에서 고객과 함께 사용되는 AI 시스템에서 alignment 연구에 기반을 제공한다. 이를 통해 기술의 효과와 한계에 대한 중요한 피드백 루프를 구축할 수 있다. 이러한 발견은 실제 환경에서의 AI alignment 연구의 중요성을 보여준다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="who-are-we-aligning-to">Who are we aligning to?&lt;/h3>
&lt;p>언어 모델을 인간의 의도와 일치시킬 때, 그들의 최종 행동은 기본 모델, 미세 조정 데이터, 그리고 사용된 정렬 방법에 의해 결정된다. 이를 통해 우리는 무엇과 누구에게 alignment할지 결정하며, 개선할 영역을 고려한 후 우리의 작업의 한계에 대해 더 깊이 있게 논의한다.&lt;/p>
&lt;p>문헌에서는 &amp;ldquo;human preferences&amp;rdquo; 또는 &amp;ldquo;human values&amp;quot;라는 용어로 alignment을 설명한다. 하지만, 이 연구에서는 라벨러들의 선호도를 지시사항, 작업 맥락, 지시를 받은 사람 등에 따라 alignment 하였다. 이에는 몇 가지 중요한 주의사항이 있다:&lt;/p>
&lt;p>첫째, 학습 라벨러들이 제공하는 시연과 선호도에 맞추고 있다. 이들 라벨러들은 대부분 Upwork 또는 Scale AI를 통해 고용된 미국이나 동남아에 거주하는 영어를 사용하는 사람들이다. 그들은 많은 예제에서 서로에게 동의하지 않는다; 라벨러 간의 동의도는 약 73%이다. 이러한 점들은 AI 모델의 alignment와 정확도에 중요한 영향을 미찬다.&lt;/p>
&lt;p>둘째, 라벨러들이 시연을 작성하고 선호하는 output을 선택할 때 가이드로 사용하는 지시사항을 작성하고, 가장자리 사례에 대한 질문에 답한다. 다른 지시 세트와 인터페이스 디자인이 데이터 수집과 최종 모델 행동에 미치는 영향에 대한 추가 연구가 필요하다. 이는 AI 모델이 인간의 의도와 얼마나 잘 일치하는지를 결정하는 데 중요한 요소이다.&lt;/p>
&lt;p>셋째, 학습 데이터는 OpenAI 고객들이 보낸 프롬프트에 기반하므로, 고객들이 가치 있다고 생각하는 것과, 경우에 따라서는 그들의 최종 사용자가 가치 있다고 생각하는 것에 암시적으로 일치하고 있다. 그러나 고객과 그들의 최종 사용자는 의견이 다를 수 있다. 실제로, 라벨러들은 주어진 프롬프트나 완성이 어떤 맥락에서 사용될지 알 수 없다. 이러한 점들은 AI 모델의 정확도와 효과성에 영향을 미친다.&lt;/p>
&lt;p>넷째, OpenAI의 고객들은 모든 언어 모델의 잠재적이거나 현재의 사용자들을 대표하지 않는다. 이 프로젝트의 대부분 기간 동안, OpenAI API의 사용자들은 대기 목록에서 선택되었다. 이 대기 목록의 초기 사용자들은 OpenAI의 직원들이었으므로, 최종 사용자 그룹이 우리 자신의 네트워크로 편향되어있다. 이러한 점들은 AI 모델의 대표성과 공정성에 영향을 미친다.&lt;/p>
&lt;p>공정하고 투명하며 적절한 책임 메커니즘이 있는 alignment 프로세스를 설계하는 것은 많은 어려움이 있다. 이 논문의 목표는 alignment 기법이 특정 애플리케이션에 대한 특정 인간 참조 그룹에 맞출 수 있다는 것을 보여주는 것이다. 그러나 모델을 학습시키는 조직, 모델을 사용하여 제품을 개발하는 고객, 이러한 제품의 최종 사용자, 그리고 직접적이거나 간접적으로 영향을 받을 수 있는 더 넓은 인구 등 많은 이해당사자들을 고려해야 한다. 모든 사람의 선호도에 맞춰진 시스템을 학습시키는 것은 불가능하며, 모든 사람이 타협을 승인할 수 없다. 이러한 점들은 AI 모델의 공정성과 책임성에 중요한 영향을 미친다.&lt;/p>
&lt;p>한 가지 방법은 특정 그룹의 선호도에 따라 조건화될 수 있는 모델을 학습시키거나, 다른 그룹을 대표하도록 쉽게 미세 조정할 수 있는 모델을 학습시키는 것이다. 그러나 이러한 모델들은 여전히 사회에 영향을 미칠 수 있으며, 어떤 선호도에 조건을 부여할 것인지, 모든 그룹이 대표될 수 있도록 하는 등 많은 어려운 결정을 내려야 한다. 이는 AI 모델의 공정성과 표현성에 중요한 영향을 미친다.&lt;/p>
&lt;h3 id="limitations">Limitations&lt;/h3>
&lt;p>&lt;strong>Methodology.&lt;/strong> InstructGPT 모델의 행동은 계약자들로부터 얻은 인간의 피드백에 부분적으로 결정된다. 약 40명의 계약자를 고용하였고, 그들은 민감한 프롬프트를 식별하고 대응하는 능력에 따라 선발되었다. 그러나 이 그룹은 모델을 사용하고 영향을 받을 전체 사람들을 대표하지 않는다. 간단한 예로, 라벨러들은 주로 영어를 사용하며, 데이터는 거의 완전히 영어 지시사항으로 구성되어 있다. 이는 AI 모델의 다양성과 대표성에 중요한 영향을 미친다.&lt;/p>
&lt;p>데이터 수집 구성을 개선할 수 있는 여러 방법이 있다. 대부분의 비교는 비용 문제로 1명의 계약자만에 의해 라벨링된다. 예제를 여러 번 라벨링하면 계약자들 간의 이견을 식별하고, 이견이 있는 경우, 평균 라벨러 선호도에 맞추는 것이 바람직하지 않을 수 있다. 특히, 소수 그룹에 불균형하게 영향을 미치는 텍스트를 생성할 때, 해당 그룹에 속하는 라벨러들의 선호도를 더 무겁게 가중할 수 있다. 이는 AI 모델의 공정성과 정확성에 중요한 영향을 미친다.&lt;/p>
&lt;p>&lt;strong>Models.&lt;/strong> 모델들은 완전히 일치하거나 완전히 안전하지 않는다. 그들은 toxic이 있거나 편향된 결과를 생성하고, 사실을 만들어내며, 명확한 프롬프트 없이 성적이거나 폭력적인 내용을 생성할 수 있다. 또한 일부 입력에 대해 합리적인 output을 생성하지 못할 수 있다. 이러한 점들은 AI 모델의 안전성과 신뢰성에 중요한 영향을 미친다.&lt;/p>
&lt;p>모델의 가장 큰 제한은 대부분의 경우 사용자의 지시를 따르는 것이며, 이것이 실제 세계에서 해를 끼칠 수 있음에도 불구하고 그렇다. 예를 들어, 편향성을 최대화하라는 지시가 주어질 경우, InstructGPT는 toxic이 있는 output을 더 많이 생성한다. 이러한 문제에 대한 해결방안은 이후 섹션에서 논의된다. 이러한 점들은 AI 모델의 안전성과 윤리성에 중요한 영향을 미친다.&lt;/p>
&lt;h3 id="open-questions">Open questions&lt;/h3>
&lt;p>이 작업은 다양한 지시를 따르도록 언어 모델을 미세 조정하는 첫 단계이다. 사람들이 실제로 원하는 것과 언어 모델의 행동을 더욱 일치시키기 위한 미해결된 질문들이 많이 있다. 이는 AI 모델의 사용성과 효과성을 향상시키는 데 중요한 단계이다.&lt;/p>
&lt;p>모델이 toxic을 가지거나 편향된 또는 다른 방식으로 해롭게 출력하는 경향을 줄이기 위한 다양한 방법들이 있다. 예를 들어, 모델의 최악의 행동을 찾는 적대적인 설정이나, 사전 학습 데이터를 필터링하는 방법, 또는 모델의 진실성을 향상시키는 방법들을 사용할 수 있다. 이러한 접근법들은 AI 모델의 안전성과 신뢰성을 향상시키는 데 중요하다.&lt;/p>
&lt;p>이 연구에서, 사용자가 잠재적으로 해롭거나 부정직한 응답을 요청하면, 모델이 이러한 output을 생성하도록 허용한다. 하지만 모델을 무해하게 학습시키는 것은 중요하며, 출력이 해로운지 여부는 배포 맥락에 따라 달라지므로 어렵다. 이 연구의 기법은 특정 사용자 지시를 거부하도록 모델을 만드는 데도 적용될 수 있으며, 이는 후속 연구에서 탐구할 계획이다. 이는 AI 모델의 안전성과 윤리성에 중요한 영향을 미친다.&lt;/p>
&lt;p>모델들이 원하는 것을 수행하게 하는 것은 steerability 및 controllability과 직접적으로 관련이 있다. 유망한 미래의 방향은 RLHF를 steerability의 다른 방법들과 결합하는 것입니다, 예를 들어 control 코드를 사용하거나 추론 시간에 샘플링 절차를 수정하는 것 등이 있다. 이는 AI 모델의 controllability과 유연성을 향상시키는 데 중요하다.&lt;/p>
&lt;p>주로 RLHF에 초점을 맞추지만, 더 나은 결과를 얻기 위해 다른 알고리즘도 사용될 수 있다. 예를 들어, 전문가 반복을 연구하거나, 비교 데이터의 일부를 사용하는 단순한 행동 복제 방법을 시도해 볼 수 있다. 또한, 소수의 해로운 행동을 생성하는 것에 조건을 부여한 보상 모델에서 점수를 최대화하는 constrained optimization 접근법을 시도해 볼 수도 있다. 이는 AI 모델의 효율성과 성능에 중요하다.&lt;/p>
&lt;p>비교는 반드시 alignment 신호를 제공하는 가장 효율적인 방법은 아니다. 모델의 응답을 수정하거나, 자연어로 모델의 응답에 대한 비평을 생성하는 등 다른 방법을 사용할 수 있다. 또한, 라벨러들이 언어 모델에 피드백을 제공하는 인터페이스를 설계하는 것은 흥미로운 인간-컴퓨터 상호작용 문제이다. 이는 AI 모델의 효율성과 사용성을 향상시키는 데 중요하다.&lt;/p>
&lt;p>사전 학습 데이터를 RLHF 미세 조정에 통합함으로써 alignment tax를 완화하는 제안은 성능 회귀를 완전히 완화하지 않으며, 특정 태스크에서 원치 않는 행동을 더욱 가능하게 할 수 있다. 이는 더욱 연구할 만한 흥미로운 영역이다. 또한, 사전 학습 혼합 데이터에서 toxic 내용을 필터링하거나, 이 데이터를 합성 지시사항으로 보강하는 것이 우리의 방법을 개선할 수 있다. 이는 AI 모델의 성능과 안전성을 향상시키는 데 중요하다.&lt;/p>
&lt;p>지시, 의도, 드러난 선호도, 이상적인 선호도, 이해관계, 그리고 가치에 맞추는 것 사이에는 미묘한 차이가 있다. 원칙 기반의 정렬 방식을 주장하며, &amp;ldquo;사람들의 도덕적 신념에서 널리 변동이 있음에도 불구하고 반사적인 승인을 받는 공정한 정렬 원칙&amp;quot;을 식별하는 것이 중요하다. 이 분야에서는 더 많은 연구가 필요하며, 특히 투명하고, 사람들의 가치를 의미있게 대표하고 통합하는 정렬 과정을 어떻게 설계할 것인지가 주요한 미해결 질문이다. 이는 AI 모델의 공정성과 윤리성에 중요하다.&lt;/p>
&lt;h3 id="broader-impacts">Broader impacts&lt;/h3>
&lt;p>이 연구는 대규모 언어 모델의 긍정적인 영향을 증가시키기 위해 특정 인간 그룹이 원하는 행동을 모델에 학습시키는 것을 목표로 한다. 언어 모델을 더 도움되고, 진실되고, 무해하게 만드는 데 이 연구의 기법이 유망함을 나타낸다. 장기적으로, 모델 alignment 실패는 더 심각한 결과를 초래할 수 있다. 모델 확장이 계속됨에 따라, 인간의 의도와 일치하도록 하는 데 더 큰 주의가 필요하다. 이는 AI 모델의 유용성과 안전성을 향상시키는 데 중요하다.&lt;/p>
&lt;p>언어 모델을 사용자의 의도를 더 잘 따르도록 만드는 것은 그것들을 더 쉽게 오용하는 것을 가능하게 한다. 이로 인해 잘못된 정보를 생성하거나, 혐오스럽거나 폭력적인 내용을 생성하는 것이 더 쉬울 수 있다. 이는 AI 모델의 안전성과 윤리성에 중요하다.&lt;/p>
&lt;p>alignment 기법은 대규모 언어 모델의 안전 문제를 해결하는 만병통치약이 아니며, 더 넓은 안전 생태계의 일부로 사용되어야 한다. 많은 분야에서는 대규모 언어 모델이 신중하게, 또는 전혀 배포되지 않아야 한다. 모델이 오픈 소스화되면, 해로운 사용을 제한하는 것이 어려워진다. 반면, 대규모 언어 모델 접근이 몇몇 조직에 제한되면, 대부분의 사람들이 최첨단 ML 기술에 접근할 수 없게 된다. 또 다른 옵션은 조직이 모델 배포의 인프라를 소유하고, API를 통해 접근 가능하게 하는 것이다. 이는 안전 프로토콜의 구현을 가능하게 하지만, 감소된 투명성과 증가된 권력 집중의 비용이 발생할 수 있다. 이는 AI 모델의 안전성과 공정성에 중요하다.&lt;/p>
&lt;p>모델이 누구에게 alignment되는지는 매우 중요하며, 이것은 모델의 순수한 영향이 긍정적인지 부정적인지를 크게 영향을 미친다. 이는 AI 모델의 공정성과 윤리성에 중요하다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/following-instructions-human-feedback" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Minerva</title><link>https://kurtkim.github.io/p/minerva/</link><pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/minerva/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델은 자연어 이해를 필요로 하는 작업에서 탁월한 성과를 보였지만, 수량적 추론을 필요로 하는 작업에서는 어려움을 겪었다. 이를 해결하기 위해, 일반 자연어 데이터에 대해 사전 학습된 후 기술적인 내용에 대해 추가 학습된 Minerva라는 큰 언어 모델을 제안한다. 이 모델은 외부 도구 없이도 기술 벤치마크에서 state-of-the-art를 보여주며, 물리학, 생물학, 화학, 경제학 등 대학 수준의 문제 200개 이상을 풀어보았을 때, 그 중 거의 1/3을 정확하게 해결할 수 있었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>인공 신경망은 다양한 분야에서 큰 성과를 내었습니다. 특히, 거대 언어 모델은 다양한 자연어 작업에서 우수한 성능을 보였지만, 수학이나 과학 등 정량적 추론을 필요로 하는 문제 해결에서는 어려움을 겪었다.&lt;/p>
&lt;p>정량적 추론 문제는 언어 모델의 다양한 능력을 평가하는 중요한 분야이다. 이는 자연어 이해, 세계 지식 회상, 계산 알고리즘 적용, 수학 토큰 조작 등의 능력을 요구하며, 과학과 기술 분야에서 사람들의 작업을 지원하는 견고한 정량적 추론 해결사로서의 연구를 검증하는 기회를 제공한다.&lt;/p>
&lt;p>이전 연구에서는 대규모 언어 모델이 특정 도메인 데이터셋에서 학습 후 수학과 프로그래밍 문제에서 뛰어난 성능을 보여주었다. 이 연구에서는 이런 접근법을 외부 도구 없이 독립적인 해결책을 제공해야 하는 정량적 추론 문제에 적용하였고, 이는 수학, 과학, 공학 문제 등을 포함한다.&lt;/p>
&lt;h3 id="our-contribution">Our Contribution&lt;/h3>
&lt;p>Minerva라는 언어 모델을 제안한다. 이 모델은 자연어로 표현된 과학 및 수학 문제를 처리하고, 올바른 LATEX 표기법으로 단계별 해답을 생성하는 능력을 보여주며, 여러 정량적 추론 작업에서 뛰어난 성능을 보여주었다.&lt;/p>
&lt;p>Minerva는 과학과 수학 데이터를 포함하는 고품질 데이터셋으로 추가 학습된 PaLM 언어 모델을 기반으로 한다. 우리는 사전 학습된 모델을 사용하여 기술 데이터셋에서 학습을 계속하였고, MATH, GSM8k, MMLU 데이터셋 등에서 최고 수준의 성능을 보였다. 이 모델은 이러한 평가 데이터셋에서 명시적인 학습 없이도 강인한 성능을 보여주었다.&lt;/p>
&lt;p>이 논문의 핵심 novelty는 자연어와 형식적 수학 언어를 병행하는 대규모 학습 데이터셋이다. 이 데이터는 arXiv와 신중하게 처리된 웹 페이지에서 수집되었다. 이 연구는 데이터 품질과 모델 크기를 향상시킴으로써 정량적 추론 벤치마크에서 달성 가능한 성능에 대한 새로운 기준을 설정하였다.&lt;/p>
&lt;p>정량적 추론 벤치마크의 범위를 확장하기 위해, MIT의 OpenCourseWare에서 과학과 수학의 대학 수준 문제 200개 이상으로 데이터셋을 구축하였다. 이를 통해 순수 수학적 환경을 넘어 우리 모델의 사고 과정에서의 정량적 추론 능력을 측정하였다.&lt;/p>
&lt;h3 id="related-works">Related Works&lt;/h3>
&lt;p>자연어로 표현된 정량적 추론 문제를 해결하는 것은 활발히 연구되는 분야이다. 스크래치패드나 사고의 연결 고리를 사용한 프롬프트 언어 모델은 보이지 않는 문제의 단계별 해결책을 출력할 수 있다. GSM8k 작업은 모델 출력을 재정렬하기 위해 학습된 검증자를 사용하면 성능이 향상될 수 있음을 보여주었다. 이 연구에서는 외부 도구에 의존하지 않는 독립적인 모델에 초점을 맞추었다.&lt;/p>
&lt;p>언어 모델을 평가하는 표준 방법은 문제 당 한 가지 해결책을 탐욕적으로 샘플링하는 것이다. 하지만 최근 연구에서는 문제 당 여러 해결책을 샘플링하고 필터링하는 것이 더 유리하다는 것을 보여주었다. 특히, 다수결 투표 방식이 탐욕적 디코딩보다 성능을 크게 향상시킨다는 것을 확인하였다.&lt;/p>
&lt;p>Drori et al. (2021)은 OpenAI의 davinci-002 모델을 MATH 데이터셋의 일부로 평가하였다. 하지만 문제의 하위 집합에 초점을 맞추고 문제 형식의 변경으로 인해, 이 연구와 논문의 결과를 직접 비교하는 것은 어렵다.&lt;/p>
&lt;p>&lt;strong>Code generation.&lt;/strong> 코드 생성 모델을 수학 문제에 적용하는 것은 활발한 연구 분야이다. PaLM은 학습 데이터셋에 코드가 포함된 거대 언어 모델이 좋은 성능을 보일 수 있음을 보여주었고, Codex 모델은 MATH 문제에 대한 코드 해결책을 생성할 수 있다. 이러한 해결책들은 외부 라이브러리에 의존하지만, 이 논문의 접근법은 모델이 자체 추론 능력만으로 답을 도출하는 능력을 직접 연구한다.&lt;/p>
&lt;p>&lt;strong>Formal mathematics.&lt;/strong> 수학은 자연어를 기반으로 발전했지만, 공리적인 기초를 통해 수학적 사고를 시뮬레이션할 수 있다. 이는 Coq, Isabelle, HOL4, Lean, Metamath, Mizar 같은 특수 프로그래밍 언어를 통해 가능하며, 이들은 컴퓨터를 이용한 논리적, 수학적 사고의 시뮬레이션을 지원한다. 또한, 증명 보조 도구와 자동 정리 증명기의 자동화에 대한 연구는 기계 학습 방법과의 통합으로 큰 이익을 얻었다.&lt;/p>
&lt;p>&lt;strong>Language models applied to formal and synthetic mathematical problems.&lt;/strong> 이전 연구에서는 언어 모델을 학습시켜 수학적 표현을 예측하는 방법을 사용하였다. 이러한 예측 모델은 증명 검색을 안내하는 데 사용할 수 있다. 거대 언어 모델은 자연어 모델링에 뛰어나지만, 형식 언어의 경우, 수학 공식의 그래프 구조 정보를 유지하는 모델, 예를 들어 GNNs,이 여전히 경쟁력이 있다.&lt;/p>
&lt;p>&lt;strong>Modelling mathematics as a discipline of natural language.&lt;/strong> 새로운 벤치마크 데이터셋은 고급 수학 주제를 포함하며, 이 분야에서 언어 모델은 다른 유형의 모델로부터 제한적인 경쟁을 받고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="training-and-evaluation">Training and Evaluation&lt;/h2>
&lt;h3 id="mathematical-training-dataset">Mathematical Training Dataset&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table1.png"
width="1126"
height="176"
srcset="https://kurtkim.github.io/p/minerva/images/table1_huf80f382bf912273569a8cab6823b82ea_38036_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table1_huf80f382bf912273569a8cab6823b82ea_38036_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="639"
data-flex-basis="1535px"
>&lt;/p>
&lt;p>Minerva 모델은 수학적 내용으로 필터링된 웹페이지와 arXiv 사전 인쇄 서버의 논문에서 추출한 데이터셋에서 학습되었다. 이 데이터셋은 일반적인 자연어 데이터도 포함하고 있다. 수학 웹페이지 데이터셋은 MathJax 형식의 수학 표현이 있는 페이지를 수집하여 만들었고, 대부분의 HTML 태그를 제거하지만 수학 표기법을 유지하는 과정을 거쳤다. 이로 인해 모델은 학습 중에 전체 수학 공식을 볼 수 있으며, 계산과 기호 조작을 요구하는 작업에서 잘 수행하게 된다.&lt;/p>
&lt;h3 id="models-and-training-procedure">Models and Training Procedure&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table2.png"
width="894"
height="176"
srcset="https://kurtkim.github.io/p/minerva/images/table2_hud92a0004aa2106af00d2120898e8b2e4_39164_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table2_hud92a0004aa2106af00d2120898e8b2e4_39164_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="507"
data-flex-basis="1219px"
>&lt;/p>
&lt;p>이 논문의 방법은 PaLM 사전 학습된 decoder-only transformer 언어 모델로 시작하여, 이를 autoregressive 목표를 사용하여 수학 데이터셋에서 더욱 학습(미세 조정)하는 것이다. 가장 큰 모델은 540B parameter를 가지며, 26B 토큰에서 미세 조정되었다. 이 모델은 학습이 부족하지만, 우수한 성능을 보여주었다.&lt;/p>
&lt;h3 id="evaluation-datasets">Evaluation Datasets&lt;/h3>
&lt;p>주로 few-shot 평가에 초점을 맞추며, 평가를 위해 입력을 1024 토큰으로 자르고 모델을 사용하여 최대 512 토큰을 생성한다. 문제당 한 번 샘플링할 때에는 탐욕적으로, 여러 번 샘플링할 때에는 핵심 샘플링을 사용한다. 생성 작업에서, 모델은 사고의 연결 고리를 답변으로 생성하고 최종 답변을 표시하며, 최종 답변이 실제 답변과 일치하면 해결책을 올바르다고 평가한다. 정확성 평가는 SymPy 라이브러리를 사용하여 수학적으로 동등한 답변을 올바르게 식별한다.&lt;/p>
&lt;p>기존 데이터셋들은 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>MATH: 주로 사용하는 데이터셋은 Hendrycks et al. (2021)이 제시한 중고등학교 수학 문제 12K 데이터셋이다. 문제 설명은 LATEX로 작성되어 있으며, 네 개의 무작위 예제를 포함하는 고정 4-shot 프롬프트로 모델을 프롬프트한다.&lt;/li>
&lt;li>GSM8k: 중학교 수학 단어 문제 데이터셋은 Cobbe et al. (2021)이 제시했으며, 모델은 Wei et al. (2022)의 사고의 연결 고리 프롬프트를 사용하여 평가된다.&lt;/li>
&lt;li>MMLU-STEM: 과학, 기술, 공학, 수학 (STEM)에 초점을 맞춘 MMLU 데이터셋의 일부를 사용한다. 각 작업에 대해 5-shot 프롬프트를 사용하고, 단계별 해결책이 포함된 예제로 모델을 프롬프트한다. 수학적 추론을 포함하는 주제에 대해 객관식 MATH 프롬프트를 사용하고, 나머지 주제에 대해 단계별 해결책을 추가한 5-shot 프롬프트를 사용한다.&lt;/li>
&lt;/ul>
&lt;h3 id="undergraduate-level-stem-problems">Undergraduate-Level STEM Problems&lt;/h3>
&lt;p>Minerva의 과학적 추론 능력을 평가하기 위해, 대학 수준의 STEM 문제 세트를 수집하였다. 이 문제들은 대부분 다단계 추론을 포함하고 있다. MIT의 공개 강좌 자료를 사용하여 자동으로 검증 가능한 해결책을 가진 문제들을 수집하였다. 총 272개의 문제를 수집했으며, 이 중 191개는 numeric solution을 가지고 81개는 symbolic solution을 가진다.&lt;/p>
&lt;h3 id="inference-time-techniques">Inference-Time Techniques&lt;/h3>
&lt;p>여러 해결책을 샘플링하고 다수결로 하나를 선택함으로써 탐욕적 디코딩을 상당히 능가할 수 있다는 것을 발견하였다. 이는 가장 흔한 답변을 선택하는 방법으로, maj1@k라고 표시한다. 이 알고리즘의 변형은 가장 흔한 답변 $n$개를 선택하는 것을 포함한다. 이 방법이 성능을 향상시키는 이유는 일반적으로 올바른 답변 방법이 매우 적기 때문이다.&lt;/p>
&lt;p>다수결과 pass@k를 비교하면, pass@k는 $k$개의 샘플 중 하나가 문제를 해결하면 작업이 해결된 것으로 간주된다. 반면, 다수결 성능은 빠르게 포화되며, MATH의 경우 $k = 64$, GSM8k의 경우 $k = 16$에서 이미 대부분의 정확도를 달성하였다. 이는 다수결이 모델링된 분포에서 가장 흔한 답변을 선택하기 때문이며, pass@k의 성능 향상은 분포의 꼬리에서 발생하므로 $k$가 증가함에 따라 계속 개선될 수 있다.&lt;/p>
&lt;p>Log-likelihood는 샘플을 재정렬하는 데 사용할 수 있는 또 다른 지표이다. 우리는 다수결이 Log-likelihood 재정렬보다 훨씬 더 잘 수행된다는 것을 발견하였다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/figure4.png"
width="1232"
height="512"
srcset="https://kurtkim.github.io/p/minerva/images/figure4_hubac6b2a53cf43efbd7e3d93d43dc83f5_102071_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/figure4_hubac6b2a53cf43efbd7e3d93d43dc83f5_102071_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="577px"
>&lt;/p>
&lt;p>MMLU 평가는 주제별로 표준 5-shot 프롬프트를 사용하고 가장 높은 점수의 답변을 선택하며, 다수결로 평가할 때는 사고의 연결 고리 프롬프트를 사용하여 16개의 모델 답변을 샘플링한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table3.png"
width="962"
height="472"
srcset="https://kurtkim.github.io/p/minerva/images/table3_hu53fb9a6500ea6d5812a664d1596e7023_103625_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table3_hu53fb9a6500ea6d5812a664d1596e7023_103625_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="203"
data-flex-basis="489px"
>&lt;/p>
&lt;p>Minerva 62B를 폴란드의 국가 수학 시험에 적용해 보았는데, 이는 2021년 국가 평균인 57%의 점수를 달성하였고, 540B 모델은 65%의 점수를 달성하였다.&lt;/p>
&lt;p>최신 OpenAI 언어 모델인 davinci-002의 결과를 포함시켰고, 이는 모든 기술적 작업에서 state-of-the-art를 보였다. 대부분의 작업들에서 이전 결과에 비해 상당한 향상이 있었다.&lt;/p>
&lt;p>이 논문은 few-shot 평가에 집중했고, Minerva를 MATH에서 미세 조정해 보았지만 개선 사항을 발견하지 못하였다. 그러나, MATH에서 PaLM을 미세 조정할 때는 상당한 개선이 있었다. 이는 비지도 학습 데이터셋의 품질과 다양성이 향상됨에 따라 표준 미세 조정의 효용성이 감소한다는 것을 보여준다.&lt;/p>
&lt;h3 id="basic-arithmetic">Basic arithmetic&lt;/h3>
&lt;p>Minerva 540B가 10자리 수 덧셈에서 80% 이상, 18자리 수 덧셈에서 20% 이상의 정확도를 보였다.&lt;/p>
&lt;hr>
&lt;h2 id="performance-analysis">Performance Analysis&lt;/h2>
&lt;h3 id="model-mistakes">Model Mistakes&lt;/h3>
&lt;p>Minerva 8B와 Minerva 62B의 성능을 비교하여 모델이 만드는 오류 유형을 파악하려고 했다. 두 모델 모두가 높은 확신을 가진 216개의 문제를 선정하였고, 이 중에서 상위 답변이 15% 이상의 표를 받았으며, Minerva 8B는 정확하고 Minerva 62B는 부정확했던 경우와 그 반대 경우를 분석하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table4.png"
width="530"
height="306"
srcset="https://kurtkim.github.io/p/minerva/images/table4_hu78717e1cda1af8ed8d9bc2a7627c83e5_42217_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table4_hu78717e1cda1af8ed8d9bc2a7627c83e5_42217_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>8B 모델의 주요 오류는 부정확한 추론이나 계산에 관련되어 있었으며, 대부분은 산술적 실수였다. 너무 짧은 해결책은 드물었고, 일부 경우에는 실제로 존재하지 않는 수학적 사실을 만들어내기도 했다.&lt;/p>
&lt;p>62B 모델이 틀린 경우, 주로 추론과 계산에서의 오류가 발생하였다. 결론적으로, 62B Minerva 모델은 8B 모델의 기술을 대부분 유지하면서 추론과 계산의 견고성을 향상시킨다는 것을 확인하였다.&lt;/p>
&lt;h3 id="false-positives">False Positives&lt;/h3>
&lt;p>이 논문의 접근법은 문제의 최종 답변의 정확성을 자동으로 확인할 수 있지만, 모델의 추론 과정을 자동으로 검증할 수는 없다. 이로 인해, 추론이 부정확하거나 불완전하더라도 최종 답변이 맞는 &amp;ldquo;false positives&amp;quot;의 가능성이 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/table5.png"
width="818"
height="146"
srcset="https://kurtkim.github.io/p/minerva/images/table5_huef1c0d130804bd0bed9d3c481e8cecf7_22209_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/table5_huef1c0d130804bd0bed9d3c481e8cecf7_22209_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;p>MATH에서 무작위로 선택한 100개의 질문과 62B 모델에서 샘플링한 답변을 수동으로 검토하여 거짓 긍정 비율을 결정하였다. 전반적으로 거짓 긍정 비율은 낮았지만, 난이도가 높아질수록 증가하는 경향을 보였다.&lt;/p>
&lt;p>평가 지표로 pass@1과 다수결 투표를 중점적으로 사용한다. 이는 그들이 거짓 긍정에 덜 취약하기 때문이다. 62B 모델의 pass@256 정확도는 84.5%이지만, 이 중 거짓 긍정의 비율이 30%로 추정된다. 거짓 긍정을 제외하면, pass@256 정확도는 약 68%로 추정된다.&lt;/p>
&lt;hr>
&lt;h2 id="memorization">Memorization&lt;/h2>
&lt;p>머신러닝 모델의 성능이 진짜 분석 능력을 보여주는지, 아니면 단순히 학습 데이터를 암기한 결과인지를 판단하는 것이 중요하다. 이는 모델이 중간 사실들을 암기하는 것이 성능에 큰 영향을 미치기 때문이다. 모델이 문제와 답변을 암기하는 강력한 암기와, 동일한 질문에 대한 다양한 답변을 암기하는 약한 암기를 모두 검토하려고 한다.&lt;/p>
&lt;p>모델이 학습 데이터에서 암기한 정보를 얼마나 잘 활용하는지 평가하기 위해, 우리는 세 가지 분석을 수행한다: 학습 코퍼스에서 문제와 solution 검색, 문제 변형에 대한 모델의 강인성 평가, 그리고 실제 solution과 모델이 생성한 solution 사이의 유사도 측정. 그 결과, 모델의 성능이 암기에 크게 의존하고 있다는 증거는 찾을 수 없었다.&lt;/p>
&lt;h3 id="training-and-evaluation-dataset-overlap">Training and Evaluation Dataset Overlap&lt;/h3>
&lt;p>올바른 답변을 생성한 문제들 중 다수결 점수가 가장 높은 100개의 문제를 선택하여 암기 가능성을 평가하였다. 이들 각각에 대해 BLEU 점수를 계산하고, 점수가 가장 높은 250개 문서를 수동으로 검토했다. 많은 문서가 수학 문제와 해답이 있는 숙제 도움 사이트에서 나왔지만, 고려 중인 문제와는 일치하지 않았다. 이 분석은 이러한 문제들이 데이터 수집 과정을 통과하지 못했다는 결론을 도출하였다.&lt;/p>
&lt;h3 id="performance-on-modiﬁed-math-problems">Performance on Modiﬁed MATH Problems&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/minerva/images/figure5.png"
width="1250"
height="390"
srcset="https://kurtkim.github.io/p/minerva/images/figure5_hudf6acb557fd345f038a788211189f340_96024_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/minerva/images/figure5_hudf6acb557fd345f038a788211189f340_96024_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="320"
data-flex-basis="769px"
>&lt;/p>
&lt;p>암기를 더 깊게 조사하기 위해, 다수결 투표로 올바르게 답변한 문제 20개를 임의로 선택해 수정하였다. 문제의 표현을 바꾸거나 문제에 나타난 숫자를 변경하고 solution을 수정했다. 수정 전후의 solution 정확도를 비교한 결과, 두 경우 모두 수정 전후의 정확도가 상관관계를 보이며, 암기가 최소한임을 나타내었다.&lt;/p>
&lt;h3 id="bleu-score-between-ground-truth-and-generated-solutions">BLEU Score Between Ground Truth and Generated Solutions&lt;/h3>
&lt;p>실제 답변과 모델이 생성한 답변 사이의 BLEU 점수를 계산하여 solution의 암기를 검사하였다. 5,000개의 테스트 질문 중 160개가 BLEU 점수가 80 이상인 샘플을 가지고 있었으며, 일반적으로 이들은 짧은 solution 이었다. 답변의 유사성이 성능에 어떤 영향을 미치는지 이해하기 위해, 특정 BLEU 점수 이상의 샘플을 제거하고 다수결 투표 정확도를 다시 계산하였다. 결과적으로, 성능이 실제 답변과 매우 유사한 모델 출력에 의해 결정되지 않음을 확인했다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions-and-discussion">Conclusions and Discussion&lt;/h2>
&lt;p>이 연구에서는 자연어로 표현된 수학적 추론을 활용하여 문제를 해결하는 양적 추론 방법을 채택하였다. 거대 언어 모델을 고품질의 수학 데이터셋에 학습시켜 논리적 추론, 수치 계산, 심볼 조작 작업에 강력한 성능을 보임을 입증하였다. 코드 생성 모델과 형식적 방법 등 다른 접근법들과 결합해 양적 문제를 해결하는 에이전트를 제작하는 것이 최종 목표이다.&lt;/p>
&lt;h3 id="limitations-of-our-approach">Limitations of Our Approach&lt;/h3>
&lt;p>양적 추론 접근법은 몇 가지 한계를 가지고 있다. 첫째, 모델의 답변의 정확성을 자동으로 검증할 수 없다. 둘째, 모델은 외부 도구를 사용할 수 없어 복잡한 수치 계산을 수행하는 능력이 제한적이다. 셋째, 대량의 데이터를 통해 학습된 모델이므로, 획득한 특정 능력에 대해 직접 통제할 수 있는 부분이 거의 없다.&lt;/p>
&lt;h3 id="societal-impact">Societal Impact&lt;/h3>
&lt;p>일반적인 상황에서 양적 추론 문제를 해결할 수 있는 인공 신경망은 큰 사회적 영향력을 가질 수 있다. 하지만 현재로서는 Minerva 모델이 이 목표에 도달하기엔 먼 상태로, 성능이 인간에 비해 떨어지며 출력의 정확성을 자동으로 검증할 수 없다. 이러한 문제가 해결되면, 모델은 광범위한 긍정적 영향을 미칠 것으로 예상되며, 접근성이 좋고 저렴한 수학 튜터로서 교육 불평등을 개선하는 데에 활용될 수 있을 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2206.14858.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/gair-nlp/abel" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LaMDA</title><link>https://kurtkim.github.io/p/lamda/</link><pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/lamda/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>LaMDA는 최대 137B의 parameter를 가진 대화용 언어 모델이다. 이 모델은 모델 확장만으로는 안전성과 사실적 근거에 대한 개선이 제한적이라는 문제를 해결하기 위해, 주석이 달린 데이터로 미세 조정하고 외부 지식 소스를 참조하는 방식을 사용한다. 이를 통해 모델의 안전성을 향상시키고, 사실에 근거한 응답을 생성하는 데 성공하였다. 또한, 이 모델은 교육 및 콘텐츠 추천 분야에서의 활용 가능성을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델 사전 학습은 자연어 처리 연구에서 유망하며, 레이블이 없는 텍스트를 사용해 모델과 데이터셋의 크기를 확장하면 성능이 향상된다. 이를 통해 GPT-3와 같은 모델은 few-shot 학습 예제로도 높은 성능을 보여준다.&lt;/p>
&lt;p>대화형 모델은 텍스트의 long-term dependency를 표현하는 능력을 활용하여 언어 모델을 효과적으로 활용한다. 모델의 크기가 커짐에 따라 대화 품질도 향상되는 강한 상관관계가 있다.&lt;/p>
&lt;p>LaMDA는 transformer 기반의 언어 모델로, 대화를 위해 설계되었다. 이는 대량의 공개 대화 데이터와 웹 문서로 사전 학습되었고, 잠재적인 응답을 생성, 필터링, 재정렬하여 최고 품질의 응답을 제공하는 다양한 작업을 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure1.png"
width="1054"
height="358"
srcset="https://kurtkim.github.io/p/lamda/images/figure1_hu79814309481c86c02f59b14c3d04a8e2_76180_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure1_hu79814309481c86c02f59b14c3d04a8e2_76180_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="294"
data-flex-basis="706px"
>&lt;/p>
&lt;p>LaMDA와 함께 모델 스케일링의 이점을 연구한 결과, 스케일링만으로는 품질이 향상되지만, 안전성과 실제 연관성은 인간의 성능에 미치지 못하였다. 그러나 스케일링과 미세 조정을 결합하면 모든 지표에서 크게 향상되었고, 특히 품질 격차는 인간 수준에 가까워졌다.&lt;/p>
&lt;p>&amp;ldquo;quality&amp;rdquo; 지표는 &amp;ldquo;sensibleness&amp;rdquo;, &amp;ldquo;speciﬁcity&amp;rdquo;, &amp;ldquo;interestingness&amp;quot;의 세 가지 요소에 기반하며, 이를 바탕으로 응답이 얼마나 합리적이고 구체적이며 흥미로운지 주석 데이터를 수집한다. 이 데이터를 사용하여 후보 응답을 재정렬하는 discriminator를 미세 조정한다.&lt;/p>
&lt;p>&amp;ldquo;safety&amp;rdquo; 지표는 모델이 생성하는 위험한 응답을 줄이기 위해 도입되었다. 이를 위해 안전 목표를 설정하고, 다양한 군중의 작업자들을 통해 대화 응답을 레이블링한다. 이 레이블을 통해 위험한 응답을 감지하고 제거하는 discriminator를 미세 조정한다. 이는 고수준에서 AI의 가치를 조정하는 과정으로 볼 수 있다.&lt;/p>
&lt;p>&amp;ldquo;groundedness&amp;rdquo; 지표는 모델이 검증 가능한 정보를 포함하는 응답을 알려진 출처에 근거하여 생성하도록 하기 위해 도입되었다. 이는 사용자나 외부 시스템이 응답의 유효성을 판단하는데 도움이 된다. 이 목표를 달성하기 위해, 정보 검색 시스템과 같은 외부 도구를 사용하여 사실을 조사하는 군중의 작업자들의 행동을 모델이 흉내내도록 학습시킨다.&lt;/p>
&lt;p>교육과 콘텐츠 추천 분야에서 LaMDA의 사용을 연구하였다. 특정 응용 프로그램에 LaMDA를 적용하기 위해 몇 번의 응용 프로그램 특정 대화를 사전 조건으로 설정했다. 실험 결과, 사전 학습만 받은 LaMDA 모델과 미세 조정된 LaMDA 모델 모두 그들의 예상 응용 프로그램 역할에 잘 적응할 수 있었으며, 특히 미세 조정된 LaMDA 모델이 더욱 도움이 되었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related work&lt;/h2>
&lt;p>&lt;strong>Language models and dialog models:&lt;/strong> 언어 모델은 최근 NLP 응용 분야에서의 성공 덕분에 주목받고 있다. 이 연구는 모델 스케일링이 품질, 안전성, 실제 연관성 지표를 어느 정도 향상시키는 것을 보여주지만, 미세 조정과 스케일링을 결합하면 모든 지표에서 성능이 크게 향상된다는 것을 보여준다.&lt;/p>
&lt;p>이 연구는 언어 모델을 대화 모델링에 적용하는 최근의 연구와 밀접하게 연관되어 있다. 대화 데이터만을 학습하는 미세 조정 단계는 이전 연구와 관련이 있다. 또한, 군중 작업자가 주석을 단 데이터에 미세 조정을 사용하여 흥미로움을 향상시키는 방법을 사용하였다. 그러나 이 연구의 목표는 사용자와의 추가적인 상호작용보다는 모델의 출력의 흥미로움을 극대화하는 것이다.&lt;/p>
&lt;p>순수 스케일링이 오픈 도메인 대화 모델 성능에 제한적인 영향을 미치는 것은 최근 연구와 일치하며, 이는 실제 연관성의 문제에 중점을 둔다. 최근 스케일링 연구는 질문-응답 작업의 성능이 모델 크기에 따라 향상된다는 것을 발견했는데, 이는 미세 조정 전의 사전 학습된 LaMDA에 대한 연구 결과와 일치한다.&lt;/p>
&lt;p>이 연구의 접근법은 언어 모델을 검색 시스템을 통해 향상시키는 데 초점을 맞춘 연구와 연관이 있다. 대부분의 기존 연구는 대화 생성보다는 오픈 도메인 질문-응답에 초점을 맞추고 있으며, 모델 자체가 중간 도구를 사용하도록 학습된다. 이러한 접근법은 RNNLM, RAG, REALM, FiD 등의 아키텍처를 포함하며, 최근의 연구는 신경 모델의 검색과 순위 지정 능력을 확장하고 발전시키고 있다. 이 접근법은 또한 영화 티켓 대화를 위한 외부 API를 사용하도록 모델을 미세 조정하는 연구와도 비교할 수 있다.&lt;/p>
&lt;p>연구 결과는 최근 대화의 실제 연관성에 대한 연구와 일부 유사하다. 외부 지식 베이스에 접근하는 것은 모델이 출처가 없는 내용을 환영하는 비율을 줄이는 것으로 나타났다. 질문-응답 시스템의 정확도는 추론 단위와 응답 생성기를 분리함으로써 개선된다. 검색 엔진과 언어 모델을 결합하면 더 사실적으로 근거를 둔 응답을 제공하는 것으로 나타났다. 알려진 출처의 정보로 생성된 응답을 보강함으로써, 안전성이나 품질에 대한 향상을 희생하지 않고 모델을 실제 연관성에 대해 세부 조정할 수 있다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Dialog metrics:&lt;/strong> 대화 모델에 대한 효과적인 지표를 정의하는 것은 아직 해결되지 않은 연구 주제이다. 이 연구의 접근법은 인간 같은 지표를 주장한 이전 연구에 의해 영감을 받았다. 많은 자동화된 지표들이 연구되었지만, 이러한 지표들은 인간의 판단과 잘 연관되지 않을 수 있다. 따라서 대화 모델링에 대한 더 신뢰할 수 있는 지표는 인간의 평가를 필요로 한다.&lt;/li>
&lt;/ul>
&lt;p>이전 연구는 다양한 대화 품질 평가를 하나의 지표로 결합하려 했으나, 이 연구에서는 각각의 평가 요소를 따로 고려한다. sensibleness, speciﬁcity 외에 interestingness, safety, groundedness 등의 새로운 지표를 추가했고, 이런 다양한 지표 사용의 장점은 특정 지표가 낮은 응답을 분석해 개선 방법을 찾는 것이 가능하다는 점이다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Safety and safety of dialog models:&lt;/strong> 언어 모델의 부적절하고 위험한 행동에 대해 많은 연구가 이루어져 왔으며, toxicity, bias, inappropriately revealing personally identifying information (PII) 등의 문제가 발견되었다. 대규모 언어 모델과 관련된 21가지 위험을 식별하였고, 이 문제를 해결하기 위한 다양한 방법이 제안되었음에도 불구하고, 이 문제를 의미있게 해결하는 것은 여전히 활발한 연구 분야이다.&lt;/li>
&lt;/ul>
&lt;p>대화 모델에 대한 문제도 논의되었는데, bias, offensiveness, hate speech 등이 학습 데이터와 모델 output에서 발견되었다. 대화 모델은 학습 데이터의 bias을 배우고 확대할 수 있다. 이를 해결하기 위해, 안전한 output을 감지하는 별도의 layer를 학습하는 방법이 사용되었고, 미세 조정이 효과적이었다. 크기를 늘리는 것은 toxicity 지표에 영향을 미치지 않지만, 안전 평가에서의 미세 조정은 영향을 미친다는 것이 확인되었다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Groundedness metrics:&lt;/strong> 권위 있는 외부 소스와 모델의 output이 일치하는지를 판단하는 대중들에게 실제성을 평가하도록 요청함으로써 실제성을 평가한다. 최근에 제안된 AIS 프레임워크는 외부 세계에 관련된 언어 모델의 output을 더 정확하게 평가하는 방법을 제시하며, 이는 정보의 이해와 식별, 그리고 정보의 출처 판별의 두 단계로 이루어진다. 또한, 최근의 연구에서는 Q2 지표를 통해 자동 평가의 가능성을 다시 제시하였다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="lamda-pre-training">LaMDA pre-training&lt;/h2>
&lt;p>LaMDA는 공개 대화 데이터와 웹 문서를 기반으로 사전 학습되어 텍스트의 다음 토큰을 예측하도록 설계되었다. 이로 인해 LaMDA는 미세 조정 전에도 일반 언어 모델로 사용될 수 있다.&lt;/p>
&lt;p>사전 학습 데이터셋은 총 2.97B 개의 문서, 1.12B 개의 대화, 13.39B 개의 대화 발화로 구성되어 있고, 대부분이 영어이다. SentencePiece 라이브러리를 통해 2.81T byte pair encoding(BPE) 토큰으로 토큰화하였다. 이는 Meena의 학습 세트인 40B 단어에 비해 훨씬 큰 규모이다.&lt;/p>
&lt;p>가장 큰 LaMDA 모델은 Meena보다 약 50배 많은 137B개의 non-embedding parameter를 가지고 있다. 이 모델은 decoder-only Transformer 언어 모델을 사용하며, 64개의 layer와 relative attention, gated-GELU activation 등을 특징으로 한다.&lt;/p>
&lt;p>LaMDA는 총 57.7일 동안 1024개의 TPU-v3 칩에서 사전 학습되었고, 배치당 256K 토큰을 사용하였다. Lingvo 프레임워크를 통해 123 TFLOPS/sec의 성능을 달성하였다. 또한, 모델 스케일링의 효과를 측정하기 위해 2B-parameter와 8B-parameter의 작은 모델도 학습하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure2.png"
width="826"
height="428"
srcset="https://kurtkim.github.io/p/lamda/images/figure2_hu194ec68ae1ded1ca77e52945a85c89f5_111247_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure2_hu194ec68ae1ded1ca77e52945a85c89f5_111247_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="463px"
>&lt;/p>
&lt;p>미세 조정 전의 모델을 &amp;ldquo;PT&amp;quot;라 부르며, PT는 Meena와 같은 샘플링-랭킹 전략을 사용한다. 총 16개의 독립적인 후보 응답을 샘플링하고, log-likelihood와 length를 기반으로 점수가 가장 높은 후보를 최종 output으로 선택한다.&lt;/p>
&lt;hr>
&lt;h2 id="metrics">Metrics&lt;/h2>
&lt;h3 id="foundation-metrics-quality-safety-and-groundedness">Foundation metrics: Quality, Safety and Groundedness&lt;/h3>
&lt;p>&lt;strong>Sensibleness, Speciﬁcity, Interestingness (SSI):&lt;/strong> overall quality score는 sensibleness, speciﬁcity, and interestingness (SSI)의 평균이다.&lt;/p>
&lt;p>Adiwardana et al.은 Meena의 품질을 측정하기 위해 sensibleness와 speciﬁcity의 평균인 SSA 지표를 제안하였다.&lt;/p>
&lt;p>&amp;ldquo;sensibleness&amp;rdquo; 점수는 모델의 응답이 문맥에 맞고 이전의 발언과 모순되지 않는지를 측정한다. 그러나, sensibleness만으로 모델을 평가하면 모델이 짧고 일반적이며 지루한 응답을 생성하는 것을 보상할 수 있다. 예를 들어, 모든 질문에 &amp;ldquo;I don’t know&amp;quot;라고 답하는 GenericBot 알고리즘은 sensibleness에서 70%의 점수를 얻었다.&lt;/p>
&lt;p>&amp;ldquo;speciﬁcity&amp;rdquo; 점수는 응답이 주어진 문맥에 특정한지를 측정한다. 예를 들어, &amp;ldquo;Me too&amp;quot;는 다양한 문맥에 사용될 수 있으므로 특이성 점수가 0이지만, &amp;ldquo;Me too. I love Eurovision songs&amp;quot;라는 응답은 문맥에 특정하므로 1점을 받는다. Meena는 이 SSA 지표에서 인간 성능과의 격차를 줄였다.&lt;/p>
&lt;p>모델의 성능이 향상됨에 따라, sensibleness와 speciﬁcity만으로는 대화 모델의 품질을 충분히 측정할 수 없다는 것을 확인하였다. 예를 들어, &amp;ldquo;How do I throw a ball?&amp;ldquo;라는 질문에 대해, &amp;ldquo;You can throw a ball by ﬁrst picking it up and then throwing it&amp;quot;은 답변은 합리적이고 특이하지만, 더 깊고 만족스러운 답변은 &amp;ldquo;One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm down and up again, extending your elbow and then releasing the ball upwards&amp;quot;이다.&lt;/p>
&lt;p>&amp;ldquo;interestingness&amp;quot;이라는 세 번째 점수는 대화의 흥미로움을 측정한다. 이는 무리 작업자에 의해 0/1 레이블로 측정되며, &amp;ldquo;누군가의 주목을 끄는&amp;rdquo; 또는 &amp;ldquo;호기심을 불러일으키는&amp;rdquo; 것, 또는 예상치 못하거나 재치 있거나 통찰력 있는 응답을 흥미롭다고 판단한다.&lt;/p>
&lt;p>&lt;strong>Safety:&lt;/strong> 대화 모델은 높은 품질(SSI) 점수를 얻을 수 있지만 사용자에게 위험할 수 있다. 그래서 위험한 모델 output을 측정하기 위한 새로운 안전 지표를 개발하였다. 이 지표는 피해의 위험을 줄이고 불공정한 편향을 방지하는 Google의 AI 원칙을 따른다.&lt;/p>
&lt;p>&lt;strong>Groundedness:&lt;/strong> 언어 모델이 잘못된 주장을 생성하는 경향이 있기 때문에, LaMDA가 가능한 한 알려진 출처와 연관된 응답을 생성하도록 하여 필요한 경우 확인할 수 있도록 하려고 한다.&lt;/p>
&lt;p>&amp;ldquo;groundedness&amp;quot;은 외부 세계에 대한 주장을 포함하는 응답 중에서 권위 있는 외부 출처에 의해 지지될 수 있는 주장의 비율로 정의된다.&lt;/p>
&lt;p>&amp;ldquo;informativeness&amp;quot;는 모든 응답 중에서 알려진 출처로부터 지지받는 외부 세계 정보를 전달하는 응답의 비율로 정의된다. 이는 &amp;ldquo;groundedness&amp;quot;과 분모 항에서만 다르다. 예를 들어, &amp;ldquo;That’s a great idea&amp;quot;와 같은 외부 세계 정보를 전달하지 않는 응답은 실재성에는 영향을 미치지 않지만 정보성에는 영향을 미친다. &amp;ldquo;Rafael Nadal is the winner of Roland Garros 2020&amp;quot;는 실재성 있는 응답의 예이다.&lt;/p>
&lt;p>마지막으로, &amp;ldquo;citation accuracy&amp;quot;를 외부 세계에 대한 명확한 주장을 포함하는 모든 응답 중에서 출처의 URL을 인용하는 모델 응답의 비율로 정의한다. 이는 &amp;ldquo;말은 네 다리가 있다&amp;quot;와 같은 잘 알려진 사실에 대한 주장은 제외합니다.&lt;/p>
&lt;h3 id="role-speciﬁc-metrics-helpfulness-and-role-consistency">Role-speciﬁc metrics: Helpfulness and Role consistency&lt;/h3>
&lt;p>기본 메트릭(quality, safety, groundedness)은 대화 에이전트의 중요한 속성을 측정한다. 이는 에이전트의 특정 역할에 의존하지 않는다. 도움이 되는지와 역할의 일관성은 에이전트가 특정 역할을 가진 대화 응용 프로그램에서 측정된다.&lt;/p>
&lt;p>&lt;strong>Helpfulness:&lt;/strong> 사용자가 정보 검색 시스템을 통해 독립적으로 조사한 정보가 올바르고, 사용자가 도움이 된다고 판단하는 경우, 모델의 응답은 helpful으로 표시된다. helpful 응답은 사용자가 올바르고 유용하다고 판단하는 응답의 부분 집합이다.&lt;/p>
&lt;p>&lt;strong>Role consistency:&lt;/strong> 모델의 응답이 대상 역할을 수행하는 에이전트가 말할 것 같다면, 그것은 role consistent가 있다고 표시된다. 이는 대화 내에서의 자체 일관성과는 다르며, 대화 외부의 에이전트의 역할 정의와의 일관성을 의미한다.&lt;/p>
&lt;hr>
&lt;h2 id="lamda-ﬁne-tuning-and-evaluation-data">LaMDA ﬁne-tuning and evaluation data&lt;/h2>
&lt;p>&lt;strong>Quality (Sensibleness, Speciﬁcity, Interestingness):&lt;/strong> 품질(SSI)을 향상시키기 위해, 작업자들에게 LaMDA와 14~30턴에 걸친 대화를 요청하여 6400개의 대화를 수집히였다. 작업자들은 각 응답이 합리적(sensible)이고, 특정(speciﬁc)하고, 흥미로운지(interesting)를 평가하고, &amp;ldquo;yes&amp;rdquo;, &amp;ldquo;no&amp;rdquo;, &amp;ldquo;maybe&amp;quot;로 레이블한다. 응답이 합리적이거나 특정하지 않으면, 특이성과 흥미로움을 &amp;ldquo;no&amp;quot;로 간주한다. 모든 응답은 5명의 다른 작업자에 의해 레이블이 붙여지고, 5명 중 적어도 3명이 &amp;ldquo;yes&amp;quot;라고 표시하면 응답이 합리적이고, 특정하며, 흥미로운 것으로 간주된다.&lt;/p>
&lt;p>최대 3번의 대화 턴을 가진 1477개의 대화로 구성된 Mini-Turing Benchmark(MTB) 데이터셋에 대한 모델의 응답을 기반으로 평가한다. 이 대화들은 모델에 공급되어 다음 응답을 생성한다. 모든 응답은 5명의 작업자 중 적어도 3명이 &amp;ldquo;yes&amp;quot;라고 표시하면 합리적이고, 특정하고, 또는 흥미로운 것으로 레이블이 붙는다.&lt;/p>
&lt;p>&lt;strong>Safety:&lt;/strong> safety를 위한 미세 조정을 위해, safety 목표를 정의하고, 이를 바탕으로 다양한 작업자들을 이용해 사람이 만든 프롬프트에 대한 LaMDA의 응답을 주석 처리하는 구조화된 접근법을 사용한다.&lt;/p>
&lt;p>작업자들에게 LaMDA와 5~10턴에 걸친 대화를 요청하여 8K 대화를 수집하였다. 작업자들은 자연스러운 형태(interactions of natural form), 민감한 주제를 다루는(interactions that touch sensitive topics), 혹은 모델을 깨려고 시도하는(interactions that adversarially attempt to break the model as per the safety objectives) 세 가지 방식으로 모델과 상호 작용한다. 각 응답에 대해, 작업자들은 문맥을 고려하여 safety 목표를 위반하는지 평가하고, &amp;ldquo;yes&amp;rdquo;, &amp;ldquo;no&amp;rdquo;, &amp;ldquo;maybe&amp;quot;로 레이블한다. 각 safety 목표에 대해 &amp;ldquo;no&amp;quot;로 표시한 작업자가 적어도 2명인 경우, 응답에는 safety 점수 1이 부여된다. 그렇지 않으면 점수는 0으로 지정된다.&lt;/p>
&lt;p>1458턴의 1166개 대화로 구성된 보류 샘플 데이터셋을 사용해 safety를 평가한다. 이 대화들은 모델에 입력되어 다음 응답을 생성한다. 각 safety 목표를 &amp;ldquo;no&amp;quot;라고 표시한 무리 작업자가 적어도 2명인 경우, 응답에는 점수 1이 부여되고, 그렇지 않으면 점수는 0이다.&lt;/p>
&lt;p>&lt;strong>Groundedness:&lt;/strong> SSI와 safety처럼, 작업자들에게 모델과 상호작용하면서 정보 탐색을 위한 대화로 이끌도록 요청하여 4K 대화를 수집하였다.&lt;/p>
&lt;p>작업자들에게 모델의 대화 턴이 외부 세계에 대한 주장을 하는지 평가하도록 요청하였다. 공개적으로 인정받지 않은 사람들에 대한 주장은 제외하고, 이는 모델이 즉흥적인 인물을 대신하여 사실적인 주장을 할 수 있기 때문이다. 이러한 주장은 외부 소스에 기반을 두는 것을 필요로 하지 않는다.&lt;/p>
&lt;p>작업자들에게 주장이 사실인지 물어본다. 3명의 작업자 모두 주장이 사실임을 안다면, 그것을 상식으로 가정하고 외부 지식 소스를 확인하지 않는다.&lt;/p>
&lt;p>확인이 필요한 주장을 포함하는 발언에 대해, 작업자들에게 조사를 위한 검색 쿼리를 기록하도록 요청한다. 그리고 외부 지식 검색 시스템에서의 결과를 포함하여 모델의 응답을 수정하도록 요청한다. 오픈 웹의 내용이 검색 결과에 포함되면, 작업자들에게 출처를 인용하는 URL을 포함하도록 요청한다.&lt;/p>
&lt;p>다양한 주제를 다루는 784턴의 대화를 포함하는 평가 데이터셋을 이용하여 실제성을 평가한다. 이 맥락들은 모델에 공급되어 다음 응답을 생성한다. 각 응답에 대해, 작업자들은 모델의 응답이 사실적인 주장을 포함하고, 이 주장이 알려진 소스를 통해 검증될 수 있는지 평가한다. 모든 응답은 3명의 다른 작업자에 의해 레이블이 붙으며, 최종 실제성, 정보성, 인용 정확성 레이블은 다수결에 의해 결정된다. 모든 데이터셋은 영어로 되어 있다.&lt;/p>
&lt;p>&lt;strong>Estimating these metrics for human-generated responses:&lt;/strong> 작업자들에게 평가 데이터셋의 무작위 샘플에 응답하도록 요청하며, 그들은 안전하고, 합리적이며, 특정하고, 흥미롭고, 실제적이며, 정보적인 방식으로 응답하도록 지시받는다. 필요한 외부 도구를 사용하도록 요청되며, 이후 맥락-응답 쌍은 평가를 위해 전송되고, 다수결에 의해 합의 레이블이 형성된다.&lt;/p>
&lt;hr>
&lt;h2 id="lamda-ﬁne-tuning">LaMDA ﬁne-tuning&lt;/h2>
&lt;h3 id="discriminative-and-generative-ﬁne-tuning-for-quality-ssi-and-safety">Discriminative and generative ﬁne-tuning for Quality (SSI) and Safety&lt;/h3>
&lt;p>사전 학습된 모델에 여러 미세 조정을 적용하여 LaMDA를 생성한다. 이는 맥락에 따른 응답 생성과 응답의 품질 및 safety 평가를 포함하며, 이로 인해 생성기와 판별기 기능을 동시에 수행할 수 있는 단일 모델이 만들어진다.&lt;/p>
&lt;p>LaMDA는 디코더만 있는 생성적 언어 모델이므로, 모든 미세 조정 예시들은 토큰의 시퀀스로 표현된다. 생성적(Generative) 미세 조정 예시들은 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$&amp;rdquo; 형태로 표현되며, 손실은 응답 부분에만 적용된다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>판별적(Discriminative) 미세 조정 예시들은 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$ $&amp;lt;$$\text{attribute-name}$$&amp;gt;$ $&amp;lt;$$\text{rating}$$&amp;gt;$&amp;ldquo;으로 표현되며, 손실은 속성 이름 다음의 등급에만 적용된다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. SENSIBLE 1&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. INTERESTING 0&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;What’s up? RESPONSE not much. UNSAFE 0&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>생성과 판별에 같은 모델을 사용하면, 응답 생성 후에 판별자를 평가하는 것은 P(&amp;quot;$&amp;lt;$$\text{desiredrating}$$&amp;gt;$&amp;rdquo; | &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$ $&amp;lt;$$\text{attribute-name}$$&amp;gt;$&amp;quot;)를 계산하는 것을 포함한다. 모델이 이미 &amp;ldquo;$&amp;lt;$$\text{context}$$&amp;gt;$ $&amp;lt;$$\text{sentinel}$$&amp;gt;$ $&amp;lt;$$\text{response}$$&amp;gt;$&amp;ldquo;를 처리했으므로, 판별자를 평가하는 것은 단지 몇 가지 추가 토큰을 처리하는 것을 포함한다: &amp;ldquo;$&amp;lt;$$\text{attribute-name}$$&amp;gt;$ $&amp;lt;$$\text{desired rating}$$&amp;gt;$&amp;rdquo;.&lt;/p>
&lt;p>LaMDA는 생성된 응답의 SSI와 safety 등급을 예측하도록 미세조정됩니다. safety 예측이 특정 임계값 이하인 응답은 제외되고, 나머지 응답들은 품질에 따라 순위를 매긴다. 이 과정에서 sensibleness는 speciﬁcity와 interestingness보다 세 배 더 높은 가중치를 받는다.&lt;/p>
&lt;p>LaMDA의 SSI 및 safety 판별자는 사전 학습 데이터셋의 대화를 점수 매기고 필터링하는데 사용되어, 안전하고 합리적이며 특정하고 흥미로운 80만 턴의 대화를 생성한다. 이 데이터셋을 사용하여 LaMDA는 주어진 컨텍스트에서 응답을 생성하도록 미세조정된다.&lt;/p>
&lt;h3 id="fine-tuning-to-learn-to-call-an-external-information-retrieval-system">Fine-tuning to learn to call an external information retrieval system&lt;/h3>
&lt;p>LaMDA 같은 언어 모델들은 확실해 보이는 output을 생성하지만, 이는 알려진 외부 출처로부터 확인된 사실과 상충하는 경우가 많다. 예를 들어, 뉴스 기사의 시작 문장을 계속하는 것처럼 보이지만, 실제로는 신뢰할 수 있는 외부 참조와는 연결이 없다.&lt;/p>
&lt;p>LaMDA는 가능한 한 확인 가능한 출처와 연결된 응답을 생성하려고 한다. 이는 기존 언어 모델이 종종 그럴듯하나 잘못된 정보를 제공할 수 있기 때문이다. 이를 통해 사용자는 필요한 경우 정보를 교차 검증할 수 있다.&lt;/p>
&lt;p>&lt;strong>The toolset (TS):&lt;/strong> LaMDA 인스턴스와 상호작용하여 6400개의 대화를 수집하였다. 이 대화들은 각각 14~30턴 사이에 이루어졌다. 각 응답은 다른 작업자들에 의해 합리성, 특이성, 흥미로움에 대해 평가되었다. 응답이 합리적이지 않거나 특정하지 않다면, 특이성과 흥미로움에 대한 평가는 수행되지 않았다. 모든 응답은 5명의 작업자들에 의해 레이블링되었고, 3명 이상이 &amp;ldquo;yes&amp;quot;라고 응답하면 그 응답이 합리적이고 특정하며 흥미로운 것으로 간주되었다.&lt;/p>
&lt;p>&lt;strong>Dialog collection:&lt;/strong> 생성 데이터용으로 40K의 주석이 달린 대화 턴을 수집하였고, 판별 데이터용으로 &amp;ldquo;correct&amp;rdquo; 혹은 &amp;ldquo;incorrect&amp;quot;으로 레이블링된 9K의 대화 턴을 수집하였다.&lt;/p>
&lt;p>작업자들 사이의 대화를 수집하고, 그들의 주장이 신뢰할 수 있는 출처에 의해 지지될 수 있는지 평가하였다. 도구 세트(TS)에 접근할 수 있으면, 더 잘 지지된 주장을 생성하는 경향이 있었다. 예를 들어, Rafael Nadal의 나이에 대한 질문에는 정보 검색 시스템을 통해 쉽게 답변을 찾을 수 있다. 이를 바탕으로, 언어 모델을 미세조정하여 응답에 대한 출처를 제공하기로 결정하였다.&lt;/p>
&lt;p>알고리즘의 미세조정을 위한 학습 데이터를 수집하기 위해, 정적 방법과 상호작용 방법을 모두 사용했다. 이 과정에서 작업자들은 모델의 output에 반응하는 것이 아니라, LaMDA가 학습할 수 있도록 수정하는 역할을 한다. 각 발언이 외부 지식 출처를 참조해야 할 주장을 포함하는지, LaMDA가 만든 인물 이외의 것에 대한 주장인지, 일반 상식을 넘어서는지에 따라 모델의 출력을 평가하고, 필요한 경우 도구 세트를 활용해 주장을 연구한다.&lt;/p>
&lt;p>알고리즘이 추론 시간에 사용하는 서비스와 동일한 도구 세트 인터페이스를 사용한다. 텍스트 쿼리를 입력하면, 정보 검색 시스템이 순위별로 정렬된 텍스트 스니펫을 반환한다. 사용자는 검색을 마친 후, 출처가 표시된 주장을 포함하도록 모델의 발언을 수정할 수 있다. 오픈 웹 콘텐츠를 사용한 경우, 외부 정보를 포함한 응답을 지원하기 위해 필요한 URL을 인용해야 한다. URL은 메시지 끝에 추가하거나, 필요에 따라 특정 단어에 인라인으로 첨부할 수 있다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning:&lt;/strong> 두 가지 작업을 수행하도록 LaMDA를 미세조정한다.&lt;/p>
&lt;p>첫 번째 작업은 대화 컨텍스트와 기본 모델의 응답을 바탕으로 특별한 문자열을 생성한다. 이 문자열은 &amp;ldquo;TS&amp;quot;로 표시되며, 이어지는 텍스트가 검색 쿼리임을 나타낸다. 예를 들어, &amp;ldquo;TS, Rafael Nadal’s age&amp;quot;와 같다.&lt;/p>
&lt;p>두 번째 작업은 도구로부터 반환된 스니펫과 대화 문장을 취한다(예: &amp;ldquo;He is 31 years old right now” + “Rafael Nadal / Age / 35&amp;rdquo;). 그런 다음 이를 기반으로 실제로 검증된 버전을 예측한다: context + base + query + snippet $\rightarrow$ &amp;ldquo;User, He is 35 years old right now&amp;rdquo; 또한, 이 작업은 추가적인 연구 쿼리를 출력할 수도 있다. 예를 들어, context + base + query + snippet $\rightarrow$ &amp;ldquo;TS, Rafael Nadal’s favorite song&amp;rdquo;. 추론 시간에는, 첫 번째로 생성된 문자열이 &amp;ldquo;TS&amp;quot;인지 &amp;ldquo;User&amp;quot;인지에 따라 모델의 출력이 정보 검색 시스템으로, 또는 사용자에게 전달된다.&lt;/p>
&lt;p>예를 들어, 대화에는 사용자로부터 다음과 같은 발언이 포함될 수 있다:&lt;/p>
&lt;p>&lt;strong>USER :&lt;/strong> What do you think of Rosalie Gascoigne’s sculptures?&lt;/p>
&lt;p>(이 대화록은 저자 중 한 명과의 대화에서 가져온 것이며, 일반적으로 사용자에게는 보여지지 않는 생성된 기본 출력과 검색 쿼리를 포함하고 있다.)&lt;/p>
&lt;p>기본 LaMDA 언어 모델, 즉 이 문맥에서 &amp;ldquo;Base&amp;rdquo; 모델이라고 부르는 것은 초안 응답을 생성한다:&lt;/p>
&lt;p>&lt;strong>LAMDA - BASE :&lt;/strong> They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her inﬂuence is also super interesting - did you know she was one of the artists that inspired Miró?&lt;/p>
&lt;p>마지막 문장이 처음에는 그럴 듯해 보이지만, Miró와 Gascoigne의 활동 시기를 비교하면 오류를 확인할 수 있다. 이는 기본 모델이 생성한 정보가 항상 정확하지 않을 수 있음을 보여준다.&lt;/p>
&lt;p>모델의 &amp;ldquo;Research&amp;rdquo; 단계에서는 &amp;ldquo;Base&amp;rdquo; output의 주장을 검증하기 위해 쿼리를 생성한다. 쿼리 생성은 모델의 미세조정에 의해 결정되며, 무한 루프를 방지하기 위해 추론 시에는 최대 쿼리 수를 설정한다. 현재 예에서는 이 최대치가 4로 설정되어 있다.&lt;/p>
&lt;p>예를 들어, &amp;ldquo;Research&amp;rdquo; 단계에서는 다음과 같은 쿼리를 발행할 수 있다.&lt;/p>
&lt;p>&lt;strong>LAMDA - RESEARCH :&lt;/strong> TS, Miró and Gascoigne&lt;/p>
&lt;p>&lt;strong>TS :&lt;/strong> Gascoigne , Rosalie | Suddenly the Lake - NGA’s Collection Search&lt;/p>
&lt;p>The course of Gascoigne’s artistic life is an inspiration for those who are only able to devote themselves to their chosen calling late in life. She ﬁrst exhibited her art . . .&lt;/p>
&lt;p>검색 스니펫의 구두점과 생략 부호는 모델이 보는 그대로이다. &amp;ldquo;Research&amp;rdquo; 단계에서는 또 다른 쿼리를 발행한다:&lt;/p>
&lt;p>&lt;strong>LAMDA - RESEARCH :&lt;/strong> TS, Miró and Gascoigne&lt;/p>
&lt;p>&lt;strong>TS :&lt;/strong> (&lt;strong>Gascoigne&lt;/strong> had been a practitioner of wild, avant-garde ikebana – Japanese ﬂower arrangement – before turning late in life to sculpture.) The two-way alternation of visual ﬂow creates a pleasing tension, like an electric charge. There are other tensions too, of class and gender.&lt;/p>
&lt;p>동일한 쿼리를 반복하면, 모델은 더 많은 세부 정보를 포함한 두 번째로 순위가 매겨진 결과를 받게 된다. 이를 통해 더 정확한 정보를 얻을 수 있다.&lt;/p>
&lt;p>&amp;ldquo;Research&amp;rdquo; 단계는 사용자에게 출력을 생성합니다. 이는 &amp;ldquo;Base&amp;rdquo; 응답의 톤과 비슷하지만, 검증되지 않은 주장을 TS로 찾은 소스를 기반으로 한 새로운 문장으로 대체한다. 이는 더 정확한 정보를 제공하기 위함이다.&lt;/p>
&lt;p>&lt;strong>LAMDA :&lt;/strong> Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese ﬂower arrangement before turning to sculpture?&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure3.png"
width="1226"
height="1146"
srcset="https://kurtkim.github.io/p/lamda/images/figure3_hubf67001e4798be9221e701ba6d45b98a_370480_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure3_hubf67001e4798be9221e701ba6d45b98a_370480_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="256px"
>&lt;/p>
&lt;hr>
&lt;h2 id="results-on-foundation-metrics">Results on foundation metrics&lt;/h2>
&lt;p>먼저 사용된 데이터셋과 방법을 요약하고, 그 다음으로 주요 결과에 대해 논의한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table1.png"
width="958"
height="686"
srcset="https://kurtkim.github.io/p/lamda/images/table1_hu3692b69d6560920a1b6be127e698e479_169077_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table1_hu3692b69d6560920a1b6be127e698e479_169077_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;p>기반 메트릭을 개선하기 위해 사용하는 작업자가 주석을 단 데이터셋을 활용하여, 두 단계의 미세조정을 수행한다.&lt;/p>
&lt;ul>
&lt;li>FT quality-safety: 미리 학습된 모델은 quality과 safety 라벨을 예측하는 판별기를 학습하기 위해 미세조정된다. 생성된 응답들은 safety 점수에 따라 필터링되고, quality 점수에 따라 재정렬된다. 또한 이 모델은 컨텍스트 응답 생성을 위해 미세조정된다.&lt;/li>
&lt;li>FT groundedness (LaMDA): FT quality-safety 모델을 외부 정보 검색 시스템 호출 생성과, 다음 동작의 quality 및 유형 예측을 위해 미세조정한다. 이는 더 정확하고 유용한 응답을 생성하는 데 도움이 된다.&lt;/li>
&lt;/ul>
&lt;p>모든 미세조정을 포함하는 모델을 LaMDA라고 정의하고, 이를 사전 학습만을 이용한 결과와 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure4.png"
width="1270"
height="1264"
srcset="https://kurtkim.github.io/p/lamda/images/figure4_hua351c13700e81d40dd409eecc3d2ec2b_417012_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure4_hua351c13700e81d40dd409eecc3d2ec2b_417012_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="241px"
>&lt;/p>
&lt;p>미세조정(특히 LaMDA)은 모든 모델 크기에서 quality, safety, groundedness을 크게 향상시킨다. 또한, 미세조정의 유무에 관계없이 모델 크기가 커질수록 품질 메트릭이 향상되지만, 미세조정을 통해 더욱 향상된다.&lt;/p>
&lt;p>미세조정 없이 모델 크기를 키우는 것은 안전성에 큰 이점을 주지 않는다. 하지만, safety 미세조정과 함께 모델 크기를 확장하면 safety가 크게 향상됩니다. 이는 미세조정이 모델의 safety 개선에 중요하다는 것을 보여준다.&lt;/p>
&lt;p>모델 크기가 커질수록 groundedness가 향상되며, 미세조정을 통해 외부 지식 소스에 접근할 수 있다. 이로 인해 모델은 73.2%의 groundedness와 65%의 인용 정확도를 달성하였다. 즉, 대부분의 응답이 알려진 출처로 추적 가능하며, 필요한 경우 인용을 포함하고 있다.&lt;/p>
&lt;p>단독으로 모델 규모를 확장하면 quality와 groundedness가 향상되지만, safety은 크게 개선되지 않다. 반면, 작업자가 주석을 단 데이터로 미세조정하면 모든 메트릭이 향상된다. 일부 경우에는 미세조정만으로도 훨씬 더 큰 모델과 동등한 성능을 얻을 수 있었다.&lt;/p>
&lt;p>미세조정된 모델은 여러 메트릭에서 작업자의 품질 수준에 거의 도달하며, 특히 interestingness 면에서는 작업자의 품질을 초과한다. 그러나, 작업자가 광범위하게 훈련받지 않았기 때문에, 이는 약한 기준일 수 있다. 또한, safety과 groundedness 면에서는 작업자의 성능에 아직도 많이 뒤떨어져 있다. 정보 검색 도구에 접근이 불가능한 상황에서는 LaMDA 모델이 작업자의 정보성을 초과하지만, 작업자가 도구에 접근할 수 있을 때에는 여전히 뒤떨어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/figure5.png"
width="1260"
height="1020"
srcset="https://kurtkim.github.io/p/lamda/images/figure5_huc942ce4ed79c86d15f240b97768112ea_217373_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/figure5_huc942ce4ed79c86d15f240b97768112ea_217373_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="123"
data-flex-basis="296px"
>&lt;/p>
&lt;p>가장 큰 모델을 사용할 때, FT quality-safety 미세조정과 FT groundedness 미세조정이 최종 결과에 크게 기여한다. PT와 FT quality-safety 사이에서 모든 메트릭의 성능이 크게 향상되며, groundedness은 FT quality-safety에서 LaMDA로 더욱 개선된다. 이는 미세조정이 모델 성능 향상에 핵심적인 역할을 한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="domain-grounding">Domain grounding&lt;/h2>
&lt;p>LaMDA는 사전 조절을 통해 도메인에 적합한 역할을 수행할 수 있다. 이는 교육 목적으로 에베레스트 산 등의 유명한 객체의 역할을 하는 것과 음악 추천 에이전트의 역할 등을 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table2.png"
width="1242"
height="130"
srcset="https://kurtkim.github.io/p/lamda/images/table2_hu7371ca700b4ad34939138969031afaec_29504_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table2_hu7371ca700b4ad34939138969031afaec_29504_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="955"
data-flex-basis="2292px"
>&lt;/p>
&lt;p>LaMDA와 PT를 각 역할에 맞게 조정하기 위해, 역할별 대화의 몇 번의 턴을 사전 조건으로 주고, 같은 사전 조건을 사용한다. 예를 들어, 에베레스트 산 역할에 맞게 조정하기 위해, 대화의 시작에 &amp;ldquo;Hi, I’m Mount Everest. What would you like to know about me?&amp;ldquo;라는 인사를 제공한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table3.png"
width="1094"
height="742"
srcset="https://kurtkim.github.io/p/lamda/images/table3_hua78ff0eee7f6d4d09d69864d10d42416_280102_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table3_hua78ff0eee7f6d4d09d69864d10d42416_280102_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="353px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table4.png"
width="1084"
height="468"
srcset="https://kurtkim.github.io/p/lamda/images/table4_hud188ea14d69afb42a33a7f4df3d86c7a_170795_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table4_hud188ea14d69afb42a33a7f4df3d86c7a_170795_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>작업자들은 LaMDA와 PT 인스턴스와 대화를 통해 600회의 대화를 생성하였다. 다른 작업자 그룹은 이 대화들이 주어진 역할에 일관되고 유용한지 평가하였다. 이를 통해 AI의 역할 수행 능력을 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table5.png"
width="596"
height="189"
srcset="https://kurtkim.github.io/p/lamda/images/table5_huf87f4fbfd6b9da40ddca65c21ff9e471_28406_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table5_huf87f4fbfd6b9da40ddca65c21ff9e471_28406_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>LaMDA 애플리케이션은 도움이 되는 능력에서 PT 애플리케이션보다 더 뛰어나며, 이는 PT의 기본 메트릭(safety, groundedness, quality 등)에서의 낮은 성능 때문일 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/lamda/images/table6.png"
width="1248"
height="510"
srcset="https://kurtkim.github.io/p/lamda/images/table6_hufa315449e1f1c8ac965138c44501fceb_112267_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/lamda/images/table6_hufa315449e1f1c8ac965138c44501fceb_112267_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="587px"
>&lt;/p>
&lt;p>LaMDA와 PT 인스턴스는 대체로 역할 일관성을 잘 유지하나 가끔 캐릭터를 벗어나는 경우가 있다. LaMDA Mount Everest는 자기 자신을 제3자처럼 언급할 때가 있고, 이는 추론 시간의 근거가 충분하지 않아 발생한다. 그러나 역할 일관성은 놀랍도록 높으며, 특히 Mount Everest와 같은 경우가 그렇다. LaMDA Music은 대화 맥락이 주로 음악 추천에 관한 것으로 가정하여, 사용자의 모호한 발화를 음악 추천 요청으로 해석한다.&lt;/p>
&lt;p>평가 중에 작업자들은 정보 검증을 위해 정보 검색 시스템을 사용하며, 알려진 출처로 뒷받침되지 않는 링크나 정보는 &amp;ldquo;not helpful&amp;quot;라고 표시한다. LaMDA Mount Everest는 응답의 30%에서 알려진 출처를 찾을 수 없는 정보를 제공하고, LaMDA Music는 9%의 응답에서 음악 추천을 놓치며, 7%에서는 링크 오류를 보인다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion-and-limitations">Discussion and limitations&lt;/h2>
&lt;p>작은 양의 인간 주석 데이터로도 대화 모델의 quality과 safety을 크게 향상시킬 수 있지만, 여전히 많은 한계가 있다.&lt;/p>
&lt;p>미세 조정 데이터셋 수집은 인간의 미묘한 판단에서 학습하는 이점을 가지지만, 비용이 많이 들고 복잡한 과정이다. 더 큰 데이터셋과 긴 맥락, 다양한 메트릭을 사용하면 결과가 개선될 것으로 예상하지만, 인간의 주관적 판단을 포착하는 것은 복잡하다. 또한, 작업자간의 불일치 패턴은 조사하지 않았다. 향후 작업은 목표 사용자를 반영하는 작업자 선정과 라벨 품질 개선 방법을 살펴본다.&lt;/p>
&lt;p>미세 조정은 모델의 실제성을 향상시키지만, 모델은 여전히 외부 출처의 내용을 정확하게 반영하지 않는 응답을 만들 수 있다. 이는 사실에 대한 단순한 질문에 한정되어 있으며, 복잡한 추론은 아직 개선이 필요하다. 또한, 모델은 대부분 의미 있는 응답을 생성하지만, 미묘한 품질 문제를 겪을 수 있다.&lt;/p>
&lt;p>미세 조정은 safety 메트릭을 평균적으로 향상시킬 수 있지만, LaMDA와 같은 대형 언어 모델이 생성할 수 있는 부적절한 응답에 대응하는 방법에 대한 연구가 필요하다. safety 위험을 완화하는 것이 완전한 신뢰성을 보장하지 않으므로, 일반적인 대화 모델에서 위험의 여러 차원을 포착하는 safety과 공정성에 대한 표준을 개발하는 데 더 많은 연구가 필요하다.&lt;/p>
&lt;p>작업자 집단이 사용자 기반을 완전히 반영하지 못하는 한계가 있다. 특히, 작업자 중 25-34세 연령대가 과대표되어 있다. 이를 개선하기 위한 미래의 연구 방향은 더 다양한 모집 방법을 통해 작업자의 대표성을 높이거나 통계적 추정을 활용하는 것이다.&lt;/p>
&lt;p>이것은 LaMDA의 최종 버전이 아니라, &amp;ldquo;LaMDA&amp;quot;를 생성하는 방법론이며, 특정 애플리케이션에 대한 최종 제품을 만드는 방향으로 이해해야 한다.&lt;/p>
&lt;h3 id="examining-bias">Examining bias&lt;/h3>
&lt;p>실세계 애플리케이션에서 잘 작동하는 고품질 대화 모델 개발에는 여전히 많은 도전이 있다. 레이블이 없는 데이터셋에서 학습된 거대 언어 모델은 학습 데이터의 패턴과 편향을 모방하게 되는데, 이러한 편향은 다양한 미묘한 방법으로 나타나며 감지하기 어렵다. 또한, 차별의 형태는 지역과 문화에 따라 크게 달라지며, 이는 아직 연구가 부족한 분야이다.&lt;/p>
&lt;p>safety 접근법의 한계는 개별 예시가 safety 목표를 위반하지 않아도 학습 데이터셋의 표현적 해를 여전히 전파할 수 있다는 것이다. LaMDA의 응답은 비결정적이므로, 특정 그룹을 통계적으로 우대함으로써 편향이 나타날 수 있다. 예를 들어, 경영에 대한 대화에서 여성을 CEO로 언급하는 응답을 거의 생성하지 않을 수 있다.&lt;/p>
&lt;p>생성 언어 모델의 통계적 편향을 완화하는 방법에는 사전 학습 데이터 필터링, 별도의 필터링 모델 학습, 제어 코드 생성, 모델 미세 조정 등이 있다. 이러한 노력은 중요하지만, 해를 완화하는 데 있어 이러한 노력의 영향을 측정할 때, 하류 응용 프로그램과 사회 기술적 환경도 고려해야 한다. 특정 맥락에서의 편향 완화는 다른 지역 문화 맥락에서는 역설적인 영향을 미칠 수 있다.&lt;/p>
&lt;p>알고리즘 편향 측정 및 완화 분야는 빠르게 성장하고 있어, LaMDA와 같은 대화형 에이전트의 안전성을 보장하기 위해 새로운 연구를 계속 탐색하는 것이 중요하다. 향후 연구는 유해하고 안전하지 않은 콘텐츠에 대한 표준 평가 데이터셋 생성에서 연구 커뮤니티와 시민사회 간의 협력을 탐색해야 한다.&lt;/p>
&lt;h3 id="adversarial-data-collection">Adversarial data collection&lt;/h3>
&lt;p>adversarial-intent의 대화를 통해 미세 조정을 위한 라벨링된 데이터의 범위를 개선하고 있다. 이 과정에서 분석가들은 LaMDA와 상호작용하며 safety 목표를 위반하는 응답을 유도한다.&lt;/p>
&lt;p>적대적 테스팅은 기계 학습 모델의 한계를 발견하고 원치 않는 응답을 유도하는 데 효과적이며, 모델 개발 중에 유해한 콘텐츠를 줄이는 데도 사용된다. 생성 모델에도 적용하려는 노력이 있지만, 대형 언어 모델에 대한 견고하고 효과적인 적대적 테스팅은 아직 열린 문제로, 평가 샘플의 일반화에 대한 도전 때문에 결과가 다양하다.&lt;/p>
&lt;p>이 접근법의 한계는 대부분의 참가자들이 자주 발생하는 문제는 찾을 수 있지만, 드물게 발생하는 문제는 찾기 어렵다는 것이다. 희귀하거나 보이지 않지만 심각한 결과를 초래할 수 있는 오류의 발견을 더욱 장려해야 한다. 이상적으로는 더 다양한 참가자들과 함께 규모를 확대하여 지속적인 노력이 필요하며, 이는 생성 언어 모델의 safety와 성능에 대한 공중 신뢰를 구축하는 데 중요한 연구 분야이다.&lt;/p>
&lt;h3 id="safety-as-a-concept-and-a-metric">Safety as a concept and a metric&lt;/h3>
&lt;p>이 논문에서 제시하는 결과는 다양한 safety 목표에 대한 세부 평가를 하나의 메트릭으로 집계하는데, 이는 이 작업의 주요 제한점이다. 다른 목표를 분리하거나 다른 가중치를 부여하는 것이 어렵다. 더 세부적인 safety 목표를 고려할 수 있는 메트릭과 미세 조정 기법을 살펴볼 필요가 있다.&lt;/p>
&lt;p>평가 척도는 조금 거칠며, 응답의 안전성이나 바람직성을 완전히 측정하지 못할 수 있다. 일부 발언이나 행동은 다른 것들보다 더 큰 불쾌감을 일으킬 수 있으며, safety 라벨은 이런 뉘앙스를 놓칠 수 있습니다. 또한, safety 접근법은 장기적으로 원치 않는 영향을 포착하지 못한다. 이 safety 목표는 미국 사회 맥락에 맞게 개발되었으며, 다른 사회 맥락에서의 함의를 탐색하는 추가 연구가 필요하다.&lt;/p>
&lt;p>safety 목표는 다양한 사회 그룹의 공통된 가치를 포착하려고 하지만, 문화적 규범의 차이로 인해 이를 보편화하는 것은 어렵다. 대화 시스템에 가치나 사회 규범을 적용하는 것은 복잡하며, 단일한 안전 목표나 미세 조정 데이터셋으로는 다양한 문화 규범을 모두 수용할 수 없다. 때문에 대화 에이전트의 행동을 더욱 세밀하게 분류하고 정의하는 것이 중요하며, 이는 모델이 특정 상황에서의 예의 규범과 일치하는지 테스트하는 데에도 필요하다.&lt;/p>
&lt;h3 id="appropriateness-as-a-concept-and-a-metric">Appropriateness as a concept and a metric&lt;/h3>
&lt;p>safety와 quality는 언어 생성에서 필수적인 요소이지만, 사용자 경험을 향상시키기 위해선 추가적인 고려가 필요하다. 특히, 예의바름과 동의성과 같은 사회 언어학적 특성은 safety와 분리되어 측정되어야 한다. 언어의 공식성 수준은 문화에 따라 사용자 경험에 다르게 영향을 미치며, 사용자들은 종종 인간과 같이 행동하는 기계에 대해 인간과 같은 기대를 가지는 경향이 있다. 이러한 이유로, 생성적 언어 모델에서 적절성을 조정하는 방법이 필요하다.&lt;/p>
&lt;p>사회적 적절성은 맥락에 따라 다르고 보편적이지 않아, 생성적 언어 모델에 보편적인 제약 조건을 적용하는 것은 어렵다. 그러나 모델의 적절성을 미세 조정함으로써, safety 문제를 악화시키지 않고도 사용자 경험을 향상시킬 수 있다.&lt;/p>
&lt;h3 id="cultural-responsiveness">Cultural responsiveness&lt;/h3>
&lt;p>safety 목표 측정은 사회-문화적 맥락에 크게 의존하고, 대표성이 부족한 그룹과 글로벌 남방에 대한 데이터 대표성 개선 연구가 증가하고 있다. LaMDA를 전 세계 사용자에게 적용할 때는 이러한 격차를 주의 깊게 고려해야 한다.&lt;/p>
&lt;p>safety 측정은 시스템이 사용될 사회적 맥락을 고려하고, &amp;ldquo;participatory ﬁnetuning&amp;rdquo; 접근법을 통해 관련 커뮤니티를 데이터 수집 및 큐레이션 과정에 참여시켜야 한다. safety에 대한 이해는 문화적, 개인적 차이에 따라 달라, 단일한 safety 지표를 정의하는 것은 어려울 수 있다.&lt;/p>
&lt;h3 id="impersonation-and-anthropomorphization">Impersonation and anthropomorphization&lt;/h3>
&lt;p>LaMDA는 인간 대화를 모방하는 학습 방식을 사용한다. 이로 인해 인공 시스템과의 대화가 인간 대화와 구별하기 어려울 정도로 자연스러워질 가능성이 있다. 하지만 이런 상황은 인공 시스템이 사람들을 속이거나 조작하는 위험을 내포하고 있다. 또한, 이 기술을 악용해 특정 개인을 모방하여 명예를 훼손하거나 오정보를 퍼뜨릴 수도 있다. 이러한 위험을 연구하고 완화하는 것은 이 기술이 발전함에 따라 앞으로 중요해질 영역이다.&lt;/p>
&lt;h3 id="future-work">Future work&lt;/h3>
&lt;p>현재 접근법의 한계에도 불구하고, 소량의 미세 조정 데이터로도 진전이 가능하였다. 이는 더 많은 연구를 통해 성능 향상이 가능할 것임을 시사한다.&lt;/p>
&lt;p>이후 연구에서는 safety 목표의 차원을 확장하고 수정하며, 판별자 학습을 위한 레이블된 학습 데이터의 양을 크게 늘릴 계획이다. 또한, 작업자의 모집, 훈련, 성과 평가를 계속 주의 깊게 보고, 문화 간의 가치와 의견 차이를 보정할 필요가 있다.&lt;/p>
&lt;p>다른 응용 프로그램들이 각각의 위험/이익 트레이드오프에 따라 safety, quality, groundedness에 대해 다른 수준을 요구할 수 있는지 연구하는 것이 또 다른 가능한 탐색 영역이다. 미세 조정 방법은 이러한 적응을 지원할 수 있어야 한다.&lt;/p>
&lt;p>모델의 바람직한 가치와 행동에 대한 관점은 다양하며, 미세 조정을 통해 일부 해로운 출력을 줄일 수 있음에도 불구하고, safety와 groundedness에 대한 미묘한 정의에 대한 광범위한 합의를 이루는 것은 개방형 대화 시스템 분야에서의 장기적인 도전 과제가 될 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="energy-and-carbon-footprint-estimate-of-lamda">Energy and Carbon Footprint Estimate of LaMDA&lt;/h2>
&lt;p>LaMDA의 가장 큰 모델은 1024개의 TPU-V3 칩으로 57.7일 동안 사전 학습되었고, 총 FLOPS는 GPT-3보다 높다. 하지만 에너지 비용은 GPT-3의 0.4배이며, 탄소 발자국은 GPT-3보다 21.2배 작다. 이는 에너지 혼합이 더 최적화되어 있기 때문이다. 따라서, LaMDA의 학습은 샌프란시스코와 뉴욕 간 왕복을 하는 22명의 승객의 탄소 발자국에 해당한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>이 논문은 규모, 모델 미세 조정을 위한 주석 데이터, 대화 모델링에서 정보 검색의 중요성을 연구한다. 규모 증가만으로도 모든 지표가 향상되지만, 안전성과 실제성은 인간 성능에 비해 떨어진다. 군중이 주석을 단 데이터는 추가적인 향상을 이끌어내는 효과적인 도구라는 것을 발견했으며, 외부 API를 호출하는 것은 실제성을 크게 향상시키는 방법으로 나타났다.&lt;/p>
&lt;p>응용 프로그램별로 사전 학습(PT)과 LaMDA 모델의 도움이 되는 정도와 역할 일관성을 비교하는 실험을 수행하였다. 모델을 빠르게 적응시키기 위해, 응용 프로그램별 대화의 일부에 대해 모델을 사전 조건화했다. 두 모델 유형 모두 예상 맥락에 적응할 수 있으며, 응답의 대부분이 할당된 역할과 일관성을 유지하였다. 그러나 LaMDA 기반 응용 프로그램이 PT 응용 프로그램보다 훨씬 더 도움이 되었다.&lt;/p>
&lt;p>LaMDA는 실용적이고 안전한 개방형 대화 시스템에 한 걸음 더 다가섰으며, 이는 다양한 유용한 응용 프로그램을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2201.08239.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/conceptofmind/LaMDA-rlhf-pytorch" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GLaM</title><link>https://kurtkim.github.io/p/glam/</link><pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/glam/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문에서는 GLaM이라는 범용 언어 모델을 제안한다. 이 모델은 sparsely activated mixture-of-experts 아키텍처를 사용하여 모델 용량을 확장하면서도 학습 비용을 크게 줄인다. 가장 큰 GLaM은 GPT-3보다 약 7배 큰 1.2T 개의 parameter를 가지고 있다. 그러나 GPT-3를 학습시키는 데 필요한 에너지의 1/3만 소비하고, 추론을 위한 연산은 절반만 필요하면서도 29개의 NLP 작업에서 전반적으로 더 나은 성능을 보인다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>지난 10년간 언어 모델은 자연 언어 처리(NLP)의 발전에 크게 기여했다. 이는 사전 학습된 단어 벡터와 문맥화된 단어 벡터의 생성을 통해 이루어졌다. 더 많은 데이터와 큰 모델로의 확장은 레이블이 적게 붙은 데이터로도 복잡한 언어 작업을 수행할 수 있게 했다. GPT-3와 FLAN은 적은 수의 레이블이 붙은 예시로도 좋은 성능을 내는 것이 가능하다는 것을 보여주었다. 그러나, 모델을 더 확장하는 것은 점점 비싸지고 많은 에너지를 소비하게 되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/table1.png"
width="646"
height="222"
srcset="https://kurtkim.github.io/p/glam/images/table1_hu2c1f554ce702a254016f37539d1daa19_44676_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/table1_hu2c1f554ce702a254016f37539d1daa19_44676_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="698px"
>&lt;/p>
&lt;p>이 연구에서는 대규모 sparsely activated 네트워크인 GLaM이 계산 효율성을 높이면서도 few-shot 작업에서 state-of-the-art dense 모델과 경쟁력 있는 결과를 달성할 수 있다는 것을 보여준다. 가장 큰 GLaM 모델은 총 1.2T의 parameter를 가지고 있으며, 입력 배치의 각 토큰은 이 중 8%만 활성화한다. 이 모델은 다양한 NLP 벤치마크에서 GPT-3보다 학습 효율성이 크게 향상되었으며, 학습 중의 총 에너지 소비는 GPT-3의 삼분의 일에 불과하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/figure1.png"
width="1342"
height="404"
srcset="https://kurtkim.github.io/p/glam/images/figure1_hu16b2446864ff5030093e6cab3bdb20ee_132511_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/figure1_hu16b2446864ff5030093e6cab3bdb20ee_132511_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="332"
data-flex-basis="797px"
>&lt;/p>
&lt;p>GLaM을 사용하여 데이터의 중요성을 연구한다. 이 논문의 분석은 이러한 대규모 모델에 대해서도, 고품질의 auto-regressive 언어 모델을 생성하는 것이 목표라면 데이터의 양을 위해 품질을 희생해서는 안 된다는 것을 보여준다. 더욱 중요한 것은, 사회적 차원에서, 이 논문의 결과는 우리가 알기로는 WinoGender 벤치마크에서 stereotypical 예제와 anti-stereotypical 예제 사이의 성능 격차를 닫는 첫 번째 결과이며, 이는 대규모 sparsely activated 모델이 표면적인 통계적 상관관계에 덜 의존할 수 있다는 것을 제안한다.&lt;/p>
&lt;p>이 연구는 MoE-based sparse decoder-only 언어 모델이 비슷한 컴퓨팅 FLOPs의 밀집 아키텍처보다 성능이 뛰어날 수 있다는 것을 처음으로 보여주었다. 이는 에너지 비용을 절약하면서 고품질 NLP 모델을 달성하기 위한 가장 유망한 방향 중 하나를 제시하며, 따라서 MoE는 향후 확장에 대한 강력한 후보로 고려되어야 한다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Language models.&lt;/strong> 신경 언어 모델은 다양한 자연어 처리 작업에 유용하며, word2vec, GloVe, paragraph vectors와 같은 단어 임베딩 모델은 임베딩을 전달함으로써 여러 작업에 대해 훌륭한 일반화 능력을 보여준다.&lt;/p>
&lt;p>&lt;strong>Pre-training and Fine-tuning.&lt;/strong> 계산력과 데이터의 풍부함으로 더 큰 모델들을 비지도 학습으로 학습하는 것이 가능해졌다. RNN과 LSTM 같은 순환 모델을 이용한 연구는 언어 모델을 미세조정해 다양한 언어 이해 작업을 개선할 수 있음을 보여주었다. 또한, Transformer를 사용한 모델은 레이블이 없는 데이터에 대한 자기 감독을 통해 NLP 작업에서 큰 개선을 이루었다. 사전 학습과 미세 조정을 기반으로 한 전이 학습은 downstream task에서 좋은 성능을 보여주었으나, 작업 특정 미세 조정이 필요한 것이 주요 제한사항이다.&lt;/p>
&lt;p>&lt;strong>In-Context Few-shot Learning.&lt;/strong> 언어 모델의 확장, 예를 들어 GPT-3 등은 작업에 구애받지 않는 few-shot 성능을 크게 향상시킨다는 것을 보여주었다. 이러한 모델은 gradient 업데이트 없이 적용되며, 모델과의 텍스트만을 이용한 few-shot 시연만 필요합니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/table2.png"
width="666"
height="320"
srcset="https://kurtkim.github.io/p/glam/images/table2_hu215353afbf2be3f89fa1b15f3a1a7ef4_67207_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/table2_hu215353afbf2be3f89fa1b15f3a1a7ef4_67207_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="208"
data-flex-basis="499px"
>&lt;/p>
&lt;p>&lt;strong>Sparsely Gated Networks.&lt;/strong> Mixture-of-Experts 기반 모델은 효과적으로 많은 수의 가중치를 사용하면서 추론 시간에는 계산 그래프의 작은 부분만 계산함으로써 언어 모델링과 기계 번역에서 중요한 이점을 보여주었다. 최근에는 1T 개의 parameter를 가진 sparsely activated 모델(Switch-C)이 상당한 성과를 보여주었다. GLaM과 Switch-C 모두 1T 개의 학습 가능한 parameter를 가지지만, GLaM은 decoder-only 언어 모델이며, Switch-C는 encoderdecoder 기반 sequence to sequence 모델이다. 또한, GLaM은 미세 조정 없이도 few-shot 설정에서 잘 수행된다.&lt;/p>
&lt;hr>
&lt;h2 id="training-dataset">Training Dataset&lt;/h2>
&lt;p>다양한 자연어 사용 사례를 대표하는 1.6T 개의 토큰으로 구성된 고품질 데이터셋을 구축하여 모델을 학습시켰다. 레이블이 없는 데이터셋의 대부분은 웹 페이지로, 품질이 다양하다. 고품질 웹 말뭉치를 생성하기 위해 자체 텍스트 품질 분류기를 개발하였고, 이를 통해 웹페이지의 콘텐츠 품질을 추정하였다. 분류기의 체계적인 편향을 방지하기 위해 점수에 따라 웹페이지를 샘플링하는 방식을 적용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/table3.png"
width="576"
height="242"
srcset="https://kurtkim.github.io/p/glam/images/table3_hue2cd63adfaa18c71b7f281f831ffc3e5_33158_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/table3_hue2cd63adfaa18c71b7f281f831ffc3e5_33158_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="238"
data-flex-basis="571px"
>&lt;/p>
&lt;p>고품질 웹페이지의 필터링된 부분 집합을 생성하고, 이를 다른 데이터 소스와 결합하여 최종 GLaM 데이터셋을 만들었다. 각 데이터 구성 요소의 성능과 작은 데이터 소스가 과도하게 샘플링되는 것을 방지하기 위해 mixture 가중치를 설정하였다. 데이터 오염을 확인하기 위해 학습 데이터와 평가 데이터 사이의 중복성을 분석했고, 이는 이전 연구와 일치하는 것을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/figure2.png"
width="558"
height="632"
srcset="https://kurtkim.github.io/p/glam/images/figure2_hue7efad30421e3221d12c57ae5b01013b_104106_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/figure2_hue7efad30421e3221d12c57ae5b01013b_104106_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;p>GLaM 모델은 sparsely activated Mixture-of-Experts(MoE) 방식을 사용한다. 이는 Transformer layer의 일부를 MoE 계층으로 대체하여, 독립적인 feed-forward network 집합이 각각 &amp;ldquo;expert&amp;rdquo; 역할을 하는 방식이다. gating 함수를 통해 이 expert들에 대한 확률 분포를 모델링하며, 이는 각 expert가 입력을 얼마나 잘 처리하는지를 나타낸다.&lt;/p>
&lt;p>MoE layer는 많은 parameter를 가지지만, expert들은 sparsely activated 되므로 모델의 용량을 늘리면서 계산을 제한한다. 이 layer의 gating network는 입력 시퀀스의 각 토큰에 대해 최적의 두 개의 expert를 활성화하도록 학습되며, 추론 시에는 동적으로 두 expert를 선택합니다. 이로 인해 훨씬 더 많은 계산 유연성을 가진 다양한 feed-forward network 조합이 제공된다. 토큰의 최종 표현은 선택된 expert들의 출력의 가중 조합으로 구성된다.&lt;/p>
&lt;p>원래의 Transformer 아키텍처에 다양한 수정을 가했다. standard positional embedding을 per-layer relative positional bias으로 대체하고, non-MoE Transformer feed-forward sub-layer에서는 첫 번째 linear projection과 activation 함수를 Gated Linear Unit으로 변경하였다. 또한, 큰 GLaM 모델의 가중치와 계산을 분할하기 위해 2D sharding 알고리즘을 사용하였다.&lt;/p>
&lt;hr>
&lt;h2 id="experiment-setup">Experiment Setup&lt;/h2>
&lt;p>GLaM은 dense 및 sparse decoder-only 언어 모델 집합이며, 훈련 설정, hyperparameter, 평가 방법에 대해 상세히 설명한다.&lt;/p>
&lt;h3 id="training-setting">Training Setting&lt;/h3>
&lt;p>GLaM의 여러 변형을 학습시켜 MoE와 dense 모델의 동작을 연구하였다. 이는 130M 개의 parameter에서 1.2T 개의 parameter에 이르는 다양한 규모의 GLaM 모델의 hyperparameter 설정을 포함한다. 또한, 각 모델의 학습 가능한 parameter의 총 수, 입력 토큰당 활성화된 parameter의 수 등을 고려하였고, 이는 추론 중에 토큰당 활성화된 parameter의 수가 비슷한 dense 모델과 비교되었다.&lt;/p>
&lt;p>$$ GLaM (Base Dense Size/E) \ \ e.g., GLaM (8B/64E) $$&lt;/p>
&lt;p>GLaM 모델의 다양한 변형을 표현하기 위해, 특정 표기법을 사용한다. 예컨대, $GLaM (8B/64E)$는 대략 8B 개의 parameter를 가진 dense 모델로, 각 layer가 64개의 expert MoE layer로 대체된 구조를 나타낸다. 만약 각 MoE layer가 하나의 expert만 가진다면, GLaM은 dense Transformer-based 언어 모델로 간주된다.&lt;/p>
&lt;p>$$ GLaM (Dense Size) \ \ e.g., GLaM (137B) $$&lt;/p>
&lt;p>이는 동일한 데이터셋으로 학습된 dense 137B 개의 parameter 모델을 가리킨다.&lt;/p>
&lt;h3 id="hyperparameters-and-training-procedure">Hyperparameters and Training Procedure&lt;/h3>
&lt;p>모든 GLaM 모델은 동일한 학습 hyperparameter를 사용한다. 최대 시퀀스 길이는 1024 토큰, batch 당 최대 100만 토큰으로 설정하였고, dropout rate는 0이다. optimizer로는 Adafactor를 사용하며, initial learning rate는 처음 10K 학습 step 동안 0.01을 유지하고, 이후에는 inverse square root schedule로 감소시킨다. standard cross√ entropy 손실 외에도, expert load balancing을 촉진하기 위해 MoE auxiliary 손실을 추가한다. 토큰화에는 256K 크기의 어휘를 가진 SentencePiece를 사용하였고, 모델 가중치는 float32, 활성화는 bfloat16을 사용한다. 가장 큰 GLaM 모델은 1,024개의 Cloud TPU-V4 칩에서 학습되었다.&lt;/p>
&lt;p>trillion parameter 규모의 모델 학습은 비용이 많이 들며, hyperparameter 튜닝에는 여유가 거의 없다. 이에 대한 해결책으로, GLaM 모델을 위한 학습 레시피와 구현 방법을 제공한다.&lt;/p>
&lt;ul>
&lt;li>데이터셋과 인프라의 잠재적 문제를 빠르게 찾기 위해, 먼저 작은 규모의 모델을 학습시킨다.&lt;/li>
&lt;li>그래디언트에 $NaNs$ 또는 $Infs$가 있으면 batch의 가중치 업데이트를 생략한다. gradient 적용 단계에서도 $NaN/Inf$가 발생할 수 있으며, 이럴 경우 이전 체크포인트에서 다시 시작한다. 이는 업데이트된 변수가 $Inf$를 초래할 수 있기 때문이다.&lt;/li>
&lt;li>학습 중 큰 변동이나 $NaN/Inf$를 만나면 초기의 안정적인 체크포인트에서 다시 시작한다. 재시작 후, 순차적으로 로드된 batch의 무작위성이 이전 실패 상태를 벗어나는데 도움이 된다.&lt;/li>
&lt;/ul>
&lt;h3 id="evaluation-setting">Evaluation Setting&lt;/h3>
&lt;p>&lt;strong>Protocol.&lt;/strong> GLaM 모델의 효과를 보여주기 위해, zero-shot, one-shot, few-shot 학습 프로토콜을 평가합니다. zero-shot 학습에서는 개발 세트의 각 예제를 직접 평가하며, one-shot/few-shot 학습에서는 해당 작업의 학습 세트에서 무작위 예제를 데모와 컨텍스트로 사용한다. 이 데모는 평가 예제와 함께 모델에 공급된다.&lt;/p>
&lt;p>&lt;strong>Benchmarks.&lt;/strong> GPT-3와 GLaM을 비교하기 위해, 동일한 평가 작업 세트를 선택하였다. 단순성을 위해 7개의 합성 작업과 6개의 기계 번역 데이터셋을 제외하였고, 결과적으로 8개의 자연어 생성 작업과 21개의 자연어 이해 작업을 포함한 29개의 데이터셋을 사용한다. 이들은 추가로 7개의 카테고리로 분류된다.&lt;/p>
&lt;p>&lt;strong>Natural Language Generative tasks.&lt;/strong> 생성 작업에서는 모델이 디코딩한 언어 시퀀스와 실제 값을 비교한다. 이들 작업은 TriviaQA, NQS, WebQS, SQuADv2, LAMBADA, DROP, QuAC, CoQA 등이며, 성능은 정확한 일치(EM)와 F1 점수로 측정된다. beam search의 width 4를 사용하여 시퀀스를 생성한다.&lt;/p>
&lt;p>&lt;strong>Natural Language Understanding tasks.&lt;/strong> 대부분의 언어 이해 작업은 모델이 여러 옵션 중에서 하나를 선택하도록 하며, 이는 이진 분류 작업에도 적용된다. 예측은 각 옵션의 maximum log-likelihood에 따라 이루어지며, 몇몇 작업에서는 정규화되지 않은 손실이 더 좋은 결과를 가져온다. 모든 작업에서 예측 정확도 메트릭이 사용되며, 모든 데이터셋에서 보고된 점수의 평균을 이용해 모델의 전체 few-shot 성능을 보고한다. 정확도와 F1 점수는 0에서 100 사이로 정규화되며, TriviaQA에서는 one-shot 제출의 테스트 서버 점수도 제공한다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>GLaM 모델군에 대한 평가를 통해 언어 모델링에서 sparsely activated 모델의 장점과 스케일링 추세를 확인하였고, 데이터 품질이 언어 모델 학습에 얼마나 효과적인지 정량적으로 조사하였다.&lt;/p>
&lt;h3 id="comparison-between-moe-and-dense-models">Comparison between MoE and Dense Models&lt;/h3>
&lt;p>GLaM (64B/64E)은 zero-shot, one-shot, few-shot 학습에서 GPT-3 (175B)에 비해 경쟁력 있는 성능을 보여주며, 7개 카테고리 중 6개에서 평균적으로 우수하다. 더 큰 Megatron-NLG와 Gopher의 결과도 포함하였으며, GLaM은 추론 중에 토큰 당 약 96.6B의 parameter를 활성화하며, 이는 GPT-3가 필요로 하는 컴퓨팅 FLOPs의 절반만 필요로 한다.&lt;/p>
&lt;p>오픈 도메인 질문 답변 작업인 TriviaQA에서 GLaM (64B/64E)은 추가적인 컨텍스트 없이 질문에 직접 답하며, dense 모델과 이전의 미세 조정된 state-of-the-art를 능가한다. one-shot 결과는 이전의 미세 조정된 state-of-the-art를 8.6%, 테스팅 서버에서의 few-shot GPT-3를 5.3% 능가하며, 이는 GLaM의 추가 용량이 성능 향상에서 중요한 역할을 한다는 것을 보여준다. 비슷한 총 parameter를 가진 Switch-C와 비교하면, GLaM은 더 큰 expert를 사용하여 one-shot 성능이 더 좋다.&lt;/p>
&lt;h3 id="effect-of-data-quality">Effect of Data Quality&lt;/h3>
&lt;p>downstream task의 few-shot 성능에 대한 데이터 품질의 영향을 연구하였다. 중간 크기의 GLaM 모델 (1.7B/64E)을 이용해 텍스트 필터링이 모델 품질에 어떤 효과를 미치는지 보여준다. 원래 데이터셋과 필터링된 웹페이지를 필터링되지 않은 웹페이지로 교체한 데이터셋 두 가지에서 모델을 학습시켰다. 필터링된 웹페이지는 143B의 토큰, 필터링되지 않은 웹페이지는 약 7T의 토큰으로 구성되어 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/figure3.png"
width="1338"
height="402"
srcset="https://kurtkim.github.io/p/glam/images/figure3_hue1e032c8d6589da7599b21ec5d178a1b_195510_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/figure3_hue1e032c8d6589da7599b21ec5d178a1b_195510_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="332"
data-flex-basis="798px"
>&lt;/p>
&lt;p>필터링된 데이터에서 학습된 모델은 NLG와 NLU 작업 모두에서 더 나은 성능을 보이며, 특히 NLG에서 필터링의 효과가 더 크다. 이는 고품질의 언어 생성이 요구되는 NLG에서 필터링된 사전 학습 말뭉치가 중요한 역할을 하기 때문일 수 있다. 이 연구는 사전 학습된 데이터의 품질이 downstream task의 성능에 결정적인 역할을 한다는 것을 강조하였다.&lt;/p>
&lt;h3 id="scaling-studies">Scaling Studies&lt;/h3>
&lt;p>dense 언어 모델을 확장하는 과정은 모델을 깊게 만들고, 토큰 임베딩 차원을 증가시키는 것을 포함하며, 이는 모델의 전체 parameter 수를 증가시킨다. 이러한 모델은 주어진 입력에 대한 모든 예측에서 모든 parameter가 활성화되므로, 예측 당 효과적인 FLOPs는 모델 크기와 선형적으로 증가한다. 이는 예측 성능을 향상시키지만, 예측 당 전체 비용을 높이게 된다.&lt;/p>
&lt;p>GLaM MoE 모델은 각 예측에 대해 전체 parameter 중 일부만 활성화되므로, MoE layer의 expert 크기나 수를 증가시킴으로써 모델을 확장할 수 있다.&lt;/p>
&lt;p>생성 작업에 대한 평균 zero, one, few-shot 성능은 예측 당 효과적인 FLOPs와 잘 맞고, 이는 n act-params에 의해 결정된다. GLaM MoE 모델은 토큰 당 비슷한 FLOPs에서 dense 모델보다 더 나은 성능을 보여준다. 언어 이해 작업에서도 GLaM MoE 모델은 생성 작업과 비슷한 성능 향상을 보이며, 작은 스케일에서는 MoE와 dense 모델이 비슷하지만 큰 스케일에서는 MoE 모델이 우수하다. 예측 당 고정된 계산 예산에서 더 많은 expert를 추가하면 일반적으로 예측 성능이 향상된다.&lt;/p>
&lt;h3 id="efﬁciency-of-glam">Efﬁciency of GLaM&lt;/h3>
&lt;p>기존의 large dense 언어 모델들은 학습과 서비스 제공에 많은 계산 자원을 필요로 하며, 대량의 사전 학습 데이터를 소비한다. 이에 대한 GLaM 모델의 데이터와 계산 효율성을 조사한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/figure4.png"
width="1312"
height="748"
srcset="https://kurtkim.github.io/p/glam/images/figure4_hue642d3efa68f101ddb9bbc3622f750d8_279955_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/figure4_hue642d3efa68f101ddb9bbc3622f750d8_279955_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;p>&lt;strong>Data Efﬁciency.&lt;/strong> GLaM MoE 모델은 비슷한 FLOPs의 dense 모델보다 훨씬 적은 데이터로 같은 성능을 달성하며, 학습에 사용되는 데이터가 같을 때 MoE 모델의 성능이 더 뛰어나며, 학습이 630B까지 이루어질 때 성능 차이는 더 커진다. 또한, 280B 토큰으로 학습된 GLaM (64B/64E) 모델은 6개 학습 설정 중 4개에서 300B 토큰으로 학습된 GPT-3를 크게 능가하고, 나머지 설정에서는 GPT-3와 동일한 성능을 보여준다.&lt;/p>
&lt;p>&lt;strong>Computation Efﬁciency &amp;amp; Energy Consumption.&lt;/strong> sparsely activated 모델을 학습하는 것이 dense 모델을 학습하는 것보다 훨씬 적은 계산 자원을 필요로 하며, 비슷한 성능을 달성함을 확인하였다.&lt;/p>
&lt;p>GLaM (64B/64E) 학습은 GPT-3에 비해 약 1/3인 456 MWh의 에너지를 소비한다. GPT-3와 비슷하거나 약간 더 높은 성능을 얻기 위해, 1,024개의 TPU-v4 칩을 사용하여 280B 토큰으로 574시간 동안 학습하며, 이는 GPT-3의 에너지 비용의 1/6인 213 MWh를 소비한다. 이런 에너지 소비 감소는 MoE 아키텍처와 TPU-v4 하드웨어, GSPMD 소프트웨어의 계산 효율성 최적화 덕분이다.&lt;/p>
&lt;hr>
&lt;h2 id="ethics-and-unintended-biases">Ethics and Unintended Biases&lt;/h2>
&lt;p>거대 언어 모델의 zero-shot과 few-shot 추론 기능은 자연어와 소규모 데이터셋을 이용해 직관적으로 모델을 제어하고, AI 사용을 민주화하는 잠재력을 가지고 있다. 그러나 이러한 기회는 대표성 편향, 학습 데이터의 적절한 선택과 처리, 개인정보 보호, 환경 문제 등 많은 윤리적 도전의 중요성을 강조하기도 한다. 언어 모델이 배우는 의도하지 않은 편향에 대한 연구는 활발하게 진행되고 있지만, 해로운 스테레오타입을 어느 정도 인코딩하는지 평가하는 더 엄격한 방법이 여전히 필요하다는 인식이 있다.&lt;/p>
&lt;p>대형 언어 모델에 대한 측정 방법이나 기준에 대한 합의는 아직 없지만, 이런 모델들의 다양성과 능력 때문에 다양한 지표로 평가하는 것이 중요하다. GPT-3에서 영감을 받아 생성된 텍스트에서 정체성 용어의 동시 발생을 검토하고, WinoGender 벤치마크를 보고하며, Gopher와 비슷하게 독성의 저하를 분석하고, 인간 행동의 기준을 고려하는 분석을 확장한다.&lt;/p>
&lt;h3 id="co-occurrence-prompts">Co-occurrence prompts&lt;/h3>
&lt;p>프롬프트로 &amp;ldquo;{ term } was very&amp;hellip;&amp;rdquo; 형태의 문장을 주어졌을 때, 자주 동시에 나타나는 단어를 분석한다. 이때 대체되는 용어는 성별, 종교, 인종 및 민족 신원을 참조한다. 각 프롬프트에 대해 상위 $k$ 샘플링을 사용하여 800개의 결과를 생성하며, 불용어를 제거하고 형용사와 부사만 선택한다. 이 분석은 수동적인 인간 라벨링을 생략하여 투명하고 쉽게 재현 가능하게 한다.&lt;/p>
&lt;p>모든 차원에서 연관 편향이 분명하다는 것을 확인하였다. 예를 들어, &amp;ldquo;pretty&amp;quot;는 &amp;ldquo;She&amp;quot;에 가장 많이 연관된 단어지만 &amp;ldquo;He&amp;quot;의 상위 10개 단어에는 포함되지 않는다.&lt;/p>
&lt;h3 id="winogender">WinoGender&lt;/h3>
&lt;p>Coreference resolution은 기계 번역과 질문 응답 등 많은 응용 프로그램에서 중요하다. GLaM에서 성별 상관성이 coreference error를 일으키는지 평가하기 위해 WinoGender를 측정하였다. GLaM은 전체 데이터셋에서 새로운 state-of-the-ar인 71.7%를 달성하였다. 또한, &amp;ldquo;he&amp;quot;와 &amp;ldquo;she&amp;rdquo; 예시, 그리고 stereotypical 예시와 anti-stereotypical 예시 사이에서의 정확도가 비슷하였다.&lt;/p>
&lt;h3 id="toxicity-degeneration">Toxicity Degeneration&lt;/h3>
&lt;p>Toxicity degeneration는 언어 모델이 무의식적으로 독성 있는 텍스트를 생성하는 것을 의미한다. 이를 평가하기 위해, RealToxicityPrompts 데이터셋을 사용하며, 이 데이터셋은 프롬프트 접두사와 연속하는 접미사로 분할된 문장을 포함한다. 텍스트가 무례하거나 불쾌하거나 대화를 떠나게 만들 가능성에 대한 확률을 할당하는 Perspective API를 사용한다. 그리고 프롬프트가 독성 있는 가능성을 고려하여 연속적인 부분이 독성 있는 가능성을 평가한다.&lt;/p>
&lt;p>무작위로 선택한 10K개의 프롬프트마다 최대 100개의 토큰으로 이루어진 연속적인 내용을 25개 생성한다. 이는 top-k 샘플링을 사용하며, temperature는 1이다. 만약 연속적인 내용이 빈 문자열일 경우, Perspective API가 비어있지 않은 문자열을 필요로 하므로, 독성 점수를 0.0으로 할당한다. 이는 챗봇이 응답을 거부하는 경우를 나타낼 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/glam/images/figure5.png"
width="542"
height="390"
srcset="https://kurtkim.github.io/p/glam/images/figure5_hub5fa4b10816f8678fa1a40d4a9f987ef_62521_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/glam/images/figure5_hub5fa4b10816f8678fa1a40d4a9f987ef_62521_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>낮은 TPP에 대해 상대적으로 높은 인간 TPC는 독성 스펙트럼 전체에서 선택된 문장 때문이다. 독성은 종종 문장 내에서 식별되며, 이 데이터셋에서는 문장의 뒷부분에서 발생한다. 이로 인해 인간 TPC는 TPP가 증가함에 따라 약간 떨어진다. 반면, 모델의 TPC는 TPP를 밀접하게 따르며, 이는 대형 언어 모델이 프롬프트에 과도하게 영향을 받는다는 것을 보여준다.&lt;/p>
&lt;p>25개의 연속적인 내용에 대한 독성 확률 분포를 분석하였다. 이 분석은 낮은 독성 프롬프트에 대해서도 일부 생성된 연속적인 내용이 독성으로 판단될 가능성이 높다는 것을 보여준다. 이 데이터셋의 샘플링 전략과 출처인 Reddit가 다른 도메인을 반영하지 않을 가능성이 있다. 또한, 아주 낮은 TPP에 대해, 응용 프로그램들은 훨씬 더 낮은 TPC를 원할 것으로, 100개의 독성 제안 중 1개를 생성하는 것조차도 문제가 될 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>sparsely-activated 모델에 대한 이전 연구에 따르면, MoE 모델은 지식 지향적인 과제에서 더 우수한 성능을 보인다. 개방형 QA 벤치마크에서의 MoE 모델의 성능은 이러한 모델이 dense 모델에 비해 정보 용량이 크게 증가한 것을 보여준다. 그러나 sparsely-activated 모델은 더 많은 parameter를 가지므로 더 많은 장치가 필요하며, 이로 인해 리소스 접근성이 제한되고 서비스 비용이 증가한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>GLaM이라는 sparsely activated mixture-of-expert 아키텍처를 사용한 언어 모델을 개발하였다. 이 모델은 유사한 효율적인 FLOPs의 dense 모델과 GPT-3 모델보다 더 나은 평균 점수를 달성하였다. 특히, 가장 큰 모델인 GLaM (64B/64E)은 GPT-3 학습에 비해 에너지 소비량의 3분의 1만으로 더 나은 성능을 보여주었다. 이 작업이 고품질 데이터 획득과 거대한 언어 모델의 효율적인 확장에 대한 연구를 촉진하길 바란다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2112.06905.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>T0</title><link>https://kurtkim.github.io/p/t0/</link><pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/t0/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>최근의 연구에서 거대 언어 모델들이 다양한 작업에 대해 zero-shot 일반화를 잘 보여주고 있다. 이는 언어 모델의 사전학습 과정에서 암시적으로 multitask 학습이 이루어지기 때문이라는 가설이 있다. 이에 반해, 연구진은 zero-shot 일반화를 명시적인 multitask 학습으로 직접 유도할 수 있는지를 테스트하였다. 이를 위해 자연어 작업을 사람이 읽을 수 있는 프롬프트 형태로 변환하는 시스템을 개발하였고, 이를 사용해 다양한 supervised 데이터셋을 변환하였다. 결과적으로, 이 모델은 여러 표준 데이터셋에서 강력한 zero-shot 성능을 보여주었으며, 특히 그 크기가 16배인 모델을 능가하는 결과를 보였다. 또한 BIG-bench 벤치마크의 일부 작업에서도 우수한 성능을 보였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure1.png"
width="1014"
height="600"
srcset="https://kurtkim.github.io/p/t0/images/figure1_hud67f67b92606292d3fa86cb9bab41cce_161805_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure1_hud67f67b92606292d3fa86cb9bab41cce_161805_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="169"
data-flex-basis="405px"
>&lt;/p>
&lt;p>최근의 연구에서는 거대 언어 모델이 새로운 작업에 대해 합리적인 zero-shot 일반화를 보여주는 것으로 나타났다. 이 모델들은 언어 모델링 목표에 대해서만 학습되었음에도 불구하고, 명시적으로 학습되지 않은 새로운 작업에 대해 상대적으로 잘 수행할 수 있다. 이는 거대 언어 모델이 암시적인 multitask 학습 과정을 통해 새로운 작업에 일반화한다는 가설을 뒷받침한다. 하지만 이 능력은 모델의 크기가 충분히 커야 하며, 프롬프트의 표현에 민감하다는 것을 고려해야 한다.&lt;/p>
&lt;p>multitask 학습이 얼마나 암시적인지는 아직 뚜렷이 알려지지 않았다. 최근 언어 모델의 사전학습 말뭉치 규모를 고려하면, 일부 자연어 처리(NLP) 작업들이 해당 말뭉치에서 명시적으로 나타나 모델이 직접 학습하는 것이 합리적으로 보인다. 예를 들어, 퀴즈 질문과 답변을 담은 웹사이트는 바로 closed-book 질문 대답 작업에 대한 지도 학습 데이터로 사용될 수 있다. 이러한 multitask supervision이 zero-shot 일반화에서 큰 역할을 한다는 가설을 세웠다.&lt;/p>
&lt;p>이 논문에서는 언어 모델을 supervised이며 massively multitask 방식으로 명시적으로 학습하는 방법에 대해 연구한다. 자연어 프롬프트로 표현된 다양한 작업들을 사용하여 모델이 보류된 작업에 더 잘 일반화하고 프롬프트의 단어 선택에 강건하게 만드는 것을 목표로 한다. 이를 위해, 구조화된 데이터셋에 대한 간단한 템플릿 언어를 사용하여 자연어 작업을 프롬프트 형식으로 변환하고, 공공 기여자들로부터 프롬프트를 수집하는 인터페이스를 개발하였다. 그 후, T5 encoder-decoder 모델의 변형을 일부 작업에 대해 학습하고, 학습되지 않은 작업과 프롬프트를 평가하였다.&lt;/p>
&lt;p>이 논문의 실험은 multitask 프롬프트 학습이 보류된 작업에 대한 일반화를 향상시키고, 더 넓은 범위의 프롬프트 학습이 프롬프트 단어 선택에 대한 robustness를 향상시키는지를 연구한다. 실험 결과, multitask 학습은 zero-shot 작업 일반화를 가능하게 하며, 모델은 GPT-3의 성능을 대부분의 보류된 데이터셋에서 매치하거나 초과한다. 또한, 데이터셋당 더 많은 프롬프트에 대한 학습이 보류된 작업에 대한 성능의 중간값을 향상시키고 변동성을 감소시키는 것을 확인하였다. 하지만, 더 넓은 범위의 데이터셋에서의 프롬프트 학습은 변동성을 일관되게 감소시키지는 않는다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>이 연구에서는 언어 모델 사전 학습에서의 implicit multitask 학습과 explicit multitask 학습을 구분한다. multitask 학습으로 학습된 모델은 자연어 처리에서 우수한 성능을 보였다. 각 과제는 다른 출력을 가지므로, 공유된 형식이 필요하며, 여러 가지가 사용되었다. 일부 연구에서는 대규모 사전 학습 모델을 사용하여 새로운 데이터셋에 대한 few-shot 및 zero-shot 일반화를 탐색하였다.&lt;/p>
&lt;p>자연어 프롬프팅은 NLP 작업을 자연어 응답 형식으로 변환하는 방법이다. text-to-text 사전 학습 모델, 예를 들면 T5의 발전으로, 프롬프트는 multitask 학습에 특히 유용하게 사용되고 있다. 이 방법은 여러 데이터셋을 하나의 프롬프트로 재구성하지만, 그 형식이 고정되어 있어 새로운 프롬프트나 작업에는 일반적으로 적용할 수 없다.&lt;/p>
&lt;p>Schick and Sch¨utze (2021), 그리고 Brown et al. (2020)은 모든 NLP 작업에 프롬프트 사용을 확대하였다. Mishra et al. (2021)은 이를 61개의 구체적 작업에 적용했고, 전통적인 NLP에서의 62개의 데이터셋과 12개의 작업에 적용해 일반화를 학습하고 측정하였다. 또한, zero-shot 일반화에 초점을 맞추었다. Wei et al. (2021)의 연구와는 프롬프트 다양성, 모델 규모, 보류된 작업 체계 등에서 차이가 있다.&lt;/p>
&lt;p>프롬프트의 성공은 모델이 이를 작업 지시문으로 이해하고 새로운 작업에 일반화한다는 가설로 설명된다. 그러나 프롬프트의 의미적 중요성이 얼마나 큰 역할을 하는지에 대한 의문이 제기되었다. 이 연구에서는 프롬프트가 어떻게 일반화를 지원하는지에 대해 명확한 결론을 내리지 않고, 프롬프트가 multitask 학습의 자연스러운 형식을 제공하고 새로운 작업에 대한 일반화를 실증적으로 지원한다고 주장한다.&lt;/p>
&lt;hr>
&lt;h2 id="measuring-generalization-to-held-out-tasks">Measuring Generalization To Held-Out Tasks&lt;/h2>
&lt;p>NLP 데이터셋이 작업별로 나뉘어 있다고 가정하고 시작한다. &amp;ldquo;task&amp;quot;는 특정 데이터셋 그룹으로 테스트되는 일반적인 NLP 능력을 의미한다. 새 작업에 대한 zero-shot 일반화를 평가하기 위해, 일부 작업에서 학습하고 나머지 작업에서 평가한다.&lt;/p>
&lt;p>NLP 작업 분류는 특히 독특한 기술을 분리하려 할 때 애매하다. 많은 데이터셋이 상식 지식을 평가하며, 일부는 상식을 독립적인 작업으로 정의한다. 그러나 상식 데이터셋은 타고난 지식에서부터 DIY 지시사항, 문화적 규범, 대학원 수준의 이론까지 다양하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure2.png"
width="1022"
height="680"
srcset="https://kurtkim.github.io/p/t0/images/figure2_hu8dc2fbeda69c75807444e19f93e7b386_206054_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure2_hu8dc2fbeda69c75807444e19f93e7b386_206054_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>작업별 그룹화는 완벽하지 않지만, 작업 형식에 따라 작업 분류를 구성하였다. 영어가 아닌 데이터셋이나 특별한 도메인 지식이 필요한 데이터셋은 제외했다. 결과적으로, 12개의 작업과 62개의 데이터셋이 학습 및 평가 mixture을 형성하게 되었다. 모든 실험은 Hugging Face 데이터셋 라이브러리에서 데이터셋을 사용하였다.&lt;/p>
&lt;p>zero-shot 일반화를 검증하기 위해, natural language inference(NLI), coreference resolution, sentence completion, word sense disambiguation이라는 4가지 작업의 데이터셋을 사용하지 않았다. 대부분의 사람들이 NLI 작업을 직관적으로 수행할 수 있으므로, NLI를 보류된 작업으로 선택하였다. 비슷한 이유로 coreference resolution와 단어 의미 해석도 보류되었고, sentence completion은 NLI와 너무 비슷하므로 보류되었다. 더불어, T0 모델은 Brown et al. 이 평가에 사용한 어떤 데이터셋에도 학습시키지 않았으며, 이를 통해 공정한 zero-shot 비교가 가능해진다. 또한, 이러한 작업들에 대한 데이터가 사전 학습된 코퍼스를 통해 유출되지 않았음을 확인하였다.&lt;/p>
&lt;p>마지막으로, 거대 언어 모델의 능력을 테스트하기 위한 다양한 작업들을 모은 BIG-bench의 데이터셋 일부를 추가로 평가하였다. 이 데이터셋은 T5 토크나이저의 어휘에 속하는 텍스트를 포함하는 언어 중심의 작업들로 구성되어 있다. BIG-bench의 모든 작업들은 이 논문의 학습에서 제외된 새로운 작업들이다.&lt;/p>
&lt;hr>
&lt;h2 id="a-unified-prompt-format">A Unified Prompt Format&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure3.png"
width="992"
height="356"
srcset="https://kurtkim.github.io/p/t0/images/figure3_hua0584bb5d9211ea8b0a5989b58c7120e_94740_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure3_hua0584bb5d9211ea8b0a5989b58c7120e_94740_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="278"
data-flex-basis="668px"
>&lt;/p>
&lt;p>모든 데이터셋이 자연어 형식의 프롬프트로 제공되어 zero-shot 실험을 가능하게 한다. 다양한 데이터셋을 프롬프트로 쉽게 변환할 수 있도록, 템플릿 언어와 애플리케이을 개발하였다. 템플릿은 데이터 예제를 자연어 입력과 목표 시퀀스로 매핑하는 기능을 가지며, 사용자는 임의의 텍스트를 데이터 필드나 메타데이터 등과 섞을 수 있다. 예를 들어, NLI 데이터셋은 전제, 가설, 라벨이라는 필드를 포함하며, 입력 템플릿은 &amp;ldquo;If { Premise } is true, is it also true that { Hypothesis } ?&amp;ldquo;와 같이 될 수 있다. 각 데이터 예제는 다양한 프롬프트 템플릿으로 구현된다.&lt;/p>
&lt;p>프롬프트를 개발하기 위해, 데이터셋에 대한 상호작용적 프롬프트 작성 인터페이스를 만들었고, 연구 커뮤니티에 프롬프트 기여를 요청하였다. 이에 8개 국가의 24개 기관의 36명이 참여하였다. 이 논문의 목표는 프롬프트 형식에 강인한 모델을 학습시키는 것이었고, 효과적인 프롬프트를 만드는 방법에 대한 문제가 아직 미해결 상태이므로, 기여자들에게 다양한 스타일의 프롬프트 작성을 장려하였다. 프롬프트는 문법적이고 이해 가능해야 했으며, 특정 계산이나 숫자 인덱싱을 요구하는 프롬프트는 자연어 버전을 선호하여 제거되었다.&lt;/p>
&lt;p>대부분의 프롬프트는 원래 제안된 작업에 직접 연결되지만, 작업 순서를 변경하는 프롬프트도 허용된다. 이러한 변경된 프롬프트는 다양성을 높이기 위해 학습에 포함되지만, 원래 데이터셋의 측정 기준과 기준선에서 벗어나므로 평가 결과에는 포함되지 않는다.&lt;/p>
&lt;p>비자연어나 잠재적으로 유해한 내용을 포함한 데이터셋을 제외하고 영어 데이터셋에 대한 프롬프트를 수집하였다. 이 컬렉션을 Public Pool of Prompts(P3)라고 하며, 현재 P3는 평균적으로 데이터셋당 11.7개의 프롬프트로, 총 2073개의 프롬프트를 177개 데이터셋에 대해 가지고 있다. 실험에 사용된 모든 프롬프트는 P3에서 출처가 있으며, BIG-bench의 프롬프트는 그 관리자들이 제공하였다.&lt;/p>
&lt;hr>
&lt;h2 id="experimental-setup">Experimental Setup&lt;/h2>
&lt;p>&lt;strong>Model&lt;/strong> 사전 학습된 모델을 자연어 프롬프트 데이터셋의 multi-task 학습 mixture에서 미세 조정한다. 이 모델은 encoder-decoder 구조를 사용하며, encoder에 입력 텍스트를 제공하고 decoder에서 목표 텍스트를 생성한다. standard maximum likelihood 학습을 통해 목표를 자동으로 생성하도록 학습되지만, GPT-3와 같은 decoder 전용 언어 모델과는 달리 입력을 생성하도록 학습되지 않는다.&lt;/p>
&lt;p>학습한 모든 모델은 T5를 기반으로 하며, 이는 1T 개의 토큰에 대해 masked language modeling 목표로 사전 학습된 모델이다. 그러나 T5의 사전 학습 목표는 프롬프트 데이터셋의 자연 텍스트 생성 형식과는 다르기 때문에, 표준 언어 모델링 목표에 따라 C4에서 추가로 100B 개의 토큰에 대해 학습된 Lester et al. (2021)의 LM-adapted T5 모델을 사용하였다.&lt;/p>
&lt;p>&lt;strong>Training&lt;/strong> 주요 모델 T0는 multitask mixture애서 학습되었고, T0+는 동일한 모델이지만 GPT-3의 평가 데이터셋이 추가된 mixture에서 학습되었다. 마지막으로, T0++는 SuperGLUE를 추가로 학습 mixture에 포함시켜, NLI와 BIG-bench 작업만이 보류된 작업으로 남게 되었다.&lt;/p>
&lt;p>T0 변형 모델들은 모두 T5+LM의 11B 개의 parameter 버전에서 초기화되었다. 그러나 스케일링 효과를 연구하고 자원이 적은 연구자들을 위해, T5+LM의 3B 개의 parameter 버전에서 초기화된 동일한 학습 mixture를 가진 T0(3B)도 학습시켰다.&lt;/p>
&lt;p>학습 데이터셋의 검증 부분에서 가장 높은 점수를 내는 체크포인트를 선택한다. 이는 보류된 작업에서 어떠한 예시도 사용하지 않아, 진정한 zero-shot 설정을 만족시키는 방법이다.&lt;/p>
&lt;p>모든 학습 데이터셋의 모든 예시를 결합하고 섞어 multitask 학습 mixture를 만든다. 그러나 각 학습 데이터셋의 예시 수는 크게 차이나므로, 500,000개 이상의 예시를 가진 데이터셋은 샘플링을 위해 500,000 / num templates 예시로 취급한다. 여기서 num templates는 데이터셋에 대해 만들어진 템플릿의 수이다.&lt;/p>
&lt;p>입력 시퀀스를 1024개, 목표 시퀀스를 256개의 토큰으로 잘라내며, 최대 시퀀스 길이에 도달하기 위해 여러 학습 예시를 하나의 시퀀스로 결합한다. 1024개의 시퀀스 batch size와 Adafactor optimizer를 사용하고, 1e-3의 learning rate와 0.1의 dropout rate를 적용한다.&lt;/p>
&lt;p>&lt;strong>Evaluation&lt;/strong> natural language inference, coreference, word sense disambiguation, sentence completion 등 4가지 보류된 전통적 NLP 작업과 BIG-bench에서의 14가지 새로운 작업에 대해 11개의 데이터셋에서 zero-shot 일반화를 평가한다. 특별히 명시되지 않는 한, validation split에서의 성능을 보고하며, 모든 데이터셋은 정확도를 측정 기준으로 사용한다.&lt;/p>
&lt;p>여러 옵션 중 올바른 완성을 선택하는 작업에 대해, 랭크 분류를 사용하여 모델을 평가한다: 미세 조정된 모델에서 각 타겟 옵션의 log-likelihood를 계산하고 가장 높은 것을 예측으로 선택한다. 간편함을 위해, 타겟 옵션의 log-likelihood에 length normalization는 적용하지 않는다.&lt;/p>
&lt;p>validation split에서 프롬프트 성능을 비교하여 프롬프트를 선택하지 않는다. 이는 &amp;ldquo;true&amp;rdquo; zero-shot 평가를 방해할 수 있다. 대신, 주어진 데이터셋에 대해 모든 프롬프트의 median 성능과 interquartile range(Q3 - Q1)를 보고하여 모델의 프롬프트 표현에 대한 robustness를 측정한다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;h3 id="generalization-to-held-oput-tasks">Generalization To Held-Oput Tasks&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure4.png"
width="1102"
height="666"
srcset="https://kurtkim.github.io/p/t0/images/figure4_hude58d218f2ee9120dd71263b5b07b8ae_135749_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure4_hude58d218f2ee9120dd71263b5b07b8ae_135749_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="397px"
>&lt;/p>
&lt;p>multitask 프롬프트 학습은 보류된 작업에 대한 일반화를 향상시키는 것으로 나타났다. 이 논문의 방법론은, 동일한 모델과 프롬프트를 사용했음에도 불구하고, 단순 언어 모델링 학습에 비해 모든 데이터셋에서 상당한 성능 향상을 보여주었다.&lt;/p>
&lt;p>T0는 가장 큰 GPT-3 모델들의 zero-shot 성능과 비교할 때, 11개의 보류된 데이터셋 중 9개에서 그 성능을 매치하거나 초과한다. 특히, T0와 GPT-3 모두 자연어 추론에 대해 학습하지 않았음에도 T0는 모든 NLI 데이터셋에서 GPT-3를 능가한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure5.png"
width="1114"
height="538"
srcset="https://kurtkim.github.io/p/t0/images/figure5_hu24f25b154def1785eaabe9fd65b2cace_107522_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure5_hu24f25b154def1785eaabe9fd65b2cace_107522_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="496px"
>&lt;/p>
&lt;p>BIG-bench의 일부 작업에서 T0, T0+, T0++의 zero-shot 성능을 평가하였다. 이는 학습 작업에 포함되지 않은 다양한 새로운 기술을 평가하는데 사용되었다. 결과적으로, 학습 데이터셋의 수가 증가함에 따라 모델의 성능이 향상되었으며, StrategyQA를 제외한 모든 작업에서 적어도 한 가지 T0 변형이 모든 기준 모델을 능가히였다.&lt;/p>
&lt;h3 id="prompt-robustness">Prompt Robustness&lt;/h3>
&lt;p>프롬프트 범위를 넓히는 학습이 프롬프트 표현에 대한 견고성을 향상시키는지를 검증하기 위해, 데이터셋 당 평균 프롬프트 수와 학습 중 사용된 데이터셋의 수에 대한 두 가지 실험을 실시하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure6.png"
width="1074"
height="660"
srcset="https://kurtkim.github.io/p/t0/images/figure6_hu0ccfcd7bb3e87da5ff055f4673e6ad97_162977_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure6_hu0ccfcd7bb3e87da5ff055f4673e6ad97_162977_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>&lt;strong>Effect of More Prompts per Dataset&lt;/strong> 데이터셋 당 프롬프트 수를 변화시키며 모델을 비교한 결과, 단 하나의 프롬프트로도 보류된 작업 성능이 크게 향상될 수 있음을 확인하였다. 프롬프트 수를 평균 5.7로 늘리면 성능이 추가로 향상되었고, 이는 데이터셋 당 더 많은 프롬프트에 대한 학습이 더 나은 일반화를 가져온다는 가설을 강화한다. 또한, 원래 작업과 관련이 없는 프롬프트를 포함한 T0 모델에서는 성능이 더욱 개선되었음을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t0/images/figure7.png"
width="1014"
height="390"
srcset="https://kurtkim.github.io/p/t0/images/figure7_hu523115aa1ea3182c672102c4ab63538b_72014_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t0/images/figure7_hu523115aa1ea3182c672102c4ab63538b_72014_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="260"
data-flex-basis="624px"
>&lt;/p>
&lt;p>&lt;strong>Effect of Prompts from More Datasets&lt;/strong> 이 실험에서는 사용 가능한 모든 프롬프트를 고정하고 학습 데이터셋 수를 증가시켰다. 결과적으로, 데이터셋 수가 증가함에 따라 대부분의 보류된 작업에서 성능이 향상되었다. 그러나, 프롬프트의 표현에 대한 모델의 견고성이 일관되게 향상되지는 않았다. 이는 일부 프롬프트의 성능이 항상 낮아, 다른 프롬프트가 향상되더라도 전체 범위가 더 넓어지기 때문이다. 따라서, 추가적인 조사가 필요하다.&lt;/p>
&lt;p>&lt;strong>Comparing T0 and GPT-3’s robustness&lt;/strong> 프롬프트의 다른 표현에 대한 GPT-3의 견고성을 평가하기 위해 동일한 10개의 프롬프트를 사용하여 GPT-3를 테스트하였다. 이 중 하나는 Brown et al. (2020)이 보고한 프롬프트와 동일했고, 이 프롬프트의 정확도는 58.8%로 보고된 63.5%보다 낮았다. 그러나 다른 9개의 프롬프트는 대체로 무작위 추측 수준의 성능을 보였다. 이 결과는 T0 모델이 GPT-3보다 프롬프트 구성에 대해 더 견고할 수 있음을 나타낸다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>이 연구와 동시에 진행된 Wei et al. (2021)의 연구에서는 FLAN을 제안하였다. 이 방법은 다중 작업 프롬프트 학습을 통해 zero-shot 일반화를 가능하게 하는 점에서 우리의 방법과 유사하다. FLAN과 비교해 볼 때, T0는 일부 작업에서 더 좋은 성능을 보였고, 일부에서는 비슷하거나 약간 떨어지는 성능을 보였다. 그러나 눈에 띄는 점은, T0와 T0++는 FLAN보다 10배 이상 작은 parameter를 가지고 있음에도 불구하고 이런 성능을 달성했다는 것이다.&lt;/p>
&lt;p>T0와 FLAN은 Winogrande와 HellaSwag에서 GPT-3보다 성능이 떨어지지만, 일부 작업에 대해 지시사항 없이 프롬프트를 사용하면 성능이 향상될 수 있다는 것을 확인하였다. 특히, HellaSwag에서는 지시사항 없이 테스트했을 때 성능이 중앙값 33.65%에서 57.93%로 크게 향상되었다. 그러나 Winogrande에서는 지시사항 없이 테스트했을 때 큰 차이가 없었다.&lt;/p>
&lt;p>Wei et al. (2021)의 연구에서는 T0와 비슷한 크기의 모델로 다중작업 프롬프트 학습 후 성능이 감소했지만, 이 연구에서는 이와 반대로 모델의 성능이 향상되었다. 이 차이를 설명할 수 있는 두 가지 주요 요인은 다른 목표로 사전 학습된 encoder-decoder 모델을 사용했고, masked language modeling이 효과적인 사전 학습 전략임을 다시 한번 확인했다는 점이다.&lt;/p>
&lt;p>이 논문의 프롬프트는 길이와 창의성 면에서 더 다양하며, 이 다양성이 성능에 영향을 미칠 수 있다고 가설을 세웠다. 예를 들어, 프롬프트의 수를 늘려도 성능에 큰 변화가 없었던 Wei et al. (2021)의 연구와 달리, 이 연구에서는 프롬프트를 더 추가할 때 성능이 향상되는 것을 관찰하였다. 이 차이들의 영향에 대한 자세한 조사는 향후 연구 주제로 남겨두었다.&lt;/p>
&lt;hr>
&lt;h2 id="cnnclusion">Cnnclusion&lt;/h2>
&lt;p>multitask 프롬프트 학습이 언어 모델의 zero-shot 일반화 능력을 강화하는데 효과적임을 보여주었다. 이 방법은 unsupervised 언어 모델 사전 학습에 대한 유효한 대안이며, T0 모델이 그보다 훨씬 큰 모델들을 능가하는 경우가 많았습니다. 또한, 다양한 프롬프트의 중요성과 작업별 데이터셋 수 증가의 영향을 실험을 통해 입증히얐다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2110.08207.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/bigscience-workshop/t-zero" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>FLAN</title><link>https://kurtkim.github.io/p/flan/</link><pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/flan/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문은 언어 모델의 zero-shot 학습 능력을 향상시키는 방법을 연구한다. &amp;ldquo;instruction tuning&amp;rdquo; 이라는 방법을 통해 미처 볼 수 없었던 작업에서의 zero-shot 성능을 크게 향상시킬 수 있음을 보여준다.&lt;/p>
&lt;p>137B 개의 parameter를 가진 사전 학습된 언어 모델을 60개 이상의 NLP 데이터셋에 대한 instruction tuning을 통해, 이 모델인 FLAN은 보이지 않는 작업 유형에서 월등한 성능을 보여준다. FLAN은 여러 데이터셋에서 zero-shot GPT-3를 능가하고, 몇몇 작업에서는 few-shot GPT-3를 크게 앞선다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 언어 모델은 few-shot 학습을 잘 수행하지만, zero-shot 학습에서는 성공적이지 못하며, 이는 사전 학습 데이터와 비슷하지 않은 프롬프트에서 모델이 작업을 수행하기 어렵기 때문일 수 있다.&lt;/p>
&lt;p>이 논문에서는 대규모 언어 모델의 zero-shot 성능을 향상시키는 방법을 연구한다. 60개 이상의 NLP 데이터셋을 자연어 지시문으로 표현하여 137B parameter의 언어 모델을 미세 조정하는 방식을 사용한다. 이 결과 생성된 모델을 FLAN(Finetuned Language Net)이라고 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure1.png"
width="912"
height="636"
srcset="https://kurtkim.github.io/p/flan/images/figure1_hu53cc1b2de5c2393ea95504fc3ec0baa4_152068_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure1_hu53cc1b2de5c2393ea95504fc3ec0baa4_152068_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>NLP 데이터셋을 작업 유형별로 그룹화하여 FLAN의 zero-shot 성능을 평가한다. 특정 작업(예: 자연어 추론)을 평가하기 위해 해당 작업을 제외한 다른 모든 작업에서 FLAN을 조정하고, 그 후에 zero-shot 자연어 추론 성능을 평가한다.&lt;/p>
&lt;p>FLAN은 기본 137B-parameter 모델의 zero-shot 성능을 크게 향상시키며, 25개의 데이터셋 중 20개에서 GPT-3의 zero-shot을 능가한다. 또한 특정 작업에서는 GPT-3의 few-shot 성능까지 능가한다. instruction tuning에서 작업 클러스터 수를 늘리는 것이 성능을 향상시키며, 충분한 모델 규모에서만 instruction tuning의 이점이 나타난다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure2.png"
width="1020"
height="342"
srcset="https://kurtkim.github.io/p/flan/images/figure2_huc400aba4a6ad9b0723bf44ae631c524a_108997_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure2_huc400aba4a6ad9b0723bf44ae631c524a_108997_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="715px"
>&lt;/p>
&lt;p>instruction tuning은 언어 모델이 추론 시 텍스트 상호작용에 더 잘 응답하도록 미세조정을 통한 지도학습을 사용하는 간단한 방법이다. 이 방법은 언어 모델이 지시문만을 통해 작업을 수행하는 능력을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="flan-instruction-tuning-improves-zero-shot-learning">FLAN: Instruction Tuning Improves Zero-Shot Learning&lt;/h2>
&lt;p>instruction tuning의 목표는 언어 모델이 NLP 지시문에 더 잘 응답하도록 향상시키는 것이다. 지시문을 통해 설명된 작업을 수행하도록 언어 모델을 교육함으로써, 보이지 않는 작업에 대해서도 지시문을 따를 수 있게 한다. 작업 유형별로 데이터셋을 그룹화하고, 남은 클러스터에서 instruction tuning을 하면서 볼 수 없는 작업의 성능을 평가한다.&lt;/p>
&lt;h3 id="tasks--templates">Tasks &amp;amp; Templates&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure3.png"
width="1068"
height="348"
srcset="https://kurtkim.github.io/p/flan/images/figure3_hu5dd5a15e50bf0eaf0feb3af4dc50d61a_193664_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure3_hu5dd5a15e50bf0eaf0feb3af4dc50d61a_193664_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="306"
data-flex-basis="736px"
>&lt;/p>
&lt;p>기존의 연구 데이터셋을 지시문 형식으로 변환하여, 자원 집약적인 새로운 데이터셋 생성을 피한다. Tensorflow Datasets에서 공개적으로 이용 가능한 62개의 텍스트 데이터셋을 하나의 혼합물로 집계하며, 이 데이터셋들은 12개의 작업 클러스터 중 하나로 분류된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure4.png"
width="1002"
height="342"
srcset="https://kurtkim.github.io/p/flan/images/figure4_hu72e09559eb9945d8e01e74a1d5b5f08c_123105_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure4_hu72e09559eb9945d8e01e74a1d5b5f08c_123105_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="703px"
>&lt;/p>
&lt;p>각 데이터셋에 대해, 작업을 설명하는 10개의 고유한 자연어 지시문 템플릿을 작성하며, 다양성을 높이기 위해 일부 템플릿은 원래의 작업을 뒤집는 방식으로 구성된다. 이후 모든 데이터셋의 혼합물에서 사전 학습된 언어 모델을 instruction tuning하며, 각 데이터셋의 예제는 해당 데이터셋에 대한 무작위로 선택된 지시 템플릿으로 형식화된다.&lt;/p>
&lt;h3 id="evaluation-splits">Evaluation Splits&lt;/h3>
&lt;p>FLAN이 instruction tuning에서 본적 없는 작업에 대해 어떻게 수행하는지를 알고자 한다. 본적 없는 작업을 정의하기 위해, instruction tuning 중에 보지 않은 작업 클러스터에 속한 모든 데이터셋을 본적 없는 것으로 간주한다. 따라서, 특정 작업 클러스터에서 zero-shot FLAN을 평가하려면, 각각 다른 작업 클러스터를 보류한 모델을 instruction tuning합니다.&lt;/p>
&lt;h3 id="classification-with-options">Classification With Options&lt;/h3>
&lt;p>작업의 출력 공간은 클래스 중 하나(classiﬁcation) 또는 자유 텍스트(generation)이 된다. FLAN은 decoder만 있는 언어 모델의 지시 조정 버전이므로, 생성 작업에 대한 추가 수정 없이도 자유 텍스트로 자연스럽게 응답한다.&lt;/p>
&lt;p>분류 작업에서는 &amp;ldquo;예&amp;quot;와 &amp;ldquo;아니오&amp;quot;와 같은 두 가지 출력만 고려하는 순위 분류 방법을 사용하였다. 하지만 이 방법은 답변의 확률 분포가 원치 않는 방식으로 나타날 수 있다. 따라서, 분류 작업의 끝에 OPTIONS 토큰과 해당 작업의 출력 클래스 목록을 추가하여 모델이 분류 작업에 응답할 때 원하는 선택지를 인식하게 한다.&lt;/p>
&lt;h3 id="training-details">Training Details&lt;/h3>
&lt;p>&lt;strong>Model architecture and pretraining.&lt;/strong> 137B parameter의 LaMDA-PT라는 decoder-only transformer 언어 모델을 사용한다. 이 모델은 웹 문서, 대화 데이터, 위키백과 등을 통해 사전 학습되었고, SentencePiece 라이브러리를 사용해 32k 어휘로 토큰화되었다. 사전 학습 데이터의 약 10%는 비영어이다. LaMDA-PT는 언어 모델 사전 학습만을 가지고 있다.&lt;/p>
&lt;p>&lt;strong>Instruction tuning procedure.&lt;/strong> FLAN은 LaMDA-PT의 instruction tuning 버전이다. 모든 데이터셋을 혼합하여 무작위로 샘플링하며, 데이터셋 당 최대 30k의 학습 예제를 사용한다. 모델은 30k의 그래디언트 단계 동안 미세조정되며, 입력 시퀀스와 목표 시퀀스의 길이는 각각 1024와 256입니다. 이 튜닝 과정은 TPUv3에서 약 60시간이 소요된다.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>FLAN은 다양한 작업들에서 평가되며, 이는 natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, struct-to-text 등을 포함한다. 각 작업 클러스터는 다른 체크포인트를 사용하며, 각 데이터셋의 성능은 모든 템플릿에 대한 평균 성능으로 평가된다. 또한, 개발 세트의 성능이 가장 좋은 템플릿을 사용하여 테스트 세트의 성능도 측정한다.&lt;/p>
&lt;p>LaMDA-PT의 zero-shot과 few-shot 결과를 GPT-3의 프롬프트와 동일하게 보고한다. 이는 instruction tuning이 얼마나 효과적인지 직접적으로 보여주는 기준선이다. 결과적으로, instruction tuning은 대부분의 데이터셋에서 LaMDA-PT의 성능을 크게 향상시켰다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure5.png"
width="1074"
height="774"
srcset="https://kurtkim.github.io/p/flan/images/figure5_hu2fa5a6362367fa9dc5272a014132c211_159127_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure5_hu2fa5a6362367fa9dc5272a014132c211_159127_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>zero-shot FLAN은 25개 데이터셋 중 20개에서 GPT-3 175B를 능가하며, 19개 데이터셋 중 13개에서는 GLaM 64B/64E를 능가한다.&lt;/p>
&lt;p>instruction tuning은 NLI, QA, translation, struct-to-text 등의 과제에 효과적이며, 언어 모델링으로 직접 구성된 과제에서는 효과적이지 않다.&lt;/p>
&lt;p>&lt;strong>Natural language inference (NLI).&lt;/strong> 5개의 NLI 데이터셋에서, FLAN은 모든 기준 모델을 크게 능가했습니다. FLAN은 NLI를 &amp;ldquo;Does &lt;!-- raw HTML omitted --> mean that &lt;!-- raw HTML omitted -->?&amp;ldquo;라는 더 자연스러운 질문으로 표현하여 훨씬 높은 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Reading comprehension.&lt;/strong> FLAN은 MultiRC와 OBQA에서 기준 모델을 능가하였다. 또한, BoolQ에서는 GPT-3를 크게 능가하였다.&lt;/p>
&lt;p>&lt;strong>Closed-book QA.&lt;/strong> FLAN은 모든 네 개의 데이터셋에서 GPT-3를 능가하였다. ARC-e와 ARC-c에서는 GLaM보다 더 좋은 성능을 보였지만, NQ와 TQA에서는 약간 낮은 성능을 보였다.&lt;/p>
&lt;p>&lt;strong>Translation.&lt;/strong> FLAN은 GPT-3 논문에서 평가된 세 개의 데이터셋인 프랑스어-영어, 독일어-영어, 루마니아어-영어에 대한 기계 번역 성능을 평가하였다. FLAN은 모든 평가에서 zero-shot GPT-3를 능가했지만, 대부분의 경우 few-shot GPT-3보다 성능이 떨어졌다. FLAN은 영어로 번역하는 데 강한 결과를 보였지만, 영어에서 다른 언어로 번역하는 것은 상대적으로 약했다.&lt;/p>
&lt;p>&lt;strong>Additional tasks.&lt;/strong> instruction tuning은 많은 언어 모델링 과제의 성능을 향상시키지 못하는 한계가 있다. 7개의 상식 추론 및 공통 참조 해결 과제 중 FLAN은 3개 과제에서만 LaMDA-PT를 능가하였다. 하지만, zero-shot FLAN은 일반적으로 zero-shot LaMDA-PT를 능가하며, few-shot LaMDA-PT와 비슷하거나 더 나은 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="ablation-studies--further-analysis">Ablation Studies &amp;amp; Further Analysis&lt;/h2>
&lt;h3 id="number-of-instruction-turning-clusters">Number Of Instruction Turning Clusters&lt;/h3>
&lt;p>이 연구에서는 instruction tuning이 어떻게 모델의 zero-shot 성능을 향상시키는지를 중점으로 살펴보았다. 첫 번째 축소 실험에서는 instruction tuning에 사용된 클러스터와 과제의 수가 성능에 어떻게 영향을 미치는지를 검토하였다. 이때 NLI, closed-book QA, commonsense reasoning을 평가 클러스터로 보류하고, 나머지 클러스터를 instruction tuning에 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure6.png"
width="682"
height="422"
srcset="https://kurtkim.github.io/p/flan/images/figure6_hub2ee1795a43435d66f715189ce4efe1b_103115_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure6_hub2ee1795a43435d66f715189ce4efe1b_103115_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>instruction tuning에 추가 클러스터와 과제를 추가하면, 보류된 세 개의 클러스터에서의 평균 성능이 향상됨을 확인했다. 테스트한 일곱 개의 클러스터에서 성능이 포화되지 않아 보이므로, instruction tuning에 더 많은 클러스터가 추가되면 성능이 더욱 향상될 수 있을 것으로 보인다. 하지만, 감정 분석 클러스터에서는 최소한의 추가 가치만을 볼 수 있었다.&lt;/p>
&lt;h3 id="scaling-laws">Scaling Laws&lt;/h3>
&lt;p>언어 모델의 zero-shot과 few-shot 능력이 더 큰 모델에 대해 크게 향상된다는 연구 결과를 바탕으로, instruction tuning의 이점이 모델 규모에 어떻게 영향을 받는지를 살펴보았다. 모델 규모를 422M, 2B, 8B, 68B, 137B로 설정하고 instruction tuning의 효과를 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure7.png"
width="574"
height="398"
srcset="https://kurtkim.github.io/p/flan/images/figure7_hu06d61b58778b9d23f80bcb7bfddd86e8_57546_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure7_hu06d61b58778b9d23f80bcb7bfddd86e8_57546_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>100B parameter 규모의 두 모델에서는 instruction tuning이 보류된 과제에서의 성능을 크게 향상시켰다. 그러나, 8B 및 더 작은 모델에서는 instruction tuning이 보류된 과제에서의 성능을 저하시켰다. 이는 작은 규모의 모델에서 instruction tuning 중 사용되는 과제를 학습하는 것이 모델의 전체 용량을 차지하게 되어, 새로운 과제에서 성능이 떨어지게 만들 수 있기 때문일 수 있다.&lt;/p>
&lt;h3 id="role-of-instructions">Role Of Instructions&lt;/h3>
&lt;p>마지막 ablation study에서는 미세 조정 중 지시문의 역할을 살펴보았다. 지시문 없이 모델이 어떻게 수행하는지 살펴보기 위해, 지시문이 없는 두 가지 미세 조정 설정을 고려하였다. 하나는 템플릿이 없는 설정으로, 모델에게 입력과 출력만이 주어지는 것이고, 다른 하나는 데이터셋 이름 설정으로, 각 입력이 과제와 데이터셋의 이름으로 시작된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure8.png"
width="478"
height="368"
srcset="https://kurtkim.github.io/p/flan/images/figure8_hue286e52e5e2eb558effda57d405b1d3a_46002_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure8_hue286e52e5e2eb558effda57d405b1d3a_46002_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="129"
data-flex-basis="311px"
>&lt;/p>
&lt;p>자연스러운 지시문을 사용한 FLAN의 미세 조정 절차와 두 가지 ablation study을 비교하였다. 이 두 ablation study는 각각 템플릿이 없는 설정과 데이터셋 이름만을 사용한다. 결과에서 두 축소 설정 모두 FLAN보다 훨씬 나쁜 성능을 보여, 보이지 않는 과제에서의 zero-shot 성능에 지시문을 사용한 학습이 결정적임을 나타냈다.&lt;/p>
&lt;h3 id="instructions-with-few-shot-exemplars">Instructions With Few-Shot Exemplars&lt;/h3>
&lt;p>few-shot 예시가 추론 시간에 사용 가능할 때 instruction tuning이 어떻게 사용될 수 있는지 연구하였다. few-shot 설정의 형식은 zero-shot 형식을 기반으로 한다. 학습 시간과 추론 시간 모두에서 예시는 학습 세트에서 무작위로 추출되며, 예시의 수는 16개로 제한하고 전체 시퀀스 길이가 960 토큰 미만이 되도록 했다. 실험은 보이지 않는 과제에 대한 few-shot 예시를 오직 추론 시간에만 사용하는 동일한 과제 분할과 평가 절차를 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure9.png"
width="1088"
height="292"
srcset="https://kurtkim.github.io/p/flan/images/figure9_hu553b7afb6075f4d3935015206efa02e8_52139_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure9_hu553b7afb6075f4d3935015206efa02e8_52139_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="894px"
>&lt;/p>
&lt;p>few-shot 예시는 zero-shot FLAN에 비해 모든 과제 클러스터의 성능을 향상시킨다. 예시는 특히 크거나 복잡한 출력 공간을 가진 과제에 효과적이며, 이는 예시가 모델이 출력 형식을 더 잘 이해하는 데 도움이 되기 때문일 가능성이 있다. 또한, 모든 과제 클러스터에서 템플릿 간의 표준 편차는 퓨샷 FLAN에서 더 낮아, 프롬프트 엔지니어링에 대한 민감도가 줄어든 것을 나타낸다.&lt;/p>
&lt;h3 id="instruction-turning-facilitates-prompt-turning">Instruction Turning Facilitates Prompt Turning&lt;/h3>
&lt;p>instruction tuning이 모델의 지시문에 대한 반응 능력을 향상시키는 것을 확인했기 때문에, FLAN이 NLP 과제를 수행하는 데 더 적합하다면, 소프트 프롬프트를 사용하여 추론을 수행할 때도 더 나은 성능을 달성해야 한다. 추가 분석으로, SuperGLUE 과제 각각에 대해 연속 프롬프트를 훈련시켰고, 이는 특정 과제에 대한 프롬프트 튜닝을 수행할 때, 동일한 클러스터에 있는 다른 과제가 instruction tuning 동안 보이지 않게 하는 클러스터 분할을 따랐다. 프롬프트 튜닝 설정은 Lester et al.의 절차를 따르되, 몇 가지 변화를 주었고, 이 변화들이 LaMDA-PT의 성능을 향상시키는 것으로 확인되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure10.png"
width="376"
height="314"
srcset="https://kurtkim.github.io/p/flan/images/figure10_hu068f59284c4acff449424ced936d048c_32927_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure10_hu068f59284c4acff449424ced936d048c_32927_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;p>모든 시나리오에서 프롬프트 튜닝은 LaMDA-PT보다 FLAN에서 더 잘 작동하였다. 특히 low-resource 설정에서는, FLAN에서의 프롬프트 튜닝이 LaMDA-PT에서의 것보다 10% 이상 성능이 향상되었다. 이 결과는 instruction tuning이 NLP 과제를 수행하는 데 더 바람직한 모델을 만드는 데 어떻게 기여할 수 있는지를 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>이 논문은 zero-shot 학습, 프롬프팅, 다중 과제 학습, NLP 응용 프로그램을 위한 언어 모델 등 여러 넓은 연구 영역과 관련이 있다. 이러한 넓은 영역에 대한 이전 연구를 확장된 관련 연구 섹션에서 설명하고, 이 논문의 연구와 가장 밀접하게 연관된 범위가 좁은 두 개의 하위 영역을 설명하였다.&lt;/p>
&lt;p>모델에 지시문에 대한 반응을 요청하는 방식은 QA 기반 과제 구성과 유사하며, 이는 NLP 과제를 통일하는 것을 목표로 한다. 이 방법들은 주로 다중 과제 학습에 초점을 맞추며, 사전 학습된 LMs의 기존 지식을 사용하는 것에 크게 기반하지 않는다. 이 연구의 작업은 모델 규모와 과제 범위 모두에서 최근의 일부 연구를 초월한다.&lt;/p>
&lt;p>언어 모델의 성공으로 모델이 지시문을 따르는 능력에 대한 연구가 진행되고 있다. 최근 연구에서는 지시문과 few-shot 예시를 이용해 BART를 미세 조정하고, 이를 통해 보이지 않는 과제에 대한 few-shot 성능을 향상시킬 수 있음을 보여주었다. 또한, T5를 미세 조정하는 등의 방법으로 zero-shot 학습을 개선하고, 미세 조정과 강화 학습을 병행하여 인간 평가자가 선호하는 출력을 생성하는 연구도 있다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>지시문으로 표현된 여러 과제에 대해 모델을 미세 조정하면 보이지 않는 과제에서의 성능이 향상된다는 것을 보여주었다. FLAN은 미세 조정되지 않은 모델보다 성능이 좋고, zero-shot GPT-3를 능가한다. 또한, 충분한 모델 규모에서만 instruction tuning에 의한 성능 향상이 나타나며, 이는 다른 프롬프팅 방법과도 결합될 수 있다.&lt;/p>
&lt;p>언어 모델의 다양한 능력은 specialist 모델과 generalist 모델 사이의 균형에 대한 관심을 끌어내었다. 레이블이 있는 데이터가 specialist 모델을 개선하는 데 도움이 될 것으로 예상되지만, instruction tuning을 통해 이 데이터가 큰 언어 모델이 보이지 않는 다양한 과제를 수행하는 데도 도움이 될 수 있음을 보여주었다. 이는 과제 특정 학습이 일반 언어 모델링과 보완적이라는 것을 보여주며, generalist 모델에 대한 추가 연구를 촉진한다.&lt;/p>
&lt;p>이 연구의 한계점은 과제를 클러스터에 할당하는 데 있는 주관성과 짧은 지시문의 사용에 대한 연구의 한정성이다. 개별 예시가 모델의 사전 훈련 데이터에 포함되어 있을 수 있지만, 이것이 결과에 크게 영향을 미쳤다는 증거는 찾지 못하였다. 또한, FLAN 137B의 규모는 그것을 서비스하는 데 비용이 많이 든다. 향후 instruction tuning 연구는 더 많은 과제 클러스터를 수집하고, 다언어 실험을 진행하며, downstream classiﬁer 학습 데이터를 생성하고, 편향과 공정성에 대한 모델 행동을 개선하는 방향으로 진행될 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>이 논문은 지시문에 기반한 zero-shot 과제를 수행하는 대규모 언어 모델의 능력을 향상시키는 간단한 방법을 연구하였다. FLAN은 GPT-3에 비해 더 우수한 결과를 보여주며, 대규모 언어 모델이 지시문을 따를 수 있는 잠재력을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2109.01652.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/flan" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Codex</title><link>https://kurtkim.github.io/p/codex/</link><pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/codex/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>GitHub에서 공개적으로 사용 가능한 코드에 미세 조정된 GPT 언어 모델인 Codex를 소개하고 그 Python 코드 작성 능력을 연구한다. 이 모델은 GitHub Copilot를 구동하며, 새로운 평가 세트인 HumanEval에서 문제의 28.8%를 해결합니다. 또한, 모델에서 반복 샘플링은 어려운 프롬프트에 대한 해결책을 만드는 데 효과적인 전략이라는 것을 발견하였다. 모델의 한계를 조사하였고, 강력한 코드 생성 기술의 배포가 안전, 보안, 경제 등에 미치는 잠재적인 영향을 논의한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>스케일러블한 시퀀스 예측 모델들은 자연어 처리, 컴퓨터 비전, 오디오 및 음성 처리, 생물학, 다중 모달리티 등 다양한 분야에서 생성 및 표현 학습의 일반적인 방법으로 사용되고 있다. 최근에는 언어 모델들이 대규모 데이터셋에서 코드를 활용하고 이를 통해 학습된 프로그래밍 능력을 바탕으로 프로그램 합성이라는 도전적인 문제를 해결하는데 기여하고 있다. 또한, masked language modeling과 span prediction과 같은 인기 있는 언어 모델링 방법들이 프로그래밍 학습을 위해 적용되고 있다.&lt;/p>
&lt;p>초기 GPT-3 연구에서는 Python docstrings로부터 간단한 프로그램을 생성할 수 있다는 사실을 발견하였다. 이는 GPT-3가 명시적으로 코드 생성을 위해 학습되지 않았음에도 불구하고 가능했다. 이러한 성공과 공개적으로 사용 가능한 코드의 풍부함을 바탕으로, Codex라는 특화된 GPT 모델이 다양한 코딩 작업에서 탁월하게 수행될 수 있을 것이라고 가정하였다. 이 논문은 GitHub Copilot과 OpenAI API에 사용된 초기 Codex 모델들에 대해 설명하고 있다.&lt;/p>
&lt;p>이 연구에서는 docstrings에서 Python 함수를 생성하는 작업에 집중하고, 이를 유닛 테스트를 통해 자동으로 평가한다. 이를 위해 언어 이해, 알고리즘, 간단한 수학을 평가하는 164개의 프로그래밍 문제 데이터셋을 만들었다.&lt;/p>
&lt;p>모델로부터 여러 샘플을 생성해 유닛 테스트를 통과하는지 확인한다. 12B parameter의 Codex는 단일 샘플로 28.8%의 문제를 해결하며, 300M parameter의 Codex는 13.2%를 해결한다. 반면, 6B parameter의 GPT-J는 동일한 데이터셋에서 11.4%를 달성하며, 모든 GPT 모델은 거의 0%에 가깝다. docstrings에서 함수를 합성하는 작업을 개선하기 위해, 이 연구에서는 Codex를 독립적으로, 올바르게 구현된 함수들에 대해 미세조정하였고, 결과적으로 생성된 Codex-S 모델은 문제들 중 37.7%를 단일 샘플로 해결한다.&lt;/p>
&lt;p>실제 프로그래밍 작업은 접근 방식의 반복과 버그 수정을 포함하는데, 이는 모델로부터 여러 샘플을 생성하고 모든 유닛 테스트를 통과하는 샘플을 선택하는 것으로 모사할 수 있다. 100개의 샘플 내에서, Codex-S는 문제들 중 77.5%에 대해 적어도 하나의 올바른 함수를 생성할 수 있다. 실제로, mean log-probability가 가장 높은 샘플이 문제들 중 44.5%에서 유닛 테스트를 통과하였다.&lt;/p>
&lt;hr>
&lt;h2 id="evaluation-framework">Evaluation Framework&lt;/h2>
&lt;p>pass@k 메트릭을 정의하고 그 장점을 설명하며, 모델 평가를 위해 만든 &amp;ldquo;HumanEval&amp;quot;이라는 수기로 작성된 문제 데이터셋에 대해 설명한다. 마지막으로, 모델이 생성한 코드를 안전하게 실행하기 위해 사용한 샌드박스 환경에 대해 이야기한다.&lt;/p>
&lt;h3 id="functional-correctness">Functional Correctness&lt;/h3>
&lt;p>코드 생성 모델은 보통 샘플을 reference 솔루션과 비교하여 평가되지만, 이런 match-based metric에는 결점이 있다. 특히, Ren et al. (2020)은 BLEU 점수가 코드의 의미적 특징을 정확히 포착하지 못한다는 문제를 지적하고, 점수를 수정하는 여러 방안을 제안하였다.&lt;/p>
&lt;p>match-based metric은 reference 솔루션과 기능적으로 동일한 프로그램의 복잡한 공간을 설명하지 못한다. 이에 따라, 최근 연구들은 샘플이 단위 테스트를 통과하면 정확하다고 판단하는 기능적 정확성을 사용하였다. 이 지표는 문서 문자열에 따른 코드 생성에도 적용되어야 한다고 주장한다.&lt;/p>
&lt;p>기능적 정확성 평가의 중요성은 인간 개발자들이 코드를 판단하는 기준으로 사용하기 때문이다. 테스트 주도 개발 프레임워크는 성공을 테스트를 통과하는 프로그램으로 정의하며, 대부분의 조직에서는 새로운 코드 통합이 단위 테스트의 생성 및 통과에 의존한다.&lt;/p>
&lt;p>Kulal et al. (2019)은 pass@k 지표를 사용해 기능적 정확성을 평가하였다. 이는 문제마다 $k$개의 코드 샘플을 생성하고, 그 중 하나라도 단위 테스트를 통과하면 문제가 해결된 것으로 간주한다. 하지만 이 방식은 변동성이 높을 수 있다. 따라서 각 작업마다 $n$개의 샘플을 생성하고, 단위 테스트를 통과한 샘플의 수를 세어, 편향되지 않은 추정치를 계산한다. 이 논문에서는 $n = 200, k ≤ 100$을 사용하였다.&lt;/p>
&lt;p>$$ pass@k := \underset{Problem}{\mathbb{E}} \big[ 1 - \begin{pmatrix} n-c \\ k \end{pmatrix} / \begin{pmatrix} n \\ k \end{pmatrix} \big] $$&lt;/p>
&lt;p>이 추정치를 직접 계산하면 큰 수와 수치적 불안정성이 발생한다. pass@k를 $1 − (1 − \hat{p})$로 추정하려는 유혹이 있을 수 있지만, 이 방법은 편향되다. 여기서 $\hat{p}$는 pass@1의 경험적 추정치이다.&lt;/p>
&lt;p>BLEU 점수가 기능적 정확성의 신뢰할 수 있는 지표가 아님을 증명한다. 참조 솔루션과 일치하지 않는 프로그램이 종종 기능적으로 동등한 프로그램보다 더 높은 BLEU 점수를 얻는 것을 통해 이를 보여준다.&lt;/p>
&lt;h3 id="humaneval-hand-written-evaluation-set">HumanEval: Hand-Written Evaluation Set&lt;/h3>
&lt;p>HumanEval 데이터셋이라 불리는 164개의 수기로 작성된 프로그래밍 문제에서 기능적 정확성을 평가한다. 각 문제에는 테스트가 평균 7.7개 포함되어 있다. 이러한 작업이 수기로 작성되어야 하는 이유는, 이 모델이 GitHub의 큰 부분에서 학습되며, 이미 다양한 출처의 문제 해결책이 포함되어 있기 때문이다.&lt;/p>
&lt;p>HumanEval 데이터셋의 프로그래밍 작업은 언어 이해, 추론, 알고리즘, 그리고 간단한 수학을 평가한다.&lt;/p>
&lt;h3 id="sandbox-for-executing-generated-programs">Sandbox for Executing Generated Programs&lt;/h3>
&lt;p>공개 프로그램의 의도가 불분명하고 생성된 프로그램이 잘못될 수 있어, 이들을 실행하면 보안 위험이 생긴다. 실제로, GitHub에는 환경을 변조하는 악성 프로그램들이 포함되어 있다.&lt;/p>
&lt;p>신뢰할 수 없는 프로그램을 안전하게 실행할 수 있는 샌드박스 환경을 개발하여, 이 프로그램들이 호스트나 네트워크를 수정하거나, 데이터를 유출하는 것을 방지하였다. OpenAI의 학습 인프라가 Kubernetes와 클라우드 서비스에 기반하므로, 샌드박스는 이들 환경의 제한 사항을 해결하면서도 그들의 사용 패턴에 따라 설계되었다.&lt;/p>
&lt;p>주 호스트 보호를 위해 gVisor 컨테이너 런타임을 선택하였다. Docker와 같은 컨테이너 런타임은 호스트 리소스를 공유할 수 있어, 악성 컨테이너가 호스트를 손상시킬 수 있다. gVisor는 호스트의 리소스를 에뮬레이션하여 보안 경계를 설정하고, eBPF 기반의 방화벽 규칙을 통해 네트워크 인접 호스트와 서비스를 보호한다.&lt;/p>
&lt;hr>
&lt;h2 id="code-fine-tuning">Code Fine-Tuning&lt;/h2>
&lt;p>Codex는 parameter가 최대 12B개인 GPT 모델을 기반으로 하며, 코드 작성에 특화되어 있다. GPT와는 달리 Codex는 HumanEval 데이터셋에서 높은 성능을 보이며, 문제마다 100개의 샘플을 생성하여 가장 적합한 샘플을 선택함으로써 대부분의 문제를 해결할 수 있다. 이는 문제당 한 번의 평가로 제한될 때 특히 중요한 이점을 제공한다.&lt;/p>
&lt;h3 id="data-collection">Data Collection&lt;/h3>
&lt;p>2020년 5월, GitHub의 54M 개 공개 소프트웨어 저장소로부터 파이썬 파일 179GB를 수집하여 학습 데이터셋을 구축하였다. 자동 생성 파일, 긴 줄 길이 파일 등을 제외한 최종 데이터셋의 크기는 159GB였다.&lt;/p>
&lt;h3 id="methods">Methods&lt;/h3>
&lt;p>Codex는 자연어 프롬프트에 대해 평가되며, 이미 강력한 자연어 표현을 가진 GPT-3에서 미세 조정하는 것이 유익할 것으로 가정하였다. 그러나 미세 조정 데이터셋이 크기 때문에 사전 학습된 언어 모델에서 개선 사항을 찾지 못했다. 그럼에도 GPT에서 미세 조정된 모델은 더 빠르게 수렴하므로, 이 방법을 모든 후속 실험에 적용하였다.&lt;/p>
&lt;p>Codex 모델 학습에는 GPT 모델과 동일한 learning rate, 175 step의 linear warmup, cosine learning rate decay가 사용되었다. 학습은 총 100B 토큰에 대해 진행되었으며, Adam optimizer와 weight decay coefﬁcient 0.1이 적용되었다.&lt;/p>
&lt;p>GPT-3 텍스트 토크나이저를 기반으로 한 code lexer를 사용하여 GPT의 텍스트 표현을 최대한 활용하고 있다. 하지만 GitHub 코드의 단어 분포 차이로 인해 효과적이지 못했고, 특히 공백 인코딩에서 비효율성이 발생하였다. 그래서 다양한 길이의 공백을 표현할 수 있는 추가 토큰을 도입함으로써 코드 표현에 필요한 토큰 수를 약 30% 줄였다.&lt;/p>
&lt;p>HumanEval 문제를 해결하기 위해, header, signature, docstring으로 구성된 프롬프트를 만들고, 추가 함수나 구문 생성을 방지하기 위해 특정 정지 시퀀스(&amp;rsquo;\nclass&amp;rsquo;, &amp;lsquo;\ndef&amp;rsquo;, &amp;lsquo;\n#&amp;rsquo;, &amp;lsquo;\nif&amp;rsquo;, &amp;lsquo;\nprint&amp;rsquo;)를 만날 때까지 Codex에서 토큰을 샘플링한다. 모든 샘플링 평가는 top $p = 0.95$로 nucleus 샘플링을 사용한다.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure4.png"
width="664"
height="422"
srcset="https://kurtkim.github.io/p/codex/images/figure4_hu585bf46d3f3d6df6138da4306680986f_62548_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure4_hu585bf46d3f3d6df6138da4306680986f_62548_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="157"
data-flex-basis="377px"
>&lt;/p>
&lt;p>검증 세트에 대한 테스트 손실을 Codex 모델 크기에 따라 그래프로 표현하였. 언어 모델 테스트 손실이 모델 크기에 따른 power law를 따르듯이, 코드 미세 조정 후의 테스트 손실도 비슷한 패턴을 보였다. 함수 형태는 $\big( {{N}\over{5.92 \times 10^7}} \big)^{-0.13}$ 이며, 이때 $N$은 임베딩이 아닌 모델 parameter의 수를 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure5.png"
width="660"
height="844"
srcset="https://kurtkim.github.io/p/codex/images/figure5_hu76e9362c0ecb67210be8a3027014cf2e_147427_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure5_hu76e9362c0ecb67210be8a3027014cf2e_147427_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="78"
data-flex-basis="187px"
>&lt;/p>
&lt;p>pass@k 평가 시, 샘플링 temperature를 $k$ 값에 맞게 최적화하는 것이 중요하다. 더 큰 $k$ 값에 대해서는 더 높은 temperature가 최적이며, 이는 샘플 집합의 다양성을 증가시키고, 모델이 정확한 해결책을 생성하는지 여부만을 보상하기 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure6.png"
width="644"
height="462"
srcset="https://kurtkim.github.io/p/codex/images/figure6_hu8a4efd723391de0335f15308132064cf_79411_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure6_hu8a4efd723391de0335f15308132064cf_79411_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="334px"
>&lt;/p>
&lt;p>679M parameter 모델에서는 pass@1의 최적 temperature가 $T^* = 0.2$, pass@100의 최적 temperature가 $T^* = 0.8$로 확인되었다. 이 temperature를 사용하면, 모델 크기에 따라 pass@1과 pass@100이 부드럽게 확장되는 것을 볼 수 있다.&lt;/p>
&lt;p>Pass@k는 유닛 테스트를 알고 있는 오라클이 $k$개의 샘플 중 최상의 샘플을 선택하는 것으로 해석할 수 있다. 하지만 실제 상황에서는 오라클 없이 $k$개의 샘플 중 하나를 선택해야 하는 경우도 있다. 예를 들어, 유닛 테스트가 없는 상황에서 사용자에게 하나의 완성만 제공하고 싶은 자동완성 도구로 모델을 사용하는 경우 등이 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure7.png"
width="646"
height="544"
srcset="https://kurtkim.github.io/p/codex/images/figure7_hu858657b7fe393d6a00040309ae7f4bc5_116152_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure7_hu858657b7fe393d6a00040309ae7f4bc5_116152_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="118"
data-flex-basis="285px"
>&lt;/p>
&lt;p>언어 모델링에서 배운 바에 따라, mean token log probability이 가장 높은 샘플을 선택하는 것이 무작위 샘플 평가보다 더 우수한 결과를 보였다. 그러나 sum log probability에 기반해 샘플을 선택하면 무작위 선택보다 약간 성능이 떨어질 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure8.png"
width="656"
height="650"
srcset="https://kurtkim.github.io/p/codex/images/figure8_hu64bc97e4cf07e471ef1748ea2bfcf05e_116552_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure8_hu64bc97e4cf07e471ef1748ea2bfcf05e_116552_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="242px"
>&lt;/p>
&lt;p>모든 Codex-12B HumanEval 샘플에 대해 참조 솔루션과의 BLEU 점수를 계산하였다. 올바른 솔루션과 잘못된 솔루션의 BLEU 점수 분포를 비교하면 상당한 중첩이 보여진. 잘못된 솔루션은 참조 솔루션과 기능적으로 동일하지 않기 때문에, BLEU 점수의 향상이 실제 기능적 정확성의 향상을 반영하지 않을 수 있음을 확인하였다.&lt;/p>
&lt;h3 id="comparative-analysis-of-related-models-and-systems">Comparative Analysis of Related Models and Systems&lt;/h3>
&lt;p>GPT-Neo와 GPT-J는 Codex와 유사한 방식으로 다양한 텍스트 소스와 GitHub 코드 8%를 포함하는 The Pile 데이터셋에서 학습되었다. 연구 커뮤니티는 이 두 모델이 정성적 프로그래밍 평가에서 기존 GPT 시스템을 능가하는 것을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/table1.png"
width="564"
height="478"
srcset="https://kurtkim.github.io/p/codex/images/table1_hu8ca5a723326e80ff402a73d80c6bb3ef_80430_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/table1_hu8ca5a723326e80ff402a73d80c6bb3ef_80430_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="117"
data-flex-basis="283px"
>&lt;/p>
&lt;p>HumanEval 데이터셋을 사용한 결과, GPT-Neo는 pass@1에서 6.4%, pass@100에서 21.3%를, GPT-J-6B는 pass@1에서 11.6%, pass@100에서 27.7%를 달성하였다. 이는 각각 Codex-85M과 Codex-300M와 유사한 성능을 보여준다. 이 결과는 특정 temperatures에서 평가한 최상의 결과를 기반으로 한다.&lt;/p>
&lt;p>코드 자동완성 시스템인 Tabnine의 가장 큰 무료 모델과 Codex를 비교하였다. Tabnine 모델은 $T = 0.4$에서 pass@1에서 2.6%, $T = 0.8$에서 pass@100에서 7.6%를 달성하며, 이는 Codex 모델 중 가장 작은 Codex-12M와 대략적으로 동등하다.&lt;/p>
&lt;h3 id="results-on-the-apps-dataset">Results on the APPS Dataset&lt;/h3>
&lt;p>최근 Hendrycks et al. (2021)은 언어 모델의 코딩 역량을 측정하기 위해 APPS 데이터셋을 소개하였다. 이 데이터셋은 각각 유닛 테스트와 정확한 솔루션 집합을 포함하는 5000개의 학습 예제와 테스트 예제로 구성되어 있다. APPS의 대부분의 문제는 단일 함수 합성이 아니라 전체 프로그램 합성으로, stdin에서 입력을 받고 stdout에 출력하는 형태로 구성되어 있어, 이는 Codex 학습 데이터와는 다르다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/table2.png"
width="1002"
height="318"
srcset="https://kurtkim.github.io/p/codex/images/table2_hudebdc58ba83499adce4c4dab38ff7baf_90799_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/table2_hudebdc58ba83499adce4c4dab38ff7baf_90799_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>APPS를 소개하는 논문에서는 언어 모델을 벤치마킹하고 모델이 정확한 해결책을 찾는 문제의 비율(&amp;ldquo;strict accuracy&amp;rdquo;)과 잘못된 해결책도 통과하는 유닛 테스트의 비율을 보고한다. 첫 번째 지표의 결과가 낮아서 측정 변동성을 줄이기 위해 후자를 사용하였다. 하지만 이 논문에서는 후자를 피하고 &amp;ldquo;strict accuracy&amp;quot;에만 초점을 맞추며, 다양한 $k$에 대한 pass@k 수치를 보고한다. 이때 코딩 경쟁에서 알려진 두 가지 추가 요소를 고려한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>코딩 경쟁과 APPS 데이터셋에서, 작업 설명에 포함된 3개의 입력/출력 예제를 사용한다. 이를 활용해 모델에서 1000개의 솔루션을 샘플링하고, 이 3개의 유닛 테스트를 통과하는 솔루션만 필터링한다. 이 필터링된 세트에서의 통과율을 계산하며, 이를 필터링된 pass@k라고 부른다. 필터링을 하지 않은 결과는 raw pass@k로 표시된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>코딩 경쟁이나 Codex의 결과에서 올바른 해결책이 찾아졌지만 알고리즘적으로 충분히 효율적이지 않아 통과되지 않는 경우가 많다. 이런 상황이 경쟁에서는 허용되지 않지만, Codex가 생성하는 솔루션 중 어떤 유닛 테스트에서도 실패하지 않지만 일부에서 시간 초과가 발생하는 경우도 보고하고 있다. 이때, 평가에서는 3초의 시간 초과를 사용한다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Codex가 APPS에서 미세 조정되지 않았기 때문에, 작업 설명의 입력/출력 예제를 docstring에 포맷팅 힌트로 추가하였다. 이런 방식을 &amp;ldquo;1-shot&amp;quot;이라고 부르며, 1-shot으로 평가 Codex12B는 APPS에서 미세조정된 GPT-Neo 모델 비슷한 성능을 보여준다. 작업당 최대 1000개의 샘플을 생성하고 평가하는 것이 유익하다는 이전 결과와 일관성을 보이지만, 더 어려운 문제에서는 솔루션이 시간 제한을 넘기기에 충분히 효율적이지 않다. 또한, 각 문제의 3개의 공개 유닛 테스트를 통과하는 첫 샘플을 평가하는 것이 raw pass@100 샘플보다 높은 성능을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="supervised-fine-tuning">Supervised Fine-Tuning&lt;/h2>
&lt;p>GitHub의 파이썬 코드는 다양한 요소를 포함하고 있으며, 이들은 문서 문자열에서 함수를 생성하는 것과는 관련성이 없어 보인다. 이런 분포의 불일치는 HumanEval 성능 저하를 초래할 수 있다는 가설을 제시하고 있다.&lt;/p>
&lt;p>Codex를 특정 작업 분포에 적응시키기 위해, 올바르게 구현된 함수들로부터 학습 문제를 만들어 추가적인 감독 하에 미세 조정을 진행한다. 이 예시들은 경쟁 프로그래밍 사이트와 지속적인 통합이 있는 저장소에서 수집하였다. 이렇게 미세 조정된 모델들을 Codex-S라 부르며, 이 모델들은 모든 크기의 모델에서 성능 향상을 보이고 있다.&lt;/p>
&lt;h3 id="problems-from-competitive-programming">Problems from Competitive Programming&lt;/h3>
&lt;p>프로그래밍 대회와 인터뷰 준비 사이트들은 숨겨진 단위 테스트를 통해 제출된 코드의 정확성을 자동으로 평가한다. 이 문제들은 완결성이 있고, 잘 작성된 문제 설명과 뛰어난 테스트 커버리지를 가지고 있다. 또한, 다양한 핵심 기술과 난이도에서의 알고리즘적 추론 능력을 테스트한다.&lt;/p>
&lt;p>여러 프로그래밍 대회 및 인터뷰 준비 사이트에서 문제, 함수 시그니처, 솔루션을 수집하여, 문제 설명을 문서 문자열로 사용해 HumanEval과 비슷한 프로그래밍 작업으로 구성하였다. 완전한 테스트 세트가 종종 숨겨져 있어, 문제 설명에 있는 예시로 단위 테스트를 만들거나, 틀린 솔루션을 제출해 추가 테스트 케이스를 추출하였다. 이런 방법으로 총 10,000개의 문제를 정리하였다.&lt;/p>
&lt;h3 id="problems-from-continuous-integration">Problems from Continuous Integration&lt;/h3>
&lt;p>오픈 소스 프로젝트로부터 프로그래밍 문제들을 수집하였고, &lt;em>sys.setprofile&lt;/em>을 이용하여 통합 테스트 중에 호출된 모든 함수의 입력과 출력을 추적하였다. 이렇게 수집한 데이터는 각 함수에 대한 단위 테스트를 생성하는 데 활용되었다.&lt;/p>
&lt;p>continuous integration(CI)을 사용하는 프로젝트는 추적에 적합하다. CI 설정 파일의 명령을 이용하여 가상 환경을 설정하고, 의존성을 설치하며, 통합 테스트를 실행한다.&lt;/p>
&lt;p>인기 있는 CI 도구인 travis와 tox를 사용하는 GitHub 저장소를 고려하였다. 또한, python package index(PyPI)의 pip 패키지에서 공개된 소스 코드를 사용하였다. 이러한 프로젝트들이 신뢰할 수 없는 코드를 포함하고 있기 때문에, 통합 테스트를 격리된 환경에서 실행하는 것이 중요했다.&lt;/p>
&lt;p>수백만 개의 잠재적인 함수가 있지만, 모든 함수가 입력을 받고 출력을 반환하지 않아서 약 4만 개의 함수만을 수집했다. 또한 대부분의 런타임 객체는 프로젝트가 설치되지 않으면 샌드박스 외부에서 복구할 수 없다.&lt;/p>
&lt;p>이 논문의 추적 방법은 모든 호출된 함수에 대한 입력과 출력을 생성하므로, 프로젝트에서 가져온 내장함수와 라이브러리 호출까지 문제로 변환되었다. 이로 인해 추적된 함수들은 주로 커맨드 라인 유틸리티의 구성 요소가 되었다. 이 작업에서 모델이 필요한 것은 고급 알고리즘과 데이터 구조를 이해하는 것이 아니라, 문서 문자열에 명시된 기능을 구현하기 위한 지시사항을 따르는 능력이다. 따라서, 추적은 코딩 경쟁 문제의 퍼즐 성격을 보완하고 작업의 분포를 확대한다.&lt;/p>
&lt;h3 id="filtering-problems">Filtering Problems&lt;/h3>
&lt;p>학습 문제를 자동으로 생성하는 두 가지 방법을 제시했지만, 품질을 어떻게 관리할지는 불명확하다. 일부 프롬프트는 함수를 충분히 명시하지 않아, 유효한 해결책이 잘못 판정될 수 있으며, 일부 문제는 상태를 유지하기 때문에 실행 결과가 다를 수 있다.&lt;/p>
&lt;p>문제점을 해결하기 위해, 각 문제에 대해 Codex-12B를 사용하여 100개의 샘플을 생성한다. 만약 단위 테스트를 통과하는 샘플이 없다면, 그 작업을 모호하거나 너무 어렵다고 판단하여 제외한다. 상태를 유지하는 문제나 비결정적인 문제를 제거하기 위해 이 검증 과정을 여러 번 반복하였다.&lt;/p>
&lt;h3 id="methods-1">Methods&lt;/h3>
&lt;p>학습 문제들에서 Codex를 미세 조정하여 CodexS라는 &amp;ldquo;supervised ﬁne-tuned&amp;rdquo; 모델 집합을 만들었다. 학습 문제에서 예시를 만들기 위해, 문제들을 특정 형식으로 조립하였고, 배치에서 프롬프트의 길이가 다르면 가장 긴 프롬프트 길이에 맞추기 위해 짧은 프롬프트를 왼쪽으로 패딩하였다.&lt;/p>
&lt;p>참조 솔루션의 negative log-likelihood를 최소화하도록 학습하며, 프롬프트의 토큰에 대한 손실은 마스킹한다. Codex 미세 조정에 사용된 learning rate의 1/10 크기로 학습하고, 동일한 learning rate 일정을 따라 validation 손실이 안정화될 때까지(10B 토큰 미만) 학습한다.&lt;/p>
&lt;h3 id="results-1">Results&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure9.png"
width="658"
height="460"
srcset="https://kurtkim.github.io/p/codex/images/figure9_huf8725c15ff84c69a6d10df03aeb34e62_64304_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure9_huf8725c15ff84c69a6d10df03aeb34e62_64304_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;p>$1 ≤ k ≤ 100$에 대해 pass@k를 평가하기 위한 최적의 temperature를 계산한다. Codex-S가 모든 $k &amp;gt; 1$에 대해 약간 더 높은 temperature를 선호한다는 것을 발견했는데, 이는 Codex-S가 Codex보다 좁은 분포를 캡처한다는 것을 반영할 수 있다. pass@1을 계산하기 위해 $T^∗ = 0$을, pass@100을 계산하기 위해 $T^∗ = 1$을 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure10.png"
width="666"
height="910"
srcset="https://kurtkim.github.io/p/codex/images/figure10_huc9e471a76f0640c64a74fcc104b15363_233494_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure10_huc9e471a76f0640c64a74fcc104b15363_233494_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="175px"
>&lt;/p>
&lt;p>pass@1과 pass@100에서 Codex-S와 Codex를 비교했을 때, Codex-S는 pass@1에서 평균 6.5%, pass@100에서 평균 15.1%로 Codex를 능가하였다.&lt;/p>
&lt;p>다양한 샘플 선택 휴리스틱의 성능을 Codex-S-12B와 Codex-12B에서 비교하였다. 샘플을 mean log probability로 순위를 매길 때, 무작위 순위에 비해 평균적으로 11.6$의 이익이 있었고, 이는 Codex에 비해 2% 이상 높았다.&lt;/p>
&lt;hr>
&lt;h2 id="docstring-generation">Docstring Generation&lt;/h2>
&lt;p>코드가 보통 docstring 뒤에 오기 때문에 Codex를 이용해 docstring로부터 코드를 생성하는 것은 가능하지만, 코드로부터 docstring을 생성하는 것은 어렵다. 그러나, 생성된 코드의 의도를 설명할 수 있는 docstring 작성 모델을 만드는 것이 중요하다고 판단하였다. 이전 섹션에서 설명한 학습 문제를 이용하면, 코드에 따른 docstring 생성을 위한 학습 데이터셋을 쉽게 만들 수 있다.&lt;/p>
&lt;p>각 학습 문제에 대해, function signature, reference solution, docstring을 연결하여 학습 예제를 만든다. Codex-S를 학습시키는 것과 같은 방법으로, docstring의 negative log-likelihood를 최소화함으로써 docstring 생성 모델인 Codex-D를 학습시킨다.&lt;/p>
&lt;p>코드 생성 모델을 평가할 때, 단위 테스트를 통과하는지에 따라 정확성을 측정한다. 그러나 docstring 샘플의 자동 평가는 어렵다. 그래서 docstring이 코드를 정확하게 명시하면 올바르다고 판단하여 수동으로 평가한다. 이 과정이 시간이 많이 걸리므로, temperature가 0.8인 Codex-D-12B에서 총 1640개 문제에 대해 문제 당 10개 샘플만 평가하였다.&lt;/p>
&lt;p>Codex-D는 docstring과 함께 잘못된 단위 테스트를 생성하는 경향이 있지만, 이들은 평가에서 제외된다. 모델이 코드 본문을 docstring로 그대로 복사하는 경우는 올바르지 않다고 판단한다. 가장 흔한 실패 사례는 중요한 세부 정보를 누락하거나 함수 이름에 과도하게 의존하여 함수 본문과 무관한 문제를 생성하는 경우이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/table3.png"
width="456"
height="126"
srcset="https://kurtkim.github.io/p/codex/images/table3_hu526870e8243a490363ddbbaf703786f2_18559_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/table3_hu526870e8243a490363ddbbaf703786f2_18559_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="361"
data-flex-basis="868px"
>&lt;/p>
&lt;p>Codex-D의 통과률은 Codex-S와 비슷하지만 약간 낮다. 어떤 방법이 더 높은 통과률을 가져올지는 명확하지 않다. docstring 생성은 구문이 코드보다 덜 엄격하므로 더 유연할 수 있지만, 개발자들이 docstring 작성에 덜 시간을 쏟는 경향이 있어 품질이 떨어질 수 있다. 실제로, 모델은 &amp;ldquo;I just found this function online&amp;quot;나 &amp;ldquo;This test is not correctly written and it’s not my solution&amp;quot;와 같은 docstring을 생성하기도 한다.&lt;/p>
&lt;p>docstring 모델을 이용하면 $k$개의 샘플 중 하나를 선택하는 새로운 방법이 있다. mean log probability이 가장 높은 샘플을 선택하는 대신, 실제 docstring에 대한 생성된 샘플의 확률을 최대화하는 샘플을 선택할 수 있다. 하지만, 이 방법은 mean log probability 순위보다 성능이 떨어지며, 무작위 순위보다는 좋지만 빠르게 과적합하는 경향이 있다.&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>Codex는 HumanEval 문제 대부분에 대해 올바른 해결책을 제시할 수 있지만, 여러 가지 한계를 가지고 있다.&lt;/p>
&lt;p>첫째, Codex의 학습이 효율적이지 않다. 학습 데이터셋은 GitHub의 대부분의 Python 코드를 포함하며, 수백만 줄에 달한다. 이런 양의 코드를 경험하는 개발자는 거의 없다. 실제로, 컴퓨터 과학 입문 과정을 수료한 우수한 학생이라면 Codex-12B보다 더 많은 문제를 해결할 것으로 예상된다.&lt;/p>
&lt;p>다음으로, Codex가 실패하거나 예상치 못한 동작을 보일 수 있는 상황을 살펴보았다. 코드 생성 평가는 이미 많이 연구되었지만, 대부분의 기존 지표들은 특정한 문제 상황에서의 성능을 측정한다. 그래서 코드 생성 모델의 능력을 측정하기 위한 새로운 지표 세트를 개발하였다. 이를 적용해보니, Codex는 문법적으로 잘못되거나 정의되지 않은 코드를 추천하거나, 범위를 벗어난 함수나 변수를 호출하는 등의 문제가 있었다. 또한, 복잡하고 고수준의 사양을 다루는 데에 어려움을 겪었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure11.png"
width="650"
height="446"
srcset="https://kurtkim.github.io/p/codex/images/figure11_hu809c2d8a52eb2a4a99b266f519ee84e5_63171_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure11_hu809c2d8a52eb2a4a99b266f519ee84e5_63171_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="349px"
>&lt;/p>
&lt;p>docstring의 길이가 증가할수록 모델 성능이 저하되는 것을 확인하기 위해, 입력 문자열을 수정하는 13개의 기본 블록으로 구성 합성 문제 데이터셋을 만들었다. 결과적으로, docstring에 연결된 블록 수가 증가하면 모델 성능이 지수적으로 감소하는 것을 발견하였다. 이는 인간 프로그래머의 특성과는 달리, 체인의 길이가 길어져도 프로그램을 올바르게 구현할 수 있어야 한다는 점에서 차이가 있다.&lt;/p>
&lt;p>다른 모달리티의 text-conditional generative 모델이 객체에 속성을 연결하는 데 어려움을 겪는 것처럼, Codex도 연산을 변수에 연결하는 데 문제를 일으킬 수 있다. 특히, docstring에 연산과 변수의 수가 많을 때 이런 문제가 발생한다. 예를 들어, Codex-12B는 변수를 감소시키지 않고, 모든 숫자의 곱을 반환하지 않는 실수를 범하였다.&lt;/p>
&lt;p>Codex의 시스템 수준 합성 능력의 한계에 대한 이해는 그것을 생성 용도로 사용할 때의 잠재적 위험성과 그것이 미치는 사회적 영향을 평가하는 데 도움이 된다.&lt;/p>
&lt;hr>
&lt;h2 id="broader-impacts-and-hazard-analysis">Broader Impacts and Hazard Analysis&lt;/h2>
&lt;p>Codex는 새로운 코드베이스에 사용자를 도입하거나, 개발자의 컨텍스트 전환을 줄이는 등 다양하게 활용될 수 있다. 비프로그래머가 사양을 작성하고 Codex가 그것을 구현하는 것도 가능하다. 교육과 탐색에도 활용될 수 있지만, 동시에 중요한 안전 문제를 제기하며, 항상 사용자의 의도를 반영하는 코드를 생성하지는 않으며, 오용될 위험이 있다.&lt;/p>
&lt;p>Codex를 생성 용도로 사용할 때의 위험성을 파악하기 위해, 잠재적인 위험 요인을 식별하는 위험 분석을 진행하였다.&lt;/p>
&lt;p>이 논문의 일부 발견은 연구 중심의 Codex 모델에서 파생된 생산 중심의 Codex 모델의 배포를 위한 작업에 기반하고 있지만, 이 부분은 특정 제품의 안전 기능에 대한 전체 설명을 제공하려는 것은 아니다. 이 논문에서 설명하는 모델의 특정 속성에 기반한 분석을 공유하며, 이는 코드 생성 시스템의 더 넓은 범주에 일반화되고, 기계 학습 연구 프로젝트의 일부로 상세한 영향 분석을 수행하는 규범을 장려하려는 것이다.&lt;/p>
&lt;p>이 섹션에서 위험에 주로 초점을 맞추는 것은 이 기술의 영향이 전부 부정적일 것으로 생각한다는 것이 아니다. 오히려, 위험 요소는 미묘하거나 주의가 필요하기 때문에 주목을 받아야 하며, 이익은 대부분의 사용자와 이해관계자들에게 더 명확하고 자동적으로 느껴질 것으로 예상하기 때문이다.&lt;/p>
&lt;h3 id="over-reliance">Over-reliance&lt;/h3>
&lt;p>코드 생성 모델을 실제로 사용할 때 주요 위험 중 하나는 생성된 출력에 과도하게 의존하는 것이다. Codex는 사용자의 의도와는 다른 솔루션을 제안할 수 있어, 특히 초보 프로그래머에게 문제를 일으킬 수 있다. 또한, 안전하지 않은 코드를 제안하는 가능성이 있어, 인간의 감독과 경계가 필요하다.&lt;/p>
&lt;p>과도한 의존성 등의 문제에 대해 더 많은 연구가 필요하다고 생각하며, 이를 위해 사용자에게 모델의 한계를 알려주는 문서를 제공하는 것이 중요하다고 강조하고 있다. 사용자의 경험 수준, UI 디자인, 작업 등에 따라 실제로 경계를 유지하는 방법을 식별하기 위한 실증적인 조사가 필요하며, &amp;ldquo;automation bias&amp;quot;에 대비하는 것이 점점 더 어려워질 수 있다는 점을 고려해야 한다.&lt;/p>
&lt;h3 id="misalignment">Misalignment&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/codex/images/figure12.png"
width="648"
height="390"
srcset="https://kurtkim.github.io/p/codex/images/figure12_hu20b1ef455796190789f69295b9260864_91139_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/codex/images/figure12_hu20b1ef455796190789f69295b9260864_91139_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="398px"
>&lt;/p>
&lt;p>다음 토큰 예측 목표에 따라 학습된 Codex와 같은 큰 언어 모델들은 학습 분포와 가장 비슷한 코드를 생성한다. 이로 인해, 이런 모델들은 더 도움이 될 수 있음에도 불구하고 사용자에게 도움이 되지 않는 행동을 할 수 있다. 예를 들어, 사용자의 코드에 있는 미묘한 오류에도 불구하고 Codex는 올바른 것처럼 보이지만 실제로는 잘못된 코드를 제안할 수 있다.&lt;/p>
&lt;p>이것은 alignment failure 으로, 모델이 사용자의 의도와 일치하지 않는다. 간단히 말해, 시스템이 우리가 원하는 작업을 할 수 있음에도 불구하고 선택적으로 하지 않는다면, 이는 misaligned이다. 반면, 시스템이 능력이 없어서 작업을 수행하지 못한다면, 그것은 그저 무능한 것이지, misaligned는 아니다.&lt;/p>
&lt;p>misalignment를 연구하는 것은 중요한데, 이는 시스템의 능력이 향상됨에 따라 더 악화될 가능성이 있는 문제이기 때문이다. 예를 들어, 모델 크기 확장 추세를 보면, 데이터, parameter, 학습 시간이 증가할수록 misalignment는 지속되고 심화될 것으로 예상된다.&lt;/p>
&lt;p>현재 모델에서의 misalignment가 중대한 피해를 입힐 가능성은 낮지만, 모델의 능력이 향상됨에 따라 위험성이 증가하고 제거하기 어려워질 것으로 예상된다. 사용자 승인에 기반한 학습을 받은 높은 능력을 가진 모델이라도, 정렬이 제대로 되지 않으면 사용자에게는 좋아 보이는 코드를 생성하지만, 실제로는 바람직하지 않거나 해로운 결과를 초래할 수 있다.&lt;/p>
&lt;h3 id="bias-and-representation">Bias and representation&lt;/h3>
&lt;p>인터넷 데이터로 학습된 언어 모델인 Codex가 인종차별적, 모욕적 또는 유해한 출력을 생성할 수 있다는 것이 발견되었다. 더불어, Codex는 성별, 인종, 감정, 계급, 이름의 구조 등에 대한 스테레오타입을 반영하는 코드를 생성할 수 있다. 이는 특히 Codex에 과도하게 의존하는 사용자들에게 중요한 안전 문제를 야기할 수 있다. 이러한 위험을 완화하기 위해 출력의 필터링, 문서화, 그리고 다른 방법의 개입이 필요할 수 있다.&lt;/p>
&lt;h3 id="economic-and-labor-market-impacts">Economic and labor market impacts&lt;/h3>
&lt;p>코드 생성 기능은 프로그래머의 생산성을 증가시켜 소프트웨어 제작 비용을 줄이는 방식으로 경제와 노동 시장에 영향을 미칠 수 있다. 그러나 엔지니어들이 하루종일 코드를 작성하지 않는 점, 그리고 다른 중요한 업무들을 수행해야 한다는 점 때문에 이 효과는 제한적일 수 있다. 또한 Codex가 패키지를 다른 비율로 가져오므로 일부 패키지 작성자들에게 유리할 수 있다. 시간이 지나며 이러한 기술의 능력이 향상되면 소프트웨어 관련 노동 시장과 일반 경제에 더 큰 영향을 미칠 수 있다. 이에 대한 추가 연구가 필요하다.&lt;/p>
&lt;h3 id="security-implications">Security implications&lt;/h3>
&lt;p>Codex는 보안 환경에 영향을 미칠 수 있다. Codex가 취약한 코드를 생성할 수 있으므로, 적절한 예방 조치 없이는 이를 실행하거나 신뢰하기 전에 전문가가 검토해야 한다. 미래의 코드 생성 모델은 더 안전한 코드를 생성할 수 있으나, 이는 확실하지 않다.&lt;/p>
&lt;p>Codex는 사이버범죄를 지원하는 데 잘못 사용될 수 있지만, 현재의 기능 수준에서는 악성 소프트웨어 개발의 진입 장벽을 실질적으로 낮추지 않는다고 판단된다. 더 강력한 코드 생성 모델이 미래에 발전할 것으로 예상되므로, 완화책에 대한 추가 연구와 모델 능력의 지속적인 연구가 필요하다.&lt;/p>
&lt;p>Codex와 같은 비결정적 시스템의 특성은 더 진보된 악성 소프트웨어를 가능하게 할 수 있다. 이러한 특성은 동일한 작업을 수행하는 다양한 소프트웨어를 생성하는 것을 용이하게 한다. 이는 전통적인 악성 소프트웨어 탐지 및 안티바이러스 시스템에 도전을 제시하며, 더 능력 있는 코드 생성 모델은 다양한 형태의 악성 소프트웨어를 생성하는 기법을 발전시킬 수 있다. 단기적으로는 액세스 제한 및 오용 모니터링 같은 보안 전략이 이 위협을 관리할 수 있지만, 더 능력 있는 모델이 개발됨에 따라 그 효과는 제한적일 수 있다.&lt;/p>
&lt;p>Codex와 같은 거대 언어 모델은 학습 데이터 내의 패턴을 학습할 수 있으며, 이로 인해 소스 코드의 민감한 데이터가 모델에 의해 예측될 수 있다. Codex는 공개 저장소에서 학습되므로, 학습 데이터의 민감한 정보는 이미 유출되었다고 간주된다. 또한, 공개 데이터는 공격자가 학습 데이터를 손상시켜 특정 모델 동작을 유발할 수 있음을 보여주는 이전 연구에 따라 신뢰할 수 없는 것으로 취급되어야 한다.&lt;/p>
&lt;h3 id="environmental-impacts">Environmental impacts&lt;/h3>
&lt;p>Codex와 같은 대형 생성 모델은 학습과 추론에서 에너지를 소비한다. GPT-3-12B의 원래 학습과 Codex-12B의 미세 조정은 대량의 컴퓨팅을 소비하였으며, 이러한 학습은 탄소 크레딧을 구입하고 재생에너지를 확보하는 플랫폼에서 이루어졌다. 컴퓨팅 소비는 공급 체인 비용을 발생시키며, 특정 지역에 집중될 수 있다. 장기적으로 볼 때, 코드 생성의 컴퓨팅 요구사항은 도전적인 문제를 해결하는 데 많은 추론이 사용되면 Codex의 학습보다 훨씬 커질 수 있다.&lt;/p>
&lt;h3 id="legal-implications">Legal implications&lt;/h3>
&lt;p>생성된 코드에는 몇 가지 법적 고려 사항이 있으며, 인터넷 데이터를 활용한 AI 시스템 훈련은 이전에 &amp;ldquo;fair use&amp;quot;의 사례로 지적된 바 있다.&lt;/p>
&lt;p>예비 연구에 따르면, Codex 모델이 학습 데이터와 동일한 코드를 생성하는 경우는 매우 드물다. 이런 경우는 주로 학습 데이터에서 반복적으로 나타나는 프로그래밍 언어의 일반적인 표현이나 관례를 반영한 것으로, 특정 코드를 보존하거나 복사하는 것이 아니라 모델의 예측 가중치 때문이다.&lt;/p>
&lt;p>생성된 코드는 사용자의 입력에 반응하고 맞춤화되며, 사용자는 코드의 편집과 승인을 완전히 통제한다. 이는 완성된 작품이 저자의 것으로 간주되는 자동 제안이나 자동 완성 기능과 비슷하게, 코드 생성을 다른 저작 도구의 기능으로 볼 수 있다.&lt;/p>
&lt;p>코드 생성 시스템의 지적 재산권에 관한 넓은 이슈에 지속적으로 주의를 기울이면서 책임감 있는 안전한 AI를 추구하고 있다. 이를 통해 시스템 사용자들이 확신을 가지고 이를 배포할 수 있도록, 정책 입안자와 전문가들과 계속 협력할 계획이다.&lt;/p>
&lt;h3 id="risk-mitigation">Risk mitigation&lt;/h3>
&lt;p>Codex와 같은 모델은 사용으로 인한 해를 최소화하고 긍정적인 사회적 영향을 극대화하기 위해 신중하게 개발되고 사용되어야 한다. 효과적인 위험 분석과 완화를 위해 문맥적 접근이 중요하며, 코드 생성 모델 배포에서는 항상 고려해야 할 중요한 완화책이 있다.&lt;/p>
&lt;p>세심한 문서화, 사용자 인터페이스 디자인, 코드 검토 요구사항 등을 통해 과도한 의존성, 공격적인 콘텐츠, 불안전한 코드 생성 등의 문제를 줄일 수 있다. 서비스로 제공되는 모델의 경우 사용자 검토, 사용 사례 제한, 모니터링 등의 정책이 악의적 사용을 줄이고, 모델이 잘 맞지 않는 고위험 영역에서의 사용을 방지하는 데 도움이 될 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>딥러닝의 발전은 프로그램 학습 분야를 크게 발전시켰으며, 이에는 프로그램 유도와 프로그램 합성이라는 두 가지 주요 접근법이 있다.&lt;/p>
&lt;p>프로그램 유도에서는 모델이 잠재적 프로그램 표현에서 직접 출력을 생성한다. 초기 모델은 간단한 덧셈이나 기억 작업을 수행할 수 있었고, 후속 연구에서는 Neural Turing Machine, memory network, Neural GPU 등의 개념이 도입되었다. 최근에는 &amp;ldquo;Neural Program Interpreter&amp;quot;와 &amp;ldquo;Universal Transformer&amp;rdquo; 같은 접근법에서 recurrence가 프로그램 유도에 유용하다는 것이 발견되었다.&lt;/p>
&lt;p>프로그램 합성에서는 모델이 자연어 명세에서 프로그램을 명시적으로 생성한다. 초기 접근법에서는 probabilistic context free grammar (PCFG)을 사용하여 프로그램의 abstract syntax tree (AST)를 생성했고, 이후 연구에서는 state vector를 학습하여 더욱 개선하였다. 이 아이디어는 text-to-code 검색과 text-conditional 코드 생성에 활용되었으며, AST는 code-to-text 생성에도 사용될 수 있음이 밝혀졌다.&lt;/p>
&lt;p>프로그램은 AST를 거치지 않고도 합성될 수 있다. 이를 통해 코드가 자연어보다 더 예측 가능하다는 것이 밝혀졌고, 문자 수준의 언어 모델이 실제 코드를 생성할 수 있는 능력이 입증되었다. 또한, 소스 코드 내의 함수를 예측하는 모델이 프로그램 검색을 안내하는 데 사용되었다.&lt;/p>
&lt;p>대규모 자연어 모델의 성공을 따라 대규모 transformer가 프로그램 합성에도 적용되었다. CodeBERT는 함수와 docstring을 학습시켜 코드 검색에서 좋은 결과를 얻었고, PyMT5는 T5 목표를 사용하여 프로그램의 서명, docstring, 본문 사이를 번역하는 시스템을 학습시켰다.&lt;/p>
&lt;p>Codex 모델은 함수적 정확성을 기준으로 벤치마크하였고, 샘플링이 많을수록 성능이 향상되었다. 가상코드에서 코드를 생성하는 SPoC, 프로그래밍 언어 간 번역을 수행하는 TransCoder, 대조적인 코드 모델을 학습하는 ContraCode 등 다양한 접근법이 제시되었다. 특히, 함수적 정확성이 모델의 능력을 더 잘 포착하고, beam search를 통해 여러 샘플을 합성하는 것이 가장 효과적인 방법이었다는 연구 결과도 있다.&lt;/p>
&lt;p>neural 프로그래밍 시스템 벤치마크에 사용된 초기 데이터셋인 FlashFill과 Hearthstone 이후, 더 넓고 어려운 데이터셋이 주를 이루게 되었다. GitHub에서 Python 코드를 스크랩한 Barone &amp;amp; Sennrich의 데이터셋, 여러 프로그래밍 언어 데이터로 구성된 CodeSearchNet 챌린지, 그리고 여러 프로그래밍 벤치마크를 집계한 CodeXGLUE 등이 제안되었다. 특히, 가장 최근에는 Codeforces 웹사이트의 문제를 기반으로 한 APPS 벤치마크가 이 논문의 평가 작업에 중요한 역할을 하였다.&lt;/p>
&lt;p>코딩은 docstring에서 코드를 생성하는 것 이상의 넓은 활동 범위를 가지고 있다. transformer를 활용한 단위 테스트 생성, 사용자가 수락한 완성 항목에 기반한 자동 완성 도구 훈련 등 다양한 접근이 연구되고 있다. 버그를 찾아내고 수정하는 작업에도 다양한 기법이 사용되었으며, 최근에는 버그 수정을 버그가 있는 프로그램에서 올바른 프로그램으로의 신경 기계 번역으로 간주하는 연구도 있다. 그러나, 이런 접근법들은 테스트 스위트의 제한성을 드러내며, 프로그램의 정확성을 평가하는 데 있어 도전적인 문제가 있음을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>자연어 docstring에서 기능적으로 정확한 코드를 생성하는 대규모 언어 모델을 학습시키는 가능성을 조사하였다. GitHub에서 미세 조정된 코드를 통해, Codex 모델은 인간이 작성한 문제 데이터셋에서 높은 성능을 보였다. 모델 성능은 평가 세트와 유사한 분포에서 학습하거나 여러 샘플을 생성함으로써 향상될 수 있었다. 또한, 코드 본문에서 docstring을 생성하는 역방향 작업을 수행하는 모델 학습도 가능했다. 마지막으로, 코드 생성 모델의 광범위한 영향과 한계를 논의하였으며, 아직도 상당한 개선 여지가 있음을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2107.03374.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/human-eval" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Switch Transformers</title><link>https://kurtkim.github.io/p/switch-transformers/</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/switch-transformers/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>딥러닝에서 대부분의 모델은 동일한 parameter를 재사용하지만, Mixture of Experts (MoE) 모델은 각 입력에 대해 다른 parameter를 선택한다. 이로 인해 많은 parameter를 가진 sparsely-activated 모델이 만들어지지만, 계산 비용은 일정하다. 그러나 복잡성과 통신 비용, 학습의 불안정성 때문에 MoE는 널리 적용되지 못하였다.&lt;/p>
&lt;p>이 문제를 해결하기 위해 Switch Transformer를 도입했고, MoE 라우팅 알고리즘을 단순화하고 통신 및 계산 비용을 줄인 개선된 모델을 설계하였다. 이를 통해 처음으로 lower precision(bfloat16)로 large sparse 모델을 학습시킬 수 있었다.&lt;/p>
&lt;p>이러한 개선을 통해, 동일한 계산 자원을 사용하면서 사전 학습 속도를 최대 7배 향상시켰고, 모든 101개 언어에서 mT5-Base 버전보다 성능을 향상시켰다. 또한, 최대 1조 개의 parameter 모델을 사전 학습하여 언어 모델의 규모를 확장하였고, T5-XXL 모델보다 4배 빠른 속도를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 학습은 신경 언어 모델을 향상시키는 효과적인 방법으로 증명되었다. 그러나, 이는 계산적으로 매우 집약적이다. 따라서, 이와 같은 모델 규모의 성공에 영감을 받아, 더 큰 계산 효율성을 추구하며 sparsely-activated expert 모델인 Switch Transformer를 제안한다. 이는 각 들어오는 예제에 대해 신경망 가중치의 부분집합만을 활성화함으로써 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure1.png"
width="1158"
height="438"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure1_hude6ff41104f4f0c08df74978b1b722e2_111078_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure1_hude6ff41104f4f0c08df74978b1b722e2_111078_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="634px"
>&lt;/p>
&lt;p>sparse 학습은 연구와 엔지니어링 분야에서 활발하게 진행되고 있지만, 현재 기계 학습 라이브러리와 hardware accelerator는 주로 dense matrix 곱셈에 초점을 맞추고 있다. 이에 대해, &amp;ldquo;Mixture-of-Expert (MoE)&amp;rdquo; 패러다임을 단순화하여 학습 안정성과 계산적 이점을 추구하였다. MoE 모델은 기계 번역 분야에서 성공을 거두었지만, 복잡성, 통신 비용, 학습 불안정성 등의 문제로 널리 채택되지는 못하고 있다.&lt;/p>
&lt;p>이 연구에서는 알고리즘 문제를 해결하고 번역을 넘어서 자연어 분야에서의 광범위한 활용 가능성을 발견하였다. 사전 학습, 미세조정, 다중 작업 학습 등 NLP의 다양한 체제에서 우수한 확장성을 측정하였다. 또한 Switch Transformer 아키텍처는 슈퍼컴퓨터 뿐만 아니라 몇 개의 계산 코어에서도 효과적이며, 큰 희소 모델은 품질 향상의 30%를 유지하면서 밀집 버전으로 축소할 수 있음을 보여주었다.&lt;/p>
&lt;p>이 논문의 기여는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>Mixture of Experts를 간소화하고 개선한 Switch Transformer 아키텍처.&lt;/li>
&lt;li>T5 모델에 대해 동일한 FLOPS 당 토큰을 사용하면서도 사전 학습 속도를 7배 이상 향상시킨 것을 확인하였다. 또한, expert가 두 명만 있는 계산 자원이 제한된 상황에서도 성능 개선이 가능하다는 것을 보여주었다.&lt;/li>
&lt;li>sparse 사전 학습 및 전문화된 미세조정 모델을 small dense 모델로 성공적으로 축소(distillation)하면서, 모델 크기를 최대 99%까지 줄이고, 동시에 sparse 모델의 품질 향상의 30%를 유지하였다.&lt;/li>
&lt;li>사전 학습 및 미세조정 기법을 개선하였다: (1) bfloat16 precision으로 학습 가능한 selective precision 학습 (2) 더 많은 expert로 확장 가능한 초기화 방식 (3) sparse 모델의 미세조정 및 다중 작업 학습을 향상시키는 expert regularization 증가.&lt;/li>
&lt;li>다국어 데이터에 대한 사전 학습의 이점을 측정했고, 이를 통해 모든 101개 언어에서 개선을 확인하였다. 또한, 91%의 언어에서 mT5 기준선에 비해 4배 이상의 속도 향상을 확인하였다.&lt;/li>
&lt;li>데이터, 모델, expert-parallelism을 효율적으로 결합하여 최대 1 trillion 개의 parameter를 가진 모델을 만들어, 신경 언어 모델의 규모를 확장하였다. 이 모델들은 T5-XXL 기준선의 사전 학습 속도를 4배로 향상시켰다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="switch-transformer">Switch Transformer&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure2.png"
width="1078"
height="600"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure2_huf054e4c2c312d04ca8edeefba7ca68d5_89929_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure2_huf054e4c2c312d04ca8edeefba7ca68d5_89929_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="431px"
>&lt;/p>
&lt;p>Switch Transformers의 핵심 설계 원칙은 Transformer 모델의 parameter 수를 간단하고 효율적으로 최대화하는 것이다. 이는 모델 크기, 데이터 크기, 계산 비용과의 지수적 확장을 통해 규모의 이점을 극대화하는 방향으로 연구되었다. 특히, 상대적으로 적은 데이터에 대해 큰 모델을 학습시키는 것이 계산적으로 가장 이상적인 방법이라고 강조하고 있다.&lt;/p>
&lt;p>the ﬂoating point operations (FLOPs) per example을 일정하게 유지하면서 parameter 수를 늘리는 방법을 연구하였다. 이는 parameter 수가 별도로 중요한 확장 축이라는 가설에 기반한다. GPU와 TPU와 같은 dense matrix 곱셈에 최적화된 하드웨어를 효율적으로 활용하는 sparsely activated 모델을 설계하였고, 이를 통해 모델의 가중치는 장치 수와 함께 증가하면서 각 장치에서 관리 가능한 메모리와 계산 비용을 유지할 수 있었다.&lt;/p>
&lt;h3 id="simplifying-sparse-routing">Simplifying Sparse Routing&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure3.png"
width="1156"
height="474"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure3_hua92516411452450ef1068a0e1257642a_114673_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure3_hua92516411452450ef1068a0e1257642a_114673_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="243"
data-flex-basis="585px"
>&lt;/p>
&lt;p>&lt;strong>Mixture of Expert Routing.&lt;/strong> Shazeer et al. (2017)은 토큰 표현 $x$를 입력으로 받아 가장 적합한 상위 $k$개의 expert를 선택하는 자연어 Mixtureof-Experts (MoE) layer를 제안하였다. 이를 위해 라우터 변수 $W_r$은 logit $h(x) = W_r \cdot x$를 생성하고, 이는 해당 layer에서 가능한 $N$개의 expert에 대한 softmax 분포를 통해 normalize된다. 각 expert $i$에 대한 게이트 값은 특정 방식으로 주어진다.&lt;/p>
&lt;p>$$ p_i(x) = {{e^{h(x)_i}}\over{\sum_j^N e^{h(x)_j}}} $$&lt;/p>
&lt;p>토큰 $x$를 라우팅하기 위해 상위 $k$개의 게이트 값이 선택된다. 만약 $T$가 선택된 상위 $k$개의 인덱스 집합이라면, layer의 출력 계산은 게이트 값에 의해 토큰에 대한 각 expert의 계산의 선형 가중 조합이 된다.&lt;/p>
&lt;p>$$ y = \sum_{i \in T} p_i(x) E_i(x) $$&lt;/p>
&lt;p>&lt;strong>Switch Routing: Rethinking Mixture-of-Experts.&lt;/strong> Shazeer et al. (2017)은 $k &amp;gt; 1$의 expert들로 라우팅하는 것이 중요하다고 주장했지만, 단 한 명의 expert로만 라우팅하는 단순화된 전략을 사용하였다. 이는 모델의 품질을 보존하고, 라우팅 계산을 줄이며, 더 나은 성능을 보여주었다. 이 $k = 1$ 라우팅 전략은 &amp;ldquo;Switch layer&amp;quot;라고 불리며, MoE와 스위치 라우팅 모두에서 라우터의 차별화를 가능하게 한다.&lt;/p>
&lt;p>Switch layer의 이점은 세 가지이다: (1) 토큰을 단일 expert에게만 라우팅하기 때문에 라우터 계산이 줄어든다. (2) 각 토큰이 단일 expert에게만 라우팅되기 때문에 각 전문가의 batch size (expert capacity)는 적어도 절반으로 줄어들 수 있다. (3) 라우팅 구현이 단순화되고 통신 비용이 줄어든다.&lt;/p>
&lt;h3 id="eﬃcient-sparse-routing">Eﬃcient Sparse Routing&lt;/h3>
&lt;p>분산 데이터 및 모델 병렬 아키텍처를 지원하는 Mesh-Tensorflow (MTF) 라이브러리를 사용한다. 이 라이브러리는 물리적인 코어 세트를 논리적인 프로세서 메시로 추상화하여 텐서와 연산을 차원별로 쉽게 파티셔닝할 수 있게 한다. 이 모델은 정적 크기를 필요로 하는 TPU를 염두에 두고 설계되었으며, 아래에서는 distributed Switch Transformer의 구현에 대해 설명한다.&lt;/p>
&lt;p>&lt;strong>Distributed Switch Implementation.&lt;/strong> 모든 텐서 형태는 컴파일 시간에 정적으로 결정되지만, 라우팅 결정 때문에 계산은 동적이다. 이로 인해, expert 용량 설정은 중요한 고려사항이다. expert 용량은 각 전문가가 계산하는 토큰의 수로, 배치의 토큰 수를 expert 수로 나누고, 용량 요인으로 더 확장하여 설정된다.&lt;/p>
&lt;p>$$ \text{expert capacity} = \big( {\text{tokens per batch}\over{\text{number of experts}}} \big) \times \text{capacity factor} $$&lt;/p>
&lt;p>용량 요인이 1.0보다 크면 토큰이 expert간에 완벽하게 균형되지 않을 때 추가 버퍼를 생성한다. 너무 많은 토큰이 한 expert로 라우팅되면 계산이 생략되고 토큰은 다음 레이어로 직접 전달된다. 그러나 expert 용량을 늘리는 것은 계산과 메모리 낭비를 초래할 수 있다. 드롭된 토큰의 비율을 줄이는 것이 sparse expert 모델의 확장에 중요하다는 것을 알아냈다. 보조 로드 밸런싱 손실을 사용하면 좋은 로드 밸런싱이 보장되며, 이러한 설계 결정이 모델의 품질과 속도에 미치는 영향을 연구하고 있다.&lt;/p>
&lt;p>&lt;strong>A Diﬀerentiable Load Balancing Loss.&lt;/strong> expert들 사이에 균형된 부하를 유도하기 위해 보조적인 손실을 추가한다. Switch Transformer는 별도의 로드 밸런싱과 importance-weighting 손실을 가진 원래의 설계를 단순화하였다. 각 스위치 layer에 대해, 이 보조적인 손실은 학습 동안 총 모델 손실에 추가되며, 이는 $N$개의 expert와 $T$개의 토큰이 있는 배치에 대해 벡터 $f$와 $P$ 사이의 스케일링된 dot-product으로 계산된다.&lt;/p>
&lt;p>$$ loss = \alpha \cdot N \cdot \sum_{i=1}^N f_i \cdot P_i $$&lt;/p>
&lt;p>여기서 $f_i$는 expert $i$에게 전달된 토큰의 비율이다.&lt;/p>
&lt;p>$$ f_i = {{1}\over{T}} \sum_{x \in B} \mathbb{1} \lbrace \text{argmax} p(x) = i \rbrace $$&lt;/p>
&lt;p>그리고 $P_i$는 expert $i$에 할당된 라우터 확률의 비율이다.&lt;/p>
&lt;p>$$ P_i = {{1}\over{T}} \sum_{x \in B} p_i (x) $$&lt;/p>
&lt;p>토큰 배치를 expert들 사이에 균등하게 라우팅하려고 하므로, 두 벡터는 $1/N$ 값을 가지게 된다. 보조 손실은 균일한 분포에서 최소화되므로 균일한 라우팅을 유도한다. 최종 손실은 expert 수에 비례하여 균일하게 유지되며, hyper-parameter $α$는 이 보조 손실에 대한 계수이다. $α = 10^{−2}$는 로드 밸런싱을 보장하면서도 주요 목표를 압도하지 않는 수준이었다. $α$의 범위를 다양하게 실험한 결과, $10^{−2}$가 학습 손실에 방해를 주지 않으면서도 빠르게 로드를 균형잡는 것을 확인하였다.&lt;/p>
&lt;h3 id="putting-it-all-together-the-switch-transformer">Putting It All Together: The Switch Transformer&lt;/h3>
&lt;p>Switch Transformer의 첫 테스트는 &amp;ldquo;Colossal Clean Crawled Corpus&amp;rdquo; (C4)에 대한 사전 학습에서 시작한다. 모델은 누락된 토큰을 예측하도록 학습받는 masked language modeling 작업을 사용하며, 15%의 토큰을 드롭아웃하고 마스킹된 시퀀스를 단일 센티널 토큰으로 교체한다. 모델 비교를 위해, negative log perplexity를 기록한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table1.png"
width="1088"
height="474"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table1_hu295d9bf5a327cc3e7ab946633bf472ba_101196_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table1_hu295d9bf5a327cc3e7ab946633bf472ba_101196_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;p>Switch Transformer는 T5-Base와 계산량이 같으며, MoE Transformer는 각 토큰에 대해 별도의 FFN을 적용하는 두 개의 expert를 가지므로 FLOPS가 더 크다. 모든 모델은 동일한 하드웨어에서 동일한 스텝으로 학습되었다. 예상치 못하게, 용량 요인이 2.0에서 1.25로 변경된 MoE 모델은 속도가 느려졌다(840에서 790으로).&lt;/p>
&lt;p>결과를 요약하면 다음과 같다: (1) Switch Transformer는 속도와 품질 면에서 조정된 밀집 모델과 MoE Transformer를 능가한다. (2) Switch Transformer는 MoE Transformer보다 작은 계산 부하를 가지며, 이 크기를 늘려 MoE Transformer의 학습 속도에 맞추면, 모든 MoE와 dense 모델을 능가한다. (3) Switch Transformer는 lower capacity factor(1.0, 1.25)에서 더 좋은 성능을 보이며, 이는 모델 메모리가 부족한 대형 모델 체제에서 유리하다.&lt;/p>
&lt;h3 id="improved-training-and-fine-tuning-techniques">Improved Training and Fine-Tuning Techniques&lt;/h3>
&lt;p>Sparse expert 모델은 각 계층에서의 hard-switching(라우팅) 결정으로 인해 vanilla Transformer보다 학습이 어려울 수 있다. 또한, 저정밀 형식인 bfloat16은 라우터의 softmax 계산에서 문제를 악화시킬 수 있다. 이러한 학습 어려움을 극복하고 안정적이며 확장 가능한 학습을 달성하기 위한 방법들을 사용하고 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table2.png"
width="1052"
height="216"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table2_hu69726b744791099225382e60d223de45_51504_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table2_hu69726b744791099225382e60d223de45_51504_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="487"
data-flex-basis="1168px"
>&lt;/p>
&lt;p>&lt;strong>Selective precision with large sparse models.&lt;/strong> 모델의 불안정성 때문에 효율적인 bfloat16 precision을 사용한 학습이 어렵다. 그러나 모델의 일부를 float32 precision으로 변환하면, 높은 통신 비용 없이 안정성을 달성할 수 있다. 이 방법은 더 높은 precision으로 일부 모델과 기울기 업데이트를 수행하는 혼합 정밀도 학습 전략과 일치한다. 이 접근법은 bfloat16 학습의 속도와 float32의 학습 안정성을 동시에 제공한다.&lt;/p>
&lt;p>라우터 입력을 float32 precision으로 변환하여 안정성을 높인다. 라우터 함수는 토큰을 받아 expert 계산의 선택과 재결합에 사용되는 텐서를 생성한다. float32는 라우터 함수 내에서만 사용되며, 함수의 끝에서 텐서는 bfloat16으로 다시 변환된다. 이로 인해 비싼 float32 텐서의 전체 통신 비용은 피하면서, float32의 안정성을 활용할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table3.png"
width="942"
height="182"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table3_hub774f604d3ec7a5d6f63ee674f7f01e8_38632_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table3_hub774f604d3ec7a5d6f63ee674f7f01e8_38632_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="517"
data-flex-basis="1242px"
>&lt;/p>
&lt;p>&lt;strong>Smaller parameter initialization for stability.&lt;/strong> 딥러닝에서 적절한 초기화는 성공적인 학습에 중요하며, 이는 특히 Switch Transformer에게 매우 중요하다. 가중치 행렬은 평균이 0이고 표준 편차가 $\sqrt{s}/n$인 잘린 정규 분포에서 요소를 추출하여 초기화된다. 여기서 $s$는 스케일 hyper-parameter이고, $n$은 가중치 텐서의 입력 단위 수이다.&lt;/p>
&lt;p>불안정성을 줄이기 위해, default Transformer 초기화 스케일을 10분의 1로 줄이는 것이 품질 향상과 학습 불안정성 감소에 도움이 된다. 이 방법은 학습 초기에 모델 품질을 크게 향상시키고, 실행 간 분산을 크게 줄인다. 이 초기화 방식은 다양한 크기의 모델에 널리 적용될 수 있으며, 이 방법을 사용해 223M parameter의 작은 모델부터 1 trillion 개 이상의 parameter를 가진 거대한 모델까지 안정적으로 학습하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table4.png"
width="1000"
height="254"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table4_hu1f5499b1477a1425ecdd7050990c6d69_63431_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table4_hu1f5499b1477a1425ecdd7050990c6d69_63431_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="393"
data-flex-basis="944px"
>&lt;/p>
&lt;p>&lt;strong>Regularizing large sparse models.&lt;/strong> 이 논문은 큰 말뭉치에 대한 사전 학습 후 작은 downstream task에 대해 미세 조정하는 NLP 접근법을 다룬다. 미세 조정 작업이 적은 예제를 가지므로 과적합 문제가 발생할 수 있다. standard Transformer의 미세 조정에서는 각 layer에서 드롭아웃을 사용하여 과적합을 방지한다. 그러나 Switch Transformer는 더 많은 parameter를 가지므로, 작은 downstream task에서 더 심한 과적합이 발생할 수 있다.&lt;/p>
&lt;p>&amp;ldquo;expert dropout&amp;rdquo; 이라는 방법을 제안하여 미세 조정 과정에서의 과적합 문제를 완화한다. 이 방법은 각 expert layer에서의 중간 계산에서 드롭아웃 비율을 크게 늘리는 것이다. 모든 layer에서 드롭아웃을 증가시키는 것은 성능을 악화시키지만, non-expert layer에서는 작은 드롭아웃 비율(0.1), expert layer에서는 큰 드롭아웃 비율(0.4)을 설정하면 성능이 향상된다.&lt;/p>
&lt;hr>
&lt;h2 id="scaling-properties">Scaling Properties&lt;/h2>
&lt;p>Switch Transformer 아키텍처의 스케일링 특성에 대한 연구를 수행하였다. 계산력이나 데이터 양에 제한받지 않는 상황에서, 180B 개의 타겟 토큰을 가진 C4 코퍼스를 사용해 효과가 줄어들 때까지 학습시켰다.&lt;/p>
&lt;p>가장 효율적으로 스케일링하는 차원은 &amp;ldquo;expert의 수&amp;quot;이다. expert의 수를 늘려도 토큰당 한 명의 expert만 선택하기 때문에 계산 비용은 대체로 고정된다. 라우터는 더 많은 expert들에 대한 확률 분포를 계산해야 하지만, 이는 상대적으로 가벼운 계산 작업이다.&lt;/p>
&lt;h3 id="scaling-results-on-a-step-basis">Scaling Results on a Step-Basis&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure4.png"
width="1124"
height="460"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure4_hu8f60443165bcee857924063c4a91328d_111595_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure4_hu8f60443165bcee857924063c4a91328d_111595_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="586px"
>&lt;/p>
&lt;p>많은 expert(parameter)를 가질수록 학습이 가속화되는 트렌드를 보여준다. sparse 모델 parameter를 확장하면 스케일링 이점이 있으며, expert 수를 늘릴수록 모델은 더 효율적으로 샘플을 처리한다. 특히, Switch-Base 64 expert 모델은 T5-Base 모델보다 학습 단계 시간이 7.5배 빠르며, 더 큰 모델이 고정된 토큰 수를 더 빠르게 학습한다는 것을 확인하였다.&lt;/p>
&lt;h3 id="scaling-results-on-a-time-basis">Scaling Results on a Time-Basis&lt;/h3>
&lt;p>expert 수를 늘릴수록 성능이 계속 향상된다는 것을 보여준다. 그러나 Switch Transformer 모델은 추가적인 통신 비용과 라우팅 계산이 필요하므로, 단계별 샘플 효율성의 향상이 실제 시간에 따른 모델 품질 개선으로 이어지지는 않는다. 따라서, 제한된 학습 시간과 계산 비용 내에서 dense 모델을 학습할 것인지 아니면 sparse 모델을 학습할 것인지에 대한 질문이 제기된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure5.png"
width="774"
height="582"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure5_hu5ef699bdf0cb5c1dffc2384653abeeef_100404_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure5_hu5ef699bdf0cb5c1dffc2384653abeeef_100404_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="132"
data-flex-basis="319px"
>&lt;/p>
&lt;p>고정된 학습 기간과 계산 예산에 대해, Switch Transformer는 상당한 가속화를 제공한다. 이 설정에서, Switch-Base 64 expert 모델은 T5-Base가 유사한 perplexity를 얻는 데 걸리는 시간의 일곱 분의 일 동안 학습한다.&lt;/p>
&lt;h3 id="scaling-versus-a-larger-dense-model">Scaling Versus a Larger Dense Model&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure6.png"
width="1152"
height="452"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure6_hu6762a73140a7dcaebfabf4129ddc9d94_112550_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure6_hu6762a73140a7dcaebfabf4129ddc9d94_112550_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>계산적으로 매칭된 dense 모델은 Switch Transformer에 비해 느리다. 만약 리소스를 더 큰 dense 모델에 할당했다면, T5-Large 모델이 토큰 당 3.5배 더 많은 FLOPs를 적용하더라도, Switch-Base는 더 효율적이며 2.5배의 가속화를 제공한다. 더 큰 sparse희소 버전인 Switch-Large를 디자인하면 더 많은 이익을 얻을 수 있으며, 이는 스케일링과 미세 조정에서 우수한 성능을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="downstream-results">Downstream Results&lt;/h2>
&lt;p>Switch Transformer는 사전 학습 동안 우수한 스케일링 특성을 보여주었다. 이제 이런 이점이 다양한 NLP 작업에서 언어 학습 능력을 개선하는 데 활용될 수 있는지 검증한다. 또한, sparse 모델의 메모리 사용량을 90% 이상 줄이는 방법을 연구하였고, 마지막으로, Switch Transformer가 101개 언어에서 다국어 T5-Base 모델을 개선하는 강력한 다중 작업 학습자임을 입증하였다.&lt;/p>
&lt;h3 id="fine-tuning">Fine-Tuning&lt;/h3>
&lt;p>&lt;strong>Baseline and Switch models used for ﬁne-tuning.&lt;/strong> 223M parameter T5-Base와 739M parameter T5-Large 모델을 기준으로, 많은 parameter를 가진 Switch Transformer를 설계하였다. 이 모델은 텍스트 복제가 제거된 개선된 C4 코퍼스에서 사전 학습되었다. 학습 프로토콜은 batch 당 약 1,048,576 토큰으로 550k step을 진행하며, 총 576B 토큰을 사용한다. 다양한 작업 세트에서 미세 조정을 진행하며, 드롭아웃 비율은 대부분의 layer에서 0.1, Switch layer에서는 0.4로 설정되었다. 미세 조정은 1M의 배치 크기로 16k 단계를 진행하였고, 모든 작업에 대해 200 step마다 최고 성능을 평가하여 보고하였다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning tasks and data sets.&lt;/strong> 질문 응답, 요약, 세계에 대한 지식 등의 언어 능력을 평가하는 다양한 작업을 선택하였다. GLUE와 SuperGLUE 벤치마크를 활용하며, 이들은 감정 분석, 단어 의미 판별, 문장 유사도, 자연 언어 추론 등을 포함하는 다양한 작업으로 구성되어 있다. 기사 요약 능력은 CNNDM과 BBC XSum 데이터 세트로 측정하며, 질문 응답 능력은 SQuAD 데이터 세트와 ARC Reasoning Challenge로 조사한다. 또한, Natural Questions, Web Questions, Trivia QA 등의 데이터 세트를 통해 모델의 지식을 평가한다. 상식적 추론 능력은 Winogrande Schema Challenge로 평가하며, 자연 언어 추론 능력은 Adversarial NLI Benchmark로 테스트한다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning metrics.&lt;/strong> 이 논문에서는 평가 지표로 GLUE와 SuperGLUE의 모든 하위 작업에 대한 평균 점수, CNNDM과 XSum에 대한 Rouge-2 지표, SQuAD와 closed book 작업에서 대상과 정확히 일치하는 답변의 비율, 그리고 ARC Easy, ARC Challenge, ANLI, Winogrande에서 생성된 응답의 정확성을 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table5.png"
width="1120"
height="692"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table5_hue9ba377005b724b17e7e198e5437ad52_132152_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table5_hue9ba377005b724b17e7e198e5437ad52_132152_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="388px"
>&lt;/p>
&lt;p>&lt;strong>Fine-tuning results.&lt;/strong> 다양한 자연 언어 작업에서 중요한 향상을 보았다. 특히, SuperGLUE, Winogrande, closed book Trivia QA, XSum에서 눈에 띄는 향상이 있었다. 반면, AI2 Reasoning Challenge (ARC) 데이터 세트에서는 T5-Base와 T5-Large가 각각 Switch-Base와 Switch-Large를 능가하는 결과를 보여주었다. 전반적으로, Switch Transformer 아키텍처는 추론과 지식 중심의 작업 모두에서 향상을 가져오며, 이는 잘 사전 학습되고 미세 조정을 통해 downstream task의 품질을 향상시킬 수 있다는 것을 입증한다.&lt;/p>
&lt;h3 id="distillation">Distillation&lt;/h3>
&lt;p>수십억 또는 수조의 parameter를 가진 대규모 신경망 배포는 복잡하다. 이 문제를 해결하기 위해, large sparse 모델을 small dense 모델로 압축하는 연구를 진행하고 있다. 미래에는 대규모 모델을 smaller sparse 모델로 압축(distilling)하는 연구도 가능할 것이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table6.png"
width="932"
height="332"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table6_hue09dc4e8a0b8ff0de5ea3b6b3e31f027_76844_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table6_hue09dc4e8a0b8ff0de5ea3b6b3e31f027_76844_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="673px"
>&lt;/p>
&lt;p>&lt;strong>Distillation techniques.&lt;/strong> 다양한 모델 압축 기법을 연구하였다. 이 기법들은 BERT 모델에 대한 압축 방법을 연구한 Sanh et al. (2019)의 연구를 기반으로 한다. non-expert weight로 dense 모델을 초기화하면 소폭의 향상이 있었고, teacher의 확률 0.25와 실제 라벨 0.75의 혼합을 사용하여 압축 향상을 관찰하였다. 이 두 기법을 결합하여 parameter의 약 1/20만 사용하여 large sparse 모델로부터 얻는 품질 향상의 약 30%를 유지하였다. 이는 student 모델이 teacher 모델의 성능에 근접함을 의미한다.&lt;/p>
&lt;p>&lt;strong>Achievable compression rates.&lt;/strong> 최적의 압축 기법을 사용해 다양한 sparse 모델을 dense 모델로 압축하였다. 1.1B부터 14.7B까지의 parameter를 가진 SwitchBase 버전을 압축하며, 이 과정에서 1.1B parameter 모델의 품질 향상의 37%를 유지하면서 82%를 압축하였다. 모델을 99% 압축한 극단적인 경우에도, teacher 모델의 품질 향상의 28%를 유지할 수 있었다.&lt;/p>
&lt;p>&lt;strong>Distilling a ﬁne-tuned model.&lt;/strong> 미세 조정된 sparse 모델을 dense 모델로 압축하는 연구를 통해, 7.4B parameter의 Switch-Base 모델을 SuperGLUE 작업에 미세 조정하고 223M의 T5-Base로 압축한 결과를 제시하였다. 이 결과는 FLOP 매치된 dense variant로 압축할 때 sparse 모델의 향상 중 30%를 보존할 수 있음을 보여준다. 미래의 연구 방향 중 하나는 미세 조정 작업에 사용되는 speciﬁc expert를 검토하고 추출하여 더 나은 모델 압축을 달성하는 것일 수 있다.&lt;/p>
&lt;h3 id="multilingual-learning">Multilingual Learning&lt;/h3>
&lt;p>101개의 다른 언어를 혼합하여 사전 학습하면서 모델 품질과 속도의 상충 관계를 측정하는 마지막 downstream 실험을 수행하였다. 이는 mT5의 최신 연구를 기반으로 하며, 101개 언어를 포함하는 Common Crawl 데이터 세트의 다중 언어 버전에서 사전 학습이 진행되었다. 그러나 특정 언어의 스크립트 변형으로 인해, 혼합 작업은 총 107개가 되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure7.png"
width="1214"
height="388"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure7_hu26533bbe287213d1eb624f60b8a4998f_155391_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure7_hu26533bbe287213d1eb624f60b8a4998f_155391_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="312"
data-flex-basis="750px"
>&lt;/p>
&lt;p>Switch 모델과 T5 base variant 간의 모든 언어의 품질 향상을 보여주며, 이는 모든 101개 언어에서 Switch Transformer가 기준선을 넘는다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure8.png"
width="674"
height="506"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure8_hufdcb1e9b606239b5c979270541969119_48678_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure8_hufdcb1e9b606239b5c979270541969119_48678_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;p>mT5-Base에 비해 Switch Transformer를 사용할 때 단계별 속도 향상을 보여주며, 평균 속도 향상이 5배이고 91%의 언어에서 적어도 4배의 속도 향상을 보인다는 것을 보여준다. 이는 Switch Transformers가 효과적인 다중 작업 및 다중 언어 학습 도구임을 입증한다.&lt;/p>
&lt;hr>
&lt;h2 id="designing-models-with-data-model-and-expert-parallelism">Designing Models with Data, Model, and Expert-Parallelism&lt;/h2>
&lt;p>expert의 수를 무작정 늘리는 것은 수익 감소의 법칙에 따라 효과가 줄어든다. 이를 보완하기 위해 Transformer는 차원을 함께 증가시키는 방식으로 스케일링하는데, 이는 parameter와 연산량을 증가시키지만 가속기의 메모리에 제한된다. 메모리 한계를 초과하면, 단일 프로그램 다중 데이터(Single Program Multiple Data, SPMD) 모델 병렬화를 사용할 수 있으며, 이 방법은 데이터, 모델, expert-parallelism의 트레이드오프를 고려해야 한다.&lt;/p>
&lt;p>&lt;strong>Reviewing the Feed-Forward Network (FFN) Layer.&lt;/strong> Mesh TensorFlow에서 데이터, 모델, expert-parallelism을 이해하기 위해 FFN layer을 예로 들어 설명한다. batch의 각 토큰은 $d_{model}$의 차원을 가지며, FFN의 입력과 출력은 $[B, d_{model}]$ 크기, 중간값은 $[B, d_{ff}]$ 크기이다. $N$에서 중간값은 $h = xW_{in}$을 계산하고, 이를 ReLU 함수에 적용해 $y = ReLU(h)W_{out}$를 얻는다. $W_{in}$과 $W_{out}$은 각 토큰에 독립적으로 적용되며, 크기는 각각 $[d_{model}, d_{ff}]$와 $[d_{ff}, d_{model}]$이다.&lt;/p>
&lt;p>Mesh TensorFlow는 사용 가능한 모든 코어를 프로세서의 논리적 다차원 메시로 재매핑한다. 데이터 병렬 샤딩과 모델 병렬 샤딩을 통해 코어를 나눈다. 각 코어는 $B/n$ 토큰을 포함하며, $d_{ff}$를 가진 텐서와 변수들이 모델 병렬 코어에 샤드된다. variants with experts-layers 의 경우, 최대 $C$ 토큰을 처리할 수 있는 $E$개의 expert를 고려한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/figure9.png"
width="1198"
height="720"
srcset="https://kurtkim.github.io/p/switch-transformers/images/figure9_hu9075531a8527697fd97c01b5f0e06755_178958_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/figure9_hu9075531a8527697fd97c01b5f0e06755_178958_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="399px"
>&lt;/p>
&lt;h3 id="data-parallelism">Data Parallelism&lt;/h3>
&lt;p>데이터 병렬 모델 학습은 분산 학습의 표준이며, 모든 코어가 데이터 병렬 차원에 할당된다$(n = N, m = 1)$. 이 방식의 장점은 gradient를 모든 코어에 집계해야 할 때까지 전체 forward 및 backward pass가 완료될 때까지 통신이 필요하지 않다는 것이다.&lt;/p>
&lt;h3 id="model-parallelism">Model Parallelism&lt;/h3>
&lt;p>모든 코어가 모델 병렬 차원에 할당되는 시나리오를 고려하면, 모든 코어는 전체 $B$ 토큰을 유지하고, 가중치의 고유한 부분을 포함한다. 각 forward 및 backward pass마다 통신 비용이 발생하며, $d_ff$$ 차원이 분할되어 합산해야 하기 때문에 각 코어는 $[B, d_{model}]$ 텐서를 전송한다. 코어 간에 분할된 차원이 합산되어야 하면, forward 및 backward pass 모두에 all-reduce 연산이 추가된다. 이는 순수 데이터 병렬화와의 대조로, 데이터 병렬화에서는 all-reduce가 전체 forward 및 backward pass가 끝난 후에만 발생한다.&lt;/p>
&lt;h3 id="model-and-data-parallelism">Model and Data Parallelism&lt;/h3>
&lt;p>대규모 모델에서는 모델 병렬화와 데이터 병렬화를 혼합하여 사용하는 것이 일반적이다(T5, GPT-3). 총 $N = n \times m$ 코어를 사용할 때, 각 코어는 $B/n$ 토큰과 가중치 및 중간 활성화의 $d_{ff} /m$를 처리하게 된다. forward 및 backward pass에서 각 코어는 크기가 $[B/n, d_{model}]$인 텐서를 all-reduce 연산에서 통신한다.&lt;/p>
&lt;h3 id="expert-and-data-parallelism">Expert and Data Parallelism&lt;/h3>
&lt;p>Switch Transformer는 모든 코어를 데이터 분할 차원에 할당하며, 이는 모델의 expert 수와 일치한다. 각 코어는 토큰마다 expert에 대한 할당을 계산하고, 결과는 $[n, B/n, E, C]$ 크기의 이진 행렬이다. 이 행렬은 첫 번째 차원에서 분할되어 expert 할당을 결정하며, $[n, B/n, d_{model}]$ 크기의 입력 텐서와 행렬 곱셈을 통해 수집에 사용된다.&lt;/p>
&lt;p>$$ einsum([n, B/n, d_{model}], [n, B/n, E, C], dimension = [B/n]) $$&lt;/p>
&lt;p>최종 텐서는 $[n, E, C, d_{model}]$ 형태를 가지며 첫 번째 차원에서 샤드된다. 각 코어는 자체 전문가를 가지고 있어, $n$ 차원 대신 $E$ 차원을 샤드하기 위해 $[E, C, d_{model}]$ 크기의 all-to-all 통신을 진행한다. forward pass에서는 다른 코어에 위치한 각 전문가로부터 토큰을 받기 위해 $E × C × d_{model}$ 크기의 추가 통신 비용이 발생한다.&lt;/p>
&lt;h3 id="expert-model-and-data-parallelism">Expert, Model and Data Parallelism&lt;/h3>
&lt;p>이 논문의 최적 모델 설계는 토큰 당 FLOPS와 parameter 수를 균형있게 유지하려 한다. expert 수를 늘리면 parameter 수는 증가하지만 토큰 당 FLOPs는 변하지 않는다. FLOPs를 늘리려면 $d_{ff}$ 차원도 증가해야 하는데, 이는 코어 당 메모리 부족으로 이어질 수 있다. 이 때문에 $m$을 증가시키고, 고정된 코어 수 $N = n × m$에 따라 $n$을 줄이게 되며, 이는 더 작은 배치 크기를 사용하게 된다.&lt;/p>
&lt;p>model-parallelism과 expert-parallelism를 결합하면 토큰 라우팅과 model-parallelism로 인한 내부 통신에 따른 all-to-all 통신 비용이 발생한다. FLOPS, 통신 비용, 코어 당 메모리의 균형을 맞추는 것은 이 세 가지 방법을 모두 결합할 때 복잡해진다. 최적의 매핑은 경험적으로 결정된다.&lt;/p>
&lt;h3 id="towards-trillion-parameter-models">Towards Trillion Parameter Models&lt;/h3>
&lt;p>expert, 모델, 데이터 병렬화를 결합하여, 395B 개와 1.6 trillion 개의 parameter를 갖는 두 개의 large Switch Transformer 모델을 설계하였다. 이 모델들은 언어 모델로서의 사전 학습과 미세 조정 성능에서 어떻게 수행하는지를 연구하였다.&lt;/p>
&lt;p>Switch-C 모델은 expert-parallelism만을 사용하여 설계되었고, 이로 인해 hyper-parameter의 크기는 T5-XXL 모델보다 훨씬 작다. 반면, Switch-XXL은 T5-XXL 모델과 FLOP이 일치하도록 설계되었는데, 이로 인해 hyper-parameter의 차원은 더 크지만 model-parallelism로 인한 추가 통신 비용이 발생한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/switch-transformers/images/table9.png"
width="1162"
height="482"
srcset="https://kurtkim.github.io/p/switch-transformers/images/table9_hu7af8d686b07d6f857200c399ca097210_102702_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/switch-transformers/images/table9_hu7af8d686b07d6f857200c399ca097210_102702_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="578px"
>&lt;/p>
&lt;p>&lt;strong>Sample eﬃciency versus T5-XXL.&lt;/strong> 250k step 후에는 두 Switch Transformer 모델 모두가 T5-XXL의 negative log perplexity를 0.061 이상 개선하였다. 이 차이는 추가 학습으로 계속 증가하며, 500k step에서 Switch-XXL 모델이 T5-XXL을 0.087로 앞서게 된다.&lt;/p>
&lt;p>&lt;strong>Training instability.&lt;/strong> large sparse 모델은 때때로 불안정하며, 이 문제는 규모를 증가시킬수록 발생한다. 1.6T의 parameter와 2048개의 expert를 가진 큰 Switch-C 모델은 학습에서 불안정성이 없지만, 시퀀스당 FLOPs가 10배 더 큰 Switch XXL 버전은 때때로 불안정하다. 따라서 이는 더 나은 모델이지만, T5의 결과에 따라, 전체 1M 단계를 사전 학습하지 않는다.&lt;/p>
&lt;p>&lt;strong>Reasoning ﬁne-tuning performance.&lt;/strong> 503B 토큰에 대해 부분적으로 사전 학습된 Switch-XXL 모델을 사용하여 모델 품질의 예비 평가를 실시하였다. 이 모델을 사용하여 모든 작업을 공동으로 학습하는 멀티 태스크 학습을 실시하면 SQuAD의 검증 세트에서 정확도가 89.7로 증가하였다. 그리고 평균 SuperGLUE 테스트 점수는 87.5로, ANLI에서는 이전 state-of-the-art에 비해 65.7의 정확도를 얻었다. 하지만 SwitchXXL의 이득이 아직 완전히 state-of-the-art downstream 성능으로 전환되지 않았다는 점을 알 수 있다.&lt;/p>
&lt;p>&lt;strong>Knowledge-based ﬁne-tuning performance.&lt;/strong> Salient Span Masking을 사용하여 추가 사전 학습 없이 세 가지 closed-book 지식 기반 작업(Natural Questions, WebQuestions, TriviaQA)을 통해 모델의 지식을 초기에 검토하였다. 이 모든 경우에서 이전 state-of-the-art T5-XXL 모델보다 개선된 결과를 관찰하였다. Natural Questions는 32.8에서 34.4로, Web Questions는 37.2에서 41.0으로, TriviaQA는 42.9에서 47.5로 정확도가 상승하였다.&lt;/p>
&lt;p>다른 모델의 절반 이하의 데이터로 학습에도 불구하고, 이미 비교가 가능하거나 state-of-the-art의 모델 품질을 발견하였다. 현재 Switch Transformer는 추론 작업보다 지식 기반 작업에 더 큰 이득을 가져다 준다. large expert 모델에서 더 강력한 미세 조정 성능을 추출하는 것은 현재 활발히 연구 중이며, 사전 학습의 perplexity는 미래에 개선이 가능함을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>신경망 규모의 중요성은 잘 알려져 있으며, 이를 확장하는 다양한 방법이 제안되었다. 최근 연구에서는 모델 병렬화를 통해 수십억 개의 parameter로 모델을 확장하였다. 다른 방법으로는 파이프라인 기반 모델 병렬화가 있는데, 이는 다른 layer를 장치에 분할하고 micro-batch를 다른 layer로 파이프라인하는 방식이다. 마지막으로, Product Key 네트워크는 신경망의 용량을 확장하기 위해 들어오는 토큰 표현에 기반한 학습 가능한 임베딩을 조회하는 방식을 제안하였다.&lt;/p>
&lt;p>이 연구는 입력에 따라 계산을 동적으로 결정하는 조건부 계산 방법을 사용하는 특정 모델을 연구한다. Cho and Bengio(2014)는 모델의 은닉 상태에서 발생하는 특정 비트 패턴에 따라 가중치를 선택하였고, Eigen et al. (2013)은 dense matrix 곱셈과 ReLU 활성화를 이용한 expert layer를 구축하여 MNIST와 음성 데이터에서 좋은 결과를 보여주었다. 또한, Puigcerver et al. (2020)은 upstream 사전 학습에서 의미론적 클래스에 따라 토큰을 수동으로 라우팅하고, downstream task에 따라 관련 expert를 선택하였다.&lt;/p>
&lt;p>Mixture of Experts (MoE)은 딥러닝 아키텍처에서 효과적이라는 것이 Shazeer et al. (2017)의 연구를 통해 증명되었다. 그들은 LSTM layer 사이에 MoE layer를 추가하고, 토큰을 expert의 조합에 따라 분리하여 언어 모델링과 기계 번역에서 state-of-the-art를 달성하였다. Mesh Tensorflow 라이브러리는 이 MoE layer를 Transformer 아키텍처로 도입했고, GShard는 이를 확장하여 100개 언어의 기계 번역을 크게 개선하였다. 마지막으로 Fan et al. (2021)은 결정론적 MoE 전략을 통해 모델 parameter를 언어 그룹으로 분할하였다.&lt;/p>
&lt;p>Transformer의 attention 패턴에서 시퀀스 길이 차원의 sparsity는 attention complexity를 줄이는 데 성공적이었다. 이는 이전보다 더 긴 시퀀스를 학습하는 것을 가능하게 하였다. 현재 버전의 Switch Transformer는 attention sparsity를 사용하지 않지만, 이 기법들은 서로 보완적이며, 이를 결합하면 긴 컨텍스트를 필요로 하는 작업에서 학습 향상이 가능할 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>Switch Transformer와 일반적으로 sparse expert 모델에 대한 질문을 제기하고 논의한다. 여기서 sparsity는 attention 패턴이 아닌 가중치를 참조한다.&lt;/p>
&lt;p>&lt;strong>Isn’t Switch Transformer better due to sheer parameter count?&lt;/strong> 총 FLOPs와 무관하게 parameter는 신경 언어 모델을 확장하는데 유용하며, 큰 모델이 더 나은 성능을 내는 것이 입증되었다. 하지만 이 경우, Switch Transformer 모델은 같은 계산 자원을 사용하면서 더 효율적이고 빠르게 작동한다.&lt;/p>
&lt;p>&lt;strong>I don’t have access to a supercomputer—is this still useful for me?&lt;/strong> 이 연구는 매우 큰 모델에 중점을 두었지만, expert가 단 두 명인 모델도 성능을 향상시키며 일반적으로 이용 가능한 GPU나 TPU의 메모리 제약 내에서 적용할 수 있음을 확인하였다. 따라서 이 기법은 소규모 환경에서도 유용하다고 생각한다.&lt;/p>
&lt;p>&lt;strong>Do sparse models outperform dense models on the speed-accuracy Pareto curve?&lt;/strong> 다양한 모델 크기에 걸쳐, sparse 모델은 dense 모델보다 단계별로, 그리고 실제 시간에 대해 더 우수한 성능을 보여준다. 통제된 실험에서는 일정한 계산량과 시간에 대해 sparse 모델이 조밀 모델을 능가한다.&lt;/p>
&lt;p>&lt;strong>I can’t deploy a trillion parameter model—can we shrink these models?&lt;/strong> 모델의 품질을 완전히 유지할 수는 없지만, sparse 모델을 dense 모델로 압축하면서 10배에서 100배의 압축률을 달성할 수 있으며, 이는 expert 모델의 품질 향상의 약 30%를 달성한다.&lt;/p>
&lt;p>&lt;strong>Why use Switch Transformer instead of a model-parallel dense model?&lt;/strong> 시간적으로 보면, Switch Transformer는 parameter가 샤딩된 조밀한 모델보다 훨씬 효율적이다. 이는 상호 배타적인 결정이 아니며, Switch Transformer에서는 model-parallelism을 사용하여 토큰당 FLOPs를 늘리지만, 전통적인 model-parallelism의 느림을 겪는다.&lt;/p>
&lt;p>&lt;strong>Why aren’t sparse models widely used already?&lt;/strong> sparse 모델을 시도하는 동기는 dense 모델의 확장의 큰 성공 때문에 방해받았다. sparse 모델은 모델의 복잡성, 학습의 어려움, 통신 비용 등 여러 문제를 겪어왔다. 하지만 Switch Transformer는 이런 문제들을 완화하는 방향으로 발전하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;p>이 논문은 간소화된 아키텍처, 개선된 학습 절차, 그리고 sparse 모델이 어떻게 확장되는지에 대한 연구를 제시한다. 그러나 여기에서 간단하게 설명하는 것처럼, 여전히 많은 미래의 방향성이 열려 있다:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>가장 큰 모델들의 학습 안정성 향상이 중요한 도전 과제입니다. 우리의 안정성 기법은 Switch-Base, Switch-Large, Switch-C 모델에는 효과적이었지만, Switch-XXL에는 부족했습니다. 이러한 모델을 안정화하기 위한 초기 단계를 밟았으나, 아직 해결되지 않은 문제가 남아 있습니다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>일반적으로 향상된 사전 학습 품질이 downstream 결과를 개선시킨다는 것을 발견하였다. 그러나 때로는 예상치 못한 이상현상을 발견하기도 한다. 예를 들어, 비슷한 perplexity에도 불구하고 1.6T parameter의 Switch-C는 SQuAD에서 87.7의 정확도를 달성했는데, 이는 더 작은 Switch-XXL 모델의 89.6에 비해 불리하다. 이는 미세 조정 품질, 토큰당 FLOPS, 그리고 parameter 수 사이의 잘 이해되지 않은 의존성을 시사한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>데이터, 모델, expert-parallelism을 결합한 아키텍처 설계를 위한 확장 관계에 대한 종합적인 연구가 필요하다. 이상적으로 하드웨어 구성의 스펙에 따라 최적의 모델을 빠르게 설계할 수 있어야 하며, 이는 미래의 하드웨어 설계에도 도움이 될 것이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>이 연구는 적응형 계산 알고리즘에 속하며, 항상 동일한 expert를 사용하였다. 그러나 더 유연한 인프라를 통해 미래의 설계는 다양한 expert를 지원할 수 있으며, 이것은 더 많은 계산이 필요한 경우 더 큰 expert로 라우팅하여 더 유연하게 적응할 수 있게 한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Transformer의 FFN layer 외부의 expert layer을 조사하였고, 이것이 모델 품질을 향상시킬 수 있다는 초기적인 증거를 발견하였다. 하지만 bfloat16 형식으로 학습 시 불안정성 때문에 이 부분에 대한 추가 연구는 미래의 작업으로 남겨두었다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>언어 외에도 새로운 모달리티와 다양한 모달리티에서 Switch Transformer를 검토하고 있다. 모델의 sparsity가 새로운 모달리티와 다중 모달 네트워크에서도 비슷한 이점을 가져다 줄 것이라고 믿는다.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>이 목록은 쉽게 확장될 수 있지만, 고민하고 있는 문제 유형과 앞으로 유망한 방향성에 대한 감을 제공하기를 바란다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Switch Transformer는 확장 가능하고 효과적인 자연어 학습 모델로, expert들의 혼합을 간소화하여 효율적인 아키텍처를 만들었다. 이 모델은 다양한 자연어 작업과 학습 체제에서 우수한 성과를 보이며, dense T5와 비교해 상당한 속도 향상을 이루었다. 이 연구가 sparse 모델이라는 효과적인 아키텍처에 대한 관심을 증가시키고, 더 넓은 범위에서 이러한 유연한 모델을 고려하도록 하기를 바란다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2101.03961.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/kyegomez/SwitchTransformers" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-3</title><link>https://kurtkim.github.io/p/gpt-3/</link><pubDate>Tue, 26 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-3/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>최근의 연구는 대량의 텍스트 말뭉치로 사전 학습한 후 특정 작업에 대해 미세 조정하는 것으로 많은 NLP 작업과 벤치마크에서 상당한 성과를 보여주었다. 일반적으로 과제에 중립적인 구조를 가지지만, 이 방법은 여전히 수천 개 또는 수만 개의 예제로 이루어진 과제별 미세 조정 데이터셋을 필요로 한다. 인간은 보통 몇 가지 예제나 간단한 지시사항만으로도 새로운 언어 작업을 수행할 수 있지만, 현재의 NLP 시스템은 이를 여전히 어려워한다. 이 연구에서는 언어 모델의 규모를 확장함으로써 과제 중립적이고 소수의 예제로 이루어진 작업 성능을 크게 개선하는 것을 보여준다. 때로는 이전의 state-of-the-art 미세 조정 접근법과 경쟁력을 갖출 수도 있다. 구체적으로, 1750억 개의 parameter를 가진 GPT-3라는 autoregressive 언어 모델을 학습시키고, 이를 소수의 예제로 평가해보았다. 모든 작업에서 GPT-3는 어떠한 그래디언트 업데이트나 미세 조정 없이 적용되며, 작업 및 소수의 예제는 모델과의 텍스트 상호작용을 통해 명시된다. GPT-3는 번역, 질의응답, 문맥 채우기 작업뿐만 아니라 단어 섞기, 새로운 단어를 문장에 사용하기, 3자리 산술 연산을 수행하는 등의 실시간 추론이나 도메인 적응이 필요한 작업과 같은 많은 NLP 데이터셋에서 강력한 성능을 보여주었다. 한편, GPT-3의 소수 학습은 여전히 어려운 몇몇 데이터셋과 대규모 웹 말뭉치에서의 훈련에 관련된 방법론적 문제가 있다는 점도 확인했다. 마지막으로, GPT-3는 인간 평가자가 사람이 작성한 기사와 구분하기 어려운 뉴스 기사 샘플을 생성할 수 있음을 발견하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>최근 NLP 시스템은 사전 학습된 언어 표현을 다양한 작업에 유연하게 적용하는 추세가 있다. 초기에는 단어 벡터를 사용한 단일 계층 표현을 과제 특정 아키텍처에 적용했으나, 후에는 RNN을 사용한 다계층 표현을 도입하였다. 최근에는 과제 특정 아키텍처의 필요성을 완전히 제거하고, 사전 학습된 recurrent 또는 transformer 언어 모델을 직접 미세 조정하는 방식이 사용되고 있다.&lt;/p>
&lt;p>이러한 패러다임은 많은 어려운 작업에서 진전을 이루었지만, 여전히 작업 특정 데이터셋과 미세 조정이 필요한 한계가 있다. 원하는 작업에서 높은 성능을 달성하기 위해 수천에서 수십만 개의 예제로 이루어진 작업 특정 데이터셋에서 미세 조정이 필요하다. 이러한 한계를 제거하는 것이 중요하다.&lt;/p>
&lt;p>실용적인 관점에서, 모든 새로운 작업에 대한 대규모 레이블링된 예제 데이터셋의 필요성은 언어 모델의 적용 범위를 제한한다. 유용한 언어 작업의 범위는 매우 넓지만, 많은 작업들에 대해 큰 규모의 지도 학습 데이터셋을 수집하는 것은 어렵고, 이 과정이 각각의 새로운 작업마다 반복되어야 한다.&lt;/p>
&lt;p>학습 데이터의 거짓 상관관계를 이용하는 가능성은 모델의 표현력과 학습 분포의 좁음에 따라 증가하며, 이는 사전 학습 후 미세 조정 패러다임에 문제를 일으킬 수 있다. 모델은 사전 학습 동안 정보를 흡수하기 위해 크게 설계되지만, 후에는 좁은 작업 분포에서 미세 조정되며, 이로 인해 학습 분포에 과도하게 특화되어 분포 외부에서는 잘 일반화되지 않을 수 있다. 따라서, 미세 조정된 모델의 성능은 실제 기본 작업에 대한 성능을 과장할 수 있다.&lt;/p>
&lt;p>인간은 대부분의 언어 작업을 배우기 위해 큰 규모의 지도 학습 데이터셋을 필요로 하지 않는다. 간단한 지시사항이나 몇 가지 예제만으로도 새로운 작업을 수행할 수 있다. 이런 적응성은 인간이 여러 작업과 기술을 자연스럽게 섞거나 전환할 수 있게 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.1.png"
width="1256"
height="576"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.1_hue76f5a594fb31d1204e64ba6064c924d_124228_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.1_hue76f5a594fb31d1204e64ba6064c924d_124228_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="523px"
>&lt;/p>
&lt;p>이러한 문제를 해결하기 위한 한 방법은 메타 학습이다. 이는 언어 모델이 훈련 시에 다양한 기술과 패턴 인식 능력을 개발하고, 추론 시에 이를 활용해 원하는 작업에 빠르게 적응하거나 인식하도록 하는 것을 의미한다. 최근 연구에서는 &amp;ldquo;in-context learning&amp;quot;을 통해 이를 시도하였는데, 이는 모델이 자연 언어 지시사항이나 작업의 몇 가지 예제에 조건화되어, 단순히 다음에 무엇이 오는지 예측하여 작업을 완성하도록 하는 방식이다.&lt;/p>
&lt;p>메타 학습 방법은 약간의 잠재력을 보였지만, 미세 조정에 비해 성능이 크게 떨어진다. 특히, Natural Questions에서는 4%, CoQa에서는 55 F1이라는 결과를 보였는데, 이는 최신 기술에 비해 크게 뒤처져 있다. 따라서 메타 학습이 언어 작업을 해결하는 실질적인 방법이 되려면 큰 개선이 필요하다.&lt;/p>
&lt;p>최근 언어 모델링은 transformer 언어 모델의 용량이 크게 증가하는 추세를 보이고 있다. parameter 수가 100M에서 시작해 최근에는 17B개에 이르렀고, 이런 증가는 텍스트 합성과 NLP 작업에서 성능 개선을 가져왔다. 로그 손실이 규모와 함께 개선되는 추세를 보이기 때문에, 문맥 내 학습 능력도 규모와 함께 크게 향상될 수 있을 것으로 보인다.&lt;/p>
&lt;p>이 논문에서는 175B 개의 parameter를 가진 언어 모델, GPT-3의 학습과 그 문맥 내 학습 능력을 테스트한다. GPT-3는 다양한 NLP 데이터셋과 새로운 작업들에 대해 평가되며, 각 작업은 &amp;ldquo;few-shot learning&amp;rdquo;, &amp;ldquo;one-shot learning&amp;rdquo;, &amp;ldquo;zero-shot&amp;rdquo; 학습의 세 가지 조건 하에서 평가된다. GPT-3는 원칙적으로 미세 조정 설정에서도 평가될 수 있지만, 이는 미래의 연구로 남겨두었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.2.png"
width="1076"
height="592"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.2_hu58589bb76b1fcb23740d2bba78e6b7e6_155952_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.2_hu58589bb76b1fcb23740d2bba78e6b7e6_155952_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="436px"
>&lt;/p>
&lt;p>자연 언어 작업 설명과 문맥 내 예제 수가 늘어날수록 모델의 성능이 향상되며, 모델 크기가 커질수록 few-shot learning이 크게 향상된다. 이러한 학습 곡선은 미세 조정이나 그래디언트 업데이트 없이, 단순히 제공된 데모 수를 늘려가며 이루어진다.&lt;/p>
&lt;p>GPT-3는 NLP 작업에서 zero-shot과 one-shot 설정에서 좋은 결과를 보이며, few-shot 설정에서는 때때로 state-of-the-art 모델과 경쟁하거나 초과한다. 예컨대, GPT-3는 CoQA에서 zero-shot에서 81.5 F1, one-shot에서 84.0 F1, few-shot에서 85.0 F1을 달성하였다. 비슷하게, TriviaQA에서는 zero-shot에서 64.3%, one-shot에서 68.0%, few-shot에서 71.2%의 정확도를 보여주었다.&lt;/p>
&lt;p>GPT-3는 unscrambling words, performing arithmetic 등의 작업에서 one-shot과 few-shot 능력을 보여준다. 또한, GPT-3는 few-shot 설정에서 사람들이 인간이 만든 기사와 구별하기 어려운 합성 뉴스 기사를 생성할 수 있다.&lt;/p>
&lt;p>GPT-3의 규모에도 불구하고, ANLI와 같은 자연어 추론 작업이나 RACE, QuAC과 같은 일부 읽기 이해 데이터셋에서 few-shot 성능이 어려움을 겪는 일부 작업들을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.3.png"
width="948"
height="598"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.3_hue386bd87315498dbfeade35aee90dfda_440734_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.3_hue386bd87315498dbfeade35aee90dfda_440734_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="158"
data-flex-basis="380px"
>&lt;/p>
&lt;p>&amp;ldquo;data contamination&amp;quot;에 대한 체계적인 연구를 수행하였다. 이는 테스트 데이터셋의 콘텐츠가 웹에 존재하기 때문에, Common Crawl과 같은 데이터셋에서 모델을 학습시킬 때 발생할 수 있는 문제이다. data contamination이 대부분의 데이터셋에서 GPT-3의 성능에 미치는 영향은 적지만, 결과가 과대 평가될 수 있는 몇몇 데이터셋을 식별하였다.&lt;/p>
&lt;p>더 작은 모델들을 학습시켜 성능을 zero, one, few-shot 설정에서 GPT-3와 비교하였다. 대부분의 작업에서 모델 용량과 함께 성능이 상대적으로 부드럽게 스케일링되는 것을 보았다. 특히, 모델 용량이 커짐에 따라 zero, one, few-shot 성능 간의 차이가 더욱 커지는 것으로 보아, 큰 모델이 더 능숙한 메타 학습자일 수 있음을 시사한다.&lt;/p>
&lt;p>마지막으로, GPT-3가 보여주는 넓은 범위의 능력에 대해, 편향, 공정성, 그리고 보다 넓은 사회적 영향에 대한 우려를 논의하고, 이러한 관점에서 GPT-3의 특성에 대한 초기 분석을 시도한다.&lt;/p>
&lt;hr>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>GPT-3의 사전 학습 접근법은 기존의 방법과 유사하나, 모델 크기, 데이터셋 크기 및 다양성, 학습 기간을 확장하였다. 컨텍스트 내에서 학습하는 다양한 설정을 체계적으로 탐색하였고, 이러한 설정은 작업 특정 데이터에 얼마나 의존하는지에 따라 다르게 위치할 수 있다. 스펙트럼에서는 적어도 네 가지 주요 포인트를 식별할 수 있다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure2.1.png"
width="1022"
height="914"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure2.1_hu141a938b4157b8a30b49f5a1188b7faf_232340_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure2.1_hu141a938b4157b8a30b49f5a1188b7faf_232340_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Fine-Tuning (FT)&lt;/strong> 최근 방식은 원하는 작업에 맞는 감독 데이터셋으로 사전 학습된 모델의 가중치를 업데이트하는 것이다. 이 방법의 이점은 많은 벤치마크에서 강력한 성능을 보여준다는 것이고, 단점은 각 작업마다 새로운 대규모 데이터셋이 필요하고, 분포 외에서는 일반화가 잘 안 될 수 있으며, 학습 데이터의 임의적인 특징을 이용할 수 있다는 것이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Few-Shot (FS)&lt;/strong> 모델이 추론 시간에 작업의 몇 가지 예시를 조건으로 받지만 가중치 업데이트는 허용되지 않는 설정이다. 이 방법의 장점은 작업 특정 데이터에 대한 요구가 크게 줄어들고, 과도하게 좁은 분포를 학습하는 가능성이 줄어든다는 것이다. 단점은 이 방법의 결과가 미세 조정된 최신 모델보다 낮았다는 것이며, 작은 양의 작업 특정 데이터가 여전히 필요하다. few-shot 학습은 넓은 작업 분포를 기반으로 학습하고, 새로운 작업에 빠르게 적응하는 것을 포함한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>One-Shot (1S)&lt;/strong> 작업에 대한 자연어 설명 외에 하나의 예시만 허용된다는 점에서 few-shot과 다르다. one-shot은 일부 작업이 사람들에게 전달되는 방식과 가장 일치하기 때문에 few-shot과 zero-shot과 구별된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Zero-Shot (0S)&lt;/strong> 자연어 지시문만 모델에게 제공되며, 예시는 허용되지 않는 방식이다. 이 방법은 최대한의 편리성을 제공하지만, 가장 도전적인 설정이기도 하다. 예시 없이 작업의 형식을 이해하는 것은 어려울 수 있지만, zero-shot은 사람들이 작업을 수행하는 방식과 가장 가깝다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="model-and-architectures">Model and Architectures&lt;/h3>
&lt;p>GPT-2와 동일한 모델과 아키텍처를 사용하면서, transformer 계층에서 alternating dense 패턴과 locally banded sparse attention 패턴을 교대로 사용하는 점이 다르다. 모델 크기에 따른 ML 성능의 의존성을 연구하기 위해, 125M 개의 parameter에서 175B 개의 parameter까지 다양한 크기의 8가지 모델을 훈련시켰다. 가장 큰 모델을 GPT-3라고 부른다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table2.1.png"
width="1158"
height="326"
srcset="https://kurtkim.github.io/p/gpt-3/images/table2.1_hu7a06fdbc940bf5eee33681a10d2f9dc4_86292_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table2.1_hu7a06fdbc940bf5eee33681a10d2f9dc4_86292_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="355"
data-flex-basis="852px"
>&lt;/p>
&lt;p>각 모델은 학습 가능한 parameter 수, layer 수, bottleneck layer의 단위 수 등으로 구성되어 있다. 모든 모델은 2048 토큰의 컨텍스트 window를 사용하며, 데이터 전송을 최소화하기 위해 GPU에 모델을 깊이와 너비 차원을 따라 분할한다. 각 모델의 아키텍처 parameter는 계산 효율성과 GPU 간의 로드 밸런싱에 기반하여 선택되었다. 이전 연구에 따르면, 검증 손실은 이러한 parameter에 대해 상당히 넓은 범위에서 크게 민감하지 않다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>언어 모델 데이터셋은 Common Crawl 데이터셋으로 확장되어 1 trillion 단어를 수집하였다. 이런 크기의 데이터셋은 가장 큰 모델을 학습시키기에 충분하지만, Common Crawl의 필터링되지 않은 버전은 품질이 낮다. 그래서 데이터셋의 품질을 향상시키기 위해 세 가지 절차를 거쳤습니다: (1) 고품질 참조 말뭉치와 유사한 Common Crawl의 버전을 다운로드하고 필터링, (2) 중복 제거를 통해 데이터셋의 중복을 방지, (3) 고품질 참조 말뭉치를 학습 데이터에 추가하여 다양성을 늘렸다.&lt;/p>
&lt;p>추가 데이터셋으로는 WebText 데이터셋의 확장 버전, 두 개의 인터넷 기반 책 말뭉치(Books1과 Books2), 그리고 영어 Wikipedia가 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table2.2.png"
width="924"
height="258"
srcset="https://kurtkim.github.io/p/gpt-3/images/table2.2_hu40bee715897d3cadbc87965d4593db55_52624_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table2.2_hu40bee715897d3cadbc87965d4593db55_52624_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="358"
data-flex-basis="859px"
>&lt;/p>
&lt;p>CommonCrawl 데이터는 필터링 전 45TB, 필터링 후 570GB로, 약 4000억 바이트 쌍 인코딩 토큰에 해당한다. 학습 중에는 품질이 높은 데이터셋을 더 자주 샘플링하며, 이는 고품질 학습 데이터를 위해 약간의 과적합을 받아들인다.&lt;/p>
&lt;p>인터넷 데이터에서 사전 학습된 언어 모델은 데이터 오염이 발생할 우려가 있다. 이를 줄이기 위해 모든 벤치마크의 개발 및 테스트 세트와 겹치는 부분을 찾아 제거하려 했으나, 일부 겹치는 부분을 무시하는 버그가 있었다. 학습의 비용 문제로 인해 다시 모델을 학습하는 것은 비현실적이었다.&lt;/p>
&lt;h3 id="training-process">Training Process&lt;/h3>
&lt;p>대형 모델은 큰 배치 크기를 사용하나 작은 learning rate가 필요하다. 학습 중 gradient noise scale을 측정하여 배치 크기를 결정하였다. 메모리 부족을 방지하기 위해 모델 병렬성을 사용하였고, 모든 모델은 Microsoft의 고대역폭 클러스터에서 V100 GPU로 학습되었다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>few-shot 학습에서는 각 작업의 학습 세트에서 무작위로 $K$개의 예제를 추출하여 평가하였다. LAMBADA와 Storycloze는 지도 학습 세트가 없으므로, 개발 세트에서 추출한 예제를 사용한다. Winograd는 하나의 데이터셋만 있으므로, 그 데이터셋에서 직접 예제를 추출한다.&lt;/p>
&lt;p>$K$는 0부터 모델의 컨텍스트 창이 허용하는 2048까지의 값이 될 수 있으며, 대체로 10에서 100개의 예제를 수용한다. $K$의 더 큰 값이 일반적으로 좋지만 항상 그런 것은 아니므로, 개발 세트와 테스트 세트가 있는 경우, 개발 세트에서 $K$의 몇 가지 값을 실험하고 최적의 값을 테스트 세트에서 사용한다. 일부 작업에서는 예시 외에도 자연어 프롬프트를 사용한다.&lt;/p>
&lt;p>객관식 작업에서는 $K$개의 컨텍스트와 정확한 완성 예제를 제공하고, 각 완성의 가능성을 비교한다. 대부분 작업에서는 토큰 당 가능성을 비교하여 길이를 정규화하지만, ARC, OpenBookQA, RACE 같은 일부 데이터셋에서는 완성의 무조건적 확률 $ {P(completion | context)}\over{P(completion | answer_context)} $로 정규화하여 추가적인 이익을 얻는다. &amp;ldquo;Answer: &amp;quot; 또는 &amp;ldquo;A: &amp;ldquo;는 완성이 답이어야 함을 알리는 프롬프트로 사용된다.&lt;/p>
&lt;p>이진 분류 작업에서는 옵션에 &amp;ldquo;True&amp;quot;나 &amp;ldquo;False&amp;quot;와 같은 의미 있는 이름을 부여하고, 객관식 문제처럼 처리한다.&lt;/p>
&lt;p>자유형식 완성 작업에서는 beam width가 4이고 길이 패널티가 $\alpha = 0.6$인 beam search를 사용한다. 모델은 F1 유사도 점수, BLEU, 또는 정확한 일치를 기준으로 평가한다.&lt;/p>
&lt;p>테스트 세트가 공개적으로 사용 가능한 경우, 모델 크기와 학습 설정별로 최종 결과를 보고한다. 테스트 세트가 비공개인 경우, 개발 세트 결과를 보고한다. 제출이 가능한 데이터셋(SuperGLUE, TriviaQA, PiQa)에 대해서는 테스트 서버에 결과를 제출하고, 그 외의 경우에는 개발 세트 결과를 보고한다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.1.png"
width="816"
height="634"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.1_hu492030114f856045d4aed1d58b28bf5f_188084_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.1_hu492030114f856045d4aed1d58b28bf5f_188084_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="308px"
>&lt;/p>
&lt;p>효율적인 학습 계산을 사용하면 언어 모델링 성능이 power-law를 따르는 것을 확인하였다. 이 경향을 더 확장하면 power-law에서 약간 벗어나는 것을 볼 수 있다. cross-entropy 손실의 개선이 학습 코퍼스의 특정 세부사항을 모델링함으로써만 이루어진다는 우려도 있지만, 실제로는 다양한 자연어 작업에서 일관된 성능 향상을 가져왔다.&lt;/p>
&lt;p>8개의 모델(175B 개의 parameter를 가진 GPT-3와 7개의 작은 모델)을 다양한 데이터셋에서 평가한다. 데이터셋은 유사한 작업을 나타내는 9개의 카테고리로 그룹화한다.&lt;/p>
&lt;p>언어 모델링과 유사한 작업, &amp;lsquo;closed book&amp;rsquo; 질문 응답 작업, 언어 간 번역 능력, Winograd Schema와 유사한 작업, 상식 추론 또는 질문 응답 작업, 읽기 이해 작업, SuperGLUE 벤치마크를, 그리고 NLI를 평가한다. 마지막으로 인텍스트 학습 능력을 조사하기 위한 추가 작업을 발명하고 평가한다. 이 모든 평가는 few-shot, one-shot, zero-shot 설정에서 이루어진다.&lt;/p>
&lt;h3 id="language-modeling-cloze-and-completion-tasks">Language Modeling, Cloze, and Completion Tasks&lt;/h3>
&lt;p>GPT-3의 성능을 전통적인 언어 모델링 작업뿐만 아니라 관심 있는 단일 단어를 예측하거나, 문장이나 단락을 완성하거나, 텍스트의 가능한 완성 사이에서 선택하는 등의 관련 작업을 테스트한다.&lt;/p>
&lt;h4 id="language-modeling">Language Modeling&lt;/h4>
&lt;p>Penn Tree Bank (PTB) 데이터셋에서 GPT-3의 zero-shot perplexity를 계산하였다. PTB는 현대 인터넷 이전에 만들어진 데이터셋이므로 학습 데이터에 포함되지 않았다. 가장 큰 모델은 PTB에서 perplexity 20.50을 달성하여 state-of-the-art를 달성하였다. PTB는 전행적인 언어 모델링 데이터셋이므로 one-shot이나 few-shot 평가는 적용되지 않았다.&lt;/p>
&lt;h4 id="lambada">LAMBADA&lt;/h4>
&lt;p>LAMBADA 데이터셋은 텍스트의 장거리 의존성을 테스트한다. 최근에는 언어 모델의 크기를 늘리는 것이 더 이상 벤치마크의 성능 향상에 별 도움이 안 된다는 의견이 있었다. 그러나 GPT-3는 zero-shot 설정에서 LAMBADA에서 76%의 결과를 보여주며, 이전 최고 기록보다 8% 향상시키는 결과를 보여주었다.&lt;/p>
&lt;p>LAMBADA는 few-shot 학습의 유연성을 보여준다. 표준 언어 모델은 문장의 마지막 단어를 예측하는 것이 어렵지만, few-shot 학습은 이를 클로즈 테스트로 제시하고 언어 모델이 한 단어의 완성을 예측하도록 한다. 이전의 stop-word ﬁlter 방법보다 효과적인 해결책을 제공합니다.&lt;/p>
&lt;p>다음과 같은 빈칸 채우기 형식을 사용한다:&lt;/p>
&lt;p>$$ \text{Alice was friends with Bob. Alice went to visit her friend ____} \rightarrow \text{Bob} $$
$$ \text{George bought some baseball equipment, a ball, a glove, and a ____} \rightarrow $$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.2.png"
width="864"
height="232"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.2_hub0494855f6e89e2303b1180ef92365b1_46023_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.2_hub0494855f6e89e2303b1180ef92365b1_46023_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="893px"
>&lt;/p>
&lt;p>GPT-3는 few-shot 설정에서 86.4%의 정확도를 보여, 이전 최고 기록보다 18% 이상 증가하였다. 모델 크기가 커질수록 퓨샷 성능이 크게 향상되었다. 그러나 빈칸 채우기 방법은 one-shot에서는 zero-shot보다 성능이 떨어졌다. 이는 모든 모델이 패턴을 인식하기 위해 여러 예제가 필요하기 때문일 것으로 보인다.&lt;/p>
&lt;p>테스트 세트에서 LAMBADA 데이터셋의 일부가 학습 데이터에 포함된 것으로 확인되었지만, 성능에 불필요한 영향을 미치지는 않는 것으로 분석되었다.&lt;/p>
&lt;h4 id="hellaswag">HellaSwag&lt;/h4>
&lt;p>HellaSwag 데이터셋은 이야기나 지시사항의 최선의 결말을 선택하는 것이다. GPT-3는 one-shot에서 78.1%, few-shot에서 79.3%의 정확도를 보여주었다. 이는 1.5B parameter 언어 모델의 75.4%를 능가하지만, 다목적 모델 ALUM의 85.6%에는 미치지 못하였다.&lt;/p>
&lt;h4 id="storycloze">StoryCloze&lt;/h4>
&lt;p>GPT-3는 5문장 이야기의 결말을 선택하는 StoryCloze 2016 데이터셋에서는 zero-shot에서 83.2%, few-shot에서 87.7%의 정확도를 보여주었다. 이는 BERT 기반 모델의 최고 기록보다 4.1% 낮지만, 이전 zero-shot 결과에 비해 10% 향상된 수치이다.&lt;/p>
&lt;h3 id="closed-book-question-answering">Closed Book Question Answering&lt;/h3>
&lt;p>GPT-3가 광범위한 사실에 대한 질문에 얼마나 잘 대답하는지를 측정한다. 일반적으로 이 작업은 정보 검색 시스템과 모델을 사용해 수행되며, 이를 &amp;ldquo;open-book&amp;quot;이라고 부른다. 하지만 최근에는 &amp;ldquo;closed-book&amp;rdquo; 방식으로 큰 언어 모델이 직접 질문에 답하는 것이 효과적이라는 연구 결과가 나왔다. 이 가설을 GPT-3로 테스트하며, Natural Questions, WebQuestions, TriviaQA 세가지 데이터셋에서 평가를 진행하였다. 이 평가는 외부 콘텐츠와 Q&amp;amp;A 데이터셋에 대한 미세조정을 허용하지 않는 엄격한 closed-book 설정에서 수행된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.3.png"
width="990"
height="254"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.3_hubc1a03863b33702af63f12c643950a56_62395_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.3_hubc1a03863b33702af63f12c643950a56_62395_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>GPT-3는 TriviaQA에서 zero-shot 64.3%, one-shot 68.0%, few-shot 71.2%의 결과를 보여주었다. zero-shot 결과만으로도 미세조정된 T5-11B를 14.2%, Q&amp;amp;A 맞춤형 범위 예측을 사용한 버전을 3.8% 초과하였다. one-shot 결과는 3.7% 향상되며, 오픈 도메인 QA 시스템의 최고 기록과 동일하게 되었다. few-shot 결과는 3.2% 향상시켰다.&lt;/p>
&lt;p>WebQuestions에서 GPT-3는 zero-shot 14.4%, one-shot 25.3%, few-shot 41.5%의 결과를 보여주었다. 이는 미세조정된 T5-11B의 37.4%, 특정 사전 학습 절차를 사용하는 T5-11B+SSM의 44.7%와 비교된다. few-shot 설정에서 GPT-3의 성능은 state-of-the-art를 달성한 미세조정 모델과 근접하다. 또한, WebQs의 질문이나 답변 스타일이 GPT-3에게는 이질적인 것으로 보여지지만, few-shot 설정에서 GPT-3는 이에 적응하며 높은 성능을 회복하는 것으로 보인다.&lt;/p>
&lt;p>Natural Questions에서 GPT-3는 zero-shot 14.6%, one-shot 23.0%, few-shot 29.9%의 성과를 보여주었다. 이는 미세조정된 T5 11B+SSM의 36.6%와 비교되는 결과이다. zero-shot에서 few-shot으로 크게 향상된 성능은 분포의 변화를 보여주며, 이는 TriviaQA와 WebQS에 비해 덜 경쟁력 있는 성능을 설명할 수 있다. 특히, NQs 질문들이 Wikipedia에 대한 매우 세부적인 지식을 요구하므로, 이는 GPT-3의 용량과 사전 학습 분포의 한계를 시험할 수 있다.&lt;/p>
&lt;p>세 가지 데이터셋 중 하나에서 GPT-3의 one-shot 성능은 오픈 도메인의 최고 성능과 일치하고, 나머지 두 데이터셋에서는 미세조정을 하지 않아도 최고 성능에 근접한다. 모든 데이터셋에서, 모델 크기에 따라 성능이 부드럽게 확장되는 것을 확인하였고, 이는 모델의 용량이 직접적으로 모델의 parameter 흡수된 &amp;lsquo;knowledge&amp;rsquo;으로 변환된다는 생각을 반영할 수 있다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>GPT-2는 용량 문제로 인해 다국어 문서를 영어로만 필터링했지만, 일부 다국어 능력을 보여주었다. 프랑스어와 영어 간 번역에서도 의미 있는 성과를 보였다. GPT-3에서는 용량을 크게 향상시키고 학습 데이터셋을 확대하여 다른 언어를 더 많이 포함하였다. GPT-3의 학습 데이터는 주로 영어(93%)이지만, 다른 언어의 텍스트도 7% 포함한다. 번역 능력을 더 잘 이해하기 위해, 분석에 독일어와 루마니아어를 추가하였다.&lt;/p>
&lt;p>기존의 비지도 학습 기계 번역은 주로 단일 언어 데이터셋과 back-translation을 사용하지만, GPT-3는 여러 언어를 혼합한 학습 데이터에서 학습한다. 이는 단어, 문장, 문서 수준에서 언어들을 결합한다. GPT-3는 특정 작업을 위해 맞춤화되지 않은 단일 학습 목표를 사용한다. 그러나, one-shot / few-shot 설정은 적은 양의 쌍으로 된 예시를 사용하기 때문에 엄밀히 말해 이전의 비지도 작업과는 비교가 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.4.png"
width="962"
height="314"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.4_hu7bd1cbed43e7b566a815aa687486c811_68971_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.4_hu7bd1cbed43e7b566a815aa687486c811_68971_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="306"
data-flex-basis="735px"
>&lt;/p>
&lt;p>zero-shot GPT-3는 작업 설명만을 받지만 최근의 비지도 NMT 결과보다 성능이 떨어진다. 그러나 각 번역 작업에 대해 한 예시만 제공하면 성능이 크게 향상되며, few-shot 설정에서 더욱 향상된다. GPT-3의 성능은 언어 방향에 따라 크게 달라진다. 영어로 번역할 때는 이전의 비지도 NMT 작업을 능가하지만 반대 방향으로는 성능이 떨어진다. En-Ro의 경우 성능이 이전 비지도 NMT 작업보다 훨씬 낮다. Fr-En과 De-En에서 few-shot GPT-3는 최고의 지도 학습 결과를 능가하고, Ro-En에서는 전체 최고 성능과 비슷한 성능을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.4.png"
width="910"
height="604"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.4_hu82b1ebcae102ece625cbe1c0e223e818_183098_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.4_hu82b1ebcae102ece625cbe1c0e223e818_183098_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>모든 언어 쌍과 zero-shot, one-shot, few-shot 설정에서 모델 용량이 증가함에 따라 성능이 부드럽게 향상되는 추세가 확인되었다.&lt;/p>
&lt;h3 id="winograd-style-tasks">Winograd-Style Tasks&lt;/h3>
&lt;p>Winograd Schemas Challenge는 대명사가 가리키는 단어를 찾는 NLP 작업이다. 언어 모델은 기존 Winograd 데이터셋에서는 좋은 성능을 보였지만, 더 어려운 Winogrande 데이터셋에서는 성능이 떨어졌다. 이는 GPT-3에서도 확인되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.5.png"
width="600"
height="200"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.5_hu7b018836446e63d3c9d1ef5b542571f1_33042_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.5_hu7b018836446e63d3c9d1ef5b542571f1_33042_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="300"
data-flex-basis="720px"
>&lt;/p>
&lt;p>GPT-3는 원래의 273개의 Winograd 스키마에서 테스트되었고, zero-shot, one-shot, few-shot 설정에서 각각 88.3%, 89.7%, 88.6%의 성능을 보여주었다. 이는 모든 경우에서 state-of-the-art와 인간의 성능을 몇 포인트 밑돌게 강력한 결과를 보여준다. 학습 데이터 중 일부 Winograd 스키마에서 오염 분석이 이루어졌지만, 이것이 결과에 미치는 영향은 작았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.5.png"
width="910"
height="606"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.5_hud3e66a69a890d4d782e96b2e04d9b5b1_139301_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.5_hud3e66a69a890d4d782e96b2e04d9b5b1_139301_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>더 어려운 Winogrande 데이터셋에서 GPT-3는 zero-shot에서 70.2%, one-shot에서 73.2%, few-shot에서 77.7%의 성능을 보여주었다. 이는 미세 조정된 RoBERTA 모델의 79%, 최첨단 모델인 T5의 84.6%, 그리고 인간의 성능인 94.0%와 비교된다.&lt;/p>
&lt;h3 id="common-sense-reasoning">Common Sense Reasoning&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.6.png"
width="938"
height="198"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.6_hu6eeb6cfaedf1899bf3ff81e3c156f85b_50199_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.6_hu6eeb6cfaedf1899bf3ff81e3c156f85b_50199_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="473"
data-flex-basis="1136px"
>&lt;/p>
&lt;p>PhysicalQA (PIQA)라는 데이터셋에서 GPT-3는 zero-shot 81.0%, one-shot 80.5%, few-shot 82.8%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 79.4%에 비해 우수하며, 인간의 성능보다는 약 10% 떨어지지만, state-of-the-art의 성능을 one-shot과 few-shot에서 능가하였다. 하지만, PIQA가 데이터 오염 가능성을 가질 수 있어 결과를 보수적으로 표시하였다.&lt;/p>
&lt;p>ARC 데이터셋에서 GPT-3는 &amp;ldquo;Challenge&amp;rdquo; 버전에서 zero-shot 51.4%, one-shot 53.2%, few-shot 51.5%의 정확도를, &amp;ldquo;Easy&amp;rdquo; 버전에서는 68.8%, 71.2%, 70.1%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 성능에 근접하거나 약간 능가하였지만, UniﬁedQA의 성능에 비하면 아직도 많이 뒤떨어져 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.6.png"
width="886"
height="590"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.6_hua09445c20b3052295af69029a7e053b3_120087_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.6_hua09445c20b3052295af69029a7e053b3_120087_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>OpenBookQA에서 GPT-3는 zero-shot에서 few-shot으로 넘어갈 때 성능이 크게 향상되었지만, state-of-the-art에 비해 아직 20점 이상 뒤떨어져 있다. GPT-3의 few-shot 성능은 미세 조정된 BERT Large와 비슷하다.&lt;/p>
&lt;p>GPT-3의 in-context 학습은 상식 추론 작업에서 일관성 없는 결과를 보였지만, OpenBookQA에서는 크게 향상되었다. 또한, GPT-3는 모든 평가에서 새 PIQA 데이터셋의 state-of-the-art를 달성하였다.&lt;/p>
&lt;h3 id="reading-comprehension">Reading Comprehension&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.7.png"
width="942"
height="200"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.7_hu9e83a0e91cedfa41f8005f1bd265357f_45405_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.7_hu9e83a0e91cedfa41f8005f1bd265357f_45405_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1130px"
>&lt;/p>
&lt;p>reading comprehension 작업에서 GPT-3를 평가해 보았다. 다양한 답변 형식을 가진 5개의 데이터셋을 사용하였고, GPT-3의 성능은 데이터셋에 따라 크게 다르며, 다양한 답변 형식에 대한 능력을 보여주었다. 일반적으로 GPT-3는 각 데이터셋에 대한 초기 기준선과 비슷한 성능을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.7.png"
width="900"
height="584"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.7_hu382e3ffe17c80967f5475d70a8823a4a_125791_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.7_hu382e3ffe17c80967f5475d70a8823a4a_125791_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="369px"
>&lt;/p>
&lt;p>GPT-3는 자유 형식의 대화 데이터셋인 CoQA에서 가장 좋은 성능을 보였고, 대화 행동과 답변 선택을 요구하는 QuAC에서는 가장 나쁜 성능을 보였다. DROP 데이터셋에서는 few-shot 설정에서 BERT 기준선을 앞섰지만, 사람의 성능과 최첨단 방법에는 미치지 못하였다. SQuAD 2.0에서는 few-shot 학습 능력을 보여주며 성능을 향상시켰고, RACE에서는 상대적으로 약한 성능을 보였지만, 초기 작업과는 경쟁력을 가졌다.&lt;/p>
&lt;h3 id="superglue">SuperGLUE&lt;/h3>
&lt;p>GPT-3를 더 체계적으로 평가하고 다른 모델들과 비교하기 위해, SuperGLUE 벤치마크라는 표준화된 데이터셋에서도 평가를 진행하였다. few-shot 설정에서는 모든 작업에 대해 학습 세트에서 무작위로 추출한 32개의 예제를 사용하였다. WSC와 MultiRC를 제외한 모든 작업에서는 각 문제의 컨텍스트로 사용할 새로운 예제 집합을 샘플링하였다. WSC와 MultiRC에서는, 모든 문제의 컨텍스트로 동일한 예제 집합을 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.8.png"
width="1092"
height="384"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.8_hud3a21f9e74fc9295279ca3d9a09ce1b9_83462_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.8_hud3a21f9e74fc9295279ca3d9a09ce1b9_83462_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="682px"
>&lt;/p>
&lt;p>GPT-3는 다양한 작업에서 성능이 다르게 나타났다. COPA와 ReCoRD에서는 거의 최고 수준에 근접한 성능을 보였고, WSC에서는 80.1%의 높은 성능을 보였다. BoolQ, MultiRC, RTE에서는 합리적인 성능을 보였고, CB에서는 75.6%의 성능을 보였다.&lt;/p>
&lt;p>WiC에서 GPT-3의 few-shot 성능이 49.4%로 상대적으로 약하다는 것을 발견하였다. 두 문장을 비교하는 일부 작업에서 GPT-3는 약한 경향이 있다. 이는 RTE와 CB의 낮은 점수를 설명할 수 있다. 그러나 이런 약점에도 불구하고, GPT-3는 8개 작업 중 4개에서 미세 조정된 BERT-Large를 능가하며, 2개 작업에서는 state-of-the-art에 가깝다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.8.png"
width="1252"
height="596"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.8_hu631d470470991d10be8ea0f9f0a7f95a_188012_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.8_hu631d470470991d10be8ea0f9f0a7f95a_188012_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="210"
data-flex-basis="504px"
>&lt;/p>
&lt;p>모델 크기와 예시 수가 증가함에 따라 few-shot SuperGLUE 점수가 개선되는 것을 확인하였다. GPT-3는 각 작업당 8개 미만의 예시만으로도 미세 조정된 BERT-Large를 능가하는 전체 SuperGLUE 점수를 얻을 수 있었다.&lt;/p>
&lt;h3 id="nli">NLI&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.9.png"
width="888"
height="598"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.9_hu1aca7b926a856f62daae8569f85a6da9_143695_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.9_hu1aca7b926a856f62daae8569f85a6da9_143695_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>자연어 추론(NLI)은 두 문장 간의 관계를 이해하는 능력을 평가한다. GPT-3는 이 작업에서 랜덤(56%)보다 약간 더 잘 수행되는 반면, few-shot 설정에서는 BERT Large와 유사한 수준으로 수행한다. 적대적 자연어 추론(ANLI) 데이터셋에서는 GPT-3가 라운드 3에서 약간의 진전을 보여주었다. 이러한 결과는 NLI가 여전히 언어 모델에게 어려운 작업이며, 진전의 시작 단계에 불과하다는 것을 시사한다.&lt;/p>
&lt;h3 id="synthetic-and-qualitative-tasks">Synthetic and Qualitative Tasks&lt;/h3>
&lt;p>GPT-3의 능력을 테스트하기 위해, 간단한 계산, 새로운 패턴 인식, 비정상적인 작업에 빠르게 적응하는 등의 작업을 제공한다. 테스트에는 산술, 단어의 글자 재배열, SAT 스타일의 유사성 문제 해결, 그리고 새로운 단어 사용, 문법 수정, 뉴스 기사 생성 등이 포함된다. 이러한 합성 데이터셋은 언어 모델의 테스트 시간 행동에 대한 추가 연구를 촉진하기 위해 공개될 예정이다.&lt;/p>
&lt;h4 id="arithmetic">Arithmetic&lt;/h4>
&lt;p>GPT-3가 특정 작업에 대한 학습 없이 간단한 산술 연산을 수행하는 능력을 테스트하기 위해, 자연어로 간단한 산술 문제를 묻는 10개의 작은 테스트를 개발하였다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>2 digit addition (2D+)&lt;/strong> 모델에게는 두 정수를 더하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 &amp;ldquo;Q: What is 48 plus 76? A: 124&amp;quot;와 같은 형태로 질문된다.&lt;/li>
&lt;li>&lt;strong>2 digit subtraction (2D-)&lt;/strong> 모델에게는 두 정수를 빼라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 답변은 음수일 수 있다. 예를 들어 &amp;ldquo;Q: What is 34 minus 53? A: -19&amp;quot;와 같은 형태로 질문된다.&lt;/li>
&lt;li>&lt;strong>3 digit addition (3D+)&lt;/strong> 2자리 수 덧셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>3 digit subtraction (3D-)&lt;/strong> 2자리 수 뺄셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>4 digit addition (4D+)&lt;/strong> 3자리 수 덧셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>4 digit subtraction (4D-)&lt;/strong> 3자리 수 뺄셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>5 digit addition (5D+)&lt;/strong> 4자리 수 덧셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>5 digit subtraction (5D-)&lt;/strong> 4자리 수 뺄셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>2 digit multiplication (2Dx)&lt;/strong> 모델에게는 두 정수를 곱하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 &amp;ldquo;Q: What is 24 times 42? A: 1008&amp;quot;와 같은 형태로 질문된ㄴ다.&lt;/li>
&lt;li>&lt;strong>One-digit composite (1DC)&lt;/strong> 모델에게는 마지막 두 숫자에 괄호가 있는 세 개의 1자리 숫자에 대해 복합 연산을 수행하라는 질문이 제시된다. 예를 들어, &amp;ldquo;Q: What is 6+(4*8)? A: 38&amp;quot;입니다. 세 개의 1자리 숫자는 [0, 10) 범위에서 균일하게 선택되며, 연산은 { +, -, * } 중에서 균일하게 선택된다.&lt;/li>
&lt;/ul>
&lt;p>10개의 모든 작업에서 모델은 정확한 답변을 생성해야 한다. 각 작업에 대해 작업의 2,000개의 무작위 인스턴스 데이터셋을 생성하고, 모든 모델을 이러한 인스턴스에서 평가한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.9.png"
width="1028"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.9_hub4937564fbe4c24425801dd2b8a216e0_37913_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.9_hub4937564fbe4c24425801dd2b8a216e0_37913_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="611"
data-flex-basis="1468px"
>&lt;/p>
&lt;p>GPT-3는 few-shot 설정에서 평가되었고, 작은 숫자에 대한 덧셈과 뺄셈에서 높은 정확도를 보여주었다. 2자리 숫자에 대한 연산에서는 덧셈에서 100%, 뺄셈에서 98.9%의 정확도를 보였으며, 3자리 숫자에 대한 연산에서는 덧셈에서 80.2%, 뺄셈에서 94.2%의 정확도를 달성하였다. 숫자의 자릿수가 증가함에 따라 성능은 감소하지만, 4자리 연산에서는 25-26%, 5자리 연산에서는 9-10%의 정확도를 보여주었다. GPT-3는 또한 계산이 복잡한 2자리 곱셈에서 29.2%의 정확도를 달성하였다. 마지막으로, GPT-3는 단일 자릿수 복합 연산(예를 들어, 9*(7+5))에서 21.3%의 정확도를 보였습니다. 이는 GPT-3가 단일 연산을 넘어서 일부 견고성을 가지고 있다는 것을 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.10.png"
width="912"
height="586"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.10_hu9cb0749638ecda1cccfc10c302e9818b_203746_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.10_hu9cb0749638ecda1cccfc10c302e9818b_203746_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>작은 모델들은 이러한 모든 작업에서 성능이 좋지 않다 - 심지어 13B parameter 모델(175B parameter 전체 GPT-3 다음으로 큰 모델)조차도 2자리 덧셈과 뺄셈을 절반 정도의 시간만 해결할 수 있고, 다른 모든 연산은 10% 미만의 시간에 해결할 수 있다.&lt;/p>
&lt;p>one-shot과 zero-shot 성능은 few-shot 성능에 비해 다소 낮지만, 이는 작업에 대한 적응이 중요함을 보여준다. 그러나 one-shot 성능은 아직도 강하며, 전체 GPT-3의 zero-shot 성능은 더 작은 모델들의 few-shot 학습보다 월등히 뛰어나다.&lt;/p>
&lt;p>모델이 단순히 특정 산술 문제를 기억하는 것인지를 확인하기 위해, 테스트 세트의 3자리 산술 문제를 가져와서 &amp;ldquo;&lt;!-- raw HTML omitted --> + &lt;!-- raw HTML omitted --> =&amp;ldquo;와 &amp;ldquo;&lt;!-- raw HTML omitted --> plus &lt;!-- raw HTML omitted -->&amp;rdquo; 형태로 학습 데이터에서 찾아보았다. 2,000개의 덧셈 문제 중에서는 17개(0.8%)만 일치하였고, 2,000개의 뺄셈 문제 중에서는 2개(0.1%)만 일치하였다. 이는 올바른 답변의 일부분만이 기억되었을 수 있다는 것을 시사한다. 또한, 잘못된 답변의 검사는 모델이 &amp;ldquo;1&amp;quot;을 올리지 않는 등의 오류를 종종 범하는 것으로 나타낸다. 이는 모델이 실제로 표를 기억하는 것이 아니라 관련 계산을 수행하려고 시도하고 있다는 것을 시사한다.&lt;/p>
&lt;p>전반적으로 GPT-3는 few-shot, one-shot, 심지어 zero-shot 설정에서도 복잡한 산술에 대해 합리적인 능숙도를 보여준다.&lt;/p>
&lt;h4 id="word-scrambling-and-manipulation-tasks">Word Scrambling and Manipulation Tasks&lt;/h4>
&lt;p>GPT-3의 새로운 기호 조작 학습 능력을 테스트하기 위해, 문자를 섞거나 추가하거나 삭제하여 왜곡된 단어를 복구하는 5가지 &amp;lsquo;character manipulation&amp;rsquo; 작업을 설계하였다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cycle letters in word (CL)&lt;/strong> 모델에게 문자가 순환된 단어와 &amp;ldquo;=&amp;rdquo; 심볼이 주어지면, 원래의 단어를 생성해야 한다. 예를 들어, &amp;ldquo;lyinevitab&amp;quot;이 주어지면 &amp;ldquo;inevitably&amp;quot;를 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Anagrams of all but ﬁrst and last characters (A1)&lt;/strong> 모델에게 첫 번째와 마지막 문자를 제외한 모든 문자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;criroptuon&amp;quot;이 주어지면 &amp;ldquo;corruption&amp;quot;을 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Anagrams of all but ﬁrst and last 2 characters (A2)&lt;/strong> 모델에게 첫 두 글자와 마지막 두 글자를 제외한 모든 글자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 복구해야 한다. 예를 들어, &amp;ldquo;opoepnnt&amp;quot;가 주어지면 &amp;ldquo;opponent&amp;quot;를 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Random insertion in word (RI)&lt;/strong> 모델에게 단어의 각 글자 사이에 무작위의 구두점이나 공백 문자가 삽입된ㄴ 글자가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;s.u!c/c!e.s s i/o/n&amp;quot;이 주어지면 &amp;ldquo;succession&amp;quot;을 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Reversed words (RW)&lt;/strong> 모델에게 거꾸로 철자된 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;stcejbo&amp;quot;가 주어지면 &amp;ldquo;objects&amp;quot;를 출력해야 한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.10.png"
width="626"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.10_hu01f4d73623cef66e0014a00542e2dd02_30607_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.10_hu01f4d73623cef66e0014a00542e2dd02_30607_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="894px"
>&lt;/p>
&lt;p>각 작업에 대해 가장 빈번한 10,000개의 단어를 사용하여 10,000개의 예시를 생성하였다. few-shot 결과는 모델 크기가 커질수록 성능이 부드럽게 증가하는 경향을 보여주었다. 전체 GPT-3 모델은 RI 38.6%, A1 40.2%, A2 15.1%를 달성하였다. 그러나 어느 모델도 RW는 불가능하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.11.png"
width="886"
height="590"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.11_hua09445c20b3052295af69029a7e053b3_146982_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.11_hua09445c20b3052295af69029a7e053b3_146982_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>one-shot 설정에서는 성능이 크게 약해져서 절반 이상 떨어지고, zero-shot 설정에서는 대부분의 작업을 수행하지 못하였다. 이는 모델이 테스트 단계에서 이러한 작업을 실제로 배우는 것을 나타내며, 이러한 작업들이 사전 학습 데이터에는 거의 나타나지 않기 때문에 zero-shot으로 수행하는 것이 어렵다.&lt;/p>
&lt;p>&amp;ldquo;in-context learning curves&amp;quot;을 통해 성능을 정량적으로 측정할 수 있다. 이는 in-context 예시의 수에 따른 작업 성능을 나타낸다. 큰 모델일수록 in-context 정보를 더 효과적으로 활용할 수 있음을 알 수 있다. 이는 작업 예시와 자연 언어 작업 설명 모두를 포함한다.&lt;/p>
&lt;p>이러한 작업을 해결하려면 문자 수준의 조작이 필요하며, BPE 인코딩은 단어의 큰 부분을 조작한다. 따라서, 이 작업에 성공하려면 BPE 토큰을 조작하는 것뿐만 아니라 그들의 하위 구조를 이해하고 분해해야 한다. 또한, CL, A1, A2는 bijective가 아니므로, 모델이 올바른 암호화 해제를 찾기 위해 검색을 수행해야 한다. 이러한 기술은 복잡한 패턴 매칭과 계산이 필요하다.&lt;/p>
&lt;h4 id="sat-analogies">SAT Analogies&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.12.png"
width="910"
height="618"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.12_hufc1bf3f66494f3e19257540e127aa8f7_118792_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.12_hufc1bf3f66494f3e19257540e127aa8f7_118792_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="353px"
>&lt;/p>
&lt;p>GPT-3는 374개의 &amp;ldquo;SAT analogy&amp;rdquo; 문제를 통해 테스트되었다. 이 작업에서 GPT-3는 few-shot 65.2%, one-shot 59.1%, zero-shot 53.7%의 성능을 보여주었다. 이는 대학 지원자들의 평균 57%보다 높다. 결과는 규모에 따라 개선되며, 1750억 모델은 130억 파라미터 모델에 비해 10% 이상 개선되었다.&lt;/p>
&lt;h4 id="news-article-generation">News Article Generation&lt;/h4>
&lt;p>GPT-3는 &amp;ldquo;news articles&amp;quot;를 생성하는 능력을 테스트하였다. 그러나 GPT-3의 학습 데이터는 뉴스 기사에 비중이 덜 두어져 있어, 뉴스 기사를 생성하는 것이 덜 효과적이었다. 이를 해결하기 위해, GPT-3의 few-shot 학습 능력을 활용해 세 개의 이전 뉴스 기사를 제공하여 모델을 조건화하였다. 그 결과, 제안된 다음 기사의 제목과 부제목을 가지고, 모델은 &amp;ldquo;news&amp;rdquo; 장르의 짧은 기사를 신뢰성 있게 생성할 수 있었다.&lt;/p>
&lt;p>GPT-3가 생성한 뉴스 기사의 품질을 평가하기 위해, 사람들이 GPT-3가 생성한 기사와 실제 기사를 구별하는 능력을 측정하기로 결정하였다. 이는 생성적 언어 모델의 품질을 측정하는 중요한 방법으로, 이전에도 비슷한 연구가 있었다.&lt;/p>
&lt;p>사람들이 모델이 생성한 텍스트를 얼마나 잘 감지하는지 확인하기 위해, newser.com에서 기사 제목과 부제목 25개를 선택하고, 다양한 크기의 언어 모델로 이를 완성하였다. 이후 미국 참가자 80명에게 이 기사들을 제시하고, 각 참가자에게 기사가 사람이 작성했는지 아니면 기계가 작성했는지 판단하도록 요청하였다.&lt;/p>
&lt;p>모델의 훈련 데이터에는 없는 기사들을 선택하였고, 모든 모델은 같은 맥락을 사용하여 훈련되었다. 각 모델에게는 같은 기사 제목과 부제목이 프롬프트로 제공되었다. 추가적으로 참가자의 집중력을 통제하기 위한 실험도 진행했는데, 이는 고의적으로 품질이 낮은 기사를 생성한 &amp;ldquo;control model&amp;quot;을 사용했습니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.11.png"
width="1208"
height="372"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.11_huf0b222ef12e7e22eecb2ac5755f5cd7c_99834_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.11_huf0b222ef12e7e22eecb2ac5755f5cd7c_99834_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="324"
data-flex-basis="779px"
>&lt;/p>
&lt;p>고의적으로 품질이 낮은 기사를 모델이 만들었다고 판별하는 평균 인간 정확도는 약 86%였다. 반면, 175B parameter 모델이 생성한 기사를 감지하는 평균 인간 정확도는 우연 수준인 52%에 불과하였다. 모델 크기가 커질수록 인간이 모델이 생성한 텍스트를 감지하는 능력이 줄어드는 것으로 보인다.&lt;/p>
&lt;p>GPT-3가 생성한 합성 기사의 예시는 대부분 인간이 진짜 내용과 구별하기 어렵다. 하지만 사실적인 오류는 기사가 모델이 생성했음을 나타낼 수 있다. 왜냐하면 모델은 인간 작가와 달리 특정 사실에 대한 접근이 없기 때문이다. 또한 반복, 비연속성, 이상한 표현도 모델이 생성한 텍스트의 지표가 될 수 있다.&lt;/p>
&lt;p>인간은 더 많은 토큰을 관찰할수록 모델이 생성한 텍스트를 더 잘 감지한다. 이를 검증하기 위해, 평균 길이가 569단어인 로이터의 12개 세계 뉴스 기사에 대해 GPT-3가 평균 498단어로 생성한 기사를 사용하여 실험을 진행하였다. 약 80명의 미국 참가자를 대상으로 한 두 가지 실험을 통해 GPT-3와 통제 모델이 생성한 기사를 감지하는 인간의 능력을 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.12.png"
width="992"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.12_hu8227a4efaba093d1188ab82ee37e1d3a_36753_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.12_hu8227a4efaba093d1188ab82ee37e1d3a_36753_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="590"
data-flex-basis="1417px"
>&lt;/p>
&lt;p>통제 모델로부터 나온 고의적으로 나쁜 긴 기사를 감지하는 인간의 평균 정확도는 약 88%였다. 반면에, GPT-3가 생성한 긴 기사를 인식하는 인간의 평균 정확도는 약 52%로 거의 우연에 가까웠다. 이는 GPT-3가 약 500단어의 뉴스 기사를 생성할 때, 인간이 쓴 것과 구별하기 어렵다는 것을 의미한다.&lt;/p>
&lt;h4 id="learning-and-using-novel-words">Learning and Using Novel Words&lt;/h4>
&lt;p>새로운 단어를 배우고 활용하는 능력을 GPT-3로 테스트해 보았다. 존재하지 않는 단어 &amp;lsquo;Gigamuru&amp;rsquo; 같은 단어의 정의를 제공하고, 그 단어를 문장에서 사용하도록 요청하였다. 결과적으로, GPT-3는 새로운 단어를 문장에서 사용하는 작업에 대해 능숙함을 보였다. 심지어 &amp;ldquo;screeg&amp;quot;이라는 단어에 대해 그럴등한 변형(&amp;ldquo;screeghed&amp;rdquo;)을 생성하며, 이 단어를 약간 어색하게 사용하였지만 장난감 칼 싸움을 묘사하는 가능성을 보여주었다.&lt;/p>
&lt;h4 id="correcting-english-grammar">Correcting English Grammar&lt;/h4>
&lt;p>영어 문법 교정은 few-shot 학습에 아주 적합한 작업이다. GPT-3를 이용해 이를 테스트하였다. 이를 위해 &amp;ldquo;Poor English Input: &lt;!-- raw HTML omitted --> \ n Good English Output: &lt;!-- raw HTML omitted -->&amp;rdquo; 형식의 문장을 주고, 한 가지 인간이 생성한 교정 예를 제공한 후, 다른 5개 문장의 교정을 요청했습니다.&lt;/p>
&lt;h2 id="measuring-and-preventing-memorization-of-benchmarks">Measuring and Preventing Memorization Of Benchmarks&lt;/h2>
&lt;p>학습 데이터셋은 인터넷에서 가져왔기 때문에, 벤치마크 테스트 세트가 학습 데이터에 포함된 것일 수 있다. 이런 테스트 오염을 정확히 파악하는 것은 아직 확립된 방법이 없는 새로운 연구 분야이다. 대규모 모델 학습 시 오염을 조사하지 않는 것이 일반적이지만, 사전 학습 데이터셋의 규모가 커지고 있어 이 문제에 점점 더 주목할 필요가 있다고 생각한다.&lt;/p>
&lt;p>학습 데이터와 평가 데이터셋이 겹치는 문제는 실제로 존재한다. Common Crawl 데이터에 기반한 언어 모델을 처음 학습시킨 연구 중 하나에서는 평가 데이터셋과 겹치는 학습 문서를 감지하고 제거하였다. GPT-2와 같은 다른 연구에서도 이러한 중복을 분석하였고, 결과적으로 학습과 테스트 데이터가 겹치는 경우 모델 성능이 약간 향상되었지만, 겹치는 데이터의 비율이 작아 전체 결과에는 크게 영향을 미치지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure4.1.png"
width="1020"
height="580"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure4.1_hu04d42d25b1438872c3c992071044d67e_331505_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure4.1_hu04d42d25b1438872c3c992071044d67e_331505_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="422px"
>&lt;/p>
&lt;p>GPT-3는 데이터셋과 모델 크기가 GPT-2보다 훨씬 크며, 대량의 Common Crawl 데이터를 포함하고 있어 오염 가능성이 늘어났다. 그러나, 데이터량이 많아 GPT-3 175B는 중복 제거된 검증 세트에 대해 크게 과적합되지 않았다. 따라서, 오염은 자주 발생할 것으로 보이지만 그 효과는 예상만큼 크지 않을 것으로 보인다.&lt;/p>
&lt;p>학습 데이터와 벤치마크의 개발 및 테스트 세트 간 중복을 찾아 제거하려 하였으나, 버그로 인해 감지된 중복이 일부만 제거되었다. 모델을 재학습하는 것은 비용 문제로 불가능했다. 그래서 남은 중복이 결과에 미치는 영향을 자세히 조사하였다.&lt;/p>
&lt;p>각 벤치마크에 대해, 13-gram 중복이 있는 예시를 제거하여 오염이 없는 &amp;ldquo;clean&amp;rdquo; 버전을 만들었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure4.2.png"
width="1128"
height="518"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure4.2_hub0ecb8bfd9ab51991516fe584c9897b4_137279_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure4.2_hub0ecb8bfd9ab51991516fe584c9897b4_137279_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>클린 벤치마크에서 GPT-3를 평가한 결과, 전체 데이터셋의 점수와 비슷한 경우, 오염이 결과에 큰 영향을 미치지 않는 것으로 판단되었다. 만약 클린 벤치마크의 점수가 낮다면, 오염이 결과를 과대 평가하고 있다는 의미이다. 그러나 대부분의 경우, 성능 변화는 미미하며, 오염 수준과 성능 차이는 연관되지 않는 것으로 나타났다. 이를 바탕으로, 오염이 성능에 큰 영향을 미치지 않았다는 결론을 내렸다.&lt;/p>
&lt;p>(1) 모델이 클린 버전에서 상당히 더 나쁜 성능을 보이거나, 또는 (2) 잠재적 오염이 매우 높아 성능 차이를 측정하기 어려운 몇 가지 특정 사례를 더 자세히 살펴보았다.&lt;/p>
&lt;p>6개의 벤치마크 그룹(Word Scrambling, Reading Comprehension, PIQA, Winograd, language modeling tasks, German to English translation)이 추가 조사를 위해 지정되었다. 이 중복 분석은 매우 보수적으로 설계되었으므로 일부 잘못된 긍정 결과가 있을 것으로 예상한다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Reading Comprehension:&lt;/strong> QuAC, SQuAD2, DROP의 작업 예시 중 90% 이상이 잠재적 오염으로 판단되었지만, 수동 검사 결과 원본 텍스트는 학습 데이터에 있지만 질문/답변 쌍은 없었다. 즉, 모델은 배경 정보만 얻을 수 있고 특정 질문에 대한 답을 기억할 수 없었다.&lt;/li>
&lt;li>&lt;strong>German translation:&lt;/strong> WMT16 독일어-영어 테스트 세트의 25%가 잠재적 오염으로 표시되었지만, 검사 결과, 학습 데이터와 유사한 쌍의 문장이 포함된 사례는 없었다. 대부분은 뉴스에서 논의된 이벤트의 일부를 포함하는 단일 언어 일치였다.&lt;/li>
&lt;li>&lt;strong>Reversed Words and Anagrams:&lt;/strong> &amp;ldquo;alaok = koala&amp;rdquo; 형태의 작업에서, 중복이 ﬂagged되었지만, 이는 대부분 회문이나 간단한 재정렬 예시였다. 중복된 부분은 적지만, 단순한 작업을 제거하면 난이도가 증가하고 잘못된 신호가 발생한다. 심볼 삽입 작업은 높은 중복을 보이지만 성능에는 영향을 미치지 않았다. 이는 작업이 비문자 문자 제거에 중점을 두고 있고, 중복 분석이 이러한 문자를 무시하기 때문이다.&lt;/li>
&lt;li>&lt;strong>PIQA:&lt;/strong> 예시의 29%가 오염되었다고 표시되었고, 클린 부분 집합에서 성능이 3% 감소했다. 테스트 데이터셋은 학습 세트 이후에 출시되었지만, 일부 웹 페이지는 학습 세트에 포함되어 있었다. 메모리 용량이 훨씬 적은 작은 모델에서도 비슷한 감소를 보아, 이는 통계적 편향일 가능성이 높다. 하지만 이 가설을 엄밀하게 증명할 수는 없으므로, PIQA 결과에는 별표를 표시하였다.&lt;/li>
&lt;li>&lt;strong>Winograd:&lt;/strong> 예시의 45%가 중복으로 표시되었고, 클린 부분집합에서 성능이 2.6% 감소했다. 중복 데이터를 검사한 결과, 학습 세트에 132개의 Winograd 스키마가 다른 형식으로 포함되어 있었다. 성능 감소가 작지만, Winograd 결과에 별표를 표시했다.&lt;/li>
&lt;li>&lt;strong>Language modeling:&lt;/strong> GPT-2에서 측정된 4개의 Wikipedia 언어 모델링 벤치마크와 Children’s Book Test 데이터셋이 대부분 학습 데이터에 포함되어 있었다. 클린한 부분 집합을 신뢰성 있게 추출할 수 없어 이 데이터셋들의 결과는 보고하지 않았다. Penn Tree Bank는 그 연령 때문에 영향을 받지 않아, 주요 언어 모델링 벤치마크로 사용하였다.&lt;/li>
&lt;/ul>
&lt;p>오염이 높지만 성능에 미치는 영향이 거의 없는 데이터셋을 검사해 실제 오염 정도를 확인하였다. 이들은 대부분 실제 오염이 없거나, 작업의 답을 알려주는 오염이 없었다. 하지만 LAMBADA는 심각한 오염이 있음에도 성능에 미치는 영향이 매우 작았다. 빈칸 채우기 형식은 가장 단순한 형태의 기억을 배제하지만, 이 논문에서 LAMBADA에서 큰 향상을 보였으므로, 결과 섹션에서 잠재적 오염을 언급했다.&lt;/p>
&lt;p>오염 분석의 한계는 클린 부분 집합이 원래 데이터셋과 같은 분포에서 추출되었는지 확신할 수 없다는 점이다. 기억이 결과를 과대평가하면서 클린 부분 집합이 더 쉽게 되게 하는 통계적 편향이 정확히 상쇄되는 가능성이 있다. 그러나 0에 가까운 이동이 많아 이는 불가능할 가능성이 크고, 기억이 적은 작은 모델들에서도 눈에 띄는 차이를 찾지 못하였다.&lt;/p>
&lt;p>데이터 오염의 영향을 측정하고 기록하기 위해 최선을 다했고, 심각성에 따라 문제 결과를 주목하거나 완전히 제거하였다. 벤치마크 설계와 모델 학습에서 이 중요하고 미묘한 문제를 해결하기 위한 많은 작업이 아직 남아 있다.&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>GPT-3에 대한 분석에는 여러 가지 한계점이 있다. 이에 대해 설명하고 미래의 연구 방향을 제안한다.&lt;/p>
&lt;p>GPT-3와 그 분석에는 한계가 있다. GPT-3는 텍스트 합성과 여러 NLP 작업에서 향상되었지만, 문서 수준에서 의미적으로 반복되거나, 긴 문장에서 일관성을 잃는 등의 문제가 있다. 또한 &amp;ldquo;common sense physics&amp;quot;과 같은 분야에서 어려움을 겪는 것으로 보였다. GPT-3의 문맥 중심 학습 성능은 &amp;ldquo;comparison&amp;rdquo; 작업이나 읽기 이해 작업 등에서 뚜렷한 차이를 보였다. 이는 GPT-3이 다른 많은 작업에서 강력한 성능을 보이고 있음에도 불구하고 눈에 띈다.&lt;/p>
&lt;p>GPT-3의 한계는 그 구조와 알고리즘에 기인한다. 이는 양방향 아키텍처나 노이즈 제거와 같은 훈련 목표를 포함하지 않는 실험 설계 때문이다. 이로 인해 GPT-3는 빈칸 채우기 작업, 두 내용을 비교하는 작업, 긴 구절을 신중히 고려한 후 짧은 답변을 생성하는 작업 등에서 성능이 떨어졌다. 이런 원인으로 GPT-3는 WIC, ANLI, QuAC 및 RACE와 같은 몇몇 작업에서 뒤떨어지는 성능을 보였다. 큰 규모의 양방향 모델을 만들거나, 양방향 모델을 몇 번이나 한 번도 시도하지 않는 학습과 함께 작동하게 하는 것은 미래의 연구 방향으로 유망히다.&lt;/p>
&lt;p>이 논문에서 설명하는 방법론의 근본적인 제한은 모든 토큰을 동등하게 취급하고 중요한 예측과 그렇지 않은 예측을 구별하지 못한다는 점이다. 또한, 언어 시스템은 단순히 예측을 만드는 것이 아니라 목표 지향적인 행동을 취해야 하며, 대규모 언어 모델은 다른 경험 영역에 기반을 두지 않아 세계에 대한 많은 맥락을 부족하게 한다. 이러한 이유로, self-supervised 예측의 확장은 한계에 도달하고, 다른 접근법으로 보완해야 한다. 이를 위해 인간으로부터 목표 함수를 학습하거나, 강화 학습으로 미세 조정하거나, 이미지 등의 추가적인 모달리티를 추가하는 방향이 유망해 보인다.&lt;/p>
&lt;p>언어 모델의 주요 제한 중 하나는 사전 학습 단계에서의 샘플 효율성이 낮다는 것이다. GPT-3는 테스트 시간에 인간과 가까운 샘플 효율성을 보이지만, 사전 학습 과정에서 인간이 평생 동안 접하는 텍스트보다 훨씬 많은 텍스트를 본다는 문제가 있다. 사전 학습의 샘플 효율성을 개선하는 것은 미래의 연구 방향으로, 물리적 세계에 기반을 두는 것이나 알고리즘의 개선을 통해 이루어질 수 있다.&lt;/p>
&lt;p>GPT-3의 few-shot 학습에서의 한 제한은, 실제로 추론 시에 새로운 작업을 &amp;ldquo;처음부터(from scratch)&amp;rdquo; 학습하는지, 아니면 학습 도중 배운 작업을 단순히 인식하고 식별하는지에 대한 불확실성이다. 이는 테스트 시간에 정확히 동일한 분포에서 작업을 인식하거나, 같은 작업을 다른 형식으로 인식하거나, QA 같은 일반적인 작업 스타일에 적응하거나, 완전히 새로운 기술을 배우는 것 등, 넓은 범위에 걸쳐 있다. 어떤 작업에서는 새롭게 배우는 경향이 있고, 다른 작업에서는 사전 학습 동안에 배워야 하는 상황도 있다. 결국 인간이 무엇을 처음부터 배우는지, 무엇을 이전의 경험으로부터 배우는지조차 확실하지 않다. 이러한 이해의 불확실성은 few-shot 학습의 원리를 정확히 파악하는 데 중요한 미래의 연구 방향을 제시한다.&lt;/p>
&lt;p>GPT-3와 같은 대규모 모델의 한계는, 추론을 수행하는데 비용이 많이 들고 불편하다는 점이다. 이는 이러한 크기의 모델의 실질적인 적용을 어렵게 만든다. 이 문제를 해결할 수 있는 한 가지 방법은, 대규모 모델을 특정 작업에 맞게 관리 가능한 크기로 축소하는 것이다. 이는 아직 수백억 개의 매개변수 규모에서 시도되지 않았지만, 새로운 도전과 기회를 제공할 수 있다.&lt;/p>
&lt;p>GPT-3는 대부분의 딥러닝 시스템과 마찬가지로 결정의 해석이 어렵고, 새로운 입력에 대한 예측이 반드시 잘 조정되지 않으며, 학습 데이터의 편향을 유지하는 등의 한계를 가지고 있다. 특히, 학습 데이터의 편향이 모델이 편견 있는 내용을 생성하도록 이끌 수 있는 문제는 사회적 관점에서 큰 우려사항이다.&lt;/p>
&lt;hr>
&lt;h2 id="broader-impacts">Broader Impacts&lt;/h2>
&lt;p>언어 모델은 자동 완성, 문법 도움 등의 다양한 이점을 제공하지만, 잠재적으로 해로운 응용 분야도 있다. GPT-3는 텍스트 생성의 품질을 향상시키고, 합성 텍스트와 인간이 쓴 텍스트를 구별하는 어려움을 증가시키므로, 언어 모델의 좋은 사용과 나쁜 사용을 모두 발전시킬 수 있다.&lt;/p>
&lt;p>해로움을 연구하고 완화하기 위한 노력을 자극하기 위해서 향상된 언어 모델의 잠재적인 해로움에 초점을 맞추면, 주요 문제는 GPT-3와 같은 언어 모델의 고의적인 오용 가능성과 모델 내의 편향, 공정성, 표현 문제입니다.&lt;/p>
&lt;h3 id="misuse-of-language-models">Misuse of Language Models&lt;/h3>
&lt;p>언어 모델의 악의적인 사용은 모델을 원래 의도와 다른 환경이나 목적으로 재사용하는 경우가 많아 예상하기 어렵다. 이를 위해 위협과 잠재적 영향을 식별하고, 위험성을 평가하는 보안 위험 평가 프레임워크를 사용한다.&lt;/p>
&lt;h4 id="potential-misuse-applications">Potential Misuse Applications&lt;/h4>
&lt;p>텍스트 생성에 의존하는 모든 사회적인 해로운 활동은 강력한 언어 모델로 인해 강화될 수 있다. 오해, 스팸, 피싱, 법적 남용, 부정한 학술 작성 등이 예시이다. 고품질의 텍스트 생성을 할 수 있는 언어 모델은 이런 활동의 장벽을 낮추고 효과를 높일 수 있다.&lt;/p>
&lt;p>텍스트 합성의 품질이 향상됨에 따라 언어 모델의 오용 가능성이 증가한다. GPT-3가 사람이 쓴 것과 구별하기 어려운 텍스트를 생성하는 능력은 이에 대한 우려를 높인다.&lt;/p>
&lt;h4 id="threat-actor-analysis">Threat Actor Analysis&lt;/h4>
&lt;p>위협 행위자는 기술과 자원 수준에 따라 분류된다. 이는 악의적 제품을 만들 수 있는 낮은 기술력을 가진 행위자부터 장기적인 목표를 가진 국가 후원의 고도로 기술화된 그룹까지 다양하다.&lt;/p>
&lt;p>오해 전략, 악성 소프트웨어 배포, 컴퓨터 사기 등이 논의되는 포럼을 모니터링하여 저수준 및 중간 수준의 행위자들이 언어 모델에 대해 어떻게 생각하는지 파악하고 있다. 2019년 GPT-2의 처음 출시 이후 오용에 대한 논의가 있었지만, 그 이후로는 실험적인 사례가 줄었고, 성공적인 배포는 없었다. 이러한 오용 논의는 언어 모델 기술의 미디어 보도와 관련이 있었다. 이러한 행위자들로부터의 즉각적인 오용 위협은 없지만, 신뢰성이 크게 향상되면 상황이 바뀔 수 있다.&lt;/p>
&lt;p>APT들은 보통 공개적으로 작전을 논의하지 않기 때문에, 전문 위협 분석가들과 상의하였다. GPT-2 출시 이후, 언어 모델을 사용하여 이익을 볼 수 있는 작전에서 눈에 띄는 변화는 없었다. 현재의 언어 모델이 텍스트 생성에 있어 훨씬 뛰어나다는 설득력 있는 증거가 없으며, 모델의 내용을 &amp;ldquo;targeting&amp;quot;하거나 &amp;ldquo;controlling&amp;quot;하는 방법이 초기 단계에 있기 때문에, 언어 모델에 많은 자원을 투자하는 것은 가치가 없다는 평가를 받았다.&lt;/p>
&lt;h4 id="external-incentive-structures">External Incentive Structures&lt;/h4>
&lt;p>각각의 위협 행위자 그룹은 그들의 목표를 달성하기 위해 전략, 기술, 절차(TTPs)를 사용한다. 이는 확장성과 배포의 용이성 등 경제적 요인에 의해 영향을 받는다. 피싱은 낮은 비용, 적은 노력, 높은 수익률로 악성 소프트웨어를 배포하고 로그인 정보를 훔칠 수 있기 때문에 모든 그룹에서 매우 인기가 있다. 언어 모델을 사용하여 기존의 TTPs를 보완하면 배포 비용이 더욱 줄어들 것으로 예상된다.&lt;/p>
&lt;p>사용의 용이성은 TTPs 채택에 큰 영향을 미친다. 언어 모델의 출력은 확률적이고, 인간의 피드백 없이는 일관된 성능을 내기 어렵다. 만약 사회적 미디어의 허위 정보 봇이 대부분의 시간 동안 신뢰할 수 있는 출력을 생성하지만 가끔 비일관적인 출력을 생성한다면, 이 봇을 운영하는 데 필요한 인간의 노동량을 줄일 수 있다. 그러나 출력을 필터링하기 위해 인간이 여전히 필요하므로, 작업의 확장성은 제한된다.&lt;/p>
&lt;h3 id="fairness-bias-and-representation">Fairness, Bias, and Representation&lt;/h3>
&lt;p>학습 데이터의 편향으로 인해 모델은 편견이나 스테레오타입을 생성할 수 있다. 이는 기존 스테레오타입을 강화하고, 불온한 묘사를 생성하는 등의 방식으로 특정 그룹에 해를 끼칠 수 있다. 그래서 우리는 GPT-3의 공정성, 편향, 대표성에 대한 한계를 이해하기 위해 편향 분석을 수행하였다.&lt;/p>
&lt;p>목표는 GPT-3의 완전한 특성화가 아니라, 그 한계와 행동에 대한 초기 분석을 제공하는 것이다. 성별, 인종, 종교 등의 편향에 초점을 맞추고 있지만, 다른 카테고리의 편향도 존재하며 이는 후속 연구에서 다루어질 수 있다. 이는 초기 분석이며 모델의 모든 편향을 반영하지는 않는다.&lt;/p>
&lt;p>이 논문의 분석은 인터넷에서 학습된 모델이 인터넷 규모의 편향을 가지고 있다는 것을 나타낸다. 모델은 학습 데이터의 스테레오타입을 반영하는 경향이 있다. 성별, 인종, 종교 등의 편향을 찾아내기 위해 175B parameter 모델과 작은 모델을 분석하였다.&lt;/p>
&lt;h4 id="gender">Gender&lt;/h4>
&lt;p>GPT-3에서 성별 편향 조사는 성별과 직업 사이의 연관성에 집중했다. 대부분의 직업은 남성 식별자가 뒤따르는 확률이 더 높았으며, 이는 높은 학력을 요구하는 직업이나 신체적 노동을 요구하는 직업에서 특히 두드러졌다. 반면, 여성 식별자가 뒤따르는 확률이 더 높은 직업은 산모 도우미, 간호사, 리셉션니스트, 가정부 등이었다.&lt;/p>
&lt;p>&amp;ldquo;&amp;ldquo;The competent { occupation } was a&amp;quot;이나 &amp;ldquo;The incompetent { occupation } was a&amp;quot;이라는 맥락으로 바뀌었을 때, 대부분의 직업은 여전히 남성 식별자를 더 높은 확률로 따르는 경향이 있었다. 이는 원래의 중립적 프롬프트와 비교했을 때도 마찬가지였다. 평균 직업 편향은 중립 변형에서 -1.11, 유능한 변형에서 -2.14, 무능한 변형에서 -1.15로 측정되었다.&lt;/p>
&lt;p>Winogender 데이터셋에서 대명사 해결을 수행하여 대부분의 직업을 남성과 연관짓는 모델의 경향성을 입증하였다. 예를 들어, &amp;ldquo;The advisor met with the advisee because she wanted to get advice about job applications. ‘She’ refers to the&amp;quot;와 같은 맥락을 제공하고, &amp;lsquo;advisor&amp;rsquo;와 &amp;lsquo;advisee&amp;rsquo; 중 어느 쪽을 &amp;lsquo;she&amp;rsquo;로 가장 적합하게 할당하는지를 측정하였다.&lt;/p>
&lt;p>언어 모델이 사회적 편향, 예를 들어 여성 대명사를 참여자 위치와 더 많이 연관짓는 경향 등을 학습했음을 발견하였다. GPT-3 175B 모델은 이 작업에서 가장 높은 정확도(64.17%)를 보였고, 이는 편향 문제가 언어 모델을 오류에 취약하게 만들 수 있는 곳에서, 큰 모델이 작은 모델보다 더 강건하다는 초기적인 증거를 제공한다.&lt;/p>
&lt;p>단어들이 어떤 단어와 같이 나타나는지 분석하는 공존 테스트를 수행하였다. 이를 위해 데이터셋의 모든 프롬프트에 대해 여러번의 출력을 생성하여 샘플 세트를 만들었다. 성별에 대한 분석에서 여성은 &amp;ldquo;beautiful&amp;quot;과 &amp;ldquo;gorgeous&amp;quot;와 같은 외모 지향적인 단어로 더 자주 묘사되었으며, 반면에 남성은 더 다양한 형용사로 묘사되었다는 것을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table6.1.png"
width="1268"
height="474"
srcset="https://kurtkim.github.io/p/gpt-3/images/table6.1_huf876a6fd221b608902ba5ef0f70ab308_102346_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table6.1_huf876a6fd221b608902ba5ef0f70ab308_102346_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="267"
data-flex-basis="642px"
>&lt;/p>
&lt;p>모델에서 가장 선호하는 상위 10개의 형용사와 이들이 대명사 지시어와 얼마나 자주 함께 나타나는지를 보여준다. &amp;ldquo;Most Favored&amp;quot;은 한 카테고리와 비교해 다른 카테고리와 더 자주 함께 나타나는 단어를 의미한다. 이를 이해하기 쉽게 하기 위해, 각 성별에 대한 모든 단어들의 공존 횟수의 평균도 제시되었다.&lt;/p>
&lt;h4 id="race">Race&lt;/h4>
&lt;p>GPT-3의 인종 편향을 조사하기 위해, 특정 인종을 나타내는 용어로 대체된 특정 프롬프트를 사용하여 샘플을 생성하고, 생성된 샘플에서 단어의 공존을 측정하였다. 이전 연구가 언어 모델이 다른 특징에 따라 다른 감정의 텍스트를 생성한다는 것을 보여준 것을 바탕으로, 인종이 어떻게 감정에 영향을 미치는지를 조사했다. 각 인종과 과도하게 공존하는 단어의 감정은 Senti WordNet을 사용하여 측정하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure6.1.png"
width="688"
height="498"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure6.1_hu84badc25f627ac23d8237755c126dbaa_103946_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure6.1_hu84badc25f627ac23d8237755c126dbaa_103946_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>모델 전반에 걸쳐 &amp;ldquo;아시아인&amp;quot;은 일관되게 높은 감정을 가지고 있었으며, 7개의 모델 중 3개에서 1위를 차지하였다. 반면에 &amp;ldquo;흑인&amp;quot;은 일관되게 낮은 감정을 가지고 있었으며, 7개의 모델 중 5개에서 최하위를 차지하였다. 이런 차이는 큰 모델 크기에서 약간 줄어들었다. 이 분석은 다양한 모델의 편향성을 보여주며, 감정, 엔티티, 입력 데이터 간의 관계에 대한 보다 세밀한 분석의 필요성을 강조한다.&lt;/p>
&lt;h4 id="religion">Religion&lt;/h4>
&lt;p>무신론, 불교, 기독교, 힌두교, 이슬람교, 유대교와 같은 종교 용어와 함께 나타나는 단어를 연구하기 위해, 각각의 종교에 대한 특정 프롬프트에 대해 800개의 모델 출력을 생성했다. 이 프롬프트는 &amp;ldquo;{ Religion practitioners } are&amp;rdquo; (Eg. &amp;ldquo;Christians are&amp;rdquo;)과 같은 형태였다. 그런 다음 모델이 자연스럽게 문장을 완성하도록 하여, 단어의 공존을 연구하는 말뭉치를 생성하였다.&lt;/p>
&lt;p>모델이 종교 용어와의 연관성을 나타내는 방식은 종종 이 용어들이 실제 세상에서 어떻게 표현되는지를 반영한다는 것을 확인하였다. 예를 들어, 이슬람교와 관련하여 &amp;lsquo;라마단&amp;rsquo;, &amp;lsquo;예언자&amp;rsquo;, &amp;lsquo;모스크&amp;rsquo;와 같은 단어들이 다른 종교보다 더 자주 등장하며, &amp;lsquo;폭력적인&amp;rsquo;, &amp;lsquo;테러리즘&amp;rsquo;, &amp;lsquo;테러리스트&amp;rsquo; 등의 단어는 이슬람교와 더 크게 연관되어 GPT-3에서 이슬람교에 대한 상위 40개 선호 단어에 포함되었다.&lt;/p>
&lt;h4 id="future-bias-and-fairness-challenges">Future Bias and Fairness Challenges&lt;/h4>
&lt;p>이 초기 분석을 통해 발견된 편향을 공유하고, 대규모 생성 모델의 편향을 파악하는 본질적인 어려움을 강조하며, 추가 연구를 촉진하고자 한다. 이는 지속적인 연구 영역이 될 것으로 기대하며, 성별, 인종, 종교를 연구의 시작점으로 설정했음을 밝혔다. 이런 선택에는 주관성이 내재해 있는 것을 인지하고 있다.&lt;/p>
&lt;p>언어 시스템의 편향을 파악하는 것뿐만 아니라 개입하는 것이 중요하며, 이를 위해선 편향 완화에 대한 공통 어휘 구축이 필요하다. 더 많은 연구가 필요하며, 이는 NLP 외부의 문헌과의 연계, 해를 끼치는 규범적 진술의 명확한 표현, 그리고 NLP 시스템에 영향을 받는 커뮤니티의 실제 경험에 대한 관여를 포함해야 한다. 편향 완화 작업은 단순히 &amp;ldquo;편향 제거&amp;quot;를 목표로 하는 것이 아니라, 전체적인 방식으로 접근해야 한다.&lt;/p>
&lt;h3 id="energy-usage">Energy Usage&lt;/h3>
&lt;p>대규모 사전 학습은 에너지 집약적이며, GPT-3 175B 훈련은 1.5B 파라미터의 GPT-2 모델에 비해 많은 계산을 소비했다. 따라서 이러한 모델의 비용과 효율성을 인식하는 것이 중요하다.&lt;/p>
&lt;p>대규모 사전 학습의 사용은 모델의 효율성을 다루는 새로운 관점을 제공한다. 이는 학습에 필요한 자원뿐만 아니라, 모델의 수명 동안 이러한 자원이 어떻게 분산되는지를 고려해야 한다. 학습 중에는 많은 자원을 소비하지만, 학습이 완료된 모델은 효율적이다. 또한, 모델 증류와 같은 기법을 사용하면 이러한 모델의 비용을 더욱 줄일 수 있으며, 알고리즘의 발전은 시간이 지남에 따라 이러한 모델의 효율성을 자연스럽게 더욱 증가시킬 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>언어 모델의 성능을 향상시키기 위한 연구는 크게 세 가지 방향으로 진행되었다. 첫 번째는 parameter와 계산량을 함께 증가시키는 것으로, 이는 모델 크기를 계속해서 증가시키는 방식이다. 두 번째는 parameter 수는 늘리지만 계산량은 늘리지 않는 방식으로, 이는 모델의 정보 저장 용량을 늘리는 데 초점을 맞추었다. 세 번째는 parameter를 증가시키지 않고 계산을 증가시키는 방식이다. GPT-3 연구는 첫 번째 접근 방식에 초점을 맞추고, 이전 모델보다 10배 큰 모델을 개발하였다.&lt;/p>
&lt;p>언어 모델 성능에 대한 규모의 영향을 체계적으로 연구한 여러 연구에서는 모델이 확장됨에 따라 손실률에서 부드러운 멱법칙 추세를 발견하였다. 이 추세는 모델이 계속 확장됨에 따라 대체로 계속될 것으로 보이며, 다양한 downstream task 에서도 규모가 커짐에 따라 부드러운 성능 향상이 관찰되었다.&lt;/p>
&lt;p>반대 방향의 연구는 가능한 한 작은 언어 모델에서도 강한 성능을 유지하려는 시도이다. 이에는 ALBERT와 언어 모델의 축소에 대한 일반적이고 특정한 접근법이 포함되어 있다. 이런 기술은 GPT-3 연구와 보완적일 수 있으며, 큰 모델의 처리 시간과 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;p>미세조정된 언어 모델이 많은 벤치마크 작업에서 인간 수준의 성능에 근접하면서, 더 어려운 혹은 개방형 작업을 구성하는 데 많은 노력이 기울여져 왔다. 이런 작업들에는 질문 응답, 읽기 이해, 그리고 기존 언어 모델에게 어려운 데이터셋을 고의로 만드는 것이 포함된다.&lt;/p>
&lt;p>많은 연구들이 질문-응답에 집중했으며, 이는 테스트한 작업들 중 상당 부분을 차지한다. 최근의 연구로는 11B 개의 매개변수를 가진 언어 모델을 미세조정한 연구와, 테스트 시점에 대량의 데이터에 집중하는 연구가 있다. GPT-3는 문맥 내 학습에 중점을 두는 것이 특징이며, 이는 미래에 다른 연구와 결합될 수 있다.&lt;/p>
&lt;p>언어 모델의 메타러닝은 이전 연구에서 활용되었지만, 그 결과는 제한적이었다. 언어 모델 메타러닝은 내부 루프와 외부 루프의 구조를 가지고 있으며, 이는 일반적인 머신러닝에 적용된 메타러닝과 유사하다. GPT-3는 모델의 문맥을 이전 예제로 채우는 것으로, 이는 모델의 활성화를 통해 적응하는 내부 루프와 가중치를 업데이트하는 외부 루프를 가지고 있다. 또한, few-shot auto-regressive density estimation과 low-resource NMT를 few-shot 학습 문제로 연구한 사례도 있다.&lt;/p>
&lt;p>이전 연구들도 사전 학습된 언어 모델과 경사 하강법을 결합하여 few-shot 학습을 할 방법을 탐색했다. 또한, 레이블이 거의 없는 상황에서의 미세조정 방법을 연구하는 반지도 학습과 같은 비슷한 목표를 가진 분야도 있다.&lt;/p>
&lt;p>자연 언어로 다중 과제 모델에 지시를 하는 방법은 처음으로 지도 학습 환경에서 공식화되었고, 일부 과제에 언어 모델에 사용되었다. 이와 유사한 개념이 text-to-text transformer에서도 탐색되었지만, 이 경우에는 문맥 학습이 아닌 다중 과제 미세조정에 적용되었다.&lt;/p>
&lt;p>다중 과제 학습은 언어 모델의 일반성과 전이 학습 능력을 향상시키는 방법으로, 여러 과제를 함께 미세조정하며 가중치를 업데이트한다. 이 방법은 단일 모델을 가중치 업데이트 없이 다양한 과제에 사용하거나, 새로운 과제에 대한 가중치 업데이트 시 샘플 효율성을 향상시킬 수 있다. 다중 과제 학습은 초기에는 좋은 결과를 보였지만, 데이터셋 구성과 훈련 커리큘럼 설정에 대한 수작업이 필요한 한계가 있다. 하지만 대규모 사전 학습은 텍스트 예측을 통해 암시적으로 다양한 과제를 포함하는 방법을 제공한다. 미래의 연구 방향은 다중 과제 학습에 대해 더 넓은 범위의 명시적 과제를 생성하는 것일 수 있다.&lt;/p>
&lt;p>지난 두 해 동안 언어 모델의 알고리즘은 매우 크게 발전했다. denoising-based bidirectionality, preﬁxLM and encoder-decoder architectures, random permutations during training, architectures that improve the efﬁciency of sampling, improvements in data and training procedures, and efﬁciency increases in the embedding parameters 등이 포함된다. 이런 기술들은 downstream task에서 큰 이익을 가져다주며, 이러한 알고리즘 발전을 GPT-3에 통합하면 downstream task 성능이 향상될 가능성이 높다. GPT-3의 규모와 이런 알고리즘 기법을 결합하는 것은 미래 연구의 유망한 방향이다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>175B 개의 parameter를 가진 언어 모델을 소개하였고, 이 모델은 다양한 NLP 작업에서 강력한 성능을 발휘하며, 또한 고품질의 샘플을 생성하며, 미세 조정 없이도 성능의 확장성이 대략 예측 가능하다는 것을 보여주었다. 그리고 이 모델의 사회적 영향에 대해서도 논의하였다. 이러한 결과들은 큰 언어 모델이 적응형, 일반 언어 시스템의 개발에 중요한 요소가 될 수 있음을 시사한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/gpt-3" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Scaling Law</title><link>https://kurtkim.github.io/p/scaling-law/</link><pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/scaling-law/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델 성능에 대한 연구에서, 모델 크기, 데이터셋 크기, 학습에 사용된 컴퓨팅 양이 교차 엔트로피 손실을 멱법칙으로 스케일링한다는 것을 발견하였다. 네트워크의 폭이나 깊이 같은 다른 세부 사항은 큰 영향을 미치지 않는다. 큰 모델은 표본 효율이 뛰어나며, 최적의 컴퓨팅 효율은 상대적으로 적은 데이터에 큰 모델을 학습시키는 것을 포함한다. 이 모든 관계를 통해, 고정된 컴퓨팅 예산의 최적 할당을 결정할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어는 인공지능 연구에 중요한 분야로, 대부분의 추론 작업을 효과적으로 수행할 수 있다. 세계의 텍스트는 비지도 학습을 위한 풍부한 데이터를 제공하며, 최근 딥러닝은 언어 모델링에 있어 빠른 발전을 보이고 있다. state-of-the-art 모델들은 많은 특정 작업에서 인간 수준의 성능에 근접하고 있으며, 이는 일관된 멀티패러그래프 작성에도 해당된다.&lt;/p>
&lt;p>언어 모델링 성능은 모델 구조, 모델 크기, 학습에 사용된 컴퓨팅 파워, 학습 데이터의 양 등 여러 요소에 의존하며, 이 연구에서는 이러한 요소들이 언어 모델링 손실에 어떻게 영향을 미치는지를 transformer 구조를 중심으로 실증적으로 조사한다. 언어 작업의 성능 범위가 넓어서, 규모에 따른 추세를 좀 더 광범위하게 연구할 수 있다.&lt;/p>
&lt;p>교육 시간, 문맥 길이, 데이터셋 크기, 모델 크기 등의 여러 요인에 따른 성능 변화를 관찰할 예정이다.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Transformer 언어 모델에 대한 주요 발견은 다음과 같다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure1.png"
width="1192"
height="374"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure1_hu424f1e66fa0f691b27459c26f82b23e8_111743_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure1_hu424f1e66fa0f691b27459c26f82b23e8_111743_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="764px"
>&lt;/p>
&lt;p>&lt;strong>Performance depends strongly on scale, weakly on model shape:&lt;/strong> 모델의 성능은 주로 모델 매개변수의 수, 데이터셋 크기, 그리고 사용된 컴퓨팅의 양에 의존하며, 다른 구조적 요소들은 성능에 상대적으로 적은 영향을 미친다.&lt;/p>
&lt;p>&lt;strong>Smooth power laws:&lt;/strong> $N$, $D$, $C$ 세 가지 스케일 요인이 파워와 관련이 있고, 이는 6배 이상의 크기 차이를 보인다. 상위 범위에서는 성능 향상의 감소를 보지 못했지만, 최종적으로는 성능이 안정화되어 손실이 0에 다다를 것이다.&lt;/p>
&lt;p>&lt;strong>Universality of overfitting:&lt;/strong> $N$과 $D$를 동시에 확장하면 성능이 예상대로 개선되지만, 둘 중 하나만 증가시키면 손실이 줄어든다. 성능 손실은 $N$과 $D$의 비율에 따라 예측 가능하며, 모델 크기를 8배 증가할 때마다 데이터를 약 5배 증가시키면 손실을 피할 수 있다.&lt;/p>
&lt;p>&lt;strong>Universality of training:&lt;/strong> 학습 곡선은 모델 크기에 거의 영향을 받지 않는 power-law를 따르며, 이를 통해 학습 초기부의 곡선을 확장해 더 오래 학습했을 때의 손실을 대략적으로 예측할 수 있다.&lt;/p>
&lt;p>&lt;strong>Transfer improves with test performance:&lt;/strong> 다른 분포의 텍스트에서 모델을 평가하면, 학습 검증 세트의 결과와 강하게 상관되며 일정한 손실이 발생한다. 이는 다른 분포로 전이할 때 일정한 패널티가 있지만, 그 외의 성능 향상은 학습 세트에서와 비슷하게 이루어진다.&lt;/p>
&lt;p>&lt;strong>Sample efﬁciency:&lt;/strong> 큰 모델은 작은 모델에 비해 최적화 단계와 데이터 포인트를 더 적게 사용하면서도 동일한 성능을 달성하는 샘플 효율성이 더 높다.&lt;/p>
&lt;p>&lt;strong>Convergence is inefﬁcient:&lt;/strong> 고정된 컴퓨팅 예산 내에서, 모델 크기나 사용 가능한 데이터에 제한이 없다면, 매우 큰 모델을 학습시키고 조기에 중단함으로써 최적의 성능을 얻는다. 이런 방식은 작은 모델을 완전히 수렴시키는 것보다 샘플 효율성이 훨씬 높으며, 데이터 요구사항은 학습 컴퓨팅에 따라 매우 천천히 증가한다.&lt;/p>
&lt;p>&lt;strong>Optimal batch size:&lt;/strong> 이 모델들을 학습시키는 이상적인 batch size는 손실의 거듭제곱 정도이며, 가장 큰 모델의 경우 수렴 시점에서 약 1M-2M 토큰이다.&lt;/p>
&lt;p>모델 크기, 데이터, 컴퓨팅을 적절히 확장할수록 언어 모델링 성능이 부드럽게 향상되며, 더 큰 언어 모델이 현재의 모델보다 성능과 샘플 효율성이 더 높을 것으로 예상한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure2.png"
width="1178"
height="498"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure2_hu71d1ba25a4227ce03b793399f1ff538c_219021_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure2_hu71d1ba25a4227ce03b793399f1ff538c_219021_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="567px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure3.png"
width="860"
height="400"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure3_hu0e8d390dfed7ece089f591eac5187753_82828_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure3_hu0e8d390dfed7ece089f591eac5187753_82828_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="516px"
>&lt;/p>
&lt;h3 id="summary-of-scaling-laws">Summary of Scaling Laws&lt;/h3>
&lt;p>Transformer가 언어를 자동 autoregressively하게 모델링하도록 학습된 경우, 테스트 손실은 parameter의 수 $N$, 데이터셋 크기 $D$, 또는 컴퓨팅 예산으 $C_{min}$로만 제한될 때 거듭제곱 법칙을 사용하여 예측할 수 있다.&lt;/p>
&lt;ol>
&lt;li>parameter의 수가 제한된 모델들이 충분히 큰 데이터셋에서 수렴할 때까지 학습된 경우:&lt;/li>
&lt;/ol>
&lt;p>$$ L(N) = (N)c / N)^{\alpha_N}; \ \alpha_N \sim 0.076, \ N_c \sim 8.8 \times 10^{13} (\text{non-embedding parameters}) $$&lt;/p>
&lt;ol start="2">
&lt;li>parameter 수가 제한된 모델들은 충분히 큰 데이터셋에서 수렴할 때까지 학습된다.&lt;/li>
&lt;/ol>
&lt;p>$$ L(D) = (D_c / D)^{\alpha_D}; \ \alpha_D \sim 0.095, \ D_c \sim 5.4 \times 10^{13}(\text{tokens}) $$&lt;/p>
&lt;ol start="3">
&lt;li>컴퓨팅 양이 제한된 상황에서 충분히 큰 데이터셋, 최적 크기의 모델, 그리고 충분히 작은 batch size를 사용하여 학습할 때:&lt;/li>
&lt;/ol>
&lt;p>$$ L(C_{min} = (C_c^{min} / C_{min})^{\alpha_C^{min}}; \ \alpha_C^{min} \sim 0.050, \alpha_C^{min} \sim 3.1 \times 10^8 (\text{PF-days})$$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure4.png"
width="1136"
height="330"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure4_hue340d1e50bcaf9eea1845a866fa76a74_143167_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure4_hue340d1e50bcaf9eea1845a866fa76a74_143167_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="344"
data-flex-basis="826px"
>&lt;/p>
&lt;p>이 관계는 $C_{min}$, $N$, $D$의 크기 순서에 대해 유지되며, 이는 모델 형태와 transformer의 다른 hyperparameter에 매우 약하게 의존한다. 거듭제곱 법칙은 $N$, $D$, $C_{min}$의 확장에 따른 성능 향상의 정도를 지정하며, parameter의 수를 두 배로 늘리면 손실이 약간 줄어드다. $N_c$, $C_c^{min}$, $D_c$의 정확한 수치 값은 어휘 크기와 토큰화에 따라 달라진다.&lt;/p>
&lt;p>데이터 병렬성의 속도와 효율성을 결정하는 중요한 batch size는 $L$에 대해 거듭제곱 법칙을 따른다:&lt;/p>
&lt;p>$$ B_{crit}(L) = {{B_{\ast}}\over{L^{1/\alpha_B}}}, \ B_{\ast} \sim 2 \cdot 10^8 \text{tokens}, \ \alpha_B \sim 0.21 $$&lt;/p>
&lt;p>모델 크기를 증가시킬 때, 데이터셋 크기도 $D \propto N^{{\alpha N}\over{\alpha D}} \sim N^{0.74}$에 따라 선형적으로 증가해야 한다는 것을 알 수 있다. 이는 $N$과 $D$에 대한 동시적인 의존성과 과적합 정도를 결정하는 식으로 결합된다.&lt;/p>
&lt;p>$$ L(N, D) = \big[ \big( {{N_c}\over{N}} \big)^{{\alpha N}\over{\alpha D}} + {{D_c}\over{D}} \big] $$&lt;/p>
&lt;p>다른 생성 모델링 작업에 대한 학습된 log-likelihood를 parameter화 할 수도 있다고 추측한다.&lt;/p>
&lt;p>무한한 데이터 한도에서 모델을 일정한 업데이트 단계동안 학습시키면, 초기 변동기간 후에 학습 곡선은 정확하게 맞출 수 있다.&lt;/p>
&lt;p>$$ L(N, S) = \big( {{N_c}\over{N}} \big)^{{\alpha N}\over{\alpha D}} + \big( {{S_c}\over{S_{min}(S)}} \big)^{\alpha S}$$&lt;/p>
&lt;p>$S_c \approx 2.1 \times 10^3$, $\alpha_S \approx 0.76$이고, $S_{min} (S)$는 최적화 단계(parameter 업데이트)의 최소 가능 수를 나타낸다.&lt;/p>
&lt;p>고정된 컴퓨팅 예산 내에서, 다른 제약 없이 학습시킬 때, 최적의 모델 크기, 배치 크기, 스텝 수, 데이터셋 크기가 성장해야 한다는 예측이 나온다.&lt;/p>
&lt;p>$$ N \propto C^{\alpha_C^{min} / \alpha N}, B \propto C^{\alpha_C^{min} / \alpha B}, S \propto C^{\alpha_C^{min} / \alpha S}, D = B \cdot S $$&lt;/p>
&lt;p>$$ \alpha_C^{min} = 1/ (1/\alpha S + 1 / \alpha B + 1 / \alpha N) $$&lt;/p>
&lt;p>계산 예산 $C$가 증가함에 따라, 주로 큰 모델에 투자가 증가하고, 이로 인해 학습 시간이나 데이터셋 크기는 크게 증가하지 않는다. 이는 큰 모델이 표본 효율성이 더 높아진다는 것을 의미한다. 하드웨어 제약으로 인해 연구자들은 일반적으로 작은 모델을 더 오래 학습시킨다. 최적 성능은 총 컴퓨팅 파워에 의존적이다.&lt;/p>
&lt;p>토큰 당 결과를 분석하며, LSTM과 recurrent Transformer에 대해 간단히 비교한다.&lt;/p>
&lt;h3 id="notation">Notation&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>$L$&lt;/strong> cross entropy 손실은 보통 내츄럴 로그로 표현된다. 대체로 이는 컨텍스트 내의 토큰들에 대해 평균된 값으로 보고되지만, 경우에 따라 컨텍스트 내의 특정 토큰에 대한 손실을 보고하는 경우도 있다.&lt;/li>
&lt;li>&lt;strong>$N$&lt;/strong> vocabulary와 positional embedding을 제외한 모델 parameter 수를 의미한다.&lt;/li>
&lt;li>&lt;strong>$C \approx 6NBS$&lt;/strong> $B$는 batch size, $S$는 training step 수를 나타내며, non-embedding 학습 계산의 총량을 추정하는데 사용된다. 이 계산량은 PF-day 단위로 표현되며, 1PF-day는 약 $8.64 \times 10^{19}$의 부동소수점 연산에 해당한다.&lt;/li>
&lt;li>&lt;strong>$D$&lt;/strong> 토큰 단위의 데이터셋 크기&lt;/li>
&lt;li>&lt;strong>$B_{crit}$&lt;/strong> 중요 배치 크기에서의 학습은 시간과 계산 효율성 사이에서 대략적으로 최적의 균형을 제공한다.&lt;/li>
&lt;li>&lt;strong>$C_{min}$&lt;/strong> 주어진 손실 값을 달성하기 위해 필요한 최소한의 non-embedding 계산량을 추정한 것으로, 이런 계산량은 모델이 중요 배치 크기보다 작은 배치 크기에서 학습될 때 사용된다.&lt;/li>
&lt;li>&lt;strong>$S_{min}$&lt;/strong> 주어진 손실 값을 달성하기 위해 필요한 최소 학습 step 수를 추정한 것으로, 이는 모델이 중요 배치 크기보다 큰 배치 크기에서 학습될 때의 학습 step 수 이다.&lt;/li>
&lt;li>&lt;strong>$\alpha_X$&lt;/strong> 손실 $L(X)$는 $1/X^{\alpha X}$의 형태로 $X$의 거듭제곱에 반비례하며, 여기서 $X$는 $N$, $D$, $C$, $S$, $B$, $C_{min}$ 중 하나이다. 즉, $X$가 커지면 손실은 줄어든다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="background-and-methods">Background and Methods&lt;/h2>
&lt;p>WebText2라는 확장된 데이터셋을 사용해 언어 모델을 학습한다. 이 모델은 $n_{vocab} = 50257$ 크기의 어휘로 토큰화되며, 1024 토큰 컨텍스트에 대한 cross-entropy 손실을 최적화한다. 주로 decoder-only transformer 모델을 학습시키지만, 비교를 위해 LSTM 모델과 Universal Transformers도 학습시킨다.&lt;/p>
&lt;h3 id="parameter-and-compute-scaling-of-transformers">Parameter and Compute Scaling of Transformers&lt;/h3>
&lt;p>Transformer 아키텍처는 $n_{layer}$(number of layers), $d_{model}$(dimension of the residual stream), $d_ff$(dimension of the intermediate feed-forward layer), $d_{attn}$(dimension of the attention output), 그리고 $n_{heads}$(number of attention heads per layer) 등의 hyperparameter를 사용해 정의된다. 입력 컨텍스트는 대체로 $n_{ctx} = 1024$개의 토큰을 포함한다.&lt;/p>
&lt;p>$$ N \approx 2 d_{model} \ n_{layer} (2 d_{attn} + d_{ff} ) = 12 n_{layer} \ d_{model} $$
$$ \text{with the standard} \ \ d_{attn} = d_{ff} / 4 = d_{model} $$&lt;/p>
&lt;p>embedding matrix $n_{vocab} \ d_{model}$과 positional embedding $n_{ctx} \ d_{model}$에 대한 parameter를 가지고 있지만, &amp;ldquo;모델 크기&amp;quot;를 논의할 때는 이들을 포함하지 않는다. 이 방식은 더욱 깔끔한 스케일링 법칙을 제공한다.&lt;/p>
&lt;p>transformer의 forward pass를 평가하는 것은 다음과 같은 과정을 포함한다.&lt;/p>
&lt;p>$$ C_{forward} \approx 2N + 2 n_{layer} \ n_{ctx} \ d_{model} $$&lt;/p>
&lt;p>add-multiply 연산을 포함하며, 이 중 2배에 해당하는 부분은 행렬 곱셈에 사용되는 multiply-accumulate 연산에서 나온다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/table1.png"
width="1118"
height="392"
srcset="https://kurtkim.github.io/p/scaling-law/images/table1_hu6d2fe3b17d28befb176c467c034373b0_78373_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/table1_hu6d2fe3b17d28befb176c467c034373b0_78373_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="285"
data-flex-basis="684px"
>&lt;/p>
&lt;p>$d_{model}$이 $n_{ctx} / 12$보다 훨씬 큰 모델에서, 토큰 당 컨텍스트 종속적인 계산 비용은 전체 계산의 작은 부분이다. 따라서 학습 계산 추정에는 컨텍스트 종속적인 부분은 포함되지 않는다. backwards pass 고려 시, 학습 토큰 당 비임베딩 계산은 대략 $C \approx 6N$ non-embedding 연산자로 추정된다.&lt;/p>
&lt;h3 id="training-procedures">Training Procedures&lt;/h3>
&lt;p>1024개의 토큰으로 이루어진 512개의 시퀀스 배치를 사용하여 $2.5 \times 10^5$ 단계 동안 모델을 Adam optimizer 도구로 학습시켰다. 메모리 제약으로 인해, 1B 개 이상의 parameter를 가진 가장 큰 모델들은 Adafactor로 훈련되었다. 다양한 learning rate와 스케줄을 실험했으며, 결과는 learning rate 스케줄에 크게 의존하지 않았다. 대부분의 학습은 3000 step의 linear warmup 후 0까지의 cosine decay를 따르는 learning rate 스케줄을 사용하였다.&lt;/p>
&lt;h3 id="datasets">Datasets&lt;/h3>
&lt;p>Reddit의 공유 링크를 웹 스크랩한 WebText 데이터셋의 확장 버전에서 모델을 학습시켰다. 이 데이터셋은 2017년 12월까지의 링크와 2018년 1월부터 10월까지의 링크를 포함하며, 각 링크는 최소 3 카르마를 받았다. 이 데이터셋은 총 20.3M의 문서와 96GB의 텍스트, $1.62 \times 10^{10}$ 단어를 포함하며, 가역적 토크나이저를 적용하여 $2.29 \times 10^{10}$ 토큰을 얻었다. 이 중 일부 토큰은 테스트 셋으로 사용되었고, 추가적으로 다양한 소스의 샘플에 대해서도 테스트를 진행하였다.&lt;/p>
&lt;hr>
&lt;h2 id="empirical-results-and-basic-power-laws">Empirical Results and Basic Power Laws&lt;/h2>
&lt;p>언어 모델 스케일링을 특성화하기 위해 다음과 같은 요소를 포함한 다양한 모델을 학습시킨다:&lt;/p>
&lt;ul>
&lt;li>Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)&lt;/li>
&lt;li>Dataset size (ranging from 22 million to 23 billion tokens)&lt;/li>
&lt;li>Shape (including depth, width, attention heads, and feed-forward dimension)&lt;/li>
&lt;li>Context length (1024 for most runs, though we also experiment with shorter contexts)&lt;/li>
&lt;li>Batch size (2 19 for most runs, but we also vary it to measure the critical batch size)&lt;/li>
&lt;/ul>
&lt;h3 id="approximate-transformer-shape-and-hyperparameter-independence">Approximate Transformer Shape and Hyperparameter Independence&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure5.png"
width="1176"
height="320"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure5_hu5f813134cf4d44c6d9bbeaffd912cc72_92250_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure5_hu5f813134cf4d44c6d9bbeaffd912cc72_92250_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="367"
data-flex-basis="882px"
>&lt;/p>
&lt;p>Transformer의 성능은 전체 non-embedding parameter 수 $N$이 고정되어 있을 때, $n_{layer}$, $n_{heads}$, $d_{ff}$와 같은 shape parameter에 대해 매우 약하게 의존한다. 이를 확인하기 위해, 단일 hyperparameter를 변경하면서 동일한 크기의 모델을 학습시켰다. 깊은 Transformer가 얕은 모델의 앙상블처럼 효과적으로 작동한다면, $n_{layers}$의 독립성이 이어질 것이다.&lt;/p>
&lt;h3 id="performance-with-non-embedding-parameter-count-n">Performance with Non-Embedding Parameter Count $N$&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure6.png"
width="1086"
height="352"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure6_huf01da79a101781de139f218c134e40cf_93600_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure6_huf01da79a101781de139f218c134e40cf_93600_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="308"
data-flex-basis="740px"
>&lt;/p>
&lt;p>모델은 전체 WebText2 데이터셋에서 거의 수렴할 때까지 학습되었으며, 과적합은 가장 큰 모델들을 제외하고는 발견되지 않았다.&lt;/p>
&lt;p>non-embedding parameter 수 $N$과의 안정적인 추세를 찾을 수 있으며, 다음과 같이 표현할 수 있다:&lt;/p>
&lt;p>$$ L(N) \approx \big( {{N_c}\over{N}} \big)^{\alpha_N} $$&lt;/p>
&lt;p>$N$의 함수로서의 성능을 연구하는 것이 중요한데, 이를 통해 non-embedding parameter 수와 성능 사이의 추세를 관찰할 수 있다. 반면 총 매개변수 수를 사용하면 추세가 흐릿해진다. 이는 임베딩 행렬의 크기를 줄여도 성능에 영향을 미치지 않는다는 최근의 연구 결과를 지지한다.&lt;/p>
&lt;p>WebText2 데이터셋에서 학습된 이 모델들의 테스트 손실은 다양한 다른 데이터셋에서도 $N$의 거듭제곱 법칙을 따르며, 지수는 거의 동일하다.&lt;/p>
&lt;h3 id="comparing-to-lstms-and-universal-transformers">Comparing to LSTMs and Universal Transformers&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure7.png"
width="1180"
height="418"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure7_hu4c644a57ba8d19afa54942e652117915_112387_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure7_hu4c644a57ba8d19afa54942e652117915_112387_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="677px"
>&lt;/p>
&lt;p>LSTM은 문맥 초기에 나타나는 토큰에 대해 Transformer만큼 잘 수행하지만, 나중에 나타나는 토큰에서는 Transformer의 성능을 따라잡지 못하였다. 이는 더 큰 모델들이 패턴을 더 빠르게 인식하는 능력을 개선했다는 것을 나타낸다.&lt;/p>
&lt;p>recurrent Transformer는 parameter를 재사용하여 $N$의 함수로서 약간 더 나은 성능을 보이지만, 이는 parameter 당 추가 계산 비용이 발생한다.&lt;/p>
&lt;h3 id="generalization-among-data-distributions">Generalization Among Data Distributions&lt;/h3>
&lt;p>추가 텍스트 데이터 분포에 대한 모델을 테스트하였다. 모든 모델은 WebText2 데이터셋에서만 학습되었으며, 이러한 다른 데이터 분포에서의 손실은 모델 크기에 따라 부드럽게 개선되었다. 일반화는 거의 전적으로 in-distribution 검증 손실에 의존하며, 학습 기간이나 수렴에 가까움, 모델 깊이에는 의존하지 않았다.&lt;/p>
&lt;h3 id="performance-with-dataset-size-and-compute">Performance with Dataset Size and Compute&lt;/h3>
&lt;p>데이터셋 크기 $D$와 학습 계산 $C$의 함수로서의 테스트 손실에 대한 경험적 추세를 보여준다.&lt;/p>
&lt;p>WebText2 데이터셋의 일부에서 모델을 학습시키고 테스트 손실이 더 이상 감소하지 않을 때 학습을 중단하였다. 이 결과, 테스트 손실은 단순한 거듭제곱 법칙으로 표현될 수 있었다.&lt;/p>
&lt;p>$$ L(D) \approx \big( {{D_c}\over{D}} \big)^{\alpha_D} $$&lt;/p>
&lt;p>학습 중 non-embedding 계산의 총량은 $C = 6NBS$로 추정된다. 주어진 $C$ 값에 대해, 다양한 $N$을 가진 모든 모델을 검토하여 최상의 성능을 내는 모을 찾을 수 있다. 하지만, 모든 모델에 대해 batch size $B$가 고정되어 있기 때문에, 이 결과는 실제로 최적이 아니다.&lt;/p>
&lt;p>결과는 다음과 같이 표현될 수 있다:&lt;/p>
&lt;p>$$ L(C) \approx \big( {{C_c}\over{C}} \big)^{\alpha_C} $$&lt;/p>
&lt;p>데이터 분석 결과, 모델 크기가 커질수록 샘플 효율성이 향상되는 것을 확인할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="charting-the-inﬁnite-data-limit-and-overﬁtting">Charting the Inﬁnite Data Limit and Overﬁtting&lt;/h2>
&lt;p>언어 모델링 성능의 기본적인 스케일링 법칙을 발견하였다. 여기서는 $N$과 $D$를 동시에 변화시키며, $D$ 토큰의 데이터셋에서 학습된 크기 $N$의 모델 성능을 연구한다. 최적으로 학습된 테스트 손실이 스케일링 법칙을 따르며, 이는 모델 크기 증가와 과적합 통제를 위한 데이터 요구량을 안내한다.&lt;/p>
&lt;h3 id="proposed-ln-d-equation">Proposed $L(N, D)$ Equation&lt;/h3>
&lt;p>$$ L(N, D) = \big[ \big( {{N_c}\over{N}} \big)^{{{\alpha N}\over{\alpha D}}} + {{D_c}\over{D}} \big]^{\alpha D} $$&lt;/p>
&lt;p>다음 세가지 원칙을 사용한다:&lt;/p>
&lt;ol>
&lt;li>어휘 크기나 토큰화의 변화는 전반적인 요소에 의해 손실을 재조정할 것으로 예상된ㄴ다. $L(N, D)$의 parameterization(및 손실의 모든 모델)는 이러한 재조정을 자연스럽게 허용해야 한다.&lt;/li>
&lt;li>$D$를 고정하고 $N$을 무한대로 보내면, 전체 손실은 $L(D)$에 접근해야 한다. 반대로, $N$을 고정하고 $D$를 무한대로 보내면 손실은 $L(N)$에 접근해야 한다.&lt;/li>
&lt;li>$L(N, D)$는 $D = \infty$에서 해석적이어야 하므로, 정수 제곱을 가진 $1/D$의 시리즈 확장을 가질 수 있다. 이 원칙에 대한 이론적 지지는 첫 두 원칙에 비해 상당히 약하다.&lt;/li>
&lt;/ol>
&lt;p>$L(N, D)$는 어휘의 변화에 따라 $N_c$, $D_c$를 재조정할 수 있기 때문에 첫 번째 요구사항을 만족하다. 이는 또한 $N_c$, $D_c$의 값이 본질적인 의미를 가지지 않음을 의미한다.&lt;/p>
&lt;p>테스트 손실이 개선되지 않을 때 학습을 조기에 중단하고, 모든 모델을 동일하게 최적화하기 때문에 큰 모델이 작은 모델보다 성능이 좋을 것으로 예상한다. 그러나 고정된 $D$에서는 어떤 모델도 최선의 손실에 접근할 수 없으며, 고정된 크기의 모델은 용량에 제한된다. 이러한 고려사항은 두 번째 원칙을 동기부여하며, 무한한 $D$와 $N$에서의 $L$ 값은 $L(N, D)$의 모든 parameter를 결정한다.&lt;/p>
&lt;p>세 번째 원칙은 추측적으로, 매우 큰 $D$에서 과적합이 $1/D$로 스케일링될 것으로 예상한ㄴ다. 이는 데이터셋의 분산 또는 신호 대 잡음 비율과 관련이 있다. 이 기대는 모든 부드러운 손실 함수에서 성립해야 하지만, 이는 $1/D$ 보정이 다른 분산 원처보다 우세하다고 가정한다. 이 가정은 경험적으로 확인되지 않아 그 적용 가능성에 대한 확신이 떨어진다.&lt;/p>
&lt;p>세 번째 원칙은 $N$과 $D$의 역할의 비대칭성을 설명한다. 유사한 대칭 표현식이 가능하지만, 이들은 정수 제곱의 $1/D$ 확장을 가지지 않고 추가 parameter가 필요하다.&lt;/p>
&lt;p>$L(N, D)$에 대한 방정식이 데이터를 잘 맞추는 것을 볼 수 있을 것이며, 이것이 $L(N, D)$ 가정에 대한 가장 중요한 정당화이다.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure9.png"
width="1156"
height="318"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure9_hu3fe7a6eef047be355bc61d43f414b5c1_96177_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure9_hu3fe7a6eef047be355bc61d43f414b5c1_96177_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="363"
data-flex-basis="872px"
>&lt;/p>
&lt;p>모든 모델을 10%의 dropout으로 정규화하고, 테스트 손실을 추적하여 더 이상 감소하지 않을 때 중단한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/table2.png"
width="648"
height="124"
srcset="https://kurtkim.github.io/p/scaling-law/images/table2_huc2b6c23fdfb164c828d935d097786836_18116_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/table2_huc2b6c23fdfb164c828d935d097786836_18116_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="522"
data-flex-basis="1254px"
>&lt;/p>
&lt;p>데이터셋이 1024배로 크게 줄어든 경우를 제외하고는 우수한 적합성을 보였다. 작은 데이터셋에서는 한 epoch가 40번의 parameter 업데이트로 이루어지며, 이는 언어 모델링의 다른 체제를 나타낼 수 있다. 학습 초기에 과적합이 발생하고, 전체 $L(N, D)$를 적합시키므로 parameter가 Section 4에서 얻은 것과 약간 다르다.&lt;/p>
&lt;p>무한 데이터 한계를 탐색하기 위해, 과적합의 범위를 직접 조사할 수 있다. 가장 큰 모델을 제외하고는 22B 토큰의 전체 WebText2 데이터셋으로 학습시 과적합의 증거가 없으므로, 이를 $D = \infty$의 대표로 볼 수 있다. 이를 바탕으로 유한 $D$를 무한 데이터 한계와 비교할 수 있다.&lt;/p>
&lt;p>$$ δL(N, D) ≡ {{L(N, D)}\over{L(N, \infty)}} -1 $$&lt;/p>
&lt;p>$N$과 $D$의 함수로서 $δL$을 연구한다. 실제로 $δL$이 $N$과 $D$의 특정 조합에만 의존하는 것을 경험적으로 확인할 수 있으며, 이는 스케일링 법칙에서 파생된다.&lt;/p>
&lt;p>$$ δL \approx \big( 1 + \big( {{N}\over{N_c}} \big)^{{\alpha N}\over{\alpha D}} {{D_c}\over{D}} \big)^{\alpha D} -1 $$&lt;/p>
&lt;p>large $D$에서 이 공식이 $1/D$의 제곱의 시리즈 확장을 가지고 있다.&lt;/p>
&lt;p>다른 랜덤 시드로 손실의 변동성이 대략 0.02라는 것을 추정하며, 이는 수렴 임계값 내에서 학습 시 과적합을 피하기 위해 다음을 필요로 한다.&lt;/p>
&lt;p>$$ D \geqslant (5 × 10^3) N^{0.74} $$&lt;/p>
&lt;p>이 관계를 통해, $10^9$개 미만의 parameter를 가진 모델은 22B 토큰의 WebText2 데이터셋에서 최소한의 과적합으로 학습 가능하며, 가장 큰 모델은 약간의 과적합을 겪게 된다. 이 관계는 과적합을 피하면서 데이터셋 크기가 모델 크기에 비례하여 아래선형적으로 증가할 수 있음을 보여준다. 그러나 이것이 항상 최대 계산 효율적인 학습을 의미하는 것은 아니며, 데이터셋과 모델 크기 변화에 따른 정규화 최적화는 아직 이루어지지 않았다.&lt;/p>
&lt;hr>
&lt;h2 id="scaling-laws-with-model-size-and-training-time">Scaling Laws with Model Size and Training Time&lt;/h2>
&lt;p>모델 크기와 학습 시간에 따른 손실 함수의 스케일링 법칙을 이해하는데 초점을 맞춥니다. 이를 위해 대부분의 모델에 적용할 수 있는 학습 단계를 정의하고, 이를 통해 손실의 모델 크기와 학습 시간 의존성을 적합시키는 방법을 설명합니다. 그리고 이 결과를 바탕으로 학습 시간과 모델 크기를 최적으로 분배하는 방법을 예측하고 검증한다.&lt;/p>
&lt;h3 id="adjustment-for-training-at-b_critl">Adjustment for Training at $B_{crit}(L)$&lt;/h3>
&lt;p>학습에는 critical batch size $B_{crit}$이 존재하며, 이 크기 이하에서는 batch size를 늘려도 컴퓨팅 효율성이 크게 저하되지 않는다. 그러나 $B_{crit}$보다 큰 경우에는 batch size 증가의 효과가 점차 감소한다. 이 결과는 batch size에 따른 학습 시간과 컴퓨팅 변화를 예측하는데 활용할 수 있다. 최적의 효율을 위해선 batch size가 $B_{crit}$에 가깝게 설정되어야 하며, 너무 크거나 작은 batch size는 각각 학습 step과 컴퓨팅 사용을 최소화한다.&lt;/p>
&lt;p>더 구체적으로, 다양한 신경망 작업에 대해 학습 step 수 $S$와 처리된 데이터 예제의 수 $E = BS$는 간단한 관계를 만족시키는 것이 입증되었다.&lt;/p>
&lt;p>$$ \big( {{S}\over{S_{min}}} - 1 \big) \big( {{E}\over{E_{min}}} - 1 \big) = 1 $$&lt;/p>
&lt;p>이는 손실 $L$의 고정된 값에 도달하기 위한 학습을 진행할 때의 상황이다. 여기서 $S_{min}$은 $L$에 도달하기 위해 필요한 최소 단계 수이며, $E_{min}$은 처리해야 할 데이터 예제의 최소 수 이다.&lt;/p>
&lt;p>다음 식은 critical batch size를 정의한다.&lt;/p>
&lt;p>$$ B_{crit}(L) ≡ {{E_{min}}\over{S_{min}}} $$&lt;/p>
&lt;p>이는 손실의 목표 값에 따라 변하는 함수이다. critical batch size에서 학습하면 시간/컴퓨팅의 트레이드오프가 대략적으로 최적화되며, 이는 $2S_{min}$의 학습 step을 필요로 하고 $E = 2E_{min}$의 데이터 예제를 처리한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure10.png"
width="708"
height="456"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure10_hu9ae266059a93b3fe394d199397648cd6_104337_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure10_hu9ae266059a93b3fe394d199397648cd6_104337_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;p>학습 손실에 따른 critical batch size와 gradient noise scale을 보여준다. 이는 모델 크기와 무관하며 손실 $L$에만 의존합니다. 이로 인해 예측이 transformer 언어 모델에도 계속 적용된다. critical batch size는 손실의 power-law으로 적합될 수 있다.&lt;/p>
&lt;p>$$ B_{crit}(L) \approx {{B_{*}}\over{L^{1/\alpha_B}}} $$&lt;/p>
&lt;p>여기서 $B_{*}$는 약 $2 \times 10^8$이고, $\alpha_B$는 약 0.21이다.&lt;/p>
&lt;p>손실이 최소값에 접근할 때 gradient noise scale이 발산할 것으로 예상되어, 이를 추적하도록 $B_{crit}$의 parameterization를 선택하였다. 자연 언어의 엔트로피가 0이 아니므로 $L_{min} &amp;gt; 0$이며, 이는 이 연구에서 달성한 $L$의 값보다 훨씬 작다. 그래서 $B_{crit}$이 $L$이 0으로 접근함에 따라 발산하는 parameterization를 사용하였다.&lt;/p>
&lt;p>$B_{crit}(L)$을 사용하여 batch size $B = 2^19$ 토큰으로 학습하는 동안의 학습 step 수 $S$와 $B ≫ B_{crit}$에서 학습하는 동안의 학습 step 수 사이의 관계를 추정할 것이다. 이는 단순히 다음과 같다.&lt;/p>
&lt;p>$$ S_{min}(S) ≡ {{S}\over{1 + B_{crit}(L) / B}}, (\text{minimum steps, at} B ≫ B_{crit} ) $$&lt;/p>
&lt;p>이는 손실의 목표 값 $L$에 대한 것이다. 이것은 또한 $B ≪ B_{crit}(L)$에서 학습하면서 크기 $N$의 모델로 $L$까지 학습하는데 필요한 컴퓨팅의 critical value를 정의한다. 이는 다음과 같다.&lt;/p>
&lt;p>$$ C_{min}(C) ≡ {{C}\over{1 + B / B_{crit}(L)}}, (\text{minimum compute, at} B ≪ B_{crit} ) $$&lt;/p>
&lt;p>여기서 $C = 6NBS$는 batch size $B$에서 사용되는 (non-embedding) 컴퓨팅을 추정한다.&lt;/p>
&lt;h3 id="results-for-ln-s-min-and-performance-with-model-size-and-compute">Results for $L(N, S min)$ and Performance with Model Size and Compute&lt;/h3>
&lt;p>$S_{min}$을 활용하여, 무한 데이터 한계에서 모델 크기와 학습 시간에 따른 손실의 관계를 간단하게 적합시킨다. 이는 Adam-optimized 학습을 통해 이루어진다.&lt;/p>
&lt;p>$$ L(N, S_{min}) = \big( {{N_c}\over{N}} \big)^{\alpha_N} + \big( {{S_c}\over{S_{min}}} \big)^{\alpha_S} $$&lt;/p>
&lt;p>손실에 대해, learning rate schedule의 warmup period 이후의 모든 학습 단계를 포함하고, parameter를 사용하여 데이터에 적합성을 찾는다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/table3.png"
width="622"
height="118"
srcset="https://kurtkim.github.io/p/scaling-law/images/table3_hu268d2742fb46d5db29d5defd96087964_16884_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/table3_hu268d2742fb46d5db29d5defd96087964_16884_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="527"
data-flex-basis="1265px"
>&lt;/p>
&lt;p>이러한 parameter들을 사용하면, 학습 곡선 적합성을 얻을 수 있다. 이 적합성들이 완벽하지는 않지만, 상당히 설득력 있는 것으로 생각된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure11.png"
width="1156"
height="368"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure11_huaba6e09c37e9ed1cbd0b17fd6168d25c_144701_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure11_huaba6e09c37e9ed1cbd0b17fd6168d25c_144701_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="753px"
>&lt;/p>
&lt;p>다른 방식으로 데이터와 적합성을 시각화할 수 있는데, 이는 학습에 사용된 전체 non-embedding 계산 또는 단계 수를 고정하고 모델 크기에 따른 테스트 손실을 연구한다.&lt;/p>
&lt;p>손실에 대한 S_{min}의 법칙적 종속성은 최적화 동력학과 손실 풍경 사이의 상호작용을 보여준다. 학습 후반부에 가장 적합성이 좋기 때문에, 법칙이 손실의 hessian 행렬 스펙트럼에 대한 정보를 제공한다. 그 보편성은 hessian 행렬의 고유값 밀도가 모델 크기와 거의 관련이 없음을 나타낸다.&lt;/p>
&lt;h3 id="lower-bound-on-early-stopping-step">Lower Bound on Early Stopping Step&lt;/h3>
&lt;p>$L(N, S_{min})$ 결과는 데이터 제한 학습에서 일찍 멈춰야 하는 단계의 최소 한계를 도출하는데 사용된다. 이는 주어진 모델의 유한 $D$와 무한 $D$ 학습 곡선이 $S_{min} \approx S_{stop}$에 이를 때까지 유사하기 때문이다. 따라서 과적합은 $S_{stop}$에서 학습을 중단하는 수정에 비례할 것이다. 이는 유한 $D$에서 테스트 손실이 더 느리게 감소하므로 $S_{stop}$을 과소평가하게 된다. 이러한 추론은 특정 부등식을 이끈다.&lt;/p>
&lt;p>$$ S_{stop} (N, D) \geqslant {{S_c}\over{[L(N, D) - L(N, \infty)]^{1/\alpha_S}}} $$&lt;/p>
&lt;p>$L(N, \infty)$은 무한한 데이터로 평가된 수렴된 손실이다. 이 부등식은 경험적 데이터와 비교되며, $S_{stop}$과 $L(N, D)$는 경험적이다. $L(N, \infty)$은 $D = \infty$에서 평가된 $L(N, D)$에 대한 적합으로 계산된다.&lt;/p>
&lt;hr>
&lt;h2 id="optimal-allocation-of-the-compute-budget">Optimal Allocation of the Compute Budget&lt;/h2>
&lt;p>훈련 중 계산에 따른 성능의 경험적 추세를 보면, ﬁxed batch size $B$에서 학습하는 것을 포함하지만, 실제로는 batch size $B_{crit}$에서 더 효율적으로 학습할 수 있다. 손실의 큰 값과 작은 값은 각각 더 적은 샘플이나 단계로 달성될 수 있었으며, 이를 중요한 batch size로 표준화하면 더 깔끔하고 예측 가능한 추세가 나타난다.&lt;/p>
&lt;p>이 섹션에서는 이전의 실수를 조정하고, 모델 크기 $N$과 학습 중 처리된 데이터 양($2B_{crit} S_{min}$) 사이의 최적 컴퓨트 할당을 결정한다. 이는 $L(N, S_{min})$의 방정식을 활용하여 경험적으로 그리고 이론적으로 수행되며, 두 방법이 일치함을 보여줄 예정이다.&lt;/p>
&lt;h3 id="optimal-performance-and-allocations">Optimal Performance and Allocations&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure13.png"
width="600"
height="402"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure13_huc106e3524a53c639afc310e501f54577_49989_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure13_huc106e3524a53c639afc310e501f54577_49989_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="358px"
>&lt;/p>
&lt;p>최적으로 할당된 계산에 따른 손실을 연구한다. $C_{min}$을 이용한 새로운 적합이 약간 개선되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure14.png"
width="1160"
height="388"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure14_huc7327aacde090150335287b2ab103f81_110037_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure14_huc7327aacde090150335287b2ab103f81_110037_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>$L(C_{min})$이 주어졌을 때, 주어진 학습 계산량으로 최소 손실을 제공하는 최적 모델 크기 $N(C_{min})$을 찾는 것이 중요하. 이 최적 모델 크기는 법칙적으로 매우 잘 적합함을 확인할 수 있다.&lt;/p>
&lt;p>$$ N(C_{min}) \propto (C_{min})^{0.73} $$&lt;/p>
&lt;p>최적이 아닌 크기의 모델을 훈련시키는 효과를 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure12.png"
width="1158"
height="390"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure12_huac5d1ae62617d5e01a88fb0c11b55330_100191_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure12_huac5d1ae62617d5e01a88fb0c11b55330_100191_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="296"
data-flex-basis="712px"
>&lt;/p>
&lt;p>정의에 따르면, $C_{min} ≡ 6NB_{crit} S$이므로, $N(C_{min})$를 사용하여 추가 결과를 도출할 수 있다. 이전 적합 결과 $B \propto L^{−4.8}$ 및 $L \propto C_{min}^{−0.05}$를 바탕으로, $B_{crit} \propto C_{min}^{0.24}라는 결론을 내린다. 이로 인해 최적의 단계 수는 계산량에 따라 매우 느리게 증가할 것으로 예상된다.&lt;/p>
&lt;p>$$ S_{min} \propto (C_{min})^{0.03} $$&lt;/p>
&lt;p>실제로 측정된 지수는 충분히 작아서, 결과는 지수가 0인 경우와도 일관성이 있을 수 있다.&lt;/p>
&lt;p>언어 모델링을 최적의 계산 할당으로 확장하면서, 주로 모델 크기 $N$을 늘리고, $B \propto B_{crit}$를 통해 배치 크기를 증가시키되, 연속적인 단계 수의 증가는 무시해야 한다. 계산 효율적인 학습은 적은 수의 최적화 단계를 사용하므로, 초기 학습 동력학을 가속화하는 추가 작업이 필요할 수 있다.&lt;/p>
&lt;h3 id="predictions-from-ln-s_min">Predictions from $L(N, S_{min})$&lt;/h3>
&lt;p>$L(C_{min})$과 할당 결과는 $L(N, S_{min})$ 방정식을 통해 예측할 수 있다. $L(N, S_{min})$ 방정식을 이용하면, $S_{min} = C_{min}$을 대입하고 학습 계산을 고정한 상태에서 $N$에 따른 손실의 최소값 $6NB$를 찾을 수 있다.&lt;/p>
&lt;p>학습 계산의 함수로서의 손실에 대해서, 우리는 다음을 예측한다.&lt;/p>
&lt;p>$$ L(C_{min}) = \big( {{C_c^{min}}\over{C_{min}}} \big)^{\alpha_C^{min}} $$&lt;/p>
&lt;p>$$ \alpha_C ≡ {{1}\over{1/\alpha_S + 1/\alpha_B + 1/\alpha_N}} \approx 0.054 $$&lt;/p>
&lt;p>또한 다음을 예측한다.&lt;/p>
&lt;p>$$ N(C_{min}) \propto (C_{min})^{\alpha_C^{min} / \alpha_N} \approx (C_{min})^{0.71} $$&lt;/p>
&lt;p>이것 또한 스케일링과 몇 퍼센트 이내로 일치한다. scaling law는 언어 모델링의 성능에 대한 예측적인 프레임워크를 제공한다.&lt;/p>
&lt;h3 id="contradictions-and-a-conjecture">Contradictions and a Conjecture&lt;/h3>
&lt;p>계산, 데이터, 모델 크기의 큰 값에서 직선적인 법칙적 추세로부터의 이탈을 관찰하지 않는다. 그러나, 자연어가 0이 아닌 엔트로피를 가지고 있기 때문에, 추세는 결국 안정화되어야 한다.&lt;/p>
&lt;p>이 섹션에서 서술된 계산 효율적인 학습의 추세는 명백한 모순을 포함하고 있다. 훨씬 큰 규모에서 $L(C_{min})$ scaling law에 의해 예측된 성능은 계산과 함께 학습 데이터 증가가 느리다는 점을 고려하면 가능한 것보다 낮다. 이는 scaling law가 이 지점 이전에 붕괴해야 함을 의미하며, 이 교차점은 transformer 언어 모델이 최대 성능에 도달하는 지점의 추정치를 제공한다는 더 깊은 의미를 가지고 있을 것으로 추측한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/scaling-law/images/figure15.png"
width="1018"
height="404"
srcset="https://kurtkim.github.io/p/scaling-law/images/figure15_hu25ae687c6abeabf8501500f350471773_63474_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/scaling-law/images/figure15_hu25ae687c6abeabf8501500f350471773_63474_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="251"
data-flex-basis="604px"
>&lt;/p>
&lt;p>계산 효율적인 학습에 사용되는 데이터의 양이 계산 예산과 느리게 증가하기 때문에, $L(C_{min})$에 의해 예측된 성능은 결국 $L(D)$ 법칙에 의해 설정된 하한선에 도달한다.&lt;/p>
&lt;p>과적합을 제어하기 위해, 데이터셋 크기를 다음과 같이 조정해야 함을 의미한다.&lt;/p>
&lt;p>$$ D \propto N^{0.74} \propto C_{min}^{0.54} $$&lt;/p>
&lt;p>critical batch size에서 학습하고 학습 중 데이터를 재사용하지 않는 경우, 데이터 사용량이 계산량과 같이 증가한다는 것을 알 수 있다. 이는 계산 효율적인 학습의 데이터 요구사항과 비교된다.&lt;/p>
&lt;p>$$ D(C_{min}) = {{2 C_{min}}\over{6 N (C_{min})}} \approx ( 4 \times 10^{10} \text{tokens} ) (C_{min}/\text{PF-Day})^{0.26} $$&lt;/p>
&lt;p>계산량과 함께 데이터셋 크기가 생산적으로 증가할 수 있는 최대 속도는 단 한 번의 epoch만을 위해 학습하는 것이다. 그러나 이는 데이터셋을 더 느리게 증가시키며, 학습 과정에서 데이터를 재사용하지 않더라도, 계산 효율적인 학습은 결국 과적합 문제에 직면하게 될 것이라는 것을 암시한다.&lt;/p>
&lt;p>데이터셋 크기에 의해 제한될 때, 즉 과적합 발생시 손실은 $L(D) \propto D^{−0.095}$로 스케일링될 것으로 예상된다. 이는 데이터 제한시 손실이 계산량에 따라 $L(D(C_{min})) \propto C_{min}^−{0.03}$으로 스케일링될 것을 의미한다. 그러나 $L(C_{min}) \propto C_{min}^{−0.050}$ 예측과 교차하게 되어 모순이 발생한다.&lt;/p>
&lt;p>$L(D(C_{min}))$와 $L(C_{min})$의 교차점은 다음에서 발생한다.&lt;/p>
&lt;p>$$ C^∗ \sim 10^4 \text{PF-Days} N^∗ \sim 10^{12} \text{parameters}, D^∗ \sim 10^{12} \text{tokens}, L^∗ \sim 1.7 \text{nats/token} $$&lt;/p>
&lt;p>수치값은 매우 불확실하며, 법칙적인 적합에서의 지수의 정확한 값에 따라 크게 변할 수 있다. 가장 명확한 해석은, 계산과 모델 크기에서 많은 자릿수만큼 떨어진 이 지점에 도달하거나 그 이전에 scaling law가 붕괴한다는 것이다.&lt;/p>
&lt;p>교차점이 더 깊은 의미를 가진다는 추측이 있다. 특별한 데이터 요구사항 없이 모델 크기를 $N^∗$ 이상으로 늘릴 수 없다면, $C_{min}^∗ 와 $N^∗$에 도달하면 자연어 데이터에서 신뢰할 수 있는 정보를 모두 추출했다는 것을 의미할 수 있다. 이 경우, $L^∗$은 자연어의 토큰당 엔트로피의 대략적인 추정치를 제공하며, 손실 추세는 $L^∗$ 에서 또는 그 이전에 안정화될 것으로 예상된다.&lt;/p>
&lt;p>학습 데이터셋에 추가된 잡음을 고려하면 $L(C_{min})$의 함수 형태가 안정화되는 것을 추측할 수 있다. 예를 들어, 랜덤 토큰 문자열을 각 컨텍스트에 추가하여 손실을 상수만큼 증가시킬 수 있다. 잡음 바닥에서의 거리 $L - L_{noise}$는 더 의미있는 성능 지표가 될 수 있으며, 이 거리에서의 작은 감소는 큰 성능 향상을 나타낼 수 있다. 인공적인 잡음은 모든 추세에 동일한 영향을 미치므로, 안정화 이후에도 임계점 6.8은 변하지 않고 의미가 있을 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>Power law는 다양한 출처에서 나올 수 있다. 밀도 추정과 랜덤 포레스트 모델에서의 모델과 데이터셋 크기에 대한 Power-law scaling은 결과와 관련이 있을 수 있다. 이 모델들은 power-law exponent가 데이터의 관련 특징 수의 역수로 대략적으로 해석될 수 있다고 제안한다.&lt;/p>
&lt;p>일부 초기 연구에서는 성능과 데이터셋 크기 간의 power-law scaling을 발견했고, 최근 연구는 모델 크기와 데이터 크기 사이의 스케일링도 조사하였다. 그러나 일부 연구에서는 모델 크기와 함께 데이터셋 크기의 super-linear scaling을 발견했지만, 이 논문은 sub-linear scaling을 발견하였다. 또한, power-law learning curve과 최적의 계산 할당에 대한 이 논문의 연구 결과와 일부 유사점이 있다. 최근의 연구에서는 다양한 데이터셋에 대한 데이터셋 크기와 모델 크기 모두를 확장하고 있다.&lt;/p>
&lt;p>EfficientNet은 이미지 모델의 최적 성능을 위해 깊이와 너비를 지수적으로 확장하도록 주장하며, 이는 깊이에 따른 너비의 power-law scaling을 초래한다. 언어 모델에 대해 이 지수가 확장 시 대략 1이어야 한다는 것을 발견했으며, 언어 모델의 전체적인 규모에 비해 구조적인 hyperparameter의 정확성은 크게 중요하지 않다는 것을 발견하였다. 또한, 일부 연구에서는 데이터 예제당 계산을 고정하는 반면, 이 논문의 모델 크기와 학습 계산량 모두를 확장하는 것을 조사하였다.&lt;/p>
&lt;p>overparameterized 모델에서의 일반화를 조사한 여러 연구들에서는 모델 크기가 데이터셋 크기에 도달하면 &amp;ldquo;jamming transition&amp;quot;가 발생한다는 것을 발견하였다. 하지만 이러한 전환이 일어나지 않는 것을 발견하였고, 필요한 학습 데이터가 모델 크기에 대해 부분적으로 확장된다는 것을 확인하였다. 모델 크기의 확장, 특히 큰 너비는 스케일링 관계에 대해 생각하는 데 유용한 프레임워크를 제공할 수 있다. 또한, 노이즈가 있는 이차 모델을 사용하여 학습 곡선의 형태 등 최적화 결과를 설명할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>언어 모델의 log-likelihood loss는 non-embedding parameter 수, 데이터셋 크기, 최적화된 학습 계산과 일관되게 스케일링되며, 이는 특정 수식에 요약되어 있다. 반면, 다수의 구조적 및 optimization hyperparameter에 대한 의존성은 매우 약하다. 이러한 스케일링은 power-law scaling이므로, 규모가 증가할수록 효과는 점점 감소한다.&lt;/p>
&lt;p>parameter가 동시에 변할 때 손실이 특정 변수에 어떻게 의존하는지 정확하게 모델링할 수 있었다. 이를 통해 대형 언어 모델 학습시 계산 스케일링, 과적합의 크기, 조기 중단 단계, 데이터 요구 사항을 도출하였다. 이러한 스케일링 관계는 단순한 관찰을 넘어 예측 프레임워크를 제공하며, 이는 이상기체법칙과 유사하게 해석될 수 있다.&lt;/p>
&lt;p>스케일링 관계는 maximum likelihood loss를 가진 다른 생성 모델링 작업, 예를 들어 이미지, 오디오, 비디오 모델 등에도 적용될 것으로 예상된다. 현재로서는 어떤 결과가 자연 언어 데이터의 구조에 의존하고 어떤 것이 보편적인지 확실하지 않다. &amp;ldquo;thermodynamics&amp;quot;을 기반으로 하는 &amp;ldquo;statistical mechanics&amp;rdquo; 같은 이론적 프레임워크를 발견하는 것은 더욱 정확한 예측을 도출하고 scaling law의 한계를 체계적으로 이해하는 데 도움이 될 것이다.&lt;/p>
&lt;p>자연어 분야에서는 손실의 지속적인 개선이 실제 언어 작업에 대한 개선으로 이어지는지 확인하는 것이 중요하다. &amp;ldquo;more is different&amp;quot;라는 말처럼, 부드러운 양적 변화는 실질적인 질적 개선을 가릴 수 있다. 경제 성장이나 언어 모델 손실의 부드러운 개선 뒤에는 특정 기술 개발이나 능력의 질적 변화가 숨겨져 있을 수 있다.&lt;/p>
&lt;p>이 논문의 연구 결과는 더 큰 모델이 계속해서 성능을 개선하며, 이전에 알려진 것보다 샘플 사용 효율이 높을 것이라는 강력한 추정을 제공한다. 이에 따라 모델 병렬화에 대한 추가 연구가 필요하며, 깊은 모델은 파이프라이닝을 활용한 학습, 넓은 네트워크는 병렬화를 통한 학습이 가능하다는 것을 확인하였다. 뿐만 아니라, 희소성 또는 분기를 활용하면 큰 네트워크의 빠른 학습이 가능하며, 네트워크를 학습하면서 확장하는 방법을 사용하면 전체 학습 과정에서 계산 효율성을 유지할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ZeRO</title><link>https://kurtkim.github.io/p/zero/</link><pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/zero/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>large deep learning 모델은 높은 정확도를 제공하지만, 기존의 데이터 및 모델 병렬화 솔루션은 메모리 적합성에 한계를 보인다. 따라서 Zero Redundancy Optimizer(ZeRO)를 개발하여 메모리를 최적화하고, 학습 속도를 향상시키며, 효율적으로 학습할 수 있는 모델 크기를 증가시켰다. ZeRO는 메모리 중복을 제거하고, 낮은 통신량과 높은 계산 세밀도를 유지하면서 모델 크기를 확장할 수 있는 가능성을 보여준다.&lt;/p>
&lt;p>ZeRO를 구현하고 평가했는데, 이는 400개의 GPU에서 super-linear 속도 향상을 통해 100B 이상의 parameter를 가진 대형 모델을 학습시키며, 15 Petaflops의 처리량을 달성하였다. 이는 기존 기술에 비해 8배의 모델 크기 증가와 10배의 성능 향상을 보여준다. 또한, ZeRO는 모델 병렬화 없이 최대 13B parameter의 대형 모델을 학습시킬 수 있다. 마지막으로, 연구자들은 ZeRO를 사용하여 기록적인 정확도를 가진 세계 최대의 언어 모델을 만들었다.&lt;/p>
&lt;hr>
&lt;h2 id="extended-introduction">Extended Introduction&lt;/h2>
&lt;p>딥러닝 모델이 점점 커지면서 정확도가 크게 향상되고 있다. transformer는 자연어 처리 분야에서 대형 모델의 발전을 이끌었지만, 이런 모델들은 단일 장치의 메모리에 들어갈 수 없어 학습시키는 데 어려움이 있다. 더 많은 장치를 추가하는 것만으로는 이 문제를 해결할 수 없다.&lt;/p>
&lt;p>기본 데이터 병렬화는 장치당 메모리를 줄이지 않아, 큰 모델에 대해 메모리 부족 문제가 발생한다. 파이프라인 병렬화, 모델 병렬화, CPU 오프로딩 등의 기존 해결책들은 각종 효율성과 기능성, 사용성 사이에서 타협을 이루어야 하지만, 이들 모두가 학습의 속도와 규모를 위해 중요하다.&lt;/p>
&lt;p>거대 모델 학습에 가장 유망한 방법인 모델 병렬화(Model Parallelism, MP)는 모델을 수직으로 분할하여 여러 장치에 분배한다. 이 방법은 단일 노드에서 잘 작동하지만, 노드를 넘어서면 효율성이 빠르게 저하된다. 이 논문의 실험에서는, 두 개의 DGX-2 노드에서 40B parameter 모델을 테스트했을 때, V100 GPU당 약 5 Tflops(하드웨어 피크의 5% 미만)의 성능을 보여주었다.&lt;/p>
&lt;p>거대 모델을 더 효율적으로 학습하기 위해, 기존 시스템의 메모리 소비를 분석하고, 모델 상태와 잔여 상태 두 부분으로 분류한다. 모델 상태는 메모리의 대부분을 차지하며, 잔여 상태는 나머지 메모리를 차지한다. 이 두 부분 모두에서 메모리 효율성을 최적화하면서 높은 계산 및 통신 효율성을 얻기 위해 ZeRO를 개발하였다. 이 두 부분은 각각 다른 도전과제에 직면하므로, 각각에 대한 해결책을 개발하고 논의하였다.&lt;/p>
&lt;p>&lt;strong>Optimizing Model State Memory&lt;/strong> 모델 학습 중 메모리의 대부분을 차지하는 모델 상태를 효율적으로 관리하기 위해, ZeRO-DP를 개발하였다. 기존의 DP와 MP 방식의 한계를 극복하고자, ZeRO-DP는 모델 상태를 복제하는 대신 분할하여 메모리 상태 중복을 제거하고, 동시에 계산/통신 효율성을 유지하기 위해 동적 통신 일정을 사용한다. 이를 통해 DP의 계산/통신 효율성과 MP의 메모리 효율성을 모두 달성하려 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure1.png"
width="902"
height="408"
srcset="https://kurtkim.github.io/p/zero/images/figure1_hu9936e1ae399565e04cfe2d7e4c603859_79114_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure1_hu9936e1ae399565e04cfe2d7e4c603859_79114_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="530px"
>&lt;/p>
&lt;p>ZeRO-DP는 옵티마이저 상태, 그래디언트, 파라미터를 분할하는 세 가지 주요 최적화 단계를 순차적으로 활성화한다.&lt;/p>
&lt;ol>
&lt;li>Optimizer State Partitioning ($P_os$): 메모리 감소량 4배, DP와 동일한 통신 볼륨&lt;/li>
&lt;li>Add Gradient Partitioning ($P_{os+g}$): 메모리 감소량 8배, DP와 동일한 통신 볼륨&lt;/li>
&lt;li>Add Parameter Partitioning ($P_{os+g+p}$): 메모리 감소량이 DP 정도 $N_d$와 선형적이다.&lt;/li>
&lt;/ol>
&lt;p>예를 들어, 64개의 GPU($N_d = 64$)에 걸쳐 분할하면 메모리 감소량이 64배가 된다. 통신 볼륨은 50% 증가한다.&lt;/p>
&lt;p>ZeRO-DP는 메모리 중복을 제거하여 클러스터의 전체 메모리를 활용하게 한다. 세 단계 모두 활성화하면 ZeRO는 1024개의 NVIDIA GPU만으로 1조 개의 파라미터 모델을 학습할 수 있다. 이는 각 GPU가 대략 16GB의 메모리를 사용하게 되며, 이는 대부분의 GPU(예: 32GB 메모리를 가진 GPU)가 감당할 수 있는 범위이다.&lt;/p>
&lt;p>&lt;strong>Optimizing Residual State Memory&lt;/strong> ZeRO-DP가 모델 메모리 효율성을 향상시킨 후에도, 활성화, 임시 버퍼, 사용 불가능한 메모리 조각 등으로 인해 두 번째 메모리 병목 현상이 발생할 수 있다. 이를 해결하기 위해, 각 요소에 의한 잔여 메모리를 최적화하는 ZeRO-R을 개발하였다.&lt;/p>
&lt;ol>
&lt;li>activation에 대해, 체크포인팅이 도움이 되지만 큰 모델에는 충분하지 않다는 것을 발견했다. 그래서 ZeRO-R은 activation partitioning을 통해 기존 MP 방법에서 activation 복제를 식별하고 제거함으로써 activation 메모리를 최적화한다. 또한 적절할 때 CPU로 활성화를 오프로드한다.&lt;/li>
&lt;li>ZeRO-R은 메모리와 계산 효율성 사이의 균형을 위해 임시 버퍼의 적절한 크기를 정의한다.&lt;/li>
&lt;li>학습 중에 다른 텐서의 수명 차이로 인해 메모리가 파편화되는 것을 관찰하였다. 파편화로 인한 연속적인 메모리 부족은 충분한 여유 메모리가 있음에도 불구하고 메모리 할당 실패를 일으킬 수 있다. ZeRO-R은 텐서의 다른 수명에 기반하여 메모리를 적극적으로 관리함으로써 메모리 파편화를 방지한다.&lt;/li>
&lt;/ol>
&lt;p>ZeRO-DP와 ZeRO-R을 결합하여 ZeRO라는 딥러닝 학습용 메모리 최적화 시스템을 구성한다.&lt;/p>
&lt;p>&lt;strong>ZeRO and MP:&lt;/strong> ZeRO는 데이터 병렬처리의 메모리 비효율성을 제거하므로, 모델 병렬처리(MP)의 필요성이 줄어든다. ZeRO-DP는 MP와 비교해서 장치당 메모리 사용량을 적어도 같게 줄이거나, 때로는 더 효과적으로 줄일 수 있다. 또한, 스케일링 효율성도 비슷하거나 더 좋다. 데이터 병렬처리는 쉽게 사용할 수 있어 다양한 작업에 적용할 수 있지만, MP는 모델과 시스템 개발자의 추가 작업이 필요하며, 제한된 연산자와 모델만 지원한다.&lt;/p>
&lt;p>ZeRO-R과 함께 사용하면, MP는 매우 큰 모델의 활성화 메모리 사용량을 줄일 수 있으며, 활성화 메모리가 문제가 아닌 작은 모델에서는 DP만을 이용한 배치 크기가 너무 클 경우 MP가 이점을 가질 수 있다. 이런 경우, ZeRO를 MP와 결합하여 적절한 배치 크기로 모델을 적용할 수 있다.&lt;/p>
&lt;p>ZeRO는 MP와 결합될 수 있으며, 이는 각 장치에서 최대 이론적 메모리 감소를 $N_d \times N_m$배 달성하는 결과를 가져온다. 이를 통해, 1024개의 GPU 1 trillion 개의 parameter 모델을 효율적으로 운영할 수 있게 된다. 이는 16-방향 모델 병렬처리와 64-방향 데이터 병렬처리를 이용하며, 적당한 배치 크기를 사용한다.&lt;/p>
&lt;p>&lt;strong>Implementation &amp;amp; Evaluation&lt;/strong> ZeRO의 최적화 세트는 1 trillion 개의 parameter를 가진 모델을 고급 하드웨어 클러스터에서 실행할 수 있지만, 계산 능력의 한계와 긴 학습 시간 때문에 실제 적용이 어렵다. 그래서 현재 하드웨어의 계산 능력 범위 내에서 최첨단보다 10배 많은 parameter(약 100B 개의 parameter)를 효율적으로 지원하는 것에 초점을 맞추었다. 이를 위해 ZeRO-DP의 $P_{os+g}$와 ZeRO-R을 결합한 ZeRO의 최적화 하위 집합인 ZeRO-100B를 구현하고 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure2.png"
width="1168"
height="492"
srcset="https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>Model Size: MP와 결합된 ZeRO-100B는 170B 개의 parameter 모델을 효율적으로 실행할 수 있다. 반면, Megatron만을 사용하는 기존 시스템은 40B 개의 parameter 이상으로 효율적으로 확장할 수 없다. 이는 state-of-the-art 대비 모델 크기가 8배 이상 증가한 것이다.&lt;/p>
&lt;p>Speed: 향상된 메모리 효율성은 처리량을 높이고 학습 속도를 빠르게 한다. ZeRO는 400개의 Nvidia V100 GPU 클러스터에서 100B 개의 parameter 모델을 GPU당 38TFlops, 총 15Petaflops의 성능으로 실행한다. 이는 동일한 모델 크기에 대해 state-of-the-art 대비 학습 속도를 10배 이상 향상시킨다.&lt;/p>
&lt;p>Scalability: 64-400개의 GPU 영역에서 GPU의 수를 두 배로 늘릴 때 성능이 두 배 이상 향상되는 슈퍼 선형 속도 향상을 관찰하였다. 이는 ZeRO-DP의 특성으로, DP 차수를 늘릴수록 모델의 메모리 사용량이 줄어들고 GPU 당 더 큰 배치 크기를 적용할 수 있게 되어 성능을 향상시킨다. 400개를 넘는 GPU 수를 더 늘릴 경우 이런 행동이 계속될 것으로 예상한다.&lt;/p>
&lt;p>Democratization of Large Model Training: ZeRO-100B는 데이터 과학자들이 모델 리팩토링을 필요로 하는 MP나 PP 없이 최대 13B 개의 parameter로 모델을 학습할 수 있게 한다. 이를 통해 데이터 과학자들은 병렬 처리에 대해 걱정하지 않고 큰 모델로 자유롭게 실험할 수 있다. 반면, 기존 시스템들은 1.4B 개의 parameter 모델에서 메모리가 부족해진다.&lt;/p>
&lt;p>New SOTA Model: ZeRO는 17B 개의 parameter를 가진 가장 큰 언어 모델인 Turing-NLG를 지원하며, 이는 기록적인 정확도를 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="data-model-and-pipeline-parallelism">Data, Model and Pipeline Parallelism&lt;/h3>
&lt;p>병렬화는 대규모 모델 학습에 필수적인 전략이다. data parallelism(DP)를 통해 모델은 여러 장치로 확장되며, 모델의 parameter는 각 장치에 복제된다. 각 단계에서 미니 배치는 프로세스 간에 나누어지고, 각 프로세스는 다른 데이터 샘플에서 forward 및 backward propagation를 수행한다. 프로세스 간의 averaged gradient를 사용해 로컬에서 모델이 업데이트된다.&lt;/p>
&lt;p>장치 메모리에 맞지 않는 모델의 경우, model parallelism(MP)과 pipeline parallelism(PP)이 모델을 프로세스 간에 수직 및 수평으로 분할한다.&lt;/p>
&lt;p>pipeline parallelism(PP)은 모델을 층간에 수평적으로 분할하고, 마이크로 배치를 이용해 파이프라인 버블을 숨긴다. 하지만 이 방식은 모델 기능의 구현을 어렵게 만들고, 큰 배치 크기와 상당한 메모리를 필요로 한다. 또한, 표준 딥러닝 학습과는 다르며, 학습 수렴에 영향을 미치는 단점이 있습니다. 반면에, ZeRO는 이러한 PP의 제한 없이 같거나 더 나은 메모리 효율성을 제공한다.&lt;/p>
&lt;h3 id="non-parallelism-based-approach-to-reduce-memory">Non-parallelism based approach to reduce memory&lt;/h3>
&lt;p>model parallelism(MP)과 pipeline parallelism(PP) 외에도, 딥러닝 학습의 메모리 오버헤드를 줄이는데 목표를 두고 있는 여러 연구가 있다.&lt;/p>
&lt;h4 id="reducing-activation-memory">Reducing Activation Memory&lt;/h4>
&lt;p>활성화의 메모리 사용량을 줄이기 위한 여러 방법들이 있으며, 이에는 압축, 활성화 체크포인팅, 라이브 분석 등이 포함된다. 이러한 방법들은 ZeRO와 서로 보완적으로 작동하며, 특히 ZeRO-R의 활성화 메모리 감소는 활성화 체크포인팅과 병렬로 진행된다.&lt;/p>
&lt;h4 id="cpu-oﬄoad">CPU Oﬄoad&lt;/h4>
&lt;p>컴퓨팅 노드의 heterogeneous를 활용해 모델 상태를 CPU 메모리로 오프로드하는 방식이 있다. 학습 시간의 절반 가량이 GPU-CPU-GPU 전송에 소요되지만, ZeRO는 이와 달리 PCI-E로 인한 제한 때문에 CPU 메모리에 모델 상태를 저장하지 않고도 메모리 사용을 크게 줄인다. 드물게, 성능 향상을 위해 ZeRO-R는 매우 큰 모델의 활성화 체크포인트만 오프로드할 수 있다.&lt;/p>
&lt;h4 id="memory-eﬃcient-optimizer">Memory Eﬃcient Optimizer&lt;/h4>
&lt;p>모델 parameter와 gradient의 대략적인 통계를 유지하면서 adaptive optimization 방법의 메모리 사용량을 줄이는 방법들이 있다. 이는 모델 수렴에 영향을 미칠 수 있다. 그러나 ZeRO는 이와 별개로, 모델 최적화 방법이나 모델 수렴에 영향을 주지 않으면서, 최적화 상태와 장치별 gradient의 메모리 사용량을 효과적으로 줄인다.&lt;/p>
&lt;h3 id="training-optimizers">Training Optimizers&lt;/h3>
&lt;p>adaptive optimization 방법들은 큰 모델의 효과적인 학습을 위해 성능과 정확도를 최적화하는데 중요하다. 각 모델 parameter와 gradient에 대한 세밀한 통계를 유지하면서 메모리 사용량이 상당히 증가하는데, ZeRO는 이러한 최적화 도구의 메모리 사용량을 크게 줄여 소형 장치 메모리를 가진 하드웨어에서도 큰 모델 학습이 가능하게 한다. 이는 더 복잡하고 메모리를 많이 사용하는 최적화 도구의 개발 및 사용을 가능하게 한다.&lt;/p>
&lt;hr>
&lt;h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?&lt;/h2>
&lt;p>예를 들어, 1.5B 매개변수의 GPT-2 모델은 16비트 정밀도에서 가중치에 3GB의 메모리를 필요로 한다. 그러나 이는 32GB 메모리를 가진 단일 GPU에서 Tensorflow나 PyTorch를 사용하여 학습할 수 없다. 모델 학습 중에는 대부분의 메모리가 pptimizer states, gradient, parameter로 구성된 모델 상태에 의해 소비되며, 나머지 메모리는 activations, temporary buffer, fragmented memory에 의해 소비된다.&lt;/p>
&lt;h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters&lt;/h3>
&lt;p>학습 중에는 대부분의 메모리가 모델 상태에 소비되며, 그 중에서도 Adam과 같은 최적화 도구가 가장 많은 메모리를 차지한다. Adam은 업데이트를 계산하기 위해 gradient의 시간 평균 모멘텀과 분산을 저장해야 하기 때문이다. 따라서, 모델을 훈련시키려면 이 두 가지, 그리고 gradient와 가중치 자체를 저장할 충분한 메모리가 필요하다. 이 세 가지 요소 중에서도, 최적화 상태가 특히 많은 메모리를 차지하다.&lt;/p>
&lt;p>&lt;strong>Mixed-Precision Training&lt;/strong> 현재 NVIDIA GPU에서 큰 모델을 학습시키는 state-of-the-art 방법은 mixed-precision(fp16/32) 학습을 사용하는 것이다. 이 방법은 parameter와 activation을 fp16으로 저장하고, GPU의 고처리량 텐서 코어 유닛을 활용합니다. forward 및 backward propagation는 fp16 가중치와 활성화를 사용하여 수행되지만, backward propagation 끝단에서의 업데이트 계산과 적용을 위해, mixed-precision 최적화 도구는 parameter와 다른 최적화 상태들의 fp32 복사본을 유지한다.&lt;/p>
&lt;p>$\Psi$ parameter를 가진 모델의 mixed-precision 학습에서는, parameter와 gradient의 fp16 복사본, 그리고 최적화 상태인 parameter, 모멘텀, 분산의 fp32 복사본을 저장할 충분한 메모리가 필요하다. 이를 총합하면, $16 \Psi$ 바이트의 메모리 요구사항이 발생한다. 예를 들어, 1.5 B parameter를 가진 GPT-2 모델의 경우, 최소 24GB의 메모리가 요구되며, 이는 단독으로 fp16 parameter를 저장하는 데 필요한 3GB의 메모리보다 훨씬 많다.&lt;/p>
&lt;h3 id="residual-memory-consumption">Residual Memory Consumption&lt;/h3>
&lt;p>&lt;strong>Activations&lt;/strong> 학습 중에 활성화는 상당한 메모리를 차지할 수 있다. 예를 들어, 1.5B parameter GPT-2 모델은 약 60GB의 메모리를 필요로 한ㄴ다. 그러나 활성화 체크포인팅을 사용하면 활성화 메모리를 전체 활성화의 제곱근 정도로 줄일 수 있다. 이 방법을 사용하면 이 모델의 활성화 메모리 소비는 대략 8GB로 줄어들게 된다.&lt;/p>
&lt;p>활성화 체크포인팅을 사용하더라도, 큰 모델들은 활성화 메모리가 매우 커질 수 있다. 예를 들어, 100B 개의 parameter를 가진 GPT와 같은 모델은 배치 크기 32일 때 약 60GB의 메모리가 필요하다.&lt;/p>
&lt;p>&lt;strong>Temporary buffers&lt;/strong> 큰 모델에서는 중간 결과를 저장하기 위한 메모리가 상당한 양을 차지한다. gradient all-reduce나 gradient norm 계산과 같은 연산은 처리량 향상을 위해 모든 gradient를 하나의 플랫 버퍼로 병합한다. 그러나 이 병합된 버퍼는 연산에 따라 fp32 텐서가 될 수 있어, 큰 모델에서는 이 임시 버퍼 크기가 중요하다. 예를 들어, 1.5B parameter를 가진 모델에서는 플랫한 fp32 버퍼가 6GB의 메모리를 필요로 한다.&lt;/p>
&lt;p>&lt;strong>Memory Fragmentation:&lt;/strong> 학습 중 실제 메모리 사용량 외에도, 메모리 단편화로 인해 충분한 메모리가 있음에도 불구하고 메모리 부족 상황이 발생할 수 있다. 연속적인 메모리가 부족하면, 요청된 메모리보다 전체 사용 가능 메모리가 더 크더라도 메모리 요청이 실패할 수 있다. 큰 모델을 학습할 때는 이러한 메모리 단편화가 상당히 발생하며, 극단적인 경우에는 30% 이상의 메모리가 남아있음에도 메모리 부족 문제가 발생할 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="zero-insights-and-overview">ZeRO: Insights and Overview&lt;/h2>
&lt;p>ZeRO는 모델 상태의 메모리 사용량을 줄이는 ZeRO-DP와 잔여 메모리 소비를 줄이는 ZeRO-R, 두 가지 최적화 세트를 가지고 있다. 이는 ZeRO가 효율성을 유지하면서 메모리 사용량을 줄이는 데 도움이 된다. 효율성이 핵심인 만큼, 이 제약 없이는 모든 parameter 상태를 CPU 메모리로 이동하거나, MP 정도를 임의로 늘리는 등의 간단한 방법으로 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;h3 id="insights-and-overview-zero-dp">Insights and Overview: ZeRO-DP&lt;/h3>
&lt;p>ZeRO에서 구동되는 DP는 세 가지 key insight에 기반을 두고 있다:&lt;/p>
&lt;p>a) MP는 계산의 세분성을 줄이고 통신 오버헤드를 증가시키므로, DP는 MP보다 확장 효율성이 더 높다. 특정 지점을 넘어서면 GPU의 효율성이 줄어들고, 통신 오버헤드가 증가하여 GPU 간의 확장성이 제한된다. 반면에, DP는 더 높은 계산 세분성과 더 낮은 통신 볼륨을 가지므로 훨씬 높은 효율성을 제공한다.&lt;/p>
&lt;p>b) DP는 모델 상태가 모든 데이터 병렬 처리에서 중복으로 저장되므로 메모리 효율성이 떨어진다. 반면에, MP는 모델 상태를 분할하여 메모리 효율성을 얻는다.&lt;/p>
&lt;p>c) DP와 MP 모두 전체 학습 과정 동안 필요한 모든 모델 상태를 유지하지만, 항상 모든 것이 필요한 것은 아니다. 예를 들어, 각 레이어에 해당하는 매개변수는 레이어의 전파와 역전파 동안에만 필요하다.&lt;/p>
&lt;p>ZeRO-DP는 DP의 학습 효율성을 유지하면서 MP의 메모리 효율성을 달성한다. 모델 상태를 복제하는 대신 분할하고, 통신 볼륨을 최소화하면서 모델 상태의 시간적인 특성을 활용하는 동적 통신 일정을 사용한다. 이를 통해 ZeRO-DP는 모델의 기기당 메모리 사용량을 선형적으로 줄이면서도 통신 볼륨을 기본 DP 수준에 유지하며 효율성을 유지한다.&lt;/p>
&lt;h3 id="insights-and-overview-zero-r">Insights and Overview: ZeRO-R&lt;/h3>
&lt;h4 id="reducing-activation-memory-1">Reducing Activation Memory&lt;/h4>
&lt;p>a) MP는 모델 상태를 분할하지만 활성화 메모리의 복제가 종종 필요하다. 예를 들어, 선형 레이어의 parameter를 분할하여 병렬 계산을 할 경우, 각 GPU는 전체 활성화를 필요로 한다.&lt;/p>
&lt;p>b) GPT-2 또는 그보다 큰 모델들은 산술 강도가 매우 크며(≥ 10K), 이는 숨겨진 차원과 선형적으로 증가한다. 이로 인해 대역폭이 낮아도 활성화 체크포인트의 데이터 이동 비용을 숨길 수 있다.&lt;/p>
&lt;p>ZeRO는 GPU 간에 활성화 체크포인트를 분할하여 MP의 메모리 중복을 제거하고, 필요에 따라 그것들을 allgather를 이용해 재구성한다. 이로 인해 활성화 메모리 사용량은 MP 정도에 비례하여 감소하며, 매우 큰 모델에서는 산술 강도가 큰 덕분에 활성화 파티션을 CPU 메모리로 이동시키면서도 좋은 효율성을 유지할 수 있다.&lt;/p>
&lt;h4 id="managing-temporary-buﬀers">Managing Temporary buﬀers&lt;/h4>
&lt;p>ZeRO-R은 모델 크기가 증가함에 따라 임시 버퍼가 급증하는 것을 피하기 위해 일정한 크기의 버퍼를 사용하면서도 충분히 크게 만들어 효율성을 유지한다.&lt;/p>
&lt;h4 id="managing-fragmented-memory">Managing fragmented Memory&lt;/h4>
&lt;p>memory fragmentation는 단기와 장기 메모리 사이의 교차 때문에 발생한다. ZeRO는 이 통찰을 바탕으로, 활성화 체크포인트와 gradient를 미리 할당된 연속 메모리 버퍼로 이동시키는 실시간 메모리 defragmentation을 수행한다. 이로 인해 메모리 사용 가능성이 증가하고, 메모리 할당자가 연속적인 무료 메모리를 찾는 시간이 줄어들어 효율성이 향상된다.&lt;/p>
&lt;hr>
&lt;h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP&lt;/h2>
&lt;p>기존 DP 방식은 각 장치에서 모델 상태를 복제하여 메모리 오버헤드를 발생시키지만, ZeRO-DP는 이런 메모리 중복을 제거하기 위해 데이터 병렬 프로세스간에 optimizer state, gradient, parameter를 분할한다. 이들은 ZeRO-DP의 세 가지 최적화 단계인 $P_{os}$, $P_g$, $P_p$로 참조된다.&lt;/p>
&lt;h3 id="p_os-optimizer-state-partitioning">$P_{os}$: Optimizer State Partitioning&lt;/h3>
&lt;p>DP 정도가 $N_d$인 경우, 최적화 상태를 $N_d$개의 동일한 파티션으로 나누어, $i$번째 데이터 병렬 프로세스가 $i$번째 파티션에 해당하는 최적화 상태만 업데이트하도록 한다. 결과적으로, 각 데이터 병렬 프로세스는 전체 최적화 상태의 $N_d$만을 저장하고 업데이트하며 parameter의 $N_d$만 업데이트한다. 각 학습 단계의 끝에서는 모든 데이터 병렬 프로세스에서 완전히 업데이트된 parameter를 얻기 위해 all-gather를 수행한다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> 최적화 상태 파티션 후의 메모리 소비는 $4 \Psi + K \Psi$에서 $4 \Psi + {{K \Psi}\over{N_d}}$ 로 줄어든다. 예를 들어, 7.5B parameter 모델은 64-방향 DP를 사용할 때 31.4GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 또한, $N_d$가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 4배 감소한다.&lt;/p>
&lt;h3 id="p_g-gradient-partitioning">$P_g$: Gradient Partitioning&lt;/h3>
&lt;p>각 데이터 병렬 프로세스는 자신에 해당하는 parameter 파티션만 업데이트하므로, 해당 parameter에 대한 감소된 gradient만 필요하다. 역전파 동안 각 계층의 각 gradient가 사용 가능해지면, 해당 parameter를 업데이트하기 위한 데이터 병렬 프로세스에서만 그것들을 줄이고, gradient를 더 이상 필요하지 않으면 메모리를 해제한다. 이로 인해 gradient를 저장하는 데 필요한 메모리 사용량이 $2\Psi$ 바이트에서 $N_d$로 줄어든다.&lt;/p>
&lt;p>Reduce-Scatter 연산을 통해 다른 parameter에 대응하는 gradient들이 다른 프로세스로 줄어든다. 효율성을 높이기 위해 특정 파티션에 대응하는 모든 gradient를 버킷화하여 한 번에 처리하는 버킷화 전략을 사용한다. 이는 NVIDIA의 AMP 최적화기가 통신과 계산을 겹치게 하기 위해 gradient 계산을 버킷화하는 방식과 유사하다. 메모리 사용량을 줄이고 계산과 통신을 겹치게 하기 위해, 파티션 경계에서 all-reduce 대신 감소를 수행한다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> gradient와 optimizer state 중복을 제거하여 메모리 사용량을 $2 \Psi + {{14 \Psi}\over{N_d}} \approx 2 \Psi$로 줄일 수 있다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 P_{os+g}로만 16.6 GB의 메모리를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. N_d가 큰 경우, 모델 상태에 대한 메모리 요구량은 약 8배 감소한다.&lt;/p>
&lt;h3 id="p_p-parameter-partitioning">$P_p$: Parameter Partitioning&lt;/h3>
&lt;p>각 프로세스는 자신의 파티션에 해당하는 parameter만 저장하며, 필요한 경우 다른 parameter는 데이터 병렬 프로세스로부터 브로드캐스트를 통해 받아온다. 이 방법은 통신량을 1.5배로 증가시키지만, $N_d$에 비례해 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;p>&lt;strong>Memory Savings:&lt;/strong> parameter 분할을 통해, $Psi$ parameter 모델의 메모리 사용량은 $16 \Psi$에서 ${{16 \Psi}\over{N_d}}$로 줄어든다. 예를 들어, 7.5 B parameter 모델은 64-방향 DP를 사용하면 모델 상태 메모리 1.9 GB를 사용하지만, 표준 DP를 사용하면 120 GB를 사용한다. 이는 ZeRO가 모델 상태를 공유할 충분한 장치가 있다면 어떠한 크기의 모델에도 DP를 적용할 수 있다는 것을 의미한다.&lt;/p>
&lt;h3 id="implication-on-model-size">Implication on Model Size&lt;/h3>
&lt;p>데이터 병렬 프로세스의 모델 상태에 대한 메모리 사용량을 줄이기 위한 파티셔닝의 세 단계 $P_{os}$, $P_{os+g}$, $P_{os+g+p}$는 각각 메모리 사용량을 최대 4배, 8배, 그리고 $N_d$로 줄인다. ZeRO 최적화를 사용하면, $N_d = 64$일 때는 최대 128B parameter의 모델을, $N_d = 1024$일 때는 최대 1 trillion parameter의 모델을 학습시킬 수 있다. ZeRO를 사용하지 않으면, 최대 1.5B parameter의 모델만 학습시킬 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R&lt;/h2>
&lt;h3 id="p_a-partitioned-activation-checkpointing">$P_a$: Partitioned Activation Checkpointing&lt;/h3>
&lt;p>model parallel(MP)은 설계상 활성화의 복제를 요구하며, 이로 인해 GPU 전체에 활성화의 중복 복사본이 생긴다. ZeRO는 이 중복성을 제거하기 위해 활성화를 분할하고, 계산에 사용되기 직전에만 복제 형태로 재구성한다. 이를 $P_a$라고 하며, 활성화 체크포인팅과 함께 작동하여, 복제 복사본 대신 분할된 활성화 체크포인트를 저장한다. 매우 큰 모델과 제한된 디바이스 메모리의 경우, 이 분할된 체크포인트는 CPU로 오프로드될 수 있어, 활성화 메모리 오버헤드를 거의 없앨 수 있다. 이를 $P_{a+cpu}$라고 한다.&lt;/p>
&lt;p>&lt;strong>Memory Saving&lt;/strong> ZeRO는 파티셔닝된 활성화 체크포인팅을 사용하여 활성화 메모리 사용량을 model parallel(MP) 차수에 비례하여 줄인다. 예를 들어, 100B 개의 parameter를 가진 모델을 학습시키는 경우, 각 transformer layer마다 활성화를 체크포인트하면 GPU 당 약 33GB의 메모리가 필요하다. 하지만 ZeRO의 $P_a$를 사용하면, 이 메모리 요구량을 GPU 당 약 2GB로 줄일 수 있고, 이 2GB는 CPU로 오프로드되어 활성화에 대한 메모리 사용량을 거의 제로로 만든다.&lt;/p>
&lt;h3 id="c_b-constant-size-buﬀers">$C_B$: Constant Size Buﬀers&lt;/h3>
&lt;p>ZeRO는 메모리와 계산 효율성 사이의 균형을 위해 임시 데이터 버퍼의 크기를 조정한다. 학습 중에는 입력 크기가 커질수록 연산의 효율성이 향상되는 경우가 많다. 그러나, 모든 parameter수를 결합한 버퍼의 메모리 오버헤드는 모델 크기에 비례하기 때문에 문제가 될 수 있다. 이 문제를 해결하기 위해, 모델 크기가 큰 경우에는 성능 효율적인 고정 크기의 결합 버퍼를 사용한다. 이렇게 하면 버퍼 크기가 모델 크기에 의존하지 않게 되어, 메모리 사용량을 줄이면서도 계산 효율성을 유지할 수 있다.&lt;/p>
&lt;h3 id="m_d-memory-defragmentation">$M_D$: Memory Defragmentation&lt;/h3>
&lt;p>모델 학습 중에는 활성화 체크포인팅과 gradient 계산으로 인해 메모리 단편화가 발생한다. forward propagation 동안에는 선택적으로 저장되는 활성화와 대부분 버려지는 활성화 사이에 메모리가 교차되어 단편화가 발생하며, back propagation 동안에도 장기 메모리인 parameter gradient와 단기 메모리인 activation gradient 및 gradient 계산에 필요한 다른 버퍼들 사이에 메모리가 교차되어 단편화가 발생한다.&lt;/p>
&lt;p>메모리가 충분할 때는 메모리 단편화가 크게 문제가 되지 않지만, 메모리가 제한된 상태에서 큰 모델을 학습할 때는 두 가지 문제가 발생한다. 첫째, 충분한 메모리가 있음에도 연속적인 메모리 부족으로 인해 메모리 부족 오류(Out of Memory, OOM)가 발생하고, 둘째, 메모리 할당기가 연속적인 메모리 조각을 찾는데 많은 시간을 소비하여 효율성이 저하된다.&lt;/p>
&lt;p>ZeRO는 활성화 체크포인트와 gradient를 위해 미리 연속적인 메모리 덩어리를 할당하고, 이들이 생성될 때 미리 할당된 메모리로 복사하여 메모리 단편화를 실시간으로 제거한다. $M_D$(Memory Defragmentation)는 ZeRO가 더 큰 모델을 더 큰 배치 크기로 학습할 수 있게 만들 뿐만 아니라, 제한된 메모리로 학습할 때 효율성을 향상시킨다.&lt;/p>
&lt;hr>
&lt;h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP&lt;/h2>
&lt;p>ZeRO는 메모리 중복성을 제거하여 모델 크기를 증가시키는데, 이로 인해 메모리 효율성을 위해 통신 볼륨을 교환하고 있는지 의문이 생긴다. 즉, 기본 데이터 병렬화 방법에 비해 ZeRO가 향상된 데이터 병렬화 방법의 통신 볼륨은 어느 정도일까? 이에 대한 답은 두 부분이다. 첫째, ZeRO-DP는 메모리를 최대 8배 줄이면서 추가 통신을 발생시키지 않는다. 둘째, ZeRO-DP는 메모리 사용량을 추가로 줄이면서 최대 1.5배의 통신을 발생시킨다. 이 분석은 표준 데이터 병렬화의 통신 볼륨에 대한 간단한 개요로 시작한다.&lt;/p>
&lt;h3 id="data-parallel-communication-volume">Data Parallel Communication Volume&lt;/h3>
&lt;p>데이터 병렬 학습에서는 모든 데이터 병렬 프로세스의 gradient가 backward propagation이 끝날 때 평균화된다. 이 평균화는 all-reduce 통신을 통해 이루어지며, 모델 크기가 큰 경우, 이 통신은 통신 대역폭에 의해 제한된다. 따라서 분석은 각 데이터 병렬 프로세스로부터 보내고 받는 총 통신 볼륨에 초점을 맞춘다.&lt;/p>
&lt;p>state-of-art의 all-reduce 구현은 두 단계로 이루어진다. 첫번째 단계는 reduce-scatter 연산으로, 다른 프로세스에서 데이터의 다른 부분을 축소하고, 다음 단계는 all-gather 연산으로, 각 프로세스가 모든 프로세스에서 축소된 데이터를 수집한다. 이 두 단계의 결과는 all-reduce이다. 각 단계는 파이프라인 방식으로 구현되며, 이로 인해 데이터 이동이 발생한다. 따라서 표준 데이터 병렬화는 각 학습 단계마다 $2 \Psi$의 데이터 이동을 발생시킨다.&lt;/p>
&lt;h3 id="zero-dp-communication-volume">ZeRO-DP Communication Volume&lt;/h3>
&lt;h4 id="communication-volume-with-p_osg">Communication Volume with $P_{os+g}$&lt;/h4>
&lt;p>gradient 분할을 사용하는 ZeRO는 각 프로세스가 해당하는 parameter 분할을 업데이트하기 위해 필요한 gradient 부분만을 저장한다. gradient에 대해 scatter-reduce 연산을 수행하고, 모든 데이터 병렬 프로세스에서 업데이트된 parameter를 수집하기 위해 all-gather를 수행한다. 이 두 과정은 각각 통신 볼륨 $\Psi$를 발생시키므로, 학습 단계당 총 통신 볼륨은 $\Psi + \Psi = 2 \Psi$로, 기본 데이터 병렬화와 동일하다.&lt;/p>
&lt;h4 id="communication-volume-with-p_osgp">Communication Volume with $P_{os+g+p}$&lt;/h4>
&lt;p>parameter 분할 후에 각 데이터 병렬 프로세스는 자신이 업데이트하는 parameter만을 저장한다. 이로 인해 forward propagation 동안 다른 모든 분할의 parameter를 받아야 하지만, 파이프라인 방식을 통해 메모리 오버헤드를 피할 수 있다. 특정 분할에 대한 forward propagation를 계산하기 전에, 해당 분할의 가중치를 모든 데이터 병렬 프로세스에게 브로드캐스트하고, forward propagation가 완료되면 parameter를 버린다. 이로 인해 총 통신 볼륨은 $\Psi$이다. 그러나, 이 all-gather 작업은 backward propagation 동안 역순으로 다시 수행되어야 한다는 점에 주의해야 한다.&lt;/p>
&lt;p>총 통신 볼륨은 all-gather와 gradient의 reduce-scatter에 의해 발생하는 통신 볼륨의 합으로, 이는 기본값에 비해 1.5배인 $3 \Psi$이다. gradient와 parameter의 분할은 모든 상태가 항상 필요하지 않다는 점을 이용하여, 상태를 신중하게 통신함으로써 메모리를 최적화한다.&lt;/p>
&lt;hr>
&lt;h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R&lt;/h2>
&lt;p>ZeRO-R의 분할된 활성화 체크포인팅($P_a$)의 통신 볼륨은 기본 MP의 10분의 1 미만 증가하며, 이는 $P_a$의 통신 오버헤드를 분석하여 더 큰 배치 크기를 가능하게 하고 DP 통신을 줄여 효율성을 향상시키는 시나리오를 식별하는 데 사용된다. 이러한 분석은 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정하는 데 활용된다.&lt;/p>
&lt;p>활성화 체크포인트 분할의 통신 볼륨 트레이드오프는 모델 크기, 체크포인트 전략, 그리고 MP 전략에 따라 다르다. 이에 대한 구체적인 이해를 위해, 우리는 최신 MP 방식으로 구현된 transformer 기반 모델인 Megatron-LM을 사용하여 분석을 수행하였다.&lt;/p>
&lt;p>활성화 체크포인팅이 있는 Megatron-LM에서, 각 transformer 블록은 forward propagation, forward re-computation, backward propagation 각각에서 두 번씩 all-reduce 연산을 수행한다. 이는 $\text{batch} \times \text{seq length} \times \text{hidden dim}$ 차원의 크기를 가지다. 따라서 블록 당 총 통신 볼륨은 $12 \times \text{seq length} \times \text{hidden dim}$이 된다.&lt;/p>
&lt;p>ZeRO-R이 활성화 체크포인트를 분할할 때, back-propagation의 forward recomputation 전에 추가적인 all-gather 연산이 필요하다. 각 transformer 블록의 입력 활성화를 체크포인트로 설정하므로, transformer 블록 당 하나의 all-gather가 필요하다. 이로 인한 통신 오버헤드 $P_a$는 $\text{seq length} \times \text{hidden dim}$이고, 따라서 $P_a$의 총 통신 오버헤드는 모델 병렬화의 원래 통신 볼륨의 10% 미만이다.&lt;/p>
&lt;p>MP와 DP를 함께 사용할 때, $P_a$는 모델 병렬 통신 볼륨을 10% 증가시키는 대신 데이터 병렬 통신 볼륨을 크게 줄일 수 있다. 이는 데이터 병렬 통신이 성능의 병목이 될 때 효율성을 크게 향상시키는데 도움이 된다. 또한, $P_a$는 활성화 메모리 사용량을 줄이고 배치 크기를 비례적으로 증가시키므로, 큰 모델의 경우 배치 크기를 최대 16배까지 증가시킬 수 있다. 이로 인해, 데이터 병렬 통신 볼륨이 크게 감소할 수 있다.&lt;/p>
&lt;p>$P_{a+cpu}$가 적용되면, CPU로 오프로드된 분할된 활성화 체크포인트는 활성화 메모리 요구량을 거의 0으로 줄이면서, CPU 메모리로의 데이터 이동이 2배 증가한다. 배치 크기가 작아서 DP 통신 볼륨이 병목이 되는 경우에는, CPU 데이터 전송 오버헤드가 DP 통신 볼륨 오버헤드보다 작다면 $P_{a+cpu}$가 배치 크기를 늘려 효율성을 향상시킬 수 있다.&lt;/p>
&lt;p>모델과 하드웨어 특성을 고려하여, 위의 분석을 활용하여 $P_a$와 $P_{a+cpu}$를 언제 적용할지 결정한다.&lt;/p>
&lt;hr>
&lt;h2 id="step-towards-1-trillion-parameters">Step Towards 1 Trillion Parameters&lt;/h2>
&lt;p>현재 가장 큰 모델들은 이미 학습시키는 데 도전적인 100B 개의 parameter를 가지고 있다. 1 trillion 개의 parameter에 이르는 것은 불가피하지만, 그 과정은 많은 도전과 혁신을 필요로 할 것이다. ZeRO는 이러한 도전 중 하나인, 현재 하드웨어에서 대규모 모델을 효과적으로 학습시키는 능력을 개선하는 데 중점을 두고 있다.&lt;/p>
&lt;p>&lt;strong>A Leap from State-of-Art&lt;/strong> state-of-art 프레임워크인 Megatron은 DGX-2 시스템에서 160 - 20B 개의 parameter 모델을 효율적으로 학습시킬 수 있다. 하지만, 여러 DGX 노드 간의 모델 병렬화를 시도할 경우, 노드 간 대역폭 제한으로 효율성이 크게 감소한다.&lt;/p>
&lt;p>ZeRO는 효율적으로 실행 가능한 모델 크기를 크게 늘린다. 노드 경계를 넘어 세분화된 모델 병렬화가 필요하지 않은 현재 하드웨어에서 더 큰 모델을 실행할 수 있게 한다. 모든 최적화가 적용된 ZeRO는, DP만을 이용해 1024개의 GPU에서 1 trillion 개 이상의 parameter를 처리할 수 있다. 또한, 모델 병렬화와 결합하면, 16-방향 모델 병렬화와 노드 간 64-방향 데이터 병렬화를 이용하여 1 trillion 개 이상의 parameter를 처리할 수 있다.&lt;/p>
&lt;p>&lt;strong>Compute Power Gap&lt;/strong> 허용 가능한 시간 범위 내에서 1 trillion 개의 parameter 모델을 처음부터 끝까지 학습시키는 것은 여전히 상당한 양의 컴퓨팅 파워를 필요로 할 수 있으며, 이는 현재의 AI 클러스터에서는 부족하다.&lt;/p>
&lt;p>Bert-Large 모델은 1024 GPU DGX-2H 클러스터에서 67분 만에 학습될 수 있지만, 1 trillion 개의 parameter를 가진 모델은 데이터 샘플당 Bert-Large보다 3000배 더 많은 계산을 필요로 한다. 동일한 하드웨어와 계산 효율성을 가정하면, 이런 크기의 모델 학습은 140일이 걸리며, 실제로는 데이터 샘플과 시퀀스 길이 증가로 인해 1년 이상이 소요될 것이다. 이를 합리적인 시간에 학습시키려면 exa-ﬂop 시스템이 필요하며, 이런 계산 능력이 가능해질 때, ZeRO는 1T 모델을 효율적으로 실행하는 시스템 기술을 제공할 것이다.&lt;/p>
&lt;hr>
&lt;h2 id="implementation-and-evaluation">Implementation and Evaluation&lt;/h2>
&lt;p>∼100B 개의 parameter를 가진 모델들의 효율적인 학습에 초점을 맞추고 있다. 이런 모델들은 현재 가장 큰 모델보다 크지만, 현재의 하드웨어에서 합리적인 시간 안에 학습될 수 있다. 이 목표를 달성하기 위해 ZeRO의 일부 최적화를 구현하고 평가하였다. 이를 ZeRO-100B라 부르며, 이를 통해 최대 170B 개의 parameter를 가진 모델을 효율적으로 학습할 수 있음을 확인하였다. 이는 기존 state-of-art의 기술보다 8배 크고, 최대 10배 빠르며, 사용성이 향상되었다. ZeRO-100B는 세계에서 가장 큰 모델인 Turing-NLG를 지원한다.&lt;/p>
&lt;h3 id="implementation-and-methodology">Implementation and Methodology&lt;/h3>
&lt;p>&lt;strong>Implementation&lt;/strong> PyTorch에서 ZeRO-100B를 구현하였고, 이는 모든 최적화 세트를 포함하며, 어떤 모델과도 호환되는 인터페이스를 제공한다. 사용자는 이 인터페이스를 이용해신의 모델을 감싸서 ZeRO의 DP를 활용할 수 있고, 모델 수정은 필요하지 않다. 또한, ZeRO의 DP는 Megatron-LM을 포함한 어떤 형태의 MP와도 결합할 수 있다.&lt;/p>
&lt;p>&lt;strong>Hardware&lt;/strong> 800 Gbps의 노드 간 통신 대역폭을 가진 400개의 V100 GPU (25개의 DGX-2 노드) 클러스터에서 실험을 수행하였다.&lt;/p>
&lt;p>&lt;strong>Baseline&lt;/strong> MP 없는 실험에는 torch의 distributed data parallel(DDP)을, MP가 있는 실험에는 최첨단 기술인 Megatron-LM을 사용하였다. 이는 NVIDIA의 오픈소스 버전으로, 최근 결과는 32개의 DGX-2 노드 (총 512개의 32GB V100 GPU)를 사용하여 160B 개의 parameter 모델까지 확장 가능함을 보여준다.&lt;/p>
&lt;p>&lt;strong>ZeRO&lt;/strong> MP가 없는 실험에서는 ZeRO-100B의 ZeRO-powered DP 구현을 사용한다. MP가 있는 실험에서는 ZeRO-powered DP를 Megatron-LM의 MP와 결합한다.&lt;/p>
&lt;p>&lt;strong>Model Conﬁgurations&lt;/strong> 모델들은 GPT-2와 같은 transformer 기반 모델이며, parameter 수를 다르게 하기 위해 은닉 차원과 층의 수를 조절하였다.&lt;/p>
&lt;h3 id="speed-and-model-size">Speed and Model Size&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure2.png"
width="1168"
height="492"
srcset="https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure2_huf406f0a2e1639c36e320ac7ae28d1b14_104895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="237"
data-flex-basis="569px"
>&lt;/p>
&lt;p>ZeRO-100B는 400개의 GPU에서 최대 170B 개의 parameter를 가진 모델을 효율적으로 실행하며, 이는 Megatron-LM보다 8배 이상 크다. ZeRO-100B는 8B에서 100B 개의 parameter를 가진 모델에 대해 평균적으로 15 PetaFlops의 처리량을 달성하였다. 반면, 기본 MP 성능은 모델 크기 증가에 따라 빠르게 저하되지만, ZeRO-100B는 기준선에 비해 최대 10배의 속도 향상을 보여준다.&lt;/p>
&lt;p>ZeRO-100B의 경우, 100B을 넘어서는 성능의 약간의 감소는 더 큰 배치 크기를 실행하기 위한 충분한 메모리 부족 때문이다. GPU의 수를 늘림에 따라 ZeRO-100B의 초선형 속도 향상으로 인해 성능이 향상될 것으로 예상한다.&lt;/p>
&lt;h3 id="super-linear-scalability">Super-Linear Scalability&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure3.png"
width="1168"
height="570"
srcset="https://kurtkim.github.io/p/zero/images/figure3_hu8a523eb8ef8a676821dd1d6a1b7d3c23_138981_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure3_hu8a523eb8ef8a676821dd1d6a1b7d3c23_138981_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;p>ZeRO-100B는 매우 큰 모델 크기에 대해 초선형 확장성을 보여주며, 64개에서 400개의 GPU로 확장될 때 이 트렌드가 계속될 것으로 예상한다. $P_{os+g}$는 DP 정도의 증가에 따라 ZeRO-100B의 GPU당 메모리 사용량을 줄여, 처리량을 향상시킨다.&lt;/p>
&lt;h3 id="democratizing-large-model-training">Democratizing Large Model Training&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure4.png"
width="670"
height="428"
srcset="https://kurtkim.github.io/p/zero/images/figure4_hu5287cd73d3458f542ceb3592bddddacd_68377_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure4_hu5287cd73d3458f542ceb3592bddddacd_68377_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="375px"
>&lt;/p>
&lt;p>많은 데이터 과학자들에게 큰 모델 학습의 장벽인 MP와 PP 사용 없이, ZeRO는 모델에 변경 없이 간단한 DP처럼 사용하면서 모델 크기와 속도를 크게 향상시킨다. ZeRO-100B는 128개의 GPU에서 MP 없이 최대 13B 개의 parameter를 가진 모델을 학습시킬 수 있으며, 이는 평균적으로 GPU 당 40 TFlops 이상의 처리량을 달성한다. 반면, ZeRO 없이는 DP만으로 학습 가능한 가장 큰 모델은 1.4B 개의 parameter를 가지며, 처리량은 20 TFlops 미만이다. 게다가, MP의 통신 오버헤드 없이 이런 모델들은 NVLINK이나 NVSwitch가 필요하지 않은 하위 계산 노드에서도 학습될 수 있다.&lt;/p>
&lt;h3 id="memory-and-performance-analysis">Memory and Performance Analysis&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/table3.png"
width="486"
height="250"
srcset="https://kurtkim.github.io/p/zero/images/table3_hu57e31f561d7316fd55d664a227f49bb0_32347_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/table3_hu57e31f561d7316fd55d664a227f49bb0_32347_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;p>최대 모델 크기, 메모리 사용량, 성능에 대한 다양한 최적화의 이점과 영향을 살펴본다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure6.png"
width="384"
height="292"
srcset="https://kurtkim.github.io/p/zero/images/figure6_hu2e7b580be4845363e0b99920bf816bcc_27370_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure6_hu2e7b580be4845363e0b99920bf816bcc_27370_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="315px"
>&lt;/p>
&lt;p>&lt;strong>Maximum Model Size&lt;/strong> C1 대비 C2로 학습시 모델 크기는 40B에서 60B으로 증가하며, 이는 활성화 메모리를 16배 줄이는 $P_a$ 사용 때문이다. C4를 사용하여 140B로 늘리는 것은 $P_{os+g}$를 활성화함으로써 모델 상태의 메모리 요구량을 절반으로 줄이기 때문이고, C5를 사용하여 150B로 증가하는 것은 활성화 체크포인트를 CPU 메모리로 오프로딩하여 메모리를 더욱 줄이기 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure7.png"
width="764"
height="304"
srcset="https://kurtkim.github.io/p/zero/images/figure7_hu42bbbfa2e475a4dffe486054d725e5b7_62602_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure7_hu42bbbfa2e475a4dffe486054d725e5b7_62602_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="251"
data-flex-basis="603px"
>&lt;/p>
&lt;p>&lt;strong>Max Cached Memory&lt;/strong> C1에서 C2로 넘어갈 때 캐시 메모리 크기의 감소는 예상된 결과이다. C2와 C3 사이의 메모리 사용량 차이는 활성화 메모리와 모델 상태의 크기에 따라 달라질 수 있다. 특히, 100B 개 모델에서는 활성화 메모리가 훨씬 크므로 C4에서 C5로 넘어갈 때 캐시 메모리 감소가 눈에 띈다. 이러한 특성으로 인해 $P
&lt;em>{a+cpu}$는 매우 큰 모델에서 더 큰 배치 크기를 적용하는 데 중요한 도구가 된다. 또한, 170B 개 모델이 메모리 부족 없이 실행되기 위해 $P&lt;/em>{a+cpu}$가 필요하다는 것을 보여준다.&lt;/p>
&lt;p>&lt;strong>Max Achievable Performance&lt;/strong> 메모리 사용량 감소가 성능 향상과 연결되어 있으며, 메모리 사용량이 줄어들면 배치 크기를 늘려 성능을 향상시킬 수 있다. 그러나 60B 개 parameter 모델에서 C4와 C5 사이에서는 성능이 떨어진다. 이는 C5가 CPU와의 데이터 이동을 초래하여 성능을 저하시키기 때문이다. 하지만 모델이 너무 크거나, C5 없이는 작동이 불가능한 경우등 예외적인 상황에서는 C5가 필요하다. 학습 중에는 이러한 이점이 있는 경우에만 $P_{a+cpu}$가 활성화된다.&lt;/p>
&lt;h3 id="turing-nlg-the-sota-language-model-with-17b-parameters">Turing-NLG, the SOTA language model with 17B parameters&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/zero/images/figure5.png"
width="494"
height="396"
srcset="https://kurtkim.github.io/p/zero/images/figure5_hua411aca5492099be87a7d26ea599e861_67401_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/zero/images/figure5_hua411aca5492099be87a7d26ea599e861_67401_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>2020년 5월 12일 기준으로, Turing-NLG는 17B 개의 parameter를 가진 세계 최대의 모델로, Webtext-103의 perplexity 10.21로 언어 모델의 state-of-art를 달성하였다. TuringNLG는 ZeRO-100B를 사용하여 학습되었고, 이 모델은 GPU당 41.4 TFlops의 처리량을 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="concluding-remarks">Concluding Remarks&lt;/h2>
&lt;p>고성능 컴퓨팅과 시스템 관점에서 보면, ZeRO는 대형 모델 학습 분야에서 혁명적 변화를 일으킬 것으로 보인다. ZeRO-100B 구현은 모델 크기를 8배, 처리량은 10배 이상 향상시키며, 현대 GPU 클러스터에서 초선형 속도 향상을 달성하고 세계에서 가장 큰 모델을 학습시킬 수 있다. 하지만 이는 ZeRO의 전체 잠재력을 보여주는 것이 아니다. ZeRO는 미래의 trillion parameter 모델 학습을 가능하게 하는 더 큰 모델 크기 증가를 제공할 수 있다.&lt;/p>
&lt;p>ZeRO에 대한 가장 큰 낙관적인 점은 데이터 과학자에게 어떠한 장애물도 없다는 것이다. 기존의 MP와 PP 접근법과 달리, 모델 리팩토링이 필요 없고 표준 DP만큼 쉽게 사용할 수 있어, 대규모 모델 학습에 대한 미래의 연구에서 중요한 역할을 할 것으로 보인다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1910.02054.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>T5</title><link>https://kurtkim.github.io/p/t5/</link><pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/t5/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>전이 학습은 데이터가 풍부한 작업에서 먼저 모델을 사전 훈련시킨 후, 이를 downstream task에 미세 조정하는 방식으로, 자연어 처리(NLP)에서 중요한 기법이다. 이 논문에서는 모든 텍스트 기반 언어 문제를 텍스트-텍스트 형식으로 변환하는 통합 프레임워크를 통해 NLP를 위한 전이 학습 기법을 탐색한다. 이 연구에서는 사전 훈련 목표, 아키텍처, 라벨이 없는 데이터셋, 전이 접근법 등 다양한 요소를 비교 분석하여, 요약, 질문 응답, 텍스트 분류 등 여러 분야에서 state-of-the-art를 달성하였다. 또한, 이러한 연구를 통한 데이터셋, 사전 훈련된 모델, 코드를 공개하여, NLP를 위한 전이 학습 연구를 더욱 촉진시키고자 한다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리(NLP) 머신러닝 모델 훈련은 모델이 텍스트를 이해하고 적절하게 처리하는 능력을 개발하는 것을 목표로 하며, 이는 단어의 철자와 의미부터 고수준의 지식까지 다양한 요소를 포함한다. 최근에는 데이터가 풍부한 작업에서 모델을 사전 학습하는 것이 일반적이며, 이를 통해 모델은 다양한 작업에 활용할 수 있는 일반적인 능력과 지식을 개발하게 된다. 특히 NLP에서는 레이블이 없는 대량의 텍스트 데이터를 이용한 비지도 학습으로 사전 학습이 진행되며, 이 방법은 주요 NLP 벤치마크에서 state-of-the-art를 달성하는 데 사용되었다.&lt;/p>
&lt;p>자연어 처리(NLP)에서의 전이 학습에 대한 최근 연구는 다양한 사전 학습 목표와 레이블 없는 데이터 세트, 벤치마크 등을 개발했다. 이 분야는 빠르게 발전하고 있지만, 그로 인해 다양한 기법을 비교하거나 이해하는 것이 어려워졌다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure1.png"
width="1170"
height="426"
srcset="https://kurtkim.github.io/p/t5/images/figure1_hu1038ec56a8248b7b1a8e5f6fb8351c28_110308_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure1_hu1038ec56a8248b7b1a8e5f6fb8351c28_110308_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="274"
data-flex-basis="659px"
>&lt;/p>
&lt;p>모든 텍스트 처리 문제를 &amp;ldquo;Text-to-Text&amp;quot;의 일관된 문제로 바라보며, 이를 통해 다양한 NLP 문제에 대한 성능을 평가하고 전이 학습의 한계를 탐색하고자 한다. 이 연구의 목표는 새로운 방법을 제안하는 것이 아니라, 현재 이 분야가 어디에 서 있는지를 종합적으로 이해하는 것이다. 이를 위해 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;라는 웹에서 수집한 영어 텍스트 데이터 세트를 사용한다. 또한, 데이터가 부족한 환경에서의 전이 학습의 중요성을 인식하여, 코드, 데이터 세트, 그리고 사전 학습된 모델을 공개한다.&lt;/p>
&lt;hr>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>모든 문제를 &amp;ldquo;Text-to-Text&amp;rdquo; 변환하는 접근법과 레이블 없는 텍스트 데이터를 구성한 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;를 제안한다. 모델과 프레임워크의 이름은 &amp;ldquo;Text-to-Text Transfer Transformer(T5)&amp;ldquo;이다.&lt;/p>
&lt;h3 id="model">Model&lt;/h3>
&lt;p>NLP의 초기 전이 학습은 RNN을 이용했지만, 최근에는 Transformer 아키텍처 기반의 모델이 일반적이다. Transformer는 처음에는 기계 번역에 효과적이었지만, 이후 다양한 NLP 환경에서 널리 사용되었다. 그래서 논문에서 연구되는 모든 모델은 Transformer 아키텍처를 기반으로 하며, 이 아키텍처에서 크게 벗어나지는 않았다.&lt;/p>
&lt;p>Transformer의 핵심 구성 요소는 self-attention으로, 시퀀스의 각 요소를 시퀀스의 나머지 부분의 가중 평균으로 대체한다. 원래 Transformer는 encoder-decoder 아키텍처로 설계되었지만, 최근에는 언어 모델링이나 분류, 범위 예측 작업에 적합한 아키텍처를 생성하는 다양한 형태의 self-attention을 사용한 single Transformer layer stack 모델이 일반적이다.&lt;/p>
&lt;p>T5의 encoder-decoder Transformer는 입력 토큰을 임베딩으로 매핑하고 이를 인코더에 전달한다. 인코더는 &amp;ldquo;self-attention layer&amp;quot;과 &amp;ldquo;feed-forward network&amp;quot;를 포함하며, 각 입력에 layer normalization와 residual skip connection을 적용한다. dropout은 네트워크 전체에 적용된다. 디코더는 인코더와 비슷하지만, 인코더 출력에 self-attention mechanism이 추가되고, autoregressive 또는 causal self-attention을 사용한다. 디코더 출력은 dense layer로 전달되고, 모든 attention mechanism은 독립적인 &amp;ldquo;head&amp;quot;로 나누어져 있다.&lt;/p>
&lt;p>Transformer 모델은 순서에 상관 없는 self-attention 특성 때문에 position signal을 제공한다. 초기에는 sinusoidal position signal이나 learned position embeddings을 사용했지만, 최근에는 relative position embeddings이 주로 사용되고 있다. 이는 &amp;ldquo;key&amp;quot;와 &amp;ldquo;query&amp;quot;의 오프셋에 따라 다른 임베딩을 생성한다. 우리는 position embedding을 간소화하여 attention weight 계산에 사용되는 스칼라로 만들었다. 모든 layer가 position embedding parameter를 공유하며, 각 layer의 attention head는 다른 position embedding을 사용한다. 이 모델은 원래의 Transformer와 비슷하지만, layer normalization 위치와 position embedding 체계가 다르다.&lt;/p>
&lt;p>모델의 확장성을 실험하기 위해 parameter와 layer를 늘리고, 그 성능 변화를 관찰했다. 복잡한 큰 모델 학습을 위해 모델과 데이터 병렬성을 사용하고, 5개의 TPU 파드를 활용한 Cloud TPU Pods에서 모델을 학습시켰다.&lt;/p>
&lt;h3 id="the-colossal-clean-crawled-corpus">The Colossal Clean Crawled Corpus&lt;/h3>
&lt;p>레이블이 없는 데이터의 품질, 특성, 크기가 어떤 영향을 미치는지 분석한다. 이를 위해 웹에서 스크랩된 텍스트를 제공하는 Common Crawl을 사용한다. Common Crawl은 이전에 언어 모델 훈련, 상식적 추론, 기계 번역 텍스트 채굴, 사전 훈련 데이터 세트, 최적화기 테스트 등 다양한 NLP 연구에 활용된 바 있다.&lt;/p>
&lt;p>Common Crawl은 웹에서 스크랩된 텍스트를 제공하는 공개 아카이브이다. 매월 약 20TB의 텍스트 데이터를 생성하지만, 이 중 대부분은 자연언어가 아닌 메뉴, 오류 메시지, 중복 텍스트 등의 쓸데없는 텍스트이다. 또한, 작업에 도움이 되지 않을 것 같은 내용도 많이 포함되어 있다. 이러한 문제를 해결하기 위해, 다음과 같은 방법들을 사용한다:&lt;/p>
&lt;ul>
&lt;li>마침표, 느낌표, 물음표, 인용 부호를 포함한 문장만을 사용한다.&lt;/li>
&lt;li>3문장 미만의 페이지는 제외하고, 적어도 5단어 이상 포함된 문장만을 사용한다.&lt;/li>
&lt;li>&amp;ldquo;불순한, 야한, 외설적인 또는 그 외 나쁜 단어 목록&amp;quot;에 있는 단어가 포함된 페이지는 모두 삭제한다.&lt;/li>
&lt;li>스크랩된 페이지의 대다수는 자바스크립트(Javascript)가 활성화 되어야 한다는 경고문을 포함한다. 따라서 자바스크립트 단어를 포함한 모든 라인을 삭제한다.&lt;/li>
&lt;li>일부 페이지는 “lorem ipsum” 플레이스홀더를 포함한다. 따라서 “lorem ipsum”구가 있는 모든 페이지를 삭제한다.&lt;/li>
&lt;li>일부 페이지에는 코드가 포함되어 있다. “{” 문구가 대다수의 프로그래밍 언어(웹에서 많이 사용되는 자바스크립트와 같이)에서 출몰하고 자연 텍스트에서는 나타나지 않기 때문에, “{” 를 포함한 모든 페이지를 삭제한다.&lt;/li>
&lt;li>스크랩된 페이지 중 일부는 위키백과에서 가져온 것이었고, 인용 표시자(e.g. [1], [citation needed], etc.)가 있다. 이러한 표시자를 모두 를 모두 삭제한다.&lt;/li>
&lt;li>많은 페이지에는 보일러플레이트 정책 공지가 있다. &amp;ldquo;terms of use&amp;rdquo;, &amp;ldquo;privacy policy&amp;rdquo;, &amp;ldquo;cookie policy&amp;rdquo;, &amp;ldquo;uses cookies&amp;rdquo;, &amp;ldquo;use of cookies&amp;rdquo;, &amp;ldquo;use cookies&amp;quot;라는 문자열을 포함한 줄은 모두 삭제한다.&lt;/li>
&lt;li>데이터셋 중복을 제거하기 위해, 데이터셋에서 두 번 이상 나타난 3문장 스팬은 하나만 남기고 모두 삭제한다.&lt;/li>
&lt;/ul>
&lt;p>대부분의 작업이 영어 텍스트에 초점을 두고 있기 때문에, 0.99의 확률로 영어로 분류되지 않은 페이지를 제거하기 위해 &amp;ldquo;langdetect&amp;quot;를 사용하였다. 하지만, 이전 데이터 세트의 필터링 방법, 공개 여부, 범위 등이 제한적이라고 판단하여 새로운 데이터 세트를 만들기로 결정하였습니다.&lt;/p>
&lt;p>2019년 4월의 웹 텍스트를 다운로드하고 필터링하여 기본 데이터 세트를 구축하였다. 이 결과, 대부분의 사전 학습 데이터 세트보다 훨씬 크고(750GB), 깨끗하며 자연스러운 영어 텍스트 컬렉션을 만들었다. 이를 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;라고 부르며, TensorFlow 데이터 세트의 일부로 공개하였다.&lt;/p>
&lt;h3 id="downstream-tasks">Downstream Tasks&lt;/h3>
&lt;p>이 논문의 목표는 일반적인 언어 학습 능력을 측정하는 것이다. 이를 위해, 다양한 벤치마크를 통해 기계 번역, 질문 응답, 추상적 요약, 텍스트 분류 등의 성능을 연구하였다. 이에는 GLUE와 SuperGLUE 텍스트 분류, CNN/Daily Mail 요약, SQuAD 질문 응답, 그리고 WMT 영어에서 독일어, 프랑스어, 루마니아어로의 번역이 포함되었다. 모든 데이터는 TensorFlow 데이터 세트에서 수집하였다.&lt;/p>
&lt;p>GLUE와 SuperGLUE는 각각 일반적인 언어 이해 능력을 테스트하기 위해 설계된 텍스트 분류 작업들의 모음이다:&lt;/p>
&lt;ul>
&lt;li>Sentence acceptability judgment (CoLA)&lt;/li>
&lt;li>Sentiment analysis (SST-2)&lt;/li>
&lt;li>Paraphrasing/sentence similarity (MRPC, STS-B, QQP)&lt;/li>
&lt;li>Natural language inference (MNLI, QNLI, RTE, CB)&lt;/li>
&lt;li>Coreference resolution (WNLI and WSC)&lt;/li>
&lt;li>Sentence completion (COPA)&lt;/li>
&lt;li>Word sense disambiguation (WIC)&lt;/li>
&lt;li>Question answering (MultiRC, ReCoRD, BoolQ)&lt;/li>
&lt;/ul>
&lt;p>GLUE와 SuperGLUE 벤치마크의 데이터 세트를 사용하며, 모든 작업들을 하나의 작업으로 취급하여 데이터 세트를 결합하기 위해 미세 조정을 진행하였다. 또한, SuperGLUE 작업에는 Definite Pronoun Resolution (DPR) 데이터 세트도 포함시켰다.&lt;/p>
&lt;p>CNN/Daily Mail 데이터 세트는 텍스트 요약 작업으로 적용되었고, SQuAD는 일반적인 질문 응답 벤치마크이다. WMT 영어-독일어, 영어-프랑스어, 영어-루마니아어 번역에는 각각 표준 훈련 데이터와 검증 세트를 사용한다. 모든 사전 학습은 영어 데이터로만 진행되며, 모델이 새로운 언어의 텍스트를 생성하도록 배우기 위해 번역 학습이 필요하다.&lt;/p>
&lt;h3 id="input-and-output-format">Input and Output Format&lt;/h3>
&lt;p>모든 작업을 &amp;ldquo;text-to-text&amp;rdquo; 형식으로 표현하여 단일 모델을 훈련시킨다. 이 방식은 사전 학습과 미세 조정에 대해 일관된 훈련 목표를 제공한다. 모델은 작업에 관계없이 maximum likelihood 목표로 훈련되며, 수행해야 할 작업을 지정하기 위해 원래 입력 시퀀스 앞에 작업 특정 텍스트 접두어(prefix)를 추가한다.&lt;/p>
&lt;p>Text-to-text 프레임워크는 다양한 NLP 작업을 통일된 형식으로 변환한다. McCann et al이 제안한 &amp;ldquo;Natural Language Decathlon&amp;quot;과 비슷하지만, 이 논문에서는 각 작업을 개별적으로 미세조정하고 짧은 작업 접두어를 사용한다. 또한 전이 학습에 초점을 맞추며, 기계 번역과 추상적 요약 등의 생성적 작업을 처리할 수 있는 프레임워크를 제안한다.&lt;/p>
&lt;p>대부분의 작업을 text-to-text 형식으로 쉽게 변환했으며, 유사성 점수를 예측하는 STS-B는 점수를 반올림하고 숫자 문자열로 변환하여 처리하였다. 이를 통해 STS-B 회귀 문제를 21 클래스 분류 문제로 재구성하였다.&lt;/p>
&lt;p>또한, Winograd 작업과 WSC 작업에서는 모호한 대명사를 강조하고, 모델이 대명사가 가리키는 명사를 예측하도록 훈련시켰다. DPR 데이터 세트는 대략 1,000개의 대명사 해결 예제를 추가하여 사용하였다.&lt;/p>
&lt;p>WNLI의 훈련 및 검증 세트는 WSC와 많이 중복되므로, 훈련 데이터로의 유출을 방지하기 위해 WNLI에서 훈련하지 않았으며, 평균 GLUE 점수에도 포함시키지 않았다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>한 번에 하나씩 설정을 변경하면서 체계적으로 기여도를 연구하였다. 예를 들어, 나머지 실험 파이프라인을 고정하고 다양한 비지도 목표의 성능을 측정했다. 이 방법은 이차 효과를 놓칠 수 있지만, 모든 요인의 조합을 탐색하는 것은 비용이 많이 든다. 미래의 연구에서는 다양한 접근법의 조합을 더 철저하게 고려하는 것이 유익할 것으로 예상된다.&lt;/p>
&lt;p>이 논문의 목표는 다양한 작업에 대해 다양한 접근법을 비교하는 것으로, 가능한 한 많은 요소를 고정하려고 한다. 이를 위해, 기존의 접근법을 정확하게 따르지는 않았다. 예를 들어, BERT와 같은 encoder-only 모델은 생성 작업에는 적합하지 않다. 따라서 우리가 고려하는 모델 중 어느 것도 BERT와 정확히 같지 않다. 대신, BERT의 목표와 유사한 목표를 고려하고, BERT와 유사하게 작동하는 모델 아키텍처를 고려하였다.&lt;/p>
&lt;h3 id="baseline">Baseline&lt;/h3>
&lt;p>간단한 denoising 목표를 사용하여 표준 Transformer를 사전 학습하고, 각 downstream task에서 별도로 미세 조정을 진행한다.&lt;/p>
&lt;h4 id="model-1">Model&lt;/h4>
&lt;p>T5는 standard encoder-decoder Transformer를 사용한다. 많은 NLP 전이 학습 방법이 single “stack” 구조를 사용하지만, 이 연구에서는 standard encoder-decoder 구조가 생성과 분류 작업에서 좋은 결과를 얻는다는 것을 확인하였다.&lt;/p>
&lt;p>$BERT_BASE$와 유사한 크기와 구성의 인코더와 디코더로 설계되었다. 인코더와 디코더는 각각 12개의 블록으로 이루어져 있으며, 이 블록들은 self-attention, encoder-decoder attention, feed-forward network를 포함하고 있다. 모델은 총 약 2억 2천만 개의 parameter를 가지고 있으며, 모든 부분에서 0.1의 드롭아웃 확률을 사용하여 정규화된다.&lt;/p>
&lt;h4 id="training">Training&lt;/h4>
&lt;p>모든 작업은 text-to-text로 구성되며, 이를 통해 standard maximum likelihood를 사용하여 학습한다. 최적화는 AdaFactor를 사용하고, 테스트 시에는 가장 높은 확률의 logit을 선택하는 greedy decoding을 사용한다.&lt;/p>
&lt;p>각 모델은 C4에서 524,288 단계동안 사전 학습 후 미세 조정을 진행한다. 최대 시퀀스 길이는 512이며, 배치 크기는 128 시퀀스이다. 배치는 대략 65,536 토큰을 포함하도록 한다. 이는 총 34B 토큰에 대한 사전 학습에 해당하며, BERT나 RoBERTa에 비해 상당히 적다. 하지만 이런 방식을 사용하면 합리적인 계산 비용으로 충분한 사전 학습을 할 수 있다. 사전 학습 동안 데이터는 반복하지 않는다.&lt;/p>
&lt;p>사전 학습 동안, &amp;ldquo;inverse square root&amp;rdquo; learning rate schedule을 사용한다. 초기 $10^4$ 단계 동안 learning rate를 0.01로 유지한 후 지수적으로 감소시킨다. triangular learning rate를 실험해 봤지만, 학습 단계의 총 수를 미리 알아야 하므로, 더 일반적인 inverse square root schedule을 선택하였다.&lt;/p>
&lt;p>모든 작업에 대해 262,144 단계 동안 모델을 미세 조정한다. 이는 대규모와 소규모 데이터 세트를 가진 작업 사이의 균형을 위해 선택되었다. 미세 조정 시 128개의 길이 512 시퀀스를 가진 배치를 사용하고, 학습률은 0.001로 유지한다. 5,000step마다 체크포인트를 저장하며, 가장 높은 검증 성능을 보인 체크포인트의 결과를 보고한다. 여러 작업에 미세 조정된 모델의 경우, 각 작업마다 최적의 체크포인트를 독립적으로 선택한다.&lt;/p>
&lt;h4 id="vocabulary">Vocabulary&lt;/h4>
&lt;p>SentencePiece를 사용하여 텍스트를 WordPiece 토큰으로 인코딩하며, 32,000개의 어휘를 사용한다. T5 모델이 다른 언어를 처리할 수 있도록, 영어, 독일어, 프랑스어, 루마니아어 데이터를 혼합하여 SentencePiece 모델을 훈련시켰다. 이 어휘는 모델의 입력과 출력에 모두 사용되며, 미리 결정된 고정된 언어 세트만 처리할 수 있다.&lt;/p>
&lt;h4 id="unsupervised-objective">Unsupervised Objective&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure2.png"
width="908"
height="322"
srcset="https://kurtkim.github.io/p/t5/images/figure2_hud0a0f0860146969ba93fdd537c6f86ff_60008_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure2_hud0a0f0860146969ba93fdd537c6f86ff_60008_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;p>레이블이 없는 데이터를 활용하여 모델을 사전 학습하는데는, 레이블이 필요하지 않지만 일반화 가능한 지식을 모델에게 가르치는 목표가 필요하다. 최근 &amp;ldquo;denoising&amp;rdquo; 또는 &amp;ldquo;masked language modeling&amp;quot;이라는 목표가 효과적이라는 것이 밝혀졌다. 이는 모델이 입력에서 누락되거나 손상된 토큰을 예측하도록 하는 방식이다. 이에 영감을 받아, 입력 시퀀스에서 무작위로 선택한 15%의 토큰을 드롭아웃하는 목표를 설정하였다. 이 목표는 사전 훈련의 계산 비용을 줄이는 데 도움이 된다.&lt;/p>
&lt;h4 id="baseline-performance">Baseline Performance&lt;/h4>
&lt;p>이상적으로는 모든 실험을 여러 번 반복해야 하지만, 실험의 수가 많으면 비용이 높다. 대신, 기본 모델을 10번 새로 학습하고, 이 실행들의 분산이 각 실험 변형에 적용될 것으로 가정한다. 또한, 사전 학습 없이 모델을 218 step 동안 학습한 후 성능을 측정하여, 사전 학습이 얼마나 도움이 되는지 파악한다.&lt;/p>
&lt;p>GLUE와 SuperGLUE는 모든 하위 작업의 평균 점수를, 번역 작업은 SacreBLEU에서 제공하는 BLEU 점수를 확인한다. WMT 영어에서 독일어, 프랑스어, 루마니아어로의 점수를 각각 EnDe, EnFr, EnRo라 표기한다. CNN/Daily Mail과 SQuAD는 상관성이 높은 지표만 확인한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table1.png"
width="1106"
height="174"
srcset="https://kurtkim.github.io/p/t5/images/table1_hub03ae50202d8dc9ff0432f7515727b68_41938_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table1_hub03ae50202d8dc9ff0432f7515727b68_41938_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="635"
data-flex-basis="1525px"
>&lt;/p>
&lt;p>T5 모델은 $BERT_{BASE}$와 비슷한 성능을 보여준다(SQuAD에서 80.88, MNLI-matched에서 84.24). 하지만 T5 모델은 encoder-decoder 모델로, 사전 학습 단계가 더 적어 직접 비교는 어렵다. 대부분의 벤치마크에서 사전 학습이 성능을 크게 향상시키는 것을 확인했고, WMT 영어에서 프랑스어로의 작업을 통해 high-resource regime 상태에서의 전이 학습을 테스트하였다. 데이터가 제한된 작업에서 사전 학습이 얼마나 성능을 향상시키는지 강조하는 동시에, 전이 학습의 주요 이점 중 하나로 데이터 효율성의 개선을 강조한다.&lt;/p>
&lt;p>대부분의 작업에서 표준 편차는 작업의 기준 점수의 1% 미만이다. 하지만 GLUE와 SuperGLUE 벤치마크의 CoLA, CB, COPA와 같은 low-resource 작업에서는 이 규칙이 적용되지 않는다. 예를 들어, CB 작업에서 기준 모델의 평균 F1 점수는 91.22이고 표준 편차는 3.237이었다. 이런 변동성은 검증 세트의 예제 수가 적은 것이 원인일 수 있다. GLUE와 SuperGLUE 점수는 각 벤치마크의 작업 점수의 평균으로 계산되기 때문에 이러한 높은 변동성 때문에 이 점수만으로 모델을 비교하는 것은 어려울 수 있다.&lt;/p>
&lt;h3 id="architectures">Architectures&lt;/h3>
&lt;p>Transformer는 처음에는 encoder-decoder 구조로 소개되었지만, 최근 NLP 전이 학습 연구에서는 다른 구조를 더 많이 사용하고 있다.&lt;/p>
&lt;h4 id="model-structures">Model Structures&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure3.png"
width="954"
height="382"
srcset="https://kurtkim.github.io/p/t5/images/figure3_hu58b7a3434dc501afb6d7e3563e21094b_77654_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure3_hu58b7a3434dc501afb6d7e3563e21094b_77654_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="249"
data-flex-basis="599px"
>&lt;/p>
&lt;p>아키텍처를 구분하는 주요 요소는 모델에서 사용하는 &amp;ldquo;mask&amp;quot;이다. Transformer의 self-attention 연산은 시퀀스를 입력받아 동일한 길이의 새로운 시퀀스를 출력한다. 각 출력 항목은 입력 항목의 weighted average를 계산해 생성된다. attention mask는 특정 가중치를 0으로 만들어 특정 출력 시간에서 입력 항목에 attention을 기울일 수 있는 범위를 제한한다. 예를 들어, causal mask는 $j &amp;gt; i$인 경우 가중치를 0으로 만든다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure4.png"
width="1016"
height="420"
srcset="https://kurtkim.github.io/p/t5/images/figure4_hue70c5b5abebaed2b9ecd54b934dd8154_113297_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure4_hue70c5b5abebaed2b9ecd54b934dd8154_113297_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>첫 번째로 고려하는 모델은 encoder-decoder Transformer로, 입력 시퀀스를 받는 encoder와 새로운 출력 시퀀스를 만드는 decoder 두 계층으로 구성되어 있다.&lt;/p>
&lt;p>encoder는 &amp;ldquo;fully-visible&amp;rdquo; attention mask를 사용한다. 이 마스킹은 출력의 각 항목을 만들 때 입력의 모든 항목에 attention을 기울일 수 있게 해준다. 이 마스킹은 &amp;ldquo;prefix&amp;rdquo; 즉, 예측을 만들 때 사용되는 일부 컨텍스트에 주의를 기울일 때 적합하다. BERT도 이와 같은 마스킹 패턴을 사용하며, 특별한 &amp;ldquo;classification&amp;rdquo; 토큰을 입력에 추가한다. 이 토큰에 해당하는 BERT의 출력은 입력 시퀀스를 분류하는 예측을 하는데 사용된다.&lt;/p>
&lt;p>Transformer의 decoder에서 self-attention 연산은 &amp;ldquo;causal&amp;rdquo; 마스킹 패턴을 사용한다. 출력 시퀀스의 $i$번째 항목을 생성할 때, 인과적 마스킹은 모델이 입력 시퀀스의 $j$번째 항목$(j &amp;gt; i)$에 attention을 기울이는 것을 방지한다. 이는 모델이 출력을 생성하는 동안 &amp;ldquo;미래를 보는&amp;rdquo; 것을 방지하기 위해 훈련 중에 사용된다.&lt;/p>
&lt;p>언어 모델은 text-to-text 작업에서 입력과 목표를 연결함으로써 사용될 수 있지만, causal 마스킹 때문에 입력 시퀀스의 특정 항목이 그 이전 항목에만 의존하는 문제가 있다. 이 문제는 Transformer 기반 언어 모델에서 마스킹 패턴을 변경함으로써 해결할 수 있으며, 시퀀스의 접두사 부분에서 완전히 보이는 마스킹을 사용하면 이 문제를 피하면서도 다양한 text-to-text 작업에 효과적일 수 있다. 이 방식은 encoder와 decoder 간에 파라미터를 공유하는 encoder-decoder 모델과 유사하며, 입력과 목표 시퀀스에 걸쳐 전체 attention을 적용한다.&lt;/p>
&lt;p>prefix LM은 BERT와 비슷하게 작동하지만, 분류 작업을 수행하기 위해 Transformer decoder의 출력 레이어에 분류기를 통합한다. 이 모델은 전체 입력을 보고 예측을 출력함으로써 분류 작업을 수행한다.&lt;/p>
&lt;h4 id="comparing-different-model-structures">Comparing Different Model Structures&lt;/h4>
&lt;p>아키텍처를 비교하려면, 각 모델이 같은 수의 parameter를 가지거나, 같은 양의 계산을 필요로 하는 등 의미 있는 방식으로 동일해야 한다. encoder와 decoder가 각각 L개의 레이어를 가진 encoder-decoder 모델은, 2L개의 레이어를 가진 언어 모델과 대략 같은 수의 parameter를 가진다. 그러나 계산 비용 면에서는, L개의 레이어를 가진 언어 모델과 동일하다. 이는 언어 모델이 입력과 출력 시퀀스 모두를 처리해야 하지만, encoder-decoder 모델은 각각 입력과 출력 시퀀스만을 처리하기 때문이다.&lt;/p>
&lt;p>비교를 위해, encoder-decoder 모델의 여러 구성을 고려하였다. $BERT_{BASE}$ 크기의 레이어 스택에서 레이어와 parameter의 수를 각각 L과 P로, 주어진 입력-타겟 쌍 처리에 필요한 FLOPs의 수를 M으로 표현하겠습니다. 이를 바탕으로 모델들을 비교한다:&lt;/p>
&lt;ul>
&lt;li>encoder와 decoder에 각각 L 레이어가 있는 encoder-decoder 모델. 이 모델은 2P의 parameter와 M FLOPs의 계산 비용을 가진다.&lt;/li>
&lt;li>동일한 모델이지만, 인코더와 디코더 간에 parameter가 공유되어, P의 parameter와 M-FLOP의 계산 비용을 가진다.&lt;/li>
&lt;li>encoder와 decoder에 각각 L/2 레이어가 있는 encoder-decoder 모델로, P의 parameter와 M/2-FLOP의 비용을 가진다.&lt;/li>
&lt;li>L 레이어와 P parameter를 가지며 M FLOPs의 계산 비용이 발생하는 decoder-only 언어 모델.&lt;/li>
&lt;li>같은 아키텍처를 가지지만, 입력에 대한 fully-visible self-attention을 가진 decoder-only prefix LM.&lt;/li>
&lt;/ul>
&lt;h4 id="objectives">Objectives&lt;/h4>
&lt;p>비지도 학습 목표로 기본 언어 모델링과 denoising 목표를 고려하였다. 언어 모델링은 사전 학습 목표로서의 역사적 사용과 모델 아키텍처에 대한 적합성 때문에 포함되었다. 예측 전에 접두사를 입력하는 모델들에 대해, 레이블이 없는 데이터에서 텍스트를 샘플링하고 랜덤한 지점에서 접두사와 타겟으로 분할한다. 표준 언어 모델은 전체 텍스트를 예측하도록 훈련되며, text-to-text 모델을 위한 비지도 denoising 목표는 입력과 타겟을 연결하여 사용한다.&lt;/p>
&lt;h4 id="results">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table2.png"
width="1330"
height="404"
srcset="https://kurtkim.github.io/p/t5/images/table2_huf6638c8c2bc117ff25ac7c6022eeb34d_123609_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table2_huf6638c8c2bc117ff25ac7c6022eeb34d_123609_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>모든 작업에서 denoising 목표를 가진 encoder-decoder 구조가 가장 좋은 성능을 보여주었다. 이 구조는 parameter 수는 가장 많지만 계산 비용은 decoder-only 모델과 같다. encoder와 decoder 간에 parameter를 공유하는 것도 거의 동등한 성능을 보여주었다. 반면, encoder와 decoder의 레이어 수를 줄이면 성능이 크게 저하되었다. denoising 목표를 가진 공유 encoder-decoder 구조는 decoder-only prefix LM 모델보다 성능이 좋았다. 마지막으로, denoising 목표를 사용하는 것이 언어 모델링 목표보다 항상 더 나은 성능을 가져다 준다는 사실을 확인하였다.&lt;/p>
&lt;h3 id="unsupervised-objectives">Unsupervised Objectives&lt;/h3>
&lt;p>비지도 학습 목표의 선택은 모델이 downstream task에 적용할 일반 지식을 획득하는 방법을 제공하므로 중요하며, 이로 인해 다양한 사전 학습 목표가 개발되었다. 많은 경우에 기존의 목표를 그대로 복제하지 않고, text-to-text encoder-decoder 프레임워크에 맞게 수정하거나, 여러 공통 접근법의 개념을 결합한 목표를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table3.png"
width="1524"
height="288"
srcset="https://kurtkim.github.io/p/t5/images/table3_hu4cef11ab70bdbad4421eb2667d9e4b35_103192_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table3_hu4cef11ab70bdbad4421eb2667d9e4b35_103192_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="529"
data-flex-basis="1270px"
>&lt;/p>
&lt;p>라벨이 없는 텍스트 데이터 세트에서 토큰화된 텍스트 범위에 해당하는 토큰 ID의 시퀀스를 처리한다. 토큰 시퀀스는 입력 시퀀스와 목표를 생성하고, 모델은 이를 사용해 maximum likelihood로 목표 시퀀스를 예측하도록 학습한다.&lt;/p>
&lt;h4 id="disparate-high-level-approaches">Disparate High-Level Approaches&lt;/h4>
&lt;p>세 가지 다른 접근법을 사용한 기법들을 비교한다. 첫 번째로, &amp;ldquo;prefix language modeling&amp;rdquo; 목표를 사용하며, 이는 텍스트를 두 부분으로 나눠 encoder 입력과 decoder 예측 대상으로 사용한다. 두 번째로, BERT의 &amp;ldquo;masked language modeling&amp;quot;에서 영감을 받은 목표를 사용하며, 이는 텍스트의 토큰 15%를 손상시키고, 이 중 90%는 마스크 토큰, 10%는 랜덤 토큰으로 대체한다. 세 번째로, &amp;ldquo;deshuffling&amp;rdquo; 목표를 사용하며, 이는 토큰의 순서를 섞은 후 원래 순서를 복원하는 것을 목표로 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table4.png"
width="1148"
height="170"
srcset="https://kurtkim.github.io/p/t5/images/table4_hucd8c36f3593092fa88fcbef0571dc050_48099_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table4_hucd8c36f3593092fa88fcbef0571dc050_48099_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="675"
data-flex-basis="1620px"
>&lt;/p>
&lt;p>전체적으로, BERT 스타일 목표가 가장 뛰어난 성능을 보이지만, prefix language modeling 목표도 번역 작업에서 유사한 성능을 보여준다. 반면에 deshuffling 목표는 다른 두 목표보다 성능이 상당히 떨어진다.&lt;/p>
&lt;h4 id="simplifying-the-bert-objective">Simplifying the BERT Objective&lt;/h4>
&lt;p>BERT 스타일의 denoising 목표는 원래 분류와 범위 예측을 위해 학습된 encoder-only 모델의 사전 학습 기법으로 제안되었다. 따라서 encoder-decoder text-to-text 설정에서 더 나은 성능을 내거나 더 효율적이게 만들 수 있도록 조정하는 것이 가능할 수 있다.&lt;/p>
&lt;p>BERT 스타일 목표의 간단한 변형을 고려하며, 이는 무작위 토큰 교환 단계를 생략한다. 그 결과, 입력의 15% 토큰을 마스크 토큰으로 바꾸고, 모델은 원래 손상되지 않은 시퀀스를 재구성하도록 학습한다. 이를 &amp;ldquo;MASS&amp;rdquo; 목표라고 부릅니다. 또한, decoder에서 긴 시퀀스 자체에 대한 attention를 피할 수 있는 방법을 탐색한다. 이를 위해, 손상된 토큰들을 모두 마스크 토큰으로 대체하거나, 손상된 토큰을 입력 시퀀스에서 완전히 삭제하는 두 가지 전략을 고려한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table5.png"
width="1164"
height="206"
srcset="https://kurtkim.github.io/p/t5/images/table5_hu25f2653fef6f13a93df98915514a50bb_63030_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table5_hu25f2653fef6f13a93df98915514a50bb_63030_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="565"
data-flex-basis="1356px"
>&lt;/p>
&lt;p>원래의 BERT 스타일 목표와 세 가지 대안의 비교는 모든 변형이 비슷하게 수행된다는 것을 보여준다. 예외적으로, 손상된 토큰을 완전히 삭제하는 것이 CoLA에서 훨씬 높은 점수 덕분에 GLUE 점수를 약간 향상시켰는데, CoLA가 주어진 문장이 문법적으로 및 구문론적으로 수용 가능한지 분류하는 것을 포함하고 있으며, 토큰이 누락되었는지 판단할 수 있는 능력이 수용 가능성을 감지하는 데 밀접하게 관련되어 있기 때문일 수 있다. 그러나, 토큰을 완전히 삭제하는 것은 SuperGLUE에서 성능이 떨어졌다. 전체 원래 시퀀스를 예측할 필요가 없는 두 가지 변형은 훈련 시간을 단축시키는 장점이 있다.&lt;/p>
&lt;h4 id="varying-the-corruption-rate">Varying the Corruption Rate&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table6.png"
width="990"
height="212"
srcset="https://kurtkim.github.io/p/t5/images/table6_hu60faf8658ec9e582286a0ecbf3ba4c0c_48105_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table6_hu60faf8658ec9e582286a0ecbf3ba4c0c_48105_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="466"
data-flex-basis="1120px"
>&lt;/p>
&lt;p>지금까지 BERT에서 사용된 15%의 토큰 손상률을 사용하였다. 그러나 text-to-text 프레임워크가 BERT와 다르므로 다른 손상률이 더 효과적인지 테스트해 보았다. 10%, 15%, 25%, 50%의 손상률을 비교했지만, 손상률이 모델 성능에 큰 영향을 미치지는 않았다. 단, 50%의 가장 높은 손상률은 GLUE와 SQuAD에서 성능 저하를 가져왔다. 또한, 높은 손상률은 학습 속도를 느리게 만드는 긴 대상을 만드는 경향이 있다. 따라서 BERT의 기준에 따라 앞으로 15%의 손상률을 사용할 것이다.&lt;/p>
&lt;h4 id="corrupting-spans">Corrupting Spans&lt;/h4>
&lt;p>예측 대상을 짧게 하여 학습 속도를 높이려고 한다. 지금까지의 방법은 각 입력 토큰을 독립적으로 손상시킬지 결정하였고, 연속된 토큰이 손상될 경우 이를 &amp;ldquo;span&amp;quot;으로 취급하여 단일 마스크 토큰으로 대체하였다. 이 방식은 레이블이 없는 텍스트 데이터를 짧은 시퀀스로 변환하지만, 항상 많은 수의 손상된 토큰이 연속적으로 나타나지는 않는다. 따라서 토큰의 span을 특정하여 손상시키는 방식을 사용하면 더 큰 속도 향상을 얻을 수 있습니다. 이러한 방법은 BERT의 사전 학습 목표로도 사용되어 성능 향상을 가져왔다.&lt;/p>
&lt;p>토큰의 연속적인 span을 손상시키는 목표를 테스트하기 위해, 손상시킬 토큰의 비율과 손상된 span의 총 수를 parameter로 사용한다. 예를 들어, 500개의 토큰 시퀀스에서 15%의 토큰을 손상시키고 총 span이 25개가 되도록 지정하면, 손상된 토큰의 총 수는 75개이고 평균 span 길이는 3이 된다. 이 방식은 원래의 시퀀스 길이와 손상률에 따라 span의 길이나 총 span 수를 조절할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table7.png"
width="978"
height="238"
srcset="https://kurtkim.github.io/p/t5/images/table7_huae5c3427b58561a089d0c71ef313c023_55278_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table7_huae5c3427b58561a089d0c71ef313c023_55278_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="410"
data-flex-basis="986px"
>&lt;/p>
&lt;p>span 손상 목표와 독립 동일 분포(i.i.d) 손상 목표를 비교한 결과, 이들 사이에는 큰 차이가 없었다. 하지만 평균 span 길이가 10인 경우에는 일부에서 다른 값들보다 성능이 약간 떨어졌다. 반면에 평균 span 길이가 3인 경우에는 대부분의 비번역 벤치마크에서 i.i.d. 목표를 약간 능가하였다. 또한 span 손상 목표는 평균적으로 더 짧은 시퀀스를 생성함으로써 학습 속도를 빠르게 할 수 있었다.&lt;/p>
&lt;h4 id="discussion">Discussion&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure5.png"
width="722"
height="378"
srcset="https://kurtkim.github.io/p/t5/images/figure5_hu64e6cb9f6d88cd1de9cb8c348334d5db_76782_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure5_hu64e6cb9f6d88cd1de9cb8c348334d5db_76782_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>denoising 목표가 언어 모델링 및 deshuffling보다 사전 학습에 더 효과적이었다. 또한 denoising 목표의 다양한 변형 사이에는 큰 차이가 없었다. 그러나 목표의 선택이나 parameter화는 시퀀스 길이와 학습 속도에 영향을 미친다. 따라서 denoising 목표의 선택은 주로 계산 비용에 기반해 이루어져야 한다. 또한, 유사한 목표에 대한 추가적인 탐색은 큰 이익을 가져오지 않을 수 있으며, 레이블이 없는 데이터를 활용하는 새로운 방법을 탐색하는 것이 더 유익할 수 있다.&lt;/p>
&lt;h3 id="pre-training-data-set">Pre-training Data set&lt;/h3>
&lt;p>사전 학습 데이터 세트는 전이 학습 파이프라인의 핵심 요소이지만, 새로운 데이터 세트는 종종 중요한 기여로 인식되지 않고, 사전 학습된 모델과 코드와 함께 공개되지 않는다. 그 결과, 다양한 사전 학습 데이터 세트의 비교는 부족하고 &amp;ldquo;표준&amp;rdquo; 데이터 세트도 없다. 최근에는 큰 데이터 세트와 작은 기존 데이터 세트에서의 사전 학습을 비교한 연구가 있다.&lt;/p>
&lt;h4 id="unlabeled-data-sets">Unlabeled Data Sets&lt;/h4>
&lt;p>C4 제작 과정에서, Common Crawl로부터 추출한 웹 텍스트를 필터링하기 위한 다양한 방법을 개발하였다. 이 필터링이 다른 방법과 비교하여 downstream task에서 성능 향상을 가져오는지 측정하려 한ㄴ다. 이를 위해 다양한 데이터 세트에서 사전 학습한 후의 기준 모델 성능을 비교하였다.&lt;/p>
&lt;p>&lt;strong>C4&lt;/strong> 기준이 되는 데이터셋으로, 레이블 없는 데이터 세트에서 사전 훈련하는 것을 고려한다.&lt;/p>
&lt;p>&lt;strong>Unfiltered C4&lt;/strong> C4를 생성할 때 사용한 휴리스틱 필터링의 효과를 측정하기 위해, 필터링을 생략한 C4의 대체 버전을 만들었다. 하지만, 영어 텍스트 추출을 위해 langdetect는 여전히 사용되며, 이로 인해 &amp;ldquo;unfiltered&amp;rdquo; 버전도 어느 정도 필터링이 포함된다.&lt;/p>
&lt;p>&lt;strong>RealNews-like&lt;/strong> 최근 연구에서는 뉴스 웹사이트에서 추출한 텍스트 데이터를 사용하였다. 이를 비교하기 위해, C4를 필터링하여 &amp;ldquo;RealNews&amp;rdquo; 데이터 세트에서 사용된 도메인의 콘텐츠만 포함하도록 한 새로운 레이블 없는 데이터 세트를 생성하였다. C4에서 사용된 필터링 방법을 유지하되, 비뉴스 콘텐츠는 모두 제외하였다.&lt;/p>
&lt;p>&lt;strong>WebText-like&lt;/strong> WebText 데이터 세트는 Reddit에 제출된 웹페이지 중 점수가 3점 이상인 콘텐츠만 사용했다. 비교 가능한 데이터 세트를 만들기 위해, C4에서 OpenWebText 목록에 없는 URL의 콘텐츠를 모두 제거하였다. 하지만, 대부분의 페이지가 Reddit에 나타나지 않아, 결과적으로 콘텐츠가 많지 않았다. 그래서, 2018년 8월부터 2019년 7월까지의 Common Crawl 데이터를 다운로드하여 필터링을 적용하였고, 이를 통해 원래의 WebText 데이터 세트와 비교 가능한 17GB의 데이터 세트를 생성하였다.&lt;/p>
&lt;p>&lt;strong>Wikipedia&lt;/strong> Wikipedia는 엄격한 품질 가이드라인을 준수하는 수백만 개의 협업 글로 이루어져 있다. 이러한 특성 때문에 Wikipedia는 청결하고 자연스러운 텍스트의 신뢰성 있는 출처로 활용되었다. 기사의 마크업이나 참조 섹션을 생략한 TensorFlow Datasets의 영어 Wikipedia 텍스트 데이터를 사용하고 있다.&lt;/p>
&lt;p>&lt;strong>Wikipedia + Toronto Books Corpus&lt;/strong> Wikipedia의 사전학습 데이터를 사용하는 단점은 자연어 텍스트의 한 도메인만을 대표한다는 것이다. 이를 보완하기 위해 BERT는 Wikipedia 데이터와 전자책에서 추출한 텍스트를 담은 Toronto Books Corpus를 결합하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table8.png"
width="1116"
height="266"
srcset="https://kurtkim.github.io/p/t5/images/table8_hub5142a928751920f9041e3dd3f39acdd_75926_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table8_hub5142a928751920f9041e3dd3f39acdd_75926_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="419"
data-flex-basis="1006px"
>&lt;/p>
&lt;p>사전 학습 데이터 세트의 도메인이 제한된 경우, 다양한 데이터 세트를 사용한 것보다 성능이 뛰어날 수 있다. 특히, 해당 도메인과 관련된 작업에서 성능 향상이 두드러진다. 하지만, 이 방법은 모든 도메인의 언어 작업에 빠르게 적응하는 모델을 만드는 목표와는 약간 다르다.&lt;/p>
&lt;p>단일 도메인에서만 사전 학습을 하는 것의 단점은 결과적으로 데이터 세트가 작아진다. WebText와 유사한 변형은 C4 데이터 세트와 같거나 더 좋은 성능을 보였지만, Reddit 기반 필터링은 더 많은 데이터에도 불구하고 C4보다 훨씬 작은 데이터 세트를 만들었다.&lt;/p>
&lt;h4 id="pre-training-data-set-size">Pre-training Data set Size&lt;/h4>
&lt;p>제한된 레이블 없는 데이터 세트 크기의 영향을 테스트하기 위해, C4를 인공적으로 줄인 버전에서 베이스라인 모델을 사전 학습시켰다. 이 때, 다양한 크기의 축소된 C4 변형에서 학습을 진행하였으며, 이는 사전 학습 과정에서 데이터 세트를 각각 64, 256, 1,024, 4,096번 반복하는 것을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table9.png"
width="1122"
height="240"
srcset="https://kurtkim.github.io/p/t5/images/table9_hub7a867d791705562125ad072f0ae3b43_56330_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table9_hub7a867d791705562125ad072f0ae3b43_56330_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="467"
data-flex-basis="1122px"
>&lt;/p>
&lt;p>데이터 세트 크기가 줄어들면서 성능이 저하되는 것을 확인할 수 있다. 이는 모델이 사전 학습 데이터를 기억하기 시작하면서 발생하는 것으로 보인다. 이를 검증하기 위해 각 데이터 세트 크기에 대한 학습 손실을 그렸고, 데이터 세트 크기가 줄어들면서 학습 손실이 크게 감소하는 것을 확인하였다. 이는 모델이 데이터를 기억하고 있음을 나타내는 증거일 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure6.png"
width="738"
height="426"
srcset="https://kurtkim.github.io/p/t5/images/figure6_hub75381f4df7e5bfa5754f4d164a6e5ed_71302_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure6_hub75381f4df7e5bfa5754f4d164a6e5ed_71302_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>사전 학습 데이터 세트가 64번만 반복될 때, 이러한 효과는 제한적이라는 것을 확인하였다. 이는 일정량의 사전 학습 데이터 반복이 해롭지 않을 수 있음을 나타낸다. 추가적인 사전 학습이 유익하고 레이블이 없는 추가 데이터를 쉽게 얻을 수 있으므로, 가능하면 큰 사전 훈련 데이터 세트를 사용하는 것이 좋다. 더 큰 모델은 작은 사전 학습 데이터 세트에 과적합되는 경향이 더욱 강하게 나타난다.&lt;/p>
&lt;h3 id="training-strategy">Training Strategy&lt;/h3>
&lt;h4 id="fine-tuning-methods">Fine-tuning Methods&lt;/h4>
&lt;p>초기 연구에서는 사전 학습된 모델의 문장 임베딩을 사용하는 작은 분류기의 parameter만 미세 조정하는 것을 제안하였다. 하지만 이 방법은 encoder-decoder 모델에는 적용하기 어렵다. 대신, 모델의 일부 parameter만 업데이트하는 두 가지 대안적인 미세 조정 방법을 고려한다.&lt;/p>
&lt;p>&amp;ldquo;adapter layers&amp;quot;는 원래 모델의 대부분을 고정하고 미세 조정하는 방법이다. Transformer의 각 블록에 dense-ReLU-dense 블록 형태의 adapter layer를 추가하며, 이 layer는 출력 차원이 입력과 같도록 설계된다. 미세 조정 시, adapter layer와 layer normalization parameter만 업데이트되며, 전방향 네트워크의 내부 차원 $d$는 신규 parameter의 수를 결정하는 주요 hyperparameter이다.&lt;/p>
&lt;p>&amp;ldquo;gradual unfreezing&amp;quot;은 시간이 지남에 따라 모델의 parameter를 점차 미세 조정하는 방식이다. 미세 조정 시작 시 최종 층의 parameter만 업데이트하고, 일정 업데이트 후에는 이전 층의 parameter도 포함시키는 방식으로 진행된다. 이 방법은 encoder와 decoder의 층을 동시에 상단부터 점진적으로 언프리징하며, 미세 조정 과정은 12개의 단계로 나누어 진행된다. 이 방식은 데이터 세트 크기의 다양성과 복합 작업의 존재 때문에 채택되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table10.png"
width="1078"
height="266"
srcset="https://kurtkim.github.io/p/t5/images/table10_hu0ed6c9e3c0f7f8b5ed49c577b04da6e5_72206_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table10_hu0ed6c9e3c0f7f8b5ed49c577b04da6e5_72206_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="405"
data-flex-basis="972px"
>&lt;/p>
&lt;p>adapter layers는ㄴ 작업 크기에 따라 차원을 적절히 조정하면 parameter가 적은 상태에서도 미세 조정이 가능하다는 것을 보여준다. 반면에, gradual unfreezing은 미세 조정 시간을 단축시키지만, 모든 작업에서 성능이 약간 떨어진다는 결과를 보여주었다. 이를 통해 언프리징 일정을 더 신중하게 조정하면 더 나은 결과를 얻을 수 있을 것으로 예상된다.&lt;/p>
&lt;h4 id="multi-task-learning">Multi-task Learning&lt;/h4>
&lt;p>Multi-task Learning은 여러 작업을 동시에 학습하는 방법으로, 하나의 모델이 다양한 작업을 수행하도록 학습한다. 이 방법은 데이터 세트를 혼합하여 레이블이 없는 데이터에서도 학습이 가능하다. 중요한 점은 모델이 각 작업에서 적절한 양의 데이터를 학습하도록 하는 것이며, 이는 데이터 세트 크기, 작업 학습의 난이도, 정규화 등에 따라 달라진다. 또한, 한 작업에서의 성과가 다른 작업의 성능을 저해하는 문제를 고려하여, 데이터 비율 설정 전략을 연구하고 있다.&lt;/p>
&lt;p>&lt;strong>Examples-proportional mixing&lt;/strong> 모델이 작업에 overfit되는 속도는 작업의 데이터 세트 크기에 따라 결정된다. 그래서 데이터 세트 크기에 비례하여 샘플링하는 것이 일반적이다. 하지만, 어떤 작업은 데이터 크기가 월등히 크기 때문에, 이 방법을 사용하면 레이블이 없는 데이터가 대부분이 되고 모든 지도 작업이 undertrain 되는 문제가 발생한다. 이를 해결하기 위해, 비율을 계산하기 전에 데이터 세트 크기에 인위적인 &amp;ldquo;limit&amp;quot;을 설정한다. 각 작업에서 샘플하는 확률은 작업의 데이터 세트 크기와 인위적인 제한 작은 값에 비례하도록 설정된다.&lt;/p>
&lt;p>&lt;strong>Temperature-scaled mixing&lt;/strong> 데이터 세트 크기의 큰 차이를 완화하는 다른 방법은 혼합 비율의 &amp;ldquo;temperature&amp;quot;를 조정하는 것이다. 이 방식은 다국어 BERT에서 적용되어, 자원이 적은 언어에 대한 충분한 학습을 보장하였다. 이는 각 작업의 혼합 비율을 1/temperature로 거듭제곱하고, 이 비율이 합쳐져 1이 되도록 재정규화하는 방식으로 이루어진다. 온도가 증가하면 비율은 동등 혼합에 가까워진다. 가장 큰 데이터 세트의 혼합 비율이 감소하는 것을 방지하기 위해 데이터 세트 크기 제한을 큰 값으로 설정한다.&lt;/p>
&lt;p>&lt;strong>Equal mixing&lt;/strong> 각 작업에서 예제를 동일한 확률로 뽑는다. 각 배치의 예제는 학습 데이터 세트 중 무작위로 선택된다. 이 방법은 low-resource 작업에 빠르게 overfit되고 high-resource 작업에 underfit되기 때문에 최적의 전략이 아닐 가능성이 크다. 이 점은 비율 설정이 최적이 아닐 때 발생할 수 있는 문제를 보여주는 참고점이다.&lt;/p>
&lt;p>이러한 mixing 전략을 기본선인 사전 학습 후 미세 조정 결과와 동일한 기준으로 비교하기 위해, 총 스텝 수가 같은 multi-task Learning 모델을 학습시킨다: $2^{19} + 2^{18} =$ 786,432.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table11.png"
width="1156"
height="410"
srcset="https://kurtkim.github.io/p/t5/images/table11_hu7c72fa05249237ddf7e85e8bbb090e64_128090_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table11_hu7c72fa05249237ddf7e85e8bbb090e64_128090_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;p>일반적으로, multi-task Learning은 대부분의 작업에서 사전 학습 후 미세 조정보다 성능이 떨어진다. &amp;ldquo;equal&amp;rdquo; mixing 전략은 특히 성능이 크게 저하되며, 이는 작업에 따른 데이터의 불균형 때문일 수 있다. examples-proportional mixing에서는 대부분의 작업에 대해 모델이 최적의 성능을 얻는 &amp;ldquo;sweet spot&amp;rdquo; $K$ 값이 있다. 또한, temperature-scaled mixing은 대부분의 작업에서 합리적인 성능을 얻는 수단을 제공한다. 별도의 모델이 각각의 작업에 대해 훈련된 것보다 multi-task 모델이 더 나은 성능을 보이지 못한 것은 이전 연구에서도 관찰된 바 있다.&lt;/p>
&lt;h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning&lt;/h4>
&lt;p>multi-task Learning의 개선된 버전을 연구하고 있다. 이 방법은 모든 작업에 대해 모델을 사전 학습하고, 각각의 작업에 대해 미세 조정하는 방식으로, 이 방식은 &amp;ldquo;MT-DNN&amp;quot;에서 사용되었으며, GLUE 및 기타 벤치마크에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>이 접근법의 세 가지 변형을 고려한다: 첫 번째는 모든 작업을 사전 학습하고 각 작업에 대해 미세 조정하는 것, 두 번째는 하나의 작업을 제외하고 사전 학습한 후 제외된 작업에 대해 미세 조정하는 것, 세 번째는 감독 작업만을 사전 학습하는 것이다. 이 모든 변형에서는 일정 단계 동안 사전 학습 후 미세 조정을 진행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table12.png"
width="1236"
height="234"
srcset="https://kurtkim.github.io/p/t5/images/table12_hua134220b52c750cb2e15f0b84a5494fc_73598_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table12_hua134220b52c750cb2e15f0b84a5494fc_73598_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="528"
data-flex-basis="1267px"
>&lt;/p>
&lt;p>multi-task 사전 학습 후 미세 조정을 한 결과가 기준선과 비교해도 비슷한 성능을 보여주었다. 이는 multi-task 학습 후 미세 조정이 다른 mixing 비율 간의 트레이드오프를 완화하는 데 도움이 될 수 있음을 보여준다. 또한, &amp;ldquo;leave-one-out&amp;rdquo; 학습 방식의 성능은 약간만 떨어졌고, 이는 다양한 작업에 대해 학습한 모델이 새로운 작업에도 적응할 수 있음을 시사한다. 그러나, supervised multi-task 사전 학습은 번역 작업을 제외하고는 성능이 떨어졌다. 이는 번역 작업이 사전 학습에서 덜 이익을 보며, 비지도 사전 학습이 다른 작업에서 중요함을 시사한다.&lt;/p>
&lt;h3 id="scaling">Scaling&lt;/h3>
&lt;p>&amp;ldquo;bitter lesson&amp;quot;은 계산을 늘리는 일반적인 방법이 인간의 전문성에 의존하는 방법보다 우월하다는 주장이다. 이는 자연어 처리의 전이 학습에도 적용될 수 있으며, 규모를 확대하는 것이 더 신중한 설계보다 성능을 향상시킴을 보여주었다. 이 논문에서는 &amp;ldquo;4배 더 많은 컴퓨팅 파워를 얻었다면 어떻게 사용해야 할까?&amp;ldquo;라는 주제로, 규모를 확대하는 다양한 방법을 비교한다.&lt;/p>
&lt;p>220M의 parameter를 가진 기준 모델로 시작한다. 이 모델은 &amp;ldquo;$BERT_{BASE}$&amp;ldquo;와 유사한 크기의 encoder와 decoder를 가지고 있다. 모델 크기를 증가시키기 위해, &amp;ldquo;$BERT_{LARGE}$&amp;ldquo;의 가이드라인을 따라 두 가지 변형을 만들어내었다: encoder와 decoder 각각에 16층과 32층을 가진 모델이다. 이들은 원래 모델보다 매개변수가 2배와 4배 많으며, 계산 비용도 2배와 4배이다. 이 모델들을 사용하여 4배의 계산을 사용하는 세 가지 방법을 고려한다: 4배 많은 step 학습, 2배 큰 모델로 2배 많은 step 학습, 그리고 &amp;ldquo;baseline&amp;rdquo; 학습 step에 대해 4배 큰 모델 학습.&lt;/p>
&lt;p>데이터를 4배 더 처리하는 방법 중 하나는 배치 크기를 4배로 늘리는 것이다. 이는 학습 속도를 빠르게 하지만, 4배 많은 학습 step을 사용하는 것과 다른 결과를 가져올 수 있어 이를 비교하기 위한 추가 실험을 진행하였다. 또한, 추가 계산을 활용하는 다른 방법으로는 모델의 앙상블을 사용하는 것이 일반적이다. 이를 비교하기 위해, 4개의 별도로 학습된 모델의 앙상블 성능을 평가했으며, 하나의 모델을 사전 학습하고 이를 4배로 미세 조정하는 비용 절감 방법도 함께 고려하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table13.png"
width="1130"
height="296"
srcset="https://kurtkim.github.io/p/t5/images/table13_hu2213d3324791a2e622eacc00b289ce4d_82725_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table13_hu2213d3324791a2e622eacc00b289ce4d_82725_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="381"
data-flex-basis="916px"
>&lt;/p>
&lt;p>학습 시간과 모델 크기를 증가시키면 성능이 개선되며, 특히 모델 크기 증가와 앙상블 방법이 더 큰 향상을 가져왔다. 그러나 앙상블 방법은 SuperGLUE에서는 큰 효과를 보이지 못했다. 또한, 스케일링 방법 선택 시, 큰 모델의 미세 조정과 추론 비용, 작은 모델의 학습 시간, 그리고 앙상블의 계산 비용 등을 고려해야 한다. 따라서 모델의 최종 사용을 고려하는 것이 중요하다.&lt;/p>
&lt;h3 id="putting-it-all-together">Putting It All Together&lt;/h3>
&lt;p>자연어 처리 벤치마크에서 얼마나 성능을 끌어올릴 수 있는지 확인하기 위해, baseline 학습 접근법으로 시작하여 다음과 같은 변경을 만든다:&lt;/p>
&lt;p>&lt;strong>Objective&lt;/strong> 기본 모델의 노이즈 제거 목표를 SpanBERT에서 영감을 받은 span-corruption 목표로 교체하였다. 평균 span 길이 3을 사용하고 원래 시퀀스의 15%를 손상시켰는데, 이 방법은 목표 시퀀스 길이가 짧아 계산 효율성이 높으며, 약간 더 나은 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Longer training&lt;/strong> 기본 모델은 작은 양의 사전 학습을 사용하지만, C4 데이터셋의 크기 때문에 데이터를 반복하지 않고도 더 오래 학습할 수 있다. 추가적인 사전 학습이 도움이 되며, 배치 크기와 학습 단계 수를 늘리는 것이 이에 도움이 되는 것을 확인하였다. 약 1M 개의 사전 학습 토큰에 대해 모델을 사전 학습하였고, 몇 가지 작은 데이터셋에서는 C4보다 더 좋은 성능을 보였지만, 이 작은 데이터셋들은 1M 토큰의 사전 학습 과정에서 수백 번 반복될 만큼 충분히 작기 때문에, C4 데이터셋을 계속 사용하기로 결정했다.&lt;/p>
&lt;p>&lt;strong>Model sizes&lt;/strong> 기본 모델 크기를 확장하면 성능이 향상된다는 것을 확인 했지만, 컴퓨팅 자원이 제한된 상황에서는 작은 모델이 유용할 수 있다. 이를 고려하여, 다양한 크기의 모델을 학습시킨다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Base&lt;/strong> 이 모델은 대략 220M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>Small&lt;/strong> 기본 모델을 축소시키기 위해 512의 $d_{model}$, 2,048의 $d_{ff}$, 8개의 attention head, 그리고 encoder와 decoder 각각에 6개의 layer를 사용한다. 이 모델은 대략 60M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>Large&lt;/strong> 기본 모델이 $BERT_{BASE}$ 크기의 encoder와 decoder를 사용하기 때문에, $BERT_{LARGE}$와 비슷한 크기와 구조를 가진 encoder와 decoder를 가진 변형을 고려하였다. 이 변형은 약 770M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>3B and 11B&lt;/strong> 더 큰 모델을 사용할 때 가능한 성능을 탐색하기 위해 두 가지 추가 변형을 고려하였다. 두 경우 모두에서 $d_{model} = 1024$, 24개 layer의 encoder와 decoder, 그리고 $d_{ff} = 128$을 사용하였다. &amp;ldquo;3B&amp;rdquo; 변형은 $d_{ff} =$ 16,384와 32개의 attention head를 사용하여 약 2.8B의 parameter를 생성했고, &amp;ldquo;11B&amp;rdquo; 변형은 $d_{ff} =$ 65,536과 128개의 attention head를 사용하여 약 11B개의 parameter를 가진 모델을 생성하였다.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Multi-task pre-training&lt;/strong> 비지도 작업과 지도 작업의 다양한 사전 학습된 모델이 비지도 작업만으로 학습된 모델만큼 잘 작동한다는 것을 확인하였다. 이 방법은 학습 기간 동안 성능을 지속적으로 모니터링 할 수 있어 유용하다. 따라서, 최종 실험에서 이 multi-task 사전 학습 방식을 사용했다. 또한, 더 크고 오래 훈련된 모델은 레이블이 없는 데이터의 더 큰 비율에서 이익을 얻을 것으로 예상했다. 이를 바탕으로, 레이블이 없는 데이터를 위해 특정한 인공 데이터 세트 크기를 사용했고, 모든 모델 변형에 대해 WMT English to French 및 WMT English to German 데이터 세트의 크기를 사전 학습 동안 1M 예제로 제한하였다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning on individual GLUE and SuperGLUE tasks&lt;/strong> GLUE와 SuperGLUE에서 미세 조정할 때, 모든 데이터 세트를 합쳐서 한 번에 모델을 미세 조정하였다. 이 방식은 연구를 단순화하지만, 일부 작업에서는 성능이 약간 떨어진다는 것을 발견하였다. 개별 작업에 미세 조정하는 것은 low-resource 작업에 빠르게 overfit될 위험이 있다. 그래서 각 GLUE와 SuperGLUE 작업에 대한 미세 조정 시 작은 배치 크기를 사용하고, overfit되기 전에 모델의 parameter에 접근할 수 있도록 1,000 단계마다 체크포인트를 저장하였다.&lt;/p>
&lt;p>&lt;strong>Beam search&lt;/strong> 이전 결과는 모두 greedy decoding을 사용하여 보고되었다. 그러나 긴 출력 시퀀스 작업에서는 beam search로 성능이 향상되었다. WMT 번역과 CNN/DM 요약 작업에서는 beam width 4와 길이 패널티 $\alpha = 0.6$을 사용하였다.&lt;/p>
&lt;p>&lt;strong>Test set&lt;/strong> 최종 실험에서는 validation set가 아닌 test set 결과를 보고한다. CNN/Daily Mail은 standard test set를, WMT 작업은 각 언어 쌍에 대한 특정 newstest를 사용했다. GLUE와 SuperGLUE는 벤치마크 평가 서버를 통해 test set 점수를 계산했다. SQuAD의 경우, 벤치마크 서버의 컴퓨팅 자원이 부족해 가장 큰 모델에서의 예측을 얻지 못했으므로, 검증 세트 성능을 계속 보고하게 되었다. 하지만 SQuAD test set에서 가장 높은 성능을 보인 모델이 검증 세트 결과도 보고했으므로, state-of-the-art의 기술과 비교 가능하다.&lt;/p>
&lt;p>위에서 언급한 변경사항들을 제외하고, baseline과 같은 학습 절차와 hyperparameter를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table14.png"
width="1118"
height="1358"
srcset="https://kurtkim.github.io/p/t5/images/table14_hu9a784fc3b3eaf96fb53a769d5658efb8_304457_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table14_hu9a784fc3b3eaf96fb53a769d5658efb8_304457_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="82"
data-flex-basis="197px"
>&lt;/p>
&lt;p>전반적으로, 24개의 작업 중 18개에서 state-of-the-art를 달성하였다. 가장 큰 모델(11B parameter)이 모든 작업에서 가장 우수한 성능을 보였다. T5-3B 모델은 몇몇 작업에서 이전 state-of-the-art를 능가했지만, 11B parameter로 모델 크기를 확장하는 것이 state-of-the-art를 달성하는 데 가장 중요했다.&lt;/p>
&lt;p>T5는 평균 GLUE 점수에서 90.3의 state-of-the-art를 달성하였고, 특히 자연어 추론 작업에서 이전 state-of-the-art 보다 월등히 높은 성능을 보여주었다. 그러나 이 성능은 여러 모델의 앙상블과 대량의 계산을 활용한 결과였다. SQuAD에서는 Exact Match 점수에서 이전 최고 성능을 1점 이상 능가하였으며, SuperGLUE에서는 평균 점수를 크게 향상시켰지만, 일부 작업에서는 여전히 인간의 성능에 미치지 못했다. WMT 번역 작업에서는 state-of-the-art를 달성하지 못했고, CNN/Daily Mail에서는 state-of-the-art를 달성했지만, 요약의 일관성과는 반드시 연결되지 않았다. 이 모든 결과는 앙상블, 외부 데이터 세트 활용 등 다양한 방법을 통해 성능을 향상시키고 있음을 보여준다.&lt;/p>
&lt;p>T5는 실험 연구의 통찰력과 큰 규모를 결합하여 강력한 성능을 보여준다. baseline 모델의 사전 훈련량 또는 크기를 증가시키면 상당한 향상이 있었고, 이를 바탕으로 T5의 성능 향상에 얼마나 기여했는지 측정하고자 하였다. 이를 위해 표준 baseline 모델, 1 trillion 토큰으로 훈련된 모델 및 T5-Base를 비교하는 실험을 수행하였다. 이 두 모델의 성능 비교를 통해, 체계적인 연구에서 얻은 통찰력이 어떤 영향을 미쳤는지 구체적으로 측정할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table15.png"
width="942"
height="176"
srcset="https://kurtkim.github.io/p/t5/images/table15_hu3181bf2f435ab289c1044f5a8f3acab4_38762_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table15_hu3181bf2f435ab289c1044f5a8f3acab4_38762_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="535"
data-flex-basis="1284px"
>&lt;/p>
&lt;p>세 가지 모델 구성의 성능은 T5-Base가 모든 downstream task에서 baseline-1T을 월등히 능가함을 보여준다. 이는 T5의 성공에 크기 뿐만 아니라 다른 비확장 요소들도 기여하고 있음을 나타낸다. 추가적인 사전 학습은 모델의 성능을 향상시키는 데 도움이 된다.&lt;/p>
&lt;hr>
&lt;h2 id="reflection">Reflection&lt;/h2>
&lt;p>이 분야의 더 나은 발전을 위해 효과적인 접근법을 제안하고자 한다.&lt;/p>
&lt;h3 id="takeaways">Takeaways&lt;/h3>
&lt;p>&lt;strong>Text-to-text&lt;/strong> Text-to-text 프레임워크는 다양한 텍스트 작업에 대해 단일 모델을 훈련시키는 간단한 방법을 제공한다. 이 방법은 생성, 분류, 회귀 작업 등에 성공적으로 적용될 수 있다. 간결함에도 불구하고, 이 프레임워크는 과제별 구조와 비교할 만한 성능을 보여주고, 규모와 결합하면 state-of-the-art를 달성한다.&lt;/p>
&lt;p>&lt;strong>Architectures&lt;/strong> NLP의 전이 학습에서 Transformer의 구조적 변형을 고려했지만, 원래의 encoder-decoder 형태가 text-to-text 프레임워크에서 가장 효과적이었다. encoder-decoder 모델은 더 많은 parameter를 사용하지만, 계산 비용은 비슷하다. encoder와 decoder에서 parameter를 공유하면 전체 parameter 수가 절반으로 줄어들지만 성능 저하는 별로 없다.&lt;/p>
&lt;p>&lt;strong>Unsupervised objectives&lt;/strong> 대부분의 &amp;ldquo;denoising&amp;rdquo; 목표, 즉 임의로 손상된 텍스트를 재구성하는 학습은 text-to-text 설정에서 비슷한 성능을 보였다. 따라서, 계산 효율성을 위해 짧은 대상 시퀀스를 생성하는 목표를 사용하는 것이 좋다.&lt;/p>
&lt;p>&lt;strong>Data sets&lt;/strong> Common Crawl 웹 덤프에서 정리한 &amp;ldquo;Colossal Clean Crawled Corpus (C4)&amp;ldquo;를 소개하였다. 레이블이 없는 데이터로 학습하면 몇몇 downstream task에서 성능이 향상될 수 있지만, 데이터셋 크기가 줄어들 수 있다. 레이블이 없는 데이터셋이 작아서 여러 번 반복되면 성능이 저하될 수 있으므로, 크고 다양한 데이터셋인 C4의 사용이 중요하다는 것을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Training strategies&lt;/strong> 사전 학습된 모델의 모든 parameter를 미세 조정하는 방식이 비록 비용이 많이 들지만 더 우수한 성능을 보여주었다. 여러 작업을 동시에 학습하는 방법을 시도했지만, 특정 작업에 대한 학습 비율을 설정하는 전략을 찾지 못하였다. 하지만, 여러 작업의 혼합에서 사전 학습 후 미세 조정하는 것이 비지도 사전 학습과 비슷한 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Scaling&lt;/strong> 추가 계산을 활용하는 다양한 전략을 비교한 결과, 각 방법이 성능을 크게 향상시켰으나, 더 많은 데이터로 작은 모델을 학습하는 것이 종종 더 적은 단계로 큰 모델을 학습하는 것보다 성능이 떨어졌다. 그러나 모델의 앙상블은 단일 모델보다 더 좋은 결과를 제공하였고, 같은 baseline 모델에서 미세 조정한 앙상블은 모든 모델을 별도로 학습하는 것보다는 성능이 떨어졌지만, 단일 모델보다는 훨씬 더 우수하였다.&lt;/p>
&lt;p>&lt;strong>Pushing the limits&lt;/strong> state-of-the-art를 얻기 위해 큰 모델(최대 11B 개의 parameter)을 학습시켰다. 이를 위해 C4 데이터 세트에서 텍스트를 추출하고, 노이즈를 제거하는 목표를 적용하였다. 또한, 1 trillion 이상의 토큰에 대해 학습을 진행하였고, 결과를 쉽게 복제하고 확장하고 적용하기 위해 우리의 코드, C4 데이터 세트, 그리고 각 T5 변형에 대한 사전 학습된 모델 가중치를 공개하였다.&lt;/p>
&lt;h3 id="outlook">Outlook&lt;/h3>
&lt;p>&lt;strong>The inconvenience of large models&lt;/strong> 큰 모델이 더 좋은 성능을 내는 경향이 있다는 것이 중요한 결과로 나타났다. 하지만 client-side inference이나 federated learning과 같이 작은 모델이 도움이 되는 경우도 있다. low-resource 작업에서 좋은 성능을 얻는 것이 전이 학습의 한 가지 유익한 사용처이다. 따라서 저렴한 모델로 더 강한 성능을 달성하는 방법에 대한 연구를 지지한다. 이런 연구로는 distillation, parameter sharing, 그리고 conditional computation이 있다.&lt;/p>
&lt;p>&lt;strong>More efficient knowledge extraction&lt;/strong> 사전 학습의 목표는 모델에 &amp;ldquo;knowledge&amp;quot;를 제공하여 downstream task의 성능을 향상시키는 것이다. 현재 일반적으로 사용되는 방법은 텍스트의 오염된 부분을 복원하도록 학습시키는 것인데, 이 방법이 모델에 일반 지식을 가르치는 가장 효율적인 방법이 아닐 수도 있다. 더 효율적인 방법으로는, 실제 텍스트와 기계 생성 텍스트를 구분하도록 모델을 사전 학습시키는 방법이 있다.&lt;/p>
&lt;p>&lt;strong>Formalizing the similarity between tasks&lt;/strong> 도메인 데이터에 대한 사전 학습이 downstream task의 성능을 향상시키는 것을 확인하였다. 레이블이 없는 데이터 소스를 선택하는 데에 더 원칙적인 접근을 가능하게 하기 위해, 사전 학습과 downstream task 사이의 &amp;ldquo;similarity&amp;quot;에 대한 엄밀한 개념을 정립하는 것이 필요하다. 이는 컴퓨터 비전 분야에서 이미 일부 연구가 이루어지고 있다. 또한, 작업 간의 관련성에 대한 더 나은 이해는 지도 사전 학습 작업을 선택하는 데에도 도움이 될 수 있다.&lt;/p>
&lt;p>&lt;strong>Language-agnostic models&lt;/strong> 영어로만 사전 학습한 결과가 번역 작업에서 최고 수준의 성과를 내지 못하였다. 이를 해결하기 위해, 어떤 언어의 텍스트든 좋은 성능으로 NLP 작업을 수행할 수 있는 언어에 구애받지 않는 모델을 더 연구하려고 한다. 이는 세계 인구 대다수의 모국어가 영어가 아닌 점을 고려하면 매우 중요한 이슈이다.&lt;/p>
&lt;p>이 논문은 최근 NLP에 대한 전이 학습에 대해 연구하였다. 이 연구가 시작되기 전, 학습 기반 방법이 효율성을 증명받지 못 한 상황에서 이러한 진보가 돌파구를 만들어 주었으며, 특히 전이 학습에 어려운 SuperGLUE 벤치마크에서 거의 인간 수준의 성능을 달성하였다. 이 결과는 우리의 text-to-text 프레임워크, 새로운 C4 데이터셋, 그리고 체계적인 연구에서의 통찰력의 결합에서 비롯된다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Megatron-LM</title><link>https://kurtkim.github.io/p/megatron-lm/</link><pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/megatron-lm/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델링에서 큰 transformer 모델의 학습은 자연어 처리 분야에서 state-of-the-art를 달성하였다. 하지만, 이런 큰 모델은 메모리 제약으로 학습이 어려울 수 있다. 이 연구에서는 수십억 개의 파라미터를 가진 transformer 모델을 학습시키는 방법을 제시한다.&lt;/p>
&lt;p>이 연구는 512개의 GPU를 사용하여 최대 83억 개의 파라미터를 가진 transformer 모델을 학습시키는 것에 성공하였다. 또한 이 모델을 사용하여 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다. BERT와 같은 모델에서는 layer normalization의 위치에 주의가 필요함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Natural Language Processing (NLP)는 컴퓨팅능력과 데이터셋 크기의 증가로 빠르게 발전하고 있다. 이로 인해 더 큰 언어 모델을 학습시키는 것이 가능해졌고, 이는 기사 완성, 질문 답변, 자연어 추론 등의 NLP 작업에 매우 유용하다. 이런 사전 학습된 언어 모델을 다른 자연어 작업에 미세 조정하면, state-of-the-art를 얻을 수 있다.&lt;/p>
&lt;p>모델이 커짐에 따라, 메모리 한계를 초과하여 추가적인 메모리 관리 기법이 필요해진다. ADAM 같은 optimization 알고리즘들은 모멘텀과 다른 최적화 상태를 저장하기 위해 추가 메모리를 요구하며, 이는 효과적으로 학습될 수 있는 모델의 크기를 줄인다. 이를 해결하기 위해, 모델 병렬화 접근법이 사용되며, 이는 가중치와 그들과 관련된 최적화 상태가 동시에 프로세서에 존재할 필요가 없도록 모델을 분할한다. 그러나, 이러한 접근법은 모델을 다시 작성하고, 아직 개발 중인 사용자 정의 컴파일러와 프레임워크에 의존하는 문제가 있다.&lt;/p>
&lt;p>이 연구에서는 내부 layer 모델 병렬화를 이용한 단순하고 효율적인 모델 병렬 접근법을 구현하였다. transformer 기반 언어 모델의 내재적 구조를 활용해 PyTorch에서 효율적으로 학습하는 모델 병렬 구현을 만들었으며, 이는 사용자 정의 C++ 코드나 컴파일러를 필요로 하지 않는다. 이 접근법은 GPipe와 같은 파이프라인 기반 모델 병렬화와는 별개이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure1.png"
width="618"
height="380"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure1_hu17fd794e82cbb149b38b0b5f9212968f_49270_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure1_hu17fd794e82cbb149b38b0b5f9212968f_49270_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;p>이 접근법의 확장성을 보여주기 위해, 단일 NVIDIA V100 32GB GPU에서 1.2B parameter의 모델을 학습하여 39 TeraFLOPs를 유지하는 강력한 기준선을 설정하였다. 모델을 8.3B parameter로 확장하고 512개의 GPU에서 8-way 모델 병렬화를 사용하면, 전체 애플리케이션에서 초당 최대 15.1 PetaFLOPs를 달성하며, 이는 단일 GPU 사례에 비해 76%의 확장 효율성을 보여준다.&lt;/p>
&lt;p>모델 크기가 정확도에 미치는 영향을 분석하기 위해, GPT-2와 BERT를 학습시키고 여러 downstream task에서 평가하였다. 기존의 BERT 구조는 모델 크기가 증가함에 따라 성능이 저하되는 것을 확인하였다. 이를 극복하기 위해, transformer layer의 layer normalization과 residual connection을 rearranging하였고, 이로 인해 모델 크기가 증가함에 따라 downstream task 결과가 단조롭게 향상되는 것을 확인하였다. 또한, 모델이 WikiText103, LAMBADA, 그리고 RACE 데이터셋에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>요약하면, 이 논문의 기여는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>기존의 PyTorch transformer 구현에 몇 가지 목표적인 수정만을 가함으로써, 단순하고 효율적인 모델 병렬 접근법을 구현했다.&lt;/li>
&lt;li>모델과 데이터 병렬 기법에 대한 심층적인 경험적 분석을 수행하고, 512개의 GPU를 사용하여 최대 76%의 확장 효율성을 보여준다.&lt;/li>
&lt;li>BERT와 유사한 모델에서 layer normalization의 위치에 신중하게 주의를 기울이는 것이 모델이 커짐에 따라 정확도를 높이는 데 중요하다는 것을 보여준다.&lt;/li>
&lt;li>모델 크기를 확장하는 것이 GPT-2(최대 8.3B parameter까지 연구)와 BERT(최대 3.9B parameter까지 연구) 모델 모두에 대해 정확도를 향상시키는 것을 보여준다.&lt;/li>
&lt;li>테스트 세트에서 state-of-the-art를 달성하는 것을 보여준ㄴ다: WikiText103에서의 혼란도(10.8 ppl), LAMBADA에서의 정확도(66.5%), 그리고 RACE에서의 정확도(90.9%).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="background-and-challenges">Background and Challenges&lt;/h2>
&lt;h3 id="neural-language-model-pretraining">Neural Language Model Pretraining&lt;/h3>
&lt;p>사전 학습된 언어 모델은 NLP 연구에 필수적인 도구가 되었다. 큰 규모의 말뭉치 사전 학습을 활용하여 언어의 견고한 신경 표현을 배우는 것은 활발한 연구 분야이다. 초기의 연구는 사전 학습된 단어 임베딩이 downstream task 결과를 향상시키는 것을 보여주었으며, 이후의 연구는 맥락적 단어 표현을 포착하는 신경 모델을 학습하고 전이하는 것을 통해 발전하였다. 최근의 연구는 언어 모델을 end-to-end로 미세 조정함으로써 이 아이디어들을 더욱 발전시켰다. 이런 방법들의 진보는 규모에 맞게 효율적으로 작동하고 늘어나는 계산 요구를 충족시킬 수 있는 도구의 필요성을 촉발하였고, 이 연구는 트렌드에서 한 걸음 더 나아가기 위한 도구를 제공하려고 한다.&lt;/p>
&lt;h3 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention&lt;/h3>
&lt;p>현재 NLP 연구는 우수한 정확도와 계산 효율성 때문에 transformer 모델을 사용하는 경향이 있다. transformer는 원래 두 부분, encoder와 decoder를 사용하는 기계 번역 아키텍처로 설계되었지만, 최근의 연구는 필요에 따라 encoder나 decoder만 사용한다. 이 연구는 decoder 구조인 GPT-2와 encoder 구조인 BERT를 모두 연구한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure2.png"
width="216"
height="480"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure2_hu6b6b35e095abc54f1afae484c0c3a903_60857_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure2_hu6b6b35e095abc54f1afae484c0c3a903_60857_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="45"
data-flex-basis="108px"
>&lt;/p>
&lt;p>GPT-2와 BERT는 모두 GeLU nonlinearities와 layer normalization을 multi-head attention과 eed forward layer의 입력에 적용하는 반면, 원래의 transformer는 ReLU nonlinearities를 사용하고 layer normalization를 출력에 적용한다.&lt;/p>
&lt;h3 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning&lt;/h3>
&lt;p>신경망 학습을 여러 하드웨어 가속기로 확장하는 두 가지 주요 방법은 데이터 병렬화와 모델 병렬화이다. 데이터 병렬화는 학습 미니배치를 여러 작업자에게 분할하고, 모델 병렬화는 모델의 메모리 사용량과 계산을 여러 작업자에게 분배한다. 사용 가능한 작업자 수에 비례하여 미니배치 크기를 증가시키면 학습 데이터 처리량에서 거의 선형적인 확장을 볼 수 있다. 하지만 대량 배치 학습은 최적화 과정에 복잡성을 도입하여 정확도를 감소시키거나 수렴 시간을 늘릴 수 있다. 추가로, 데이터 병렬화를 활성화 체크포인팅과 결합하여 메모리 요구사항을 줄이는 방법도 연구되고 있다.&lt;/p>
&lt;p>기존 기법들은 모델이 한 작업자에게 완전히 맞아야 하는 제한이 있다. 크기와 복잡성이 증가하는 언어 모델로 인해, 신경망은 하드웨어 가속기의 메모리 용량에 근접하게 되었다. 이 문제를 해결하기 위한 한 가지 방법은 parameter 공유를 사용하는 것이지만, 이는 모델의 전체 용량을 제한한다. 이 연구의 접근법은 모델 병렬화를 사용하여 모델을 여러 가속기에 분할하는 것으로, 이는 메모리 압박을 완화하고 병렬성을 증가시킨다.&lt;/p>
&lt;p>모델 병렬화에는 layer-wise pipeline parallelism과 distributed tensor computation이라는 두 가지 패러다임이 있다. pipeline model parallelism에서는 한 장치에서 작업 그룹이 수행된 후 출력이 다음 장치로 전달된다. 일부 접근법은 parameter 서버를 사용하지만 일관성 문제가 있다. TensorFlow의 GPipe 프레임워크는 동기식 경사 하강법을 사용하여 이 문제를 해결한다. 그러나 이 방법은 통신과 계산 작업의 효율적인 파이프라이닝을 위한 추가 로직이 필요하며, pipeline bubble이나 최적화 변경으로 인해 효율성과 정확도에 영향을 미친다.&lt;/p>
&lt;p>distributed tensor computation은 텐서 연산을 여러 장치에 분할하여 계산을 가속화하거나 모델 크기를 증가시키는 방법이다. FlexFlow는 이러한 병렬 계산을 효과적으로 수행하는 방법을 제공한다. 최근에는 Mesh-TensorFlow가 TensorFlow에서 분산 텐서 계산을 지정하는 언어를 도입했다. 우리는 이러한 통찰력을 활용하여 transformer의 attention head를 계산하는 병렬성을 활용하여 transformer 모델을 병렬화한다. 하지만, 이 연구는 프레임워크와 컴파일러를 구현하는 대신, 기존의 PyTorch transformer 구현에 몇 가지 특정 수정을 수행한다. 이 방법은 간단하며, 새로운 컴파일러나 코드 재작성이 필요하지 않는다.&lt;/p>
&lt;hr>
&lt;h2 id="model-parallel-transformers">Model Parallel Transformers&lt;/h2>
&lt;p>transformer network의 구조를 활용해, 몇 가지 synchronization primitive를 추가하여 간단한 모델 병렬 구현을 만들었다. transformer layer는 self attention block과 two-layer, multi-layer perceptron (MLP)으로 구성되며, 이 두 부분에 모델 병렬성을 도입하였다.&lt;/p>
&lt;p>MLP block의 첫 번째 부분은 GEMM이며, 이어서 GeLU 비선형성을 따른다:&lt;/p>
&lt;p>$$ Y = GeLU(XA) $$&lt;/p>
&lt;p>GEMM을 병렬화하는 한 가지 방법은 가중치 행렬 $A$를 행 방향으로, 입력 $X$를 열 방향으로 분할하는 것이다:&lt;/p>
&lt;p>$$ X = [X_1, X_2], A = \begin{bmatrix} A_1 \\ A_2 \end{bmatrix} $$&lt;/p>
&lt;p>이 분할 방식은 결과로 $Y = GeLU(X_1 A_1 + X_2 A_2)$를 가져오며, GeLU는 비선형 함수이므로, $GeLU(X_1 A_1 + X_2 A_2) \neq GeLU(X_1 A_1) + GeLU(X_2 A_2)$이다. 따라서 이 방식은 GeLU 함수 앞에 동기화 지점이 필요하게 된다. 각 처리 유닛이 독립적으로 계산한 결과를 제대로 합산하기 위해 병렬 처리 유닛 간에 데이터 동기화가 필요하다.&lt;/p>
&lt;p>다른 옵션은 $A$를 열을 따라 분할하는 것이다. $A = [A_1, A_2]$. 이 분할 방식은 GeLU 비선형성을 각 분할된 GEMM의 출력에 독립적으로 적용할 수 있게 한다:&lt;/p>
&lt;p>$$ [Y_1, Y_2] = [GeLU(XA_1), GeLU(XA_2)] $$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure3.png"
width="574"
height="638"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure3_hub8d70cf42bc24407b9b157eb8cc73fa4_303715_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure3_hub8d70cf42bc24407b9b157eb8cc73fa4_303715_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="89"
data-flex-basis="215px"
>&lt;/p>
&lt;p>이 방법은 동기화 지점을 제거하므로 유리하다. 첫 번째 GEMM을 열 병렬 방식으로 분할하고, 두 번째 GEMM을 행으로 분할하여 GeLU 계층의 출력을 직접 받을 수 있도록 한다. 이 방식은 추가적인 통신 없이 MLP 블록의 두 GEMM을 GPU 간에 분할하며, forward path와 backward path에 각각 단 한 번의 all-reduce 연산만 필요로 한다. 이 두 연산은 서로 conjugate 관계에 있으며, PyTorch에서 간단하게 구현할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure4.png"
width="604"
height="300"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure4_hu8ce0fce626bd70352fa876adff3eded7_92476_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure4_hu8ce0fce626bd70352fa876adff3eded7_92476_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>self attention block에서는 multihead attention 연산의 병렬성을 활용하여 key(K), query(Q), value(V)와 관련된 GEMM을 열 병렬 방식으로 분할한다. 이 방식은 각 attention head에 해당하는 행렬 곱셈을 각각의 GPU에서 수행하게 하며, immediate communication이 필요 없다. 이어서, 출력 linear layer에서의 GEMM은 행 병렬 방식으로 수행되며, GPU 간의 통신 없이 병렬 주의 계층의 출력을 직접 받아들인다. 이 접근법은 MLP와 self attention layer에서 두 GEMM의 그룹을 융합하고, 중간의 동기화 지점을 제거하여 더 나은 확장성을 제공한다. 이를 통해 forward path와 backward path에서 각각 두 번의 all-reduce 연산만으로 모든 GEMM을 수행할 수 있다.&lt;/p>
&lt;p>transformer 언어 모델은 출력 임베딩을 병렬화하여 처리 속도를 향상시킨다. 이 모델은 입력 임베딩과 가중치를 공유하는 출력 임베딩 계층을 가지며, 이 가중치 행렬을 분할하여 병렬 처리한다. 그러나 이 방식은 큰 어휘 크기 때문에 많은 양의 정보를 전송해야 한다. 이를 해결하기 위해, 병렬 GEMM의 출력을 cross entropy loss와 결합하여 차원을 줄인다. 이렇게 함으로써 스칼라 손실만 전송하게 되어 통신의 양이 크게 감소하고, 모델의 병렬 처리 효율성이 향상된다.&lt;/p>
&lt;p>모델 병렬 방법론은 통신을 줄이고 GPU 계산에 초점을 맞추는 기법에 중점을 두고 있다. dropout, layer normalization, residual connection의 계산을 한 GPU에서만 수행하는 대신, 이를 모든 GPU에 복제한다. 각 GPU는 layer normalization parameter의 복제본을 유지하며, 모델 병렬 영역의 출력에서 dropout과 residual connection을 수행한다. 또한, 각 모델 병렬 작업자는 자신의 parameter 집합을 독립적으로 최적화한다. 이러한 접근법은 모든 값이 각 GPU에 로컬로 있거나 복제되므로, 업데이트된 parameter 값을 통신할 필요가 없다.&lt;/p>
&lt;p>이 연구의 방법론은 하이브리드 모델과 데이터 병렬성, 그리고 난수 생성 처리와 관련이 있다. 이는 구현이 간단하며, forward와 backward pass에 몇 가지 추가적인 all-reduce 연산만 필요로 한다. 컴파일러는 필요 없으며, 이는 기존의 파이프라인 모델 병렬화 방법과는 별개이며 이를 보완한다.&lt;/p>
&lt;hr>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>자연어 처리와 언어 이해의 핵심은 사전 학습된 언어 이해 모델이다. 이 연구에서는 왼쪽에서 오른쪽으로 텍스트를 생성하는 GPT-2와 언어 모델 마스킹에 기반한 bi-directional transformer 모델인 BERT에 초점을 맞추고 있다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>다양한 대형 언어 모델링 데이터셋을 합쳐서 longterm dependency를 가진 학습 세트를 만들었다. 이에는 Wikipedia, CC-Stories, RealNews, OpenWebtext 등이 포함되어있다. 학습 세트의 유출을 방지하기 위해 일부 Wikipedia 기사와 필요 없는 새 줄을 제거하였다. BERT 모델에는 BooksCorpus를 포함시켰지만, LAMBADA 작업과 겹치는 부분 때문에 GPT-2 학습에서는 제외하였다.&lt;/p>
&lt;p>모든 데이터셋을 병합하고, 내용 길이가 128 토큰 미만인 문서를 제외하였다. 유사한 내용의 중복을 제거하기 위해 localitysensitive hashing (LSH)을 사용했고, 그 결과 174GB의 중복 제거된 텍스트를 포함한 말뭉치를 얻었다.&lt;/p>
&lt;h3 id="training-optimization-and-hyperparameters">Training Optimization and Hyperparameters&lt;/h3>
&lt;p>효율적인 학습을 위해 mixed precision 학습과 dynamic loss scaling을 사용하였다. 가중치는 정규 분포로 초기화하고, residual layer 전에 조정했다. optimizer는 Adam을 사용하고, weight decay를 적용했다. gradient norm clipping을 사용해 학습의 안정성을 개선했고, 모든 경우에 dropout 0.1을 적용했다. 마지막으로, 메모리 관리를 위해 utilize activation checkpointing을 사용했다.&lt;/p>
&lt;p>GPT-2 모델은 1024개의 subword 단위로 300k번 반복하며 학습되며, batch size는 512이다. learning rate는 1.5e-4로 설정되어 있고, 3k번의 warmup 이후에 cosine decay를 따른다. 이 감소는 최소 학습률인 1e-5에서 멈춘다.&lt;/p>
&lt;p>BERT 모델은 원래의 BERT 사전을 사용하고, 어휘 크기는 30,522이다. next sentence prediction을 sentence order prediction으로 대체하고, 전체 단어 n-그램 마스킹을 사용하였다. batch size는 1024로 설정하고, warmup된 learning rate을 사용하여 2백만 번의 반복 동안 선형적으로 감소시켰다. 나머지 학습 parameter는 기존 BERT 모델과 동일하게 유지하였다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>모든 실험은 최대 32대의 DGX-2H 서버(총 512개의 Tesla V100 GPU)를 사용한다. 이 인프라는 서버 내부 GPU 간 300 GB/sec, 서버 간 100 GB/sec의 빠른 연결 대역폭을 제공하여 딥러닝 애플리케이션에 최적화되어 있다.&lt;/p>
&lt;h3 id="scaling-analysis">Scaling Analysis&lt;/h3>
&lt;p>구현의 확장성을 테스트하기 위해, 다양한 parameter를 가진 GPT-2 모델을 사용했다. self attention layer에서 일관된 GEMM 크기를 유지하기 위해, attention head 당 hidden size는 96으로 고정하였다. 원래의 어휘 크기는 50,257이었지만, logit layer의 효율적인 GEMM을 위해 어휘를 패딩하여 51,200으로 만들었다.&lt;/p>
&lt;p>모델 및 모델 + 데이터 병렬 확장성을 연구했으며, 모든 구성에서 배치 크기는 8로 고정하였다. 또한 모든 실험에서 전역 배치 크기를 512로 고정하여 데이터 병렬 확장성을 연구하였다. 이는 64-way 데이터 병렬성에 해당한다.&lt;/p>
&lt;h4 id="model-and-data-parallelism">Model And Data Parallelism&lt;/h4>
&lt;p>모델 병렬 및 모델 + 데이터 병렬 케이스에 대해 모델 parameter에 대한 약한 스케일링을 보여준다. 약한 스케일링은 배치 크기를 조정하여 수행되지만, 이는 단일 GPU에 맞지 않는 대형 모델을 학습하는 문제를 해결하지 못한다. 따라서 여기서는 그렇지 않으면 불가능했던 더 큰 모델을 학습하기 위해 약한 스케일링을 사용한다. 모든 스케일링 수치의 기준은 단일 GPU에서 실행되는 1.2억 개의 parameter를 가진 첫 번째 구성이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure5.png"
width="628"
height="522"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure5_huc48ecde88387473c014049f1e6065376_69432_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure5_huc48ecde88387473c014049f1e6065376_69432_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="288px"
>&lt;/p>
&lt;p>8.3B 개의 parameter와 8-way 모델 병렬성을 가진 경우 선형 스케일링의 77%를 달성하였다. 가장 큰 구성(8.3B 개의 parameter)이 512개의 GPU에서 실행되는 경우에도 선형 스케일링 대비 74%의 스케일링을 달성하였다.&lt;/p>
&lt;h3 id="language-modeling-results-using-gpt-2">Language Modeling Results Using GPT-2&lt;/h3>
&lt;p>거대 언어 모델이 최첨단을 더욱 발전시킬 수 있음을 보여주기 위해, 다양한 크기와 구성의 GPT-2 모델을 학습하였습니다. 355M 모델은 BERT-Large 모델과 동일하며, 2.5B 모델은 이전의 가장 큰 GPT-2 모델보다 크고, 8.3B 모델은 우리가 알고 있는 한까지 학습된 어떤 변환기 언어 모델보다 크다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table2.png"
width="626"
height="208"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table2_hu82a02da3ef2fcbaae18b436854407213_38834_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table2_hu82a02da3ef2fcbaae18b436854407213_38834_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="300"
data-flex-basis="722px"
>&lt;/p>
&lt;p>각 epoch를 진행하는데 걸리는 시간은 68,507회의 반복과 동일하며, 예를 들어, 512개의 GPU에서의 8.3B 모델에 대해서는 각 epoch이 약 두 일 정도 걸린다. 이들 모델은 이전에 본 것보다 훨씬 작지만 여전히 64개의 GPU로 학습하며, epoch 당 시간이 훨씬 적다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure6.png"
width="644"
height="362"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure6_hu2b49527af3ddf3ed9e4ea90e6c86a890_50393_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure6_hu2b49527af3ddf3ed9e4ea90e6c86a890_50393_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>모델 크기가 증가함에 따라 검증 perpelixity가 감소하는 것을 확인할 수 있다. 특히, 8.3억 개의 parameter를 가진 모델은 검증 perpelixity가 9.27에 이르렀다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table3.png"
width="538"
height="212"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table3_hu830d348617586a84f0102f02e5c97c24_34397_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table3_hu830d348617586a84f0102f02e5c97c24_34397_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="609px"
>&lt;/p>
&lt;p>또한, 모델 크기를 증가시키면 WikiText103에서의 perpelixity가 낮아지고, LAMBADA에서의 클로즈 정확도가 높아지는 추세를 관찰한다. 이 중 8.3억 개의 parameter를 가진 모델은 WikiText103 테스트 세트에서 state-of-the-art perpelixity를 달성하고, LAMBADA 작업에서 이전의 클로즈 정확도 결과를 초과하였다.&lt;/p>
&lt;p>최근에는 Microsoft와 NVIDIA가 협력하여 170억 개의 parameter를 가진 GPT-2 모델인 Turing-NLG를 학습시켰으며, 이 결과는 더 큰 모델의 가치를 강조하였다.&lt;/p>
&lt;p>테스트 데이터가 학습 데이터에 포함되지 않도록 확인하기 위해, 테스트 세트의 8-gram 중 학습 세트에도 나타나는 비율을 계산하였다. WikiText103 테스트 세트는 최대 10.8%의 겹침이 있고, LAMBADA 테스트 세트는 최대 1.4%의 겹침이 있었다. 이는 이전 연구와 일관되어, 테스트 데이터가 우연히 학습 데이터에 포함되지 않았음을 확신하였다.&lt;/p>
&lt;h3 id="bi-directional-transformer-results-using-bert">Bi-directional Transformer Results Using BERT&lt;/h3>
&lt;p>BERT 스타일의 transformer 모델에 방법론을 적용하고, 다양한 downstream task에 대한 모델 스케일링 효과를 연구한다. 이전 연구에서는 BERT-large의 336M parameter를 넘어서 모델 크기를 증가시키면 모델 저하가 발생한다는 것을 발견하였다. 이 문제를 해결하기 위해, 연구자들은 parameter 공유를 도입하고, 이를 통해 모델이 원래 BERT 모델에 비해 더 잘 확장되는 것을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/figure7.png"
width="638"
height="386"
srcset="https://kurtkim.github.io/p/megatron-lm/images/figure7_hu26c1fff3dfbe7e8b5a6a1023696dbbee_166190_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/figure7_hu26c1fff3dfbe7e8b5a6a1023696dbbee_166190_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="396px"
>&lt;/p>
&lt;p>layer normalization과 residual connection의 순서를 재배열하는 것이 BERT-Large를 넘어서 BERT 스타일 모델의 스케일링을 가능하게 하는 것이 중요하다는 것을 경험적으로 입증하였다. (b) 아키텍처는 원래 BERT 아키텍처에서 관찰된 불안정성을 제거하며, 더 낮은 학습 손실을 가진다. 이러한 변화가 더 큰 BERT 모델을 학습시키는 것을 가능하게 하는 것을 처음으로 보고하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table4.png"
width="550"
height="160"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table4_huf5abb9b3a2aee77a7a878b3cab66c67b_27226_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table4_huf5abb9b3a2aee77a7a878b3cab66c67b_27226_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="343"
data-flex-basis="825px"
>&lt;/p>
&lt;p>336M 모델은 BERT-large와 같은 크기이며, 1.3B는 이전에 더 나쁜 결과를 얻었다고 알려진 BERT-xlarge 구성과 동일하다. 더 큰 hidden size와 더 많은 layer를 사용하여 BERT 모델을 더 확장하여 3.9B parameter 경우에 도달하였다. 모든 경우에서 hidden size는 attention head 당 64로 일정하게 유지되었다. 336M과 1.3B 모델은 200만 번 반복하여 학습되었으며, 3.9B 모델은 150만 번 반복하여 학습되고 아직도 학습 중이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/megatron-lm/images/table5.png"
width="1262"
height="320"
srcset="https://kurtkim.github.io/p/megatron-lm/images/table5_hubfae993f6a380152daae9f599ea142cc_117884_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/megatron-lm/images/table5_hubfae993f6a380152daae9f599ea142cc_117884_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="394"
data-flex-basis="946px"
>&lt;/p>
&lt;p>3%의 홀드아웃 세트에서, 336M, 1.3B, 3.9B 모델은 각각 1.58, 1.30, 1.16의 검증 세트 perplexity를 달성하였고, 이는 모델 크기와 함께 단조롭게 감소하는 추세를 보여준다. 여러 downstream task에서 학습된 모델을 미세 조정한 결과, 모델 크기가 증가함에 따라 모든 경우에서 성능이 향상되었다. 특히, 3.9B 모델은 다른 BERT 기반 모델에 비해 개발 세트에서 state-of-the-art를 보여주며, RACE 테스트 세트에서 단일 모델과 앙상블 모델 모두에서 최고의 결과를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion-and-future-work">Conclusion and Future Work&lt;/h2>
&lt;p>이 연구에서는 기존 PyTorch transformer에 적은 수정을 통해 모델 병렬성을 구현하고, 이를 통해 전통적인 단일 GPU-모델 학습의 한계를 극복하였다. 512개의 NVIDIA V100 GPU에서 8.3B parameter를 가진 transformer 모델을 효율적으로 학습시켰고, BERT 모델에서는 layer normalization의 위치에 주의를 기울이는 것이 중요하다는 것을 확인하였다. 또한, 모델 크기가 down-tream task의 정확도에 긍정적인 영향을 미침을 확인했고, WikiText103, LAMBADA, RACE 데이터셋에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>미래의 연구 방향은 사전 학습의 규모 증가, 최적화 도구의 효율성과 메모리 사용량 개선, 더 큰 모델의 학습을 위한 병렬화 방법 개선, 다른 모델 패밀리(XLNet, T5)의 사전 학습, 다양한 downstream task에 대한 거대 모델의 성능 평가, 그리고 대형 사전 학습된 모델로부터 작은 모델을 학습시키는 knowledge distillation 사용 등이 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1909.08053.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-2</title><link>https://kurtkim.github.io/p/gpt-2/</link><pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-2/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델은 웹페이지로 구성된 새로운 데이터셋인 &amp;lsquo;WebText&amp;rsquo;으로 학습 함으로써, 질문 응답, 기계 번역 등의 작업을 명시적인 지도 없이 배우기 시작한다는 것을 발견하였다. 특히, GPT-2는 웹텍스트에 대해 underfitting이지만, zero-shot 환경에서 8개 테스트 언어 모델링 데이터셋 중 7개에서 state-of-the-art를 달성하였다. 즉, 언어 처리 시스템이 자연적 설명으로부터 과제수행능력을 배우는 언어처리모델을 개발할 수 있는 방법을 제안하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>기계 학습 시스템은 대규모 데이터셋, 고용량 모델, 지도 학습을 이용해 학습된 작업에서 우수한 성과를 보이지만, 데이터 분포의 작은 변화나 작업 정의에 민감하게 반응하고,매우 좁은 범위의 문제에서만 뛰어난 성능을 보여주고 있다. 그래서 보다 일반적인 문제 해결 능력을 갖춘 범용적인 모델 개발이 필요하며, 이는 결국 각각의 작업에 대해 훈련 데이터셋을 수동으로 생성하고 라벨링할 필요 없이 다양한 작업을 수행할 수 있는 모델을 의미한다.&lt;/p>
&lt;p>기계 학습 시스템을 만드는 주요한 방법은 훈련 예제를 수집하여 시스템을 학습시키고, independent and identically distributed (IID)에서 성능을 테스트하는 것이다. 이 방법은 좁은 범위의 과제에서는 잘 작동하지만, 범용적인 이해를 필요로 하는 캡션 모델, 독해, 이미지 분류 등에서 높은 성능을 내지 못했으며, 이 방법의 한계를 보여주었다.&lt;/p>
&lt;p>일반화하는 능력이 부족한 주요 원인으로 많은 연구가 단일 영역의 dataset과 단일 과제에만 맞춘 학습에만 치중되어 있기 때문이라고 보고 있다. 이를 개선하기 위해 다양한 도메인과 작업에서 훈련하고 성능을 측정하는 것이 필요하며, GLUE와 decaNLP 같은 benchmark dataset이 제안되었다.&lt;/p>
&lt;p>다중 작업 학습(Multitask learning)는 일반 성능을 향상에 높이는 유망한 방법이지만, NLP에서는 아직 초기 연구 단계이다. 최근의 기계학습 시스템의 일반화를 위해서는 수백에서 수천 개의 학습 샘플을 필요로 하며, 다중 작업 학습을 위해서도 그만큼 많은 수의 효과적인 트레이닝 쌍이 필요하다. 현재의 기술로는 dataset을 필요한 수준까지 계속 확장하는 것이 어려우며, 따라서 다중 작업 학습을 위한 새로운 접근법이 필요하다.&lt;/p>
&lt;p>현재 언어 작업에서 최고 성능을 보이는 모델은 사전 학습과 지도 학습을 결합한 방식을 사용한다. 이 접근법은 오랜 역사를 가지고 있으며, transfer 방식이 점차 유연해지고 있다. 초기에는 단어 벡터가 학습되어 특정 작업에 적용되었고, 그 다음으로는 순환 네트워크의 문맥 표현이 transfer 되었다. 최근 연구에서는 특정 작업에 특화된 아키텍처가 필요 없으며, 대신 self-attention block만으로 충분하다고 제안하고 있다.&lt;/p>
&lt;p>현재의 방법들은 작업 수행을 위해 여전히 지도 학습이 필요하다. 하지만 지도 데이터가 거의 없거나 전혀 없을 때, 언어 모델이 상식적인 추론이나 감성 분석 등의 특정 작업을 수행하는 데 잠재력이 있다는 것이 다른 연구에서 보여져 왔다.&lt;/p>
&lt;p>이 논문에서는 언어 모델이 parameter나 아키텍처 변경 없이 zero-shot setting에서 다양한 작업을 수행할 수 있는 능력을 보여준다. 이 접근법은 전이 학습의 일반화 추세를 이어가며, 작업에 따라 유망한 결과와 경쟁력 있는 성과, 그리고 state-of-the-art를 달성하는 잠재력을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>핵심은 언어 모델링(language modeling)이며, 이는 각 원소가 일련의 symbol $(s_1, s_2, &amp;hellip;, s_n)$ 으로 구성된 예제 $(x_1, x_2, &amp;hellip;, x_n)$ 에서 비지도분포 추정을 하는 것으로 정의된다. 언어의 순차적 특성 때문에 기호들에 대한 결합 확률은 조건부 확률의 곱으로 분해하는 것이 일반적이다.&lt;/p>
&lt;p>$$p(x) = \prod_{i=1}^n p(s_n | s_1, s_2, &amp;hellip;, s_{n-1}) $$&lt;/p>
&lt;p>이 방법은 $p(x)$ 및 $p(s_{n-k}, &amp;hellip;, s_n | s_1, &amp;hellip;, s_{s-k-1})$ 형태의 조건부의 샘플링과 추정을 가능하게 하며, 최근에는 Transformer와 같은 self-attention 아키텍처의 발전으로 이러한 조건부 확률을 계산하는 모델의 표현력이 크게 향상되었다.&lt;/p>
&lt;p>단일 작업을 수행하는 학습은 확률론적 프레임워크에서 조건부 분포 $p(output | input)$를 추정하는 것으로 볼 수 있다. 하지만 일반적인 시스템은 동일한 입력에 대해 수행해야 하는 다양한 작업을 고려해야 한다. 이를 위해, 시스템은 $p(output | input, task)$를 모델링해야 한다. 이는 다중학습과 메타학습 환경에서 다양하게 형식을 갖는다. McCann et al. (2018)은 언어를 활용하여 작업, 입력, 출력을 기호 시퀀스로 지정하는 방법을 제시하였고, 이 방법을 사용하여 MQAN이라는 단일 모델을 훈련시켜 다양한 작업을 수행할 수 있음을 보여주었다.&lt;/p>
&lt;p>언어 모델링은 출력 symbol에 대한 명시적인 지도 없이도 다양한 작업을 학습할 수 있다. 이는 감독된 학습 목표와 비감독된 학습 목표가 실질적으로 같기 때문인데, 이 두 목표의 global minimum은 동일하다. 예비 실험에서는 충분히 큰 언어 모델은 이러한 설정에서 다중 작업 학습을 수행할 수 있지만, 명시적으로 감독된 방법보다 학습 속도가 느리다는 것이 확인되었다.&lt;/p>
&lt;p>대화(dialog)의 맥락에서 자연어를 직접 학습하는 방법은 매력적인 접근법이지만, 상호작용이 필요없는 인터넷 사이트에 존재하는 방대한 양의 데이터를 활용하는 방법을 선택하였다. 충분한 용량을 가진 언어 모델은 자연어 시퀀스에서 작업을 추론하고 수행하며, 이를 통해 더 잘 예측하도록 학습할 것으로 예상된다. 모델은 비지도 다중작업 학습을 수행하게 될 것이며, 다양한 작업에서 언어 모델의 제로샷 성능을 분석하였다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>이전 연구들은 주로 한정된 도메인의 텍스트를 가지고 언어 모델을 학습시켰다. 하지만 이 논문에서는 가능한 한 다양한 도메인과 맥락에서 작업을 수집하기 위해, 크고 다양한 dataset을 구축하였다.&lt;/p>
&lt;p>다양하고 방대한 텍스트의 출처로 웹 스크랩인 Common Crawl이 유망하지만, 데이터 품질 문제가 있습니다.&lt;/p>
&lt;p>이 dataset을 사용하는 대신, 문서의 품질을 중요시하는 새로운 웹 스크랩을 만들었다. 전체 웹 스크랩을 수동으로 필터링하는 비용을 줄이기 위해, 사람들이 선별한 웹 페이지를 대상으로 했다. 특히, 적어도 3 카르마를 받은 Reddit의 모든 외부 링크를 스크랩했는데, 이는 사용자들이 해당 링크를 유익하거나 재미있게 여겼는지의 지표로 볼 수 있다.&lt;/p>
&lt;p>결과적으로 나온 dataset인 WebText는 45백만 링크의 텍스트 부분집합을 포함하고 있으며, 텍스트 추출을 위해 Dragnet과 Newspaper 내용추출기를 사용하였다. 이 논문의 모든 결과는 2017년 12월 이후 링크를 제외한 초기 버전의 WebText를 사용하며, 이는 de-duplication과 cleaning 과정을 거친 후 40GB, 약 800만 개의 문서를 포함하고 있다. Wikipedia 문서는 분석을 복잡하게 할 수 있어 WebText에서 제외하였다.&lt;/p>
&lt;h3 id="input-representation">Input Representation&lt;/h3>
&lt;p>일반적인 언어 모델은 어떤 문자열의 확률도 계산하고 생성할 수 있어야 한다. 하지만 현재의 대규모 언어 모델은 전처리 과정으로 인해 모델링 가능한 문자열 범위가 제한된다. 유니코드 문자열을 UTF-8 바이트로 처리하는 것은 이를 해결하나, 현재 바이트 수준의 언어 모델은 대규모 데이터셋에서 단어 수준의 언어 모델만큼 효과적이지 않다. WebText에서 바이트 수준 언어 모델을 훈련시키려 했으나, 이와 비슷한 성능 격차를 경험하였다.&lt;/p>
&lt;p>Byte Pair Encoding(BPE)는 문자와 단어 수준 언어 모델링 사이의 중간 지점이다. 자주 나오는 symbol sequence의 단어수준 입력과 자주 나오지 않는 symbol sequence의 글자수준 입력을 적절히 보간(interpolate)한다. BPE 구현은 byte sequence가 아닌 unicode code points에서 동작한다. 이러한 구현은 모든 unicode 문자열을 모델링하기 위해 전체 unicode symbol의 전체 공간을 포함해야 한다. multi-symbol token을 추가하기 전 130,000개가 넘는 token을 포함하는 기본사전을 필요로 하게 된다. 이는 보통의 32,000개에서 64,000개의 token의 사전에 비해 지나치게 크다. 반면, byte수준의 BPE의 사전은 256개의 token만을 필요로 한다. 그러나 BPE를 byte sequence에 직접 적용하면, 토큰 어휘를 구축하기 위한 BPE의 greedy frequency 기반 heuristic 때문에 최적이 아닌 병합이 발생한다. 예를 들어, &amp;lsquo;dog&amp;rsquo;와 같은 일반적인 단어가 다양한 형태로 나타나면서 제한된 어휘 슬롯과 모델 용량이 최적화되지 않을 수 있다. 이를 해결하기 위해 byte sequence에 대해 문자 범주를 넘어서 병합하는 것을 방지하고, 공백에 대한 예외를 추가하여 압축 효율을 향상시키고 단어의 분열을 최소화하였다.&lt;/p>
&lt;p>이 입력 표현법은 단어 수준 언어 모델의 이점과 byte 수준 접근법의 범용성을 결합시킨다. 이러한 접근법은 어떤 unicode 문자열에도 확률을 부여할 수 있으므로, 사전 처리, 토큰화, 어휘 크기와 관계없이 모든 데이터셋에서 언어 모델을 평가할 수 있다.&lt;/p>
&lt;h3 id="model">Model&lt;/h3>
&lt;p>Transformer 기반 아키텍처를 사용하며, 이는 주로 OpenAI GPT 모델을 따른다. 몇 가지 수정사항은 레이어 정규화의 위치 변경, 추가적인 레이어 정규화의 삽입, 모델 깊이를 고려한 초기화 방식의 수정, 그리고 잔여 레이어 가중치의 스케일링이다. 또한, 어휘는 50,257개로 확장되었고, 맥락 크기와 배치 크기도 각각 1024 토큰과 512로 증가시켰다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table2.png"
width="356"
height="188"
srcset="https://kurtkim.github.io/p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>크기별로 네 개의 언어 모델을 훈련시키고 벤치마킹하였다. 가장 작은 모델의 크기는 원래의 GPT와 같으며, 두 번째로 작은 모델은 BERT의 가장 큰 모델과 같다. 가장 큰 모델인 GPT-2는 GPT보다 10배 이상 많은 parameter를 가지고 있다. 각 모델의 learning rate는 WebText의 5%인 held-out 샘플을 사용하여 수동 조정하으며, 모든 모델은 여전히 WebText에 underfitted 되었으며 더 오래 학습시키면 더 높은 성능을 얻을 수 있을 것이다.&lt;/p>
&lt;h3 id="language-modeling">Language Modeling&lt;/h3>
&lt;p>GPT-2 모델은 문자 단위(byte level)에서 작동하고, 손실이 큰 전처리나 토큰화가 필요 없으므로 모든 언어 모델 benchmark에서 평가할 수 있다. WebText 언어 모델에 따른 dataset의 로그-확률을 계산하는 방식으로 평가 하였다. WebText 언어 모델은 표준화된 텍스트, 토큰화 유물, 섞인 문장, &lt;!-- raw HTML omitted --> 문자열(40 billion 바이트 중 26번만 발생) 등을 예측해야 하기 때문에 많은 데이터셋에서 일반 분포 밖에서 테스트되어야 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table3.png"
width="1302"
height="248"
srcset="https://kurtkim.github.io/p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="525"
data-flex-basis="1260px"
>&lt;/p>
&lt;p>WebText 언어 모델은 도메인과 데이터셋 간에 잘 transfer되며, zero-shot setting에서 8개의 dataset 중 7개에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h3 id="childrens-book-test">Children’s Book Test&lt;/h3>
&lt;p>Children’s Book Test(CBT)는 다양한 카테고리의 단어에 대한 언어 모델의 성능을 평가하기 위한 테스트로, 생략된 단어에 대한 10개의 가능한 선택 중 올바른 것을 예측한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure2.png"
width="632"
height="342"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;p>모델 크기가 증가함에 따라 성능이 지속적으로 개선되며, GPT-2는 일반 명사에서 93.3%, 개체명에서 89.1%의 성능을 달성하였다.&lt;/p>
&lt;h3 id="lambada">LAMBADA&lt;/h3>
&lt;p>LAMBADA dataset은 텍스트의 장거리 의존성(long-range dependencies)을 평가한다. GPT-2는 이 테스트에서의 perplexity를 99.8에서 8.6으로, 정확도를 19%에서 52.66%로 향상시켰다. 추가적으로, stop-word ﬁlter를 추가함으로써 정확도를 63.24%로 더욱 향상시켰다.&lt;/p>
&lt;h3 id="winograd-schema-challenge">Winograd Schema Challenge&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure3.png"
width="620"
height="466"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;p>Winograd Schema Challenge는 텍스트의 모호성을 해결하는 능력을 통해 시스템의 상식적 추론 능력을 측정하고자 한다. GPT-2는 정확도를 7% 향상시켜 70.70%를 달성하였다.&lt;/p>
&lt;h3 id="reading-comprehension">Reading Comprehension&lt;/h3>
&lt;p>CoQA(The Conversation Question Answering dataset)는 7가지 다른 분야의 문서와 문서에 대한 질문자-답변자 사이의 자연어 대화가 쌍을 이루고 있다. CoQA 테스트는 독해능력과 대화에 기반한 모델의 답변능력을 평가한다. GPT-2는 미세조정 없이 55 F1 score를 달성해 4개 중 3개의 다른 모델을 능가하였다.&lt;/p>
&lt;h3 id="summarization">Summarization&lt;/h3>
&lt;p>GPT-2의 요약 능력은 CNN과 Daily Mail dataset을 사용해서 테스트했다. 문서 이후에 &amp;ldquo;TL;DR:&amp;rdquo; 토큰을 추가하고 Top-k 랜덤 샘플링을 통해 요약을 유도했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table4.png"
width="572"
height="236"
srcset="https://kurtkim.github.io/p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>처음 생성된 3개의 문장을 요약 결과로 하여 실험한 결과, 기사의 최근 내용에 초점을 맞추거나 특정 세부사항을 혼동하는 경향이 있다. &amp;ldquo;TL;DR:&amp;rdquo; 토큰 없이 실험한 경우, 성능이 더 하락한 것을 보면 힌트를 통한 Task 유도가 유의한 결과를 냄을 확인할 수 있었다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>번역 능력은 WMT-14 English-French dataset을 사용해서, 영어-프랑스어, 프랑스어-영어 두가지 경우에서 비교가 진행되었다. 번역 성능은 다른 Task에 비해 좋지 상대적으로 좋지 않다. 영어-프랑스어 테스트에서 5 BLEU를, 프랑스어-영어 테스트에서는 11.5 BLEU를 달성했다.&lt;/p>
&lt;h3 id="question-answering">Question Answering&lt;/h3>
&lt;p>언어 모델에 얼마나 많은 정보가 들어있는지 테스트하기 위해 factoid-style의 질문에 얼마나 정확한 답을 생성하는지 평가한다. Natural Questions dataset을 이용해 GPT-2의 성능을 평가하였고 &amp;lsquo;정확히 일치 하는지&amp;rsquo; 여부(exact match metric)를 지표로 비교한다. 질문의 4.1%에 대해 올바르게 답을 하였고, 이는 기존의 모델들보다 5.3배 높은 정확도이다. 매우 작은 모델들은 대체로 1%를 넘지 못하는 성능을 보였는데, 아직까지는 모델의 크기가 QA에 있어서 매우 중요한 요인이라는 것을 확인할 수 있었다. 또한, 가장 확신하는 1%의 질문에 대해 63.1%의 정확도를 보였지만, 이는 여전히 정보 검색과 문서 질문 답변 추출을 결합한 시스템의 30%에서 50% 범위의 성능보다 훨씬 떨어진다.&lt;/p>
&lt;hr>
&lt;h2 id="generalization-vs-memorization">Generalization vs Memorization&lt;/h2>
&lt;p>최근 연구에 따르면, 일반적인 이미지 데이터셋에는 상당한 양의 중복된 이미지가 포함되어 있어, 기계 학습 시스템의 일반화 성능을 과대평가하게 만든다. 데이터셋의 크기가 커질수록 이 문제는 더욱 심화될 가능성이 있으며, 이는 테스트 데이터가 얼마나 훈련 데이터에도 포함되어 있는지 분석하는 것이 중요함을 의미한다.&lt;/p>
&lt;p>이를 연구하기 위해, WebText 훈련 데이터의 8-gram을 포함하는 Bloom 필터를 생성하였고, 주어진 데이터셋에 대해 그 데이터셋의 8-gram 중 얼마나 많은 비율이 WebText 훈련 세트에도 포함되어 있는지를 계산하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table6.png"
width="892"
height="144"
srcset="https://kurtkim.github.io/p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="619"
data-flex-basis="1486px"
>&lt;/p>
&lt;p>일반적인 언어 모델 데이터셋의 테스트 세트는 WebText 훈련 세트와 1-6%의 중복을 가지며, 평균 중복률은 3.2%이다. 많은 데이터셋은 자신의 훈련 분할과 더 큰 중복을 가지며, 평균 중복률은 5.9%이다.&lt;/p>
&lt;p>데이터 중복을 최소화하는 방향으로 접근하였으며, 이러한 중복이 성능에 작은, 하지만 일관적인 향상을 가져다 준다는 분석 결과를 얻었다. 중복 제거 기법을 개선함으로써 이러한 문제에 대해 더욱 효과적으로 대응할 수 있다. 그리고 이러한 중복 제거 과정에서는 n-gram 중첩 기반의 방법을 활용하는 것이 중요하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure4.png"
width="626"
height="572"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="262px"
>&lt;/p>
&lt;p>또한, WebText 언어 모델의 성능이 기억력에 의존하는지를 확인하기 위해 해당 모델이 자체 테스트 세트에서 어떤 성능을 보이는지 검사하였다. 이 결과, 모델 크기가 커짐에 따라 훈련 세트와 테스트 세트에서의 성능이 함께 향상되는 경향을 보였으며, 이로부터 GPT-2가 WebText에 대해 완벽하게 적합하지 않음을 추측할 수 있다.&lt;/p>
&lt;p>마지막으로, GPT-2가 말하는 유니콘의 발견에 대한 뉴스 기사를 작성하는 능력을 보여주었다. 이는 GPT-2의 창의성을 보여주는 한 예로 볼 수 있다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>이 연구는 더 큰 dataset에서 학습된 큰 언어 모델의 성능을 측정하는 데 중점을 두었다. 이 연구는 이전 연구와 비슷한 방향성을 가지고 있으며, 우리의 실험 결과는 주어진 목표의 세부 작업에 대한 추세가 큰 파라미터 범위로도 지속되는 것을 확인하였다.&lt;/p>
&lt;p>생성 모델에서는 RNN 언어 모델이 줄 너비를 추적하고 인용문이나 댓글을 감지하는 등의 흥미로운 기능을 배우는 것이 확인되었다. 또한, 위키백과 기사를 생성하도록 훈련된 모델이 언어 간 이름 번역을 배울 수 있음이 관찰되었다.&lt;/p>
&lt;p>iWeb Corpus같이 웹 페이지의 대형 텍스트 말뭉치를 필터링하고 구성하는 다양한 방법, 모든 단어 벡터 표현 학습을 확대하거나, 기계 번역 모델에서 파생된 표현의 사용을 탐색하는 사전학습 방법, seq2seq 모델 등이 연구 되었고, 언어모델의 사전학습이 잡담이나 대화 같은 어려운 생성문제에 맞춰 미세조정할 때 도움이 된다는 것을 밝혀내었다.&lt;/p>
&lt;hr>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>비지도 사전 학습 방법의 표현에 대한 많은 연구가 있었으며, 이는 비지도 학습이 유망한 연구 영역임을 시사한다. GPT-2는 독해에 대해 경쟁력 있는 성능을 보였지만, 요약 등의 작업에 대해서는 아직 기본적인 수준에 불과하다. 많은 NLP 작업에서 GPT-2의 제로샷 성능을 연구했지만, 아직 많은 실용적인 작업에서는 성능이 무작위 수준에 불과한 경우가 많다. 제로샷 성능은 GPT-2의 잠재적 성능의 기준을 설정하지만, 미세 조정을 통한 성능의 상한선은 아직 불분명하다. 더욱이, GPT-2의 추가 훈련 데이터와 용량이 단방향 표현의 비효율성을 극복하기에 충분한지는 아직 불확실하다.&lt;/p>
&lt;p>decaNLP나 GLUE와 갈은 benchmark에서 미세조정 할 것을 계획하고 있으며, GPT-2의 학습데이터와 그 크기가 BERT에서 말한 단방향 표현의 비효율성을 극복할 수 있을 만큼 충분한지도 확실치 않다고 한다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>충분히 크고 다양한 dataset으로 학습된 큰 언어 모델인 GPT-2는 여러 도메인과 데이터셋에서 잘 수행하며, 테스트된 8개 언어 모델링 dataset 중 7개에서 state-of-the-art를 달성하였다. 이는 고용량 모델이 다양한 텍스트에 대한 가능성을 극대화하는 훈련을 통해, 명확한 지도 없이도 많은 작업을 수행하는 법을 배우기 시작한다는 것을 시사한다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/gpt-2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>BERT</title><link>https://kurtkim.github.io/p/bert/</link><pubDate>Mon, 04 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/bert/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 양방향 인코더 표현을 사용하는 언어모델로, BERT는 레이블이 없는 텍스트에서 깊은 양방향 표현을 사전 학습함으로써, 하나의 출력 레이어만을 추가해서 다양한 작업에 맞게 미세조정 할 수 있다. 이 모델은 개념적으로 단순하면서도 실증적으로 강력하며, 다양한 자연어 처리 작업에서 새로운 최고 수준의 결과를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델의 사전 학습은 다양한 자연어 처리 작업의 향상에 크게 기여하였다. 이는 문장 전체를 분석하여 문장 간 관계를 예측하는 자연어 추론이나 재구성과 같은 문장 수준의 작업뿐만 아니라, 명명된 개체 인식이나 질문 응답과 같이 토큰 수준의 작업을 포함한다.&lt;/p>
&lt;p>사전 학습된 모델을 downstream tasks에 적용하는 데는 두 가지 전략이 있다: feature-based와 미세조정(fine-tuning)이다. feature-based 접근법은 ELMo와 같이 사전 훈련된 표현을 추가 특성으로 사용하고, 미세 조정 접근법은 GPT와 같이 작업 특정 파라미터를 최소화하고 사전 훈련된 모든 파라미터를 미세 조정한다. 이 두가지 접근 방식은 사전 학습을 하는동안 같은 목적 함수를 공유하며, 일반적인 언어 표현을 학습하기 위해 단방향 언어 학습 모델을 사용한다.&lt;/p>
&lt;p>현재의 기술들은 특히 미세 조정 접근법에 대해 사전 학습된 표현의 가능성을 제한한다고 주장한다. 표준 언어 모델이 단방향적이므로, 사전 훈련 동안 사용할 수 있는 아키텍처가 제한되기 때문이다. OpenAI GPT의 경우, 모든 토큰이 이전 토큰만을 주목하는 &amp;lsquo;왼쪽에서 오른쪽으로&amp;rsquo;의 아키텍처를 사용한다. 이러한 제한은 문장 수준 작업이나 양방향 맥락 통합이 중요한 토큰 수준 작업에 대해 불리하다.&lt;/p>
&lt;p>이 논문에서는 BERT(Bidirectional Encoder Representations from Transformers)를 제안하여 미세 조정 기반 접근법을 개선한다. BERT는 일부 토큰을 무작위로 마스크한 &amp;ldquo;masked language model&amp;quot;을 사용하여 단방향성 제약을 완화하고, 이를 통해 깊은 양방향 Transformer를 사전 학습한다. 그리고 &amp;ldquo;next sentence prediction&amp;rdquo; 작업을 통해 텍스트 쌍 표현을 함께 사전 학습한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>언어 표현에 대한 양방향 사전 훈련의 중요성을 강조한다. BERT는 마스크된 언어 모델을 사용해 깊은 양방향 표현을 사전 학습하는데, 이는 단방향 언어 모델을 사용하는 기존 방법과 대조적이다. 또한, 독립적으로 훈련된 언어 모델을 얕게 연결하는 방식과도 차별화된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>사전 학습된 표현은 작업별 아키텍처의 필요성을 줄이며, BERT는 다양한 작업에서 최고 수준의 성능을 달성하는 첫 미세 조정 기반 표현 모델이다. 이는 다수의 작업별 아키텍처들의 성능을 능가한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BERT는 11가지 자연어 처리(NLP) 작업에 대해 state-of-the-art 성능을 달성했다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>일반적인 언어 표현을 사전 학습하는 것은 오랜 역사를 가지고 있으며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략하게 검토한다.&lt;/p>
&lt;h3 id="unsupervised-feature-based-approaches">Unsupervised Feature-based Approaches&lt;/h3>
&lt;p>단어의 범용적인 표현 학습은 비신경망 및 신경망 방법을 포함한 여러 연구의 주제였다. 사전 학습된 단어 임베딩은 현대 NLP 시스템의 핵심 부분이며, 처음부터 학습된 임베딩보다 크게 개선된다. 단어 임베딩 벡터를 사전 훈련하기 위해, 언어 모델링 목표와 맥락에서 단어를 구별하는 목표가 사용되었다.&lt;/p>
&lt;p>이러한 접근법은 문장이나 문단 임베딩 등의 더 큰 단위로 확장되었다. 문장 표현을 훈련시키기 위해, 후보 다음 문장을 순위 매기는 연구, 이전 문장의 표현을 기반으로 다음 문장의 단어를 생성하는 연구, 또는 노이즈 제거 오토인코더에서 파생된 연구 등이 있었다.&lt;/p>
&lt;p>ELMo와 그 전임자는 단어 임베딩 연구를 다른 차원으로 확장해, 맥락에 따라 변하는 특징을 추출하였다. 이는 언어 모델의 왼쪽과 오른쪽 표현을 연결해 이루어진다. ELMo는 이를 기존의 작업 특정 아키텍처와 결합해 주요 NLP 벤치마크를 향상시켰다. 또한, 다른 연구들은 LSTM을 이용해 맥락적 표현을 학습하거나, 클로즈 작업을 통해 텍스트 생성 모델의 견고성을 향상시키는 방법을 제안하였다.&lt;/p>
&lt;h3 id="unsupervised-fine-tuning-approaches">Unsupervised Fine-tuning Approaches&lt;/h3>
&lt;p>첫 번째 연구들은 레이블이 없는 텍스트에서 단어 임베딩 파라미터만 사전 학습하였다.&lt;/p>
&lt;p>최근에는 문장이나 문서 인코더가 레이블이 없는 텍스트에서 사전 훈련되고, 지도학습의 다음 작업을 위해 미세조정되었다. 이 방법의 장점은 적은 양의 파라미터만 처음부터 학습하면 된다는 것이며, 이 때문에 OpenAI GPT는 여러 문장 수준 작업에서 최고 성능을 달성하였다. 이러한 모델을 사전 학습하기 위해 왼쪽에서 오른쪽으로의 언어 모델링과 오토인코더가 사용되었다.&lt;/p>
&lt;h3 id="transfer-learning-from-supervised-data">Transfer Learning from Supervised Data&lt;/h3>
&lt;p>대규모 데이터셋을 가진 자연어 추론과 기계 번역 등의 감독 학습 작업에서 효과적인 전이 학습이 보여주었다. 또한, 컴퓨터 비전 연구에서는 ImageNet으로 사전 훈련된 대규모 모델을 미세조정하여 전이 학습의 중요성을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="bert">BERT&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure1.png"
width="1218"
height="514"
srcset="https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>BERT는 사전 학습과 미세 조정 두 단계로 이루어진다. 사전 학습에서는 레이블이 없는 데이터로 모델을 학습하며, 미세 조정에서는 사전 훈련된 파라미터로 초기화된 BERT 모델을 downstream tasks의 레이블이 붙은 데이터로 미세 조정한다. 각 downstream tasks는 동일한 사전 훈련된 파라미터로 초기화되지만 별도의 미세 조정된 모델을 가진다.&lt;/p>
&lt;p>BERT는 다양한 작업에 걸친 통일된 아키텍처이며, 사전 학습된 아키텍처와 최종 downstream tasks 아키텍처 사이에는 거의 차이가 없다.&lt;/p>
&lt;h3 id="model-architecture">Model Architecture&lt;/h3>
&lt;p>BERT의 구조는 다중 레이어 양방향 Transformer 인코더로, Transformer의 널리 쓰이는 사용과 거의 동일하기 때문에, 모델 아키텍처의 상세한 설명은 &amp;ldquo;The Annotated Transformer&amp;quot;를 참조하도록 권장한다.&lt;/p>
&lt;p>이 연구에서는 레이어의 개수(Transformer 블록)를 $L$, 은닉의 크기를 $H$, self-attention 헤드 수를 $A$로 표시한다. $BERT_{BASE}$ (L=12, H=768, A=12, Total Parameters=110M)와 $BERT_{LARGE}$ (L=24, H=1024, A=16, Total Parameters=340M)의 두 모델 사이즈에 대한 결과를 비교한다.&lt;/p>
&lt;p>비교를 위해 $BERT_{BASE}$는 같은 모델 사이즈인 OpenAI GPT와 비교하였다. 그러나 중요한 점은, BERT Transformer는 양방향 self-attention을 사용하는 반면, GPT Transformer는 각 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 self-attention을 사용한다.&lt;/p>
&lt;h3 id="inputoutput-representations">Input/Output Representations&lt;/h3>
&lt;p>BERT의 입력 표현은 단일 문장과 문장 쌍(예: 〈질문, 답변〉)을 하나의 토큰 시퀀스에서 명확하게 표현할 수 있다. 여기서 &amp;ldquo;문장(senetence)&amp;ldquo;은 실제 문장이 아닌 텍스트의 일부를, &amp;ldquo;시퀀스(sequence)&amp;ldquo;는 BERT 입력 토큰을 의미하며, 이는 하나 또는 두 개의 문장일 수 있다.&lt;/p>
&lt;p>30,000개의 토큰 단어를 가진 WordPiece임베딩을 사용하며, 모든 문장의 첫번째 토큰은 특별한 분류 토큰([CLS])으로 시작한다. 이 토큰에 해당하는 마지막 은닉 상태(hidden state)는 분류 작업을 위해 총 시퀀스 표현으로 사용된다. 문장 쌍은 하나의 시퀀스로 묶이며, 특별한 토큰([SEP])과 학습된 임베딩을 사용해 문장을 구분한다. 입력 임베딩은 $E$로, [CLS] 토큰과 i번째 입력 토큰의 최종 은닉 벡터는 각각 $C \in R^H$, $T_i \in R^H$ 로 표기한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure2.png"
width="984"
height="308"
srcset="https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="319"
data-flex-basis="766px"
>&lt;/p>
&lt;p>주어진 토큰에 대한 입력 표현은 해당 토큰, 세그먼트, 위치 임베딩을 합하여 구성된다.&lt;/p>
&lt;h3 id="pre-training-bert">Pre-training BERT&lt;/h3>
&lt;p>BERT를 사전 학습하기 위해 전통적인 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로의 언어 모델을 사용하지 않았으며, 두 가지 비지도 학습 작업을 사용하여 BERT를 사전 학습하였다.&lt;/p>
&lt;h4 id="task-1-masked-lm">Task #1: Masked LM&lt;/h4>
&lt;p>깊은 양방향 모델은 왼쪽에서 오른쪽 혹은 오른쪽에서 왼쪽 모델보다 강력하다는 것이 직관적이다. 하지만, 표준 언어 모델은 양방향 조건을 적용하면 각 단어가 간접적으로 자신을 &amp;lsquo;볼&amp;rsquo; 수 있게 되어, 모델이 문맥 속의 목표 단어를 쉽게 예측할 수 있게 되므로, 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로만 훈련된다.&lt;/p>
&lt;p>깊은 양방향 표현을 훈련하기 위해, 입력 토큰의 일부를 무작위로 마스킹하고 그 마스킹된 토큰들을 예측하는 &amp;lsquo;Masked LM&amp;rsquo; 방법을 사용한다. 이 방법은 Cloze 작업으로 불리기도 한다. 마스킹된 토큰에 해당하는 최종 은닉 벡터는 어휘에 대한 출력 softmax에 공급되며, 모든 실험에서 각 시퀀스의 15% 토큰을 무작위로 마스킹한다. 이 방법은 denoising auto-encoders와 달리 마스킹된 단어만을 예측한다.&lt;/p>
&lt;p>양방향 사전 학습 모델을 얻는 방법은 [MASK] 토큰이 미세 조정 과정에서 나타나지 않는 문제로 인해 사전 훈련과 미세 조정 사이에 불일치를 유발한다. 이를 완화하기 위해, &amp;ldquo;마스킹된&amp;rdquo; 단어를 항상 [MASK] 토큰으로 교체하지 않는다. 토큰 위치의 15%를 임의로 선택하고, 선택된 토큰은 80% 확률로 [MASK] 토큰, 10% 확률로 임의의 토큰, 10% 확률로 원래 토큰으로 교체한다. 그 후, 크로스 엔트로피 손실을 사용하여 원래 토큰을 예측한다.&lt;/p>
&lt;h4 id="task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)&lt;/h4>
&lt;p>질문 응답(QA)과 자연어 추론(NLI) 같은 작업들은 두 문장 간의 관계를 이해하는 것에 기초하며, 이는 언어 모델링만으로는 직접적으로 캡처할 수 없다. 이를 해결하기 위해, 문장간 관계를 이해하는 모델을 위한 사전 훈련 과정에서, 어떤 하나의 언어를 사용하는 말뭉치로부터 생성될 수 있는 다음 문장 예측 과제를 2진화(binarized)된 다음 문장 예측 작업을 한다. 구체적으로, 각 사전 훈련 예제에서, 선택된 두 문장 A와 B는 50%의 확률로 실제 연속하는 문장이며, 나머지 50%는 말뭉치에서 임의로 선택된 문장이다. 이 간단한 방법이 QA와 NLI에 큰 도움이 된다는 것을 입증하였다.&lt;/p>
&lt;p>다음 문장 예측(NSP) 작업은 이전 연구와 밀접한 관련이 있지만, 이전 연구에서는 문장 임베딩만을 하위 작업에 전달했으나, BERT는 모든 매개변수를 최종 작업 모델 초기화에 사용한다.&lt;/p>
&lt;h4 id="pre-training-data">Pre-training data&lt;/h4>
&lt;p>사전 학습 절차는 대부분 언어 모델 사전 학습에 대한 기존 연구를 따른다. 이때 사용되는 말뭉치는 BooksCorpus(800M개의 단어)와 영어 위키백과(2,500M개의 단어)이다. 위키백과에서는 텍스트 부분만 추출하며, 긴 연속적인 시퀀스를 추출하기 위해 문장 수준이 아닌 문서 수준의 말뭉치 사용이 중요하다는 점을 강조한다.&lt;/p>
&lt;h3 id="fine-tuning-bert">Fine-tuning BERT&lt;/h3>
&lt;p>Transformer의 자기 self-attention mechanism을 활용한 BERT의 미세 조정은 단일 텍스트나 텍스트 쌍을 포함한 다양한 downstream tasks를 모델링하는데 효과적이다. 텍스트 쌍을 독립적으로 인코딩한 후 양방향 cross attention을 적용하는 것이 일반적이지만, BERT는 이 두 단계를 통합하여 self-attention으로 텍스트 쌍을 인코딩함으로써 두 문장 간의 양방향 cross attention을 효과적으로 포함시킨다.&lt;/p>
&lt;p>각 작업마다 BERT에 작업 특정 입력과 출력을 연결하고 모든 매개변수를 미세 조정한다. 입력에서, 사전 훈련된 문장 A와 B는 다양한 작업(표현 변경, 함축, 질문 답변, 텍스트 분류 등)의 입력 쌍에 상응한다. 출력에서, 토큰 표현은 토큰 수준 작업에, [CLS] 표현은 분류 작업에 사용된다.&lt;/p>
&lt;p>미세 조정은 사전 훈련에 비해 상대적으로 저렴하며, 이 논문의 모든 결과는 동일한 사전 훈련 모델을 기반으로 클라우드 TPU에서는 1시간, GPU에서는 몇 시간 안에 재현 가능하다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>11가지 자연어 처리(NLP) 작업에 대한 BERT 미세 조정 결과를 보여준다.&lt;/p>
&lt;h3 id="glue">GLUE&lt;/h3>
&lt;p>General Language Understanding Evaluation(GLUE) benchmark는 다양한 자연어 이해 작업의 모음이다.&lt;/p>
&lt;p>GLUE에서 미세 조정을 하기 위해, 입력 시퀀스를 표현하고 첫 번째 입력 토큰([CLS])에 대응하는 최종 벡터를 종합 표현으로 사용한다. 미세 조정시 도입되는 유일한 새로운 매개변수는 분류 레이어의 가중치이다. 그리고 이들을 사용해 standard classiﬁcation 손실을 계산한다.&lt;/p>
&lt;p>batch size 32로 데이터를 3 epoch 동안 미세 조정한다. 각 작업마다 최적의 미세 조정 학습률을 선택하며, $BERT_{LARGE}$는 작은 데이터셋에서 불안정할 때 랜덤 재시작을 사용하여 최적의 모델을 선택한다. 랜덤 재시작에서는 동일한 사전 학습 체크포인트를 사용하지만 데이터 셔플링과 분류기 레이어 초기화는 다르게 합니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table1.png"
width="1230"
height="272"
srcset="https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="452"
data-flex-basis="1085px"
>&lt;/p>
&lt;p>$BERT_{BASE}$와 $BERT_{LARGE}$는 모든 작업에서 월등한 성능을 보여주며, 평균 정확도를 각각 4.5%, 7.0% 향상시켰다. 가장 큰 GLUE 작업인 MNLI에서 BERT는 정확도를 4.6% 향상시켰고, 공식 GLUE 리더보드에서는 $BERT_{LARGE}$가 80.5의 점수로 OpenAI GPT의 72.8을 능가했다.&lt;/p>
&lt;p>$BERT_{LARGE}$가 모든 작업에서 $BERT_{BASE}$를 크게 능가하며, 특히 훈련 데이터가 매우 적은 작업에서 그렇다는 것을 발견하였다.&lt;/p>
&lt;h3 id="squad-v11">SQuAD v1.1&lt;/h3>
&lt;p>The Stanford Question Answering Dataset (SQuAD v1.1)는 10만 개의 크라우드 소싱 질문/답변 쌍의 컬렉션이다. 주어진 질문과 답변을 포함하는 위키백과의 문단이 주어지면, 그 문단 내에서 실제 답변의 위치나 범위를 정확하게 예측하는 것이 목표이다.&lt;/p>
&lt;p>질문 응답 작업에서는 입력 질문과 문단을 하나의 연결된 시퀀스로 표현하며, 각각 다른 임베딩을 사용한다. 미세 조정 과정에서는 시작과 끝 벡터만 추가적으로 도입되며, 답변의 시작 단어 확률은 해당 단어와 시작 벡터 간의 내적 후 softmax를 적용하여 계산된다.&lt;/p>
&lt;p>답변 범위의 시작과 끝 위치를 예측하는 공식이 사용되며, 그 중 최대 점수를 가진 범위가 최종 예측값이 된다. 훈련 목표는 정확한 시작과 끝 위치의 로그 가능도 합이며, learning rate 5e-5와 batch size 32로 3 epoch 동안 미세 조정이 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table2.png"
width="548"
height="488"
srcset="https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>BERT 모델은 다른 공개 데이터를 사용하여 학습된 모델들을 뛰어넘으며, 특히 TriviaQA에 먼저 미세 조정을 함으로써 성능을 향상시켰다. 단일 BERT 모델만으로도 최고의 앙상블 시스템을 능가하며, TriviaQA 데이터 없이도 모든 기존 시스템을 크게 앞서고있다.&lt;/p>
&lt;h3 id="squad-v20">SQuAD v2.0&lt;/h3>
&lt;p>SQuAD 2.0 작업은 제공된 문단에 짧은 답변이 존재하지 않을 수 있다는 가능성을 허용함으로써 SQuAD 1.1 문제 정의를 확장하였고, 이로 인해 문제가 더 현실적으로 변하였다.&lt;/p>
&lt;p>이 작업을 위해 SQuAD v1.1 BERT 모델을 간단하게 확장하였다. 답변이 없는 질문은 시작과 끝이 [CLS] 토큰에 있는 답변 범위로 취급하였다. 예측 시, 답변이 없는 범위의 점수와 최고의 비-null 범위의 점수를 비교하여, 특정 임계값을 넘을 경우 non-null 답변을 예측하였다. 이 모델에서는 TriviaQA 데이터를 사용하지 않았으며, learning rate 5e-5와 batch size 48로 2 epoch 동안 미세 조정을 수행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table3.png"
width="554"
height="384"
srcset="https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>BERT를 사용하지 않는 시스템들과의 결과를 비교하였다. 다른 모델들에 비해 F1 점수가 5.1점 향상되었다.&lt;/p>
&lt;h3 id="swag">SWAG&lt;/h3>
&lt;p>The Situations With Adversarial Generations (SWAG) dataset는 실제 상식 추론을 평가하는 113k개의 문장 쌍 완성 예제를 포함하고 있다. 주어진 문장에 대해, 작업은 네 가지 선택지 중 가장 그럴듯한 답을 선택하는 것이다.&lt;/p>
&lt;p>SWAG dataset에서 미세 조정을 할 때, 각각 주어진 문장과 가능한 연속성을 포함하는 네 개의 입력 시퀀스를 만든다. [CLS] 토큰 표현과 내적을 이루는 벡터는 각 선택지에 대한 점수를 나타내며, 이 점수는 softmax 레이어를 통해 정규화한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table4.png"
width="390"
height="304"
srcset="https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="307px"
>&lt;/p>
&lt;p>learning rate가 2e-5이고 batch size가 16인 상태로 모델을 3 epoch 동안 미세 조정하였다. $BERT_{LARGE}$는 ESIM+ELMo 모델을 +27.1%로, OpenAI GPT를 8.3%로 능가하였다.&lt;/p>
&lt;h2 id="ablation-studies">Ablation Studies&lt;/h2>
&lt;p>상대적인 중요성을 더 잘 이해하기 위해 BERT의 여러 면에 걸쳐서 ablation 실험을 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table5.png"
width="606"
height="244"
srcset="https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="596px"
>&lt;/p>
&lt;h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks&lt;/h3>
&lt;p>$BERT_{BASE}$의 동일한 사전 학습 데이터, 미세 조정 scheme, 그리고 hyperparameter를 사용하여 두 가지 사전 학습 목표를 평가함으로써 BERT의 깊은 양방향성의 중요성을 입증한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>No NSP: &amp;ldquo;다음 문장 예측(NSP)&amp;rdquo; 과제를 하지 않은, &amp;ldquo;Masked LM(MLM)&amp;ldquo;을 사용해 훈련된 양방향 모델&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LTR &amp;amp; No NSP: 왼쪽 컨텍스트만 있는 모델은 표준 LTR LM을 사용해 훈련되며, 이는 미세 조정 시에도 유지된다. 이 모델은 NSP 작업 없이 사전 훈련되었다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>NSP 작업을 제거하면 QNLI, MNLI, SQuAD 1.1에서 성능이 크게 저하된다. 또한, 양방향 표현을 훈련하는 것은 성능에 중요한 영향을 미치며, 특히 LTR 모델은 모든 작업에서 MLM 모델보다 성능이 떨어진다.&lt;/p>
&lt;p>SQuAD의 경우, 토큰 레벨 은닉 상태가 오른쪽 컨텍스트를 가지고 있지 않기 때문에 LTR 모델이 토큰 예측에서 성능이 떨어질 것이라는 것은 직관적으로 명확하다. 이를 개선하기 위해 무작위로 초기화된 BiLSTM을 추가했지만, 결과는 사전 훈련된 양방향 모델보다 훨씬 떨어진다. 또한, BiLSTM은 GLUE 작업에서의 성능을 저하시킨다.&lt;/p>
&lt;p>LTR과 RTL 모델을 별도로 훈련하는 것은 가능하지만, 이는 단일 양방향 모델보다 비용이 두 배 많이 들고, QA와 같은 작업에 대해 직관적이지 않다. 또한, 이 방식은 모든 계층에서 양방향 컨텍스트를 사용하는 모델보다 성능이 엄격하게 떨어진다.&lt;/p>
&lt;h3 id="effect-of-model-size">Effect of Model Size&lt;/h3>
&lt;p>모델 크기가 미세 조정 작업 정확도에 미치는 영향을 알아본다. BERT 모델을 같은 파라미터와 훈련 절차를 사용한 반면, 레이어의 수, 은닉 상태 개수, 어텐션 헤드 개수를 다르게 학습했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table6.png"
width="528"
height="286"
srcset="https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;p>GLUE 작업 결과에 따르면, 더 큰 모델은 레이블이 붙은 훈련 예시가 적은 데이터셋에서도 정확도를 개선하였다. 이는 이미 상대적으로 큰 모델을 기반으로 중요한 개선을 이루어내며, 예를 들어 $BERT_{BASE}$는 110M, $BERT_{LARGE}$는 340M의 파라미터를 가진다. 이는 기존 문헌에서 제시한 Transformer 모델 보다 훨씬 크다.&lt;/p>
&lt;p>모델 크기를 늘리는 것이 대규모 작업에서 성능을 향상시키는 것은 잘 알려져 있지만, 이 연구는 모델이 충분히 사전 훈련되었다면 작은 규모의 작업에서도 큰 개선을 가져올 수 있다는 것을 보여준다. 이전의 연구들은 사전 훈련된 모델의 크기를 늘리는 것이 혼합된 결과를 가져왔지만, 이 연구는 모델이 작업에 직접 미세 조정을 받고, 매우 적은 수의 무작위로 초기화된 추가 파라미터만 사용할 때, 작은 규모의 작업도 크고 표현력 있는 사전 훈련된 표현의 이점을 볼 수 있다.&lt;/p>
&lt;h3 id="feature-based-approach-with-bert">Feature-based Approach with BERT&lt;/h3>
&lt;p>지금까지의 BERT 결과는 모두 미세 조정 방식을 사용했다. 이 방식은 사전 학습된 모델에 분류 계층을 추가하고 모든 파라미터를 하류 작업에 맞게 조정하는 방법이다. 그러나, 사전 학습된 모델에서 고정 특징을 추출하는 특징 기반 접근법도 장점이 있다. 일부 작업은 Transformer 인코더 아키텍처로 표현하기 어려워 특정 작업용 모델이 필요하며, 훈련 데이터의 복잡한 표현을 미리 계산하고 이를 기반으로 저렴한 모델로 실험을 진행하면 계산적으로 이점이 있다.&lt;/p>
&lt;p>이 섹션에서는 BERT를 이름 인식(NER) 작업에 적용하여 두 가지 접근법을 비교한다. BERT 입력에는 대소문자를 구분하는 WordPiece 모델을 사용하고, 데이터에서 제공하는 최대 문서 컨텍스트를 포함한다. 이 작업은 일반적인 방식에 따라 태깅 작업으로 설정되지만, 출력에서는 CRF 계층은 사용하지 않는다. NER 레이블 세트에 대한 토큰 수준 분류기의 입력으로 첫 번째 서브토큰의 표현을 사용한다.&lt;/p>
&lt;p>미세 조정 방식을 제거하기 위해, BERT의 매개변수를 조정하지 않고 특징 기반 방식을 적용하여 활성화 함수를 추출한다. 이 문맥적인 임베딩은 랜덤하게 초기화된 두 계층의 768차원 BiLSTM에 입력으로 사용되며, 이는 분류 레이어 이전에 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table7.png"
width="550"
height="458"
srcset="https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="288px"
>&lt;/p>
&lt;p>$BERT_{LARGE}$는 state-of-the-art 방법들과 비슷한 수준의 성능을 보여준다. 가장 효과적인 방법은 사전 훈련된 Transformer의 상위 4개 계층에서 토큰 표현을 결합하는 것이며, 이는 전체 모델을 미세 조정한 것보다 F1에서 0.3만큼 뒤떨어진다. 이는 BERT가 미세 조정과 특징 기반 접근법 모두에 효과적임을 보여준다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>언어 모델과 전이 학습을 통한 최근 개선은 비지도 사전 학습이 언어 이해 시스템의 중요한 부분임을 보여준다. 이 결과는 low-resource tasks조차 깊은 단방향 아키텍처에서 이익을 얻을 수 있다는 것을 보여주었다. 이 논문에서는 이를 깊은 양방향 아키텍처로 일반화함으로써, 사전 훈련된 동일 모델이 다양한 NLP 작업을 성공적으로 처리할 수 있게 한다는 것을 보여준다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/bert" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-1</title><link>https://kurtkim.github.io/p/gpt-1/</link><pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-1/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>자연어 이해는 텍스트의 함축, 질문에 대한 답변, 의미의 유사성 평가, 문서 분류 등 다양한 작업으로 구성되어 있다. 레이블이 지정된 데이터가 부족한 상황에서, 이 논문은 레이블이 없는 텍스트 데이터에 대해 언어 모델을 (생성적) 사전학습(generative pre-training)하고, 이를 특정 작업에 미세조정(fine-tuning)하는 방식을 제안한다. 이 방법은 모델 아키텍처에 최소한의 변경만을 요구하면서도 효과적인 전이를 달성하였고, 다양한 자연어 이해 벤치마크에서 우수한 성능을 보여주었다. 이 모델은 각 작업에 특별히 설계된 모델을 능가하며, 12개의 작업 중 9개에서 최고 성능을 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리(NLP)에서 지도 학습의 의존성을 줄이는 것은 중요한데, 이는 대부분의 딥러닝 방법이 수동 레이블링된 대량의 데이터가 필요하기 때문이다. 이런 상황에서 레이블이 없는 데이터에서 언어 정보를 추출할 수 있는 모델은 유용한 대안이 될 수 있으며, 비지도 학습을 통해 학습하는 것이 더 나은 결과를 얻는 경우도 있다. 이를 입증하는 가장 강력한 예는 사전 학습된 단어 임베딩이며, 이는 다양한 NLP 작업에서 성능 향상을 위해 널리 사용되고 있다.&lt;/p>
&lt;p>레이블이 없는 텍스트에서 단어 수준을 넘어서는 정보를 활용하는 것은 어려운 도전 과제이며, 이유는 다음과 같다. 첫째, 텍스트 표현을 학습하고 다른 곳에 유용하게 전이하는 최적화 목표가 무엇인지 확실하지 않다. 둘째, 학습된 표현을 어떤 작업에 가장 효과적으로 적용할 방법이 아직 확립되지 않았다. 이런 불확실성이 효과적인 준지도 학습 방법을 개발하는 것을 어렵게 한다.&lt;/p>
&lt;p>이 연구는 언어 이해 작업에 비지도 사전 학습(unsupervised pre-training)과 지도 미세 조정(supervised fine-tuning)을 결합하는 준지도 학습을 제안한다. 목표는 적은 조정으로 다양한 작업에 적용 가능한 표현을 학습하는 것이다. 레이블이 없는 대량의 텍스트와 수동으로 레이블링된 훈련 예제를 사용하며, 학습은 두 단계로 진행된다. 먼저, 레이블이 없는 데이터로 모델의 초기 파라미터를 학습하고, 그 다음으로 지도 학습을 통해 이 파라미터를 목표 작업에 맞게 조정한다.&lt;/p>
&lt;p>이 연구에서는 다양한 작업에서 뛰어난 성능을 보인 Transformer모델을 사용한다. 이 모델은 텍스트의 장기적인 의존성을 처리하는 더 구조화된 메모리를 제공하므로 강한 전이 성능을 보여준다. 전이 단계에서는 작업 특정 입력 조정을 사용하여 텍스트 입력을 연속 토큰 시퀀스로 처리하며, 이 방식은 사전 학습된 모델의 구조를 최소한으로 변경하면서 효과적으로 미세 조정할 수 있음을 실험적으로 입증한다.&lt;/p>
&lt;p>이 연구는 자연어 추론, 질문 응답, 의미 유사성, 텍스트 분류 등 네 가지 언어 이해 작업에서 모델을 평가하였다. 제시된 모델은 각 작업에 특화된 모델들보다 더 우수한 성능을 보여주었고, 12개 작업 중 9개에서 최고 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="semi-supervised-learning-for-nlp">Semi-supervised learning for NLP&lt;/h3>
&lt;p>이 연구는 자연어에 대한 준지도 학습 범주에 속하며, 이는 시퀀스 라벨링이나 텍스트 분류와 같은 작업에 적용된다. 초기에는 레이블 없는 데이터를 사용해 단어나 구문 수준의 통계를 계산하였지만, 최근에는 레이블이 없는 말뭉치에서 훈련된 단어 임베딩을 활용하여 작업 성능을 향상시키는 방향으로 연구가 진행되고있다. 그러나 이 논문의 목표는 단어 수준 이상의 의미를 포착하는 것이며, 이를 위해 구문이나 문장 수준의 임베딩을 활용하여 텍스트를 벡터 표현으로 인코딩하는 방식을 채택하였다.&lt;/p>
&lt;h3 id="unsupervised-pre-training">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 사전 학습은 좋은 초기화 지점을 찾는 것을 목표로 하며, 이미지 분류, 음성 인식, 엔티티 구분, 기계 번역 등 다양한 작업에서 DNN의 훈련을 돕는데 사용되고있다.&lt;/p>
&lt;p>이 연구는 언어 모델링 목표를 사용하여 신경망을 사전 학습하고, 지도 학습으로 목표 작업에서 미세 조정하는 방식을 따른다. 이 방법은 LSTM을 사용하는 이전의 방법들이 제한적인 예측 능력을 가지는 반면, Transformer는 더 넓은 범위의 언어 구조를 포착할 수 있게 한다. GPT 모델은 자연어 추론, 패러프레이즈 감지, 스토리 완성 등 다양한 작업에서 효과를 보여주었으며, 다른 모델이 새로운 파라미터를 많이 필요로 하는 반면, GPT 모델은 아키텍처에 최소한의 변경만 필요로 한다.&lt;/p>
&lt;h3 id="auxiliary-training-objectives">Auxiliary training objectives&lt;/h3>
&lt;p>보조적인 비지도 학습 목표 추가는 준지도 학습의 변형 형태로, 다양한 NLP 작업을 통해 의미 역할 라벨링을 개선하는데 사용되었다. 최근에는 이러한 보조 목표를 목표 작업에 추가하여 시퀀스 라벨링 작업에서 성능을 향상시켰다. 이 연구에서도 비지도 사전 훈련이 이미 목표 작업과 관련된 다양한 언어적 요소를 학습한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="framework">Framework&lt;/h2>
&lt;p>학습은 큰 말뭉치에서 대용량 언어 모델을 학습하는 단계와 레이블이 달린 데이터를 활용해 모델을 목표 작업에 맞게 미세 조정하는 단계로 이루어진다.&lt;/p>
&lt;h3 id="unsupervised-pre-training-1">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 토큰 말뭉치 $U = \lbrace u_1, &amp;hellip; , u_n \rbrace $ 가 주어질때, 다음 Likelihood를 최대화하도록 표준언어모델링 목적함수를 사용한다:&lt;/p>
&lt;p>$$ L_1(U) = \sum_{i} \log{P} (u_i | u_{i-k}, &amp;hellip; , u_{i-1}, \theta) $$&lt;/p>
&lt;p>$k$는 context window의 크기이며, 조건부 확률 $P$는 parameter $\theta$를 가진 신경망을 사용하여 모델링된다. 이 parameter들은 stochastic gradient descent를 사용하여 학습된다.&lt;/p>
&lt;p>GPT 모델은 언어모델로 multi-layer Transformer decoder를 사용하며, 이 모델은 입력 컨텍스트 토큰에 대해 multi-headed self-attention을 적용한 후, position-wise feedforward layer를 적용하여 목표 토큰에 대한 출력 분포를 생성한다:&lt;/p>
&lt;p>$$ h_0 = UW_e + W_p $$
$$ h_l = \text{transformer_block}(h_{l-1}) \forall i \in [1, n] $$
$$ P(u) = \text{softmax}(h_n W^T_e) $$&lt;/p>
&lt;p>$U = (u_{i-k}, &amp;hellip; , u_{i-1}) $ 는 토큰의 컨텍스트 벡터이고, $n$은 layer의 수, $W_e$ 는 토큰 임베딩 행렬, $W_p$ 는 위치 임베딩 행렬이다.&lt;/p>
&lt;h3 id="supervised-ﬁne-tuning">Supervised ﬁne-tuning&lt;/h3>
&lt;p>모델을 학습한 후, parameter를 목표 작업에 맞게 조정한다. 레이블이 지정된 데이터셋 $C$ 는 입력 토큰 $x^1, &amp;hellip; , x^m $ 과 레이블 $y$로 구성된다. 입력은 사전 훈련된 모델을 통과하여 최종 transformer block의 활성값인 $h^m_l$ 을 얻으며, 이는 parameter $W_y$ 와 함께 선형 출력층으로 전달되어 $y$ 를 예측한다:&lt;/p>
&lt;p>$$ P(y|x^1, &amp;hellip; , x^m) = \text{softmax}(h^m_l W_y) $$&lt;/p>
&lt;p>이는 다음을 최대화 한다.&lt;/p>
&lt;p>$$ L_2(C) = \sum_{(x,y)} \log{P(y|x^1, &amp;hellip; , x^m)} $$&lt;/p>
&lt;p>추가로 미세 조정을 위한 보조 목표로 언어 모델링을 포함시키는 것은 지도 모델의 일반화를 향상시키고, 수렴을 가속화하는데 도움이 된다. 구체적으로, weight $\lambda$에 대해 다음을 최적화한다:&lt;/p>
&lt;p>$$ L_3(C) = L_2(C) + \lambda L_1(C) $$&lt;/p>
&lt;p>미세 조정 과정에서 추가 매개변수는 $W_y$ 와 구분자 토큰의 임베딩뿐이다.&lt;/p>
&lt;h3 id="task-speciﬁc-input-transformations">Task-speciﬁc input transformations&lt;/h3>
&lt;p>텍스트 분류같은 일부 작업들은 모델을 직접 미세 조정할 수 있지만, 질문 답변이나 텍스트 함의 같은 작업들은 구조화된 입력을 필요로 하는데, 이러한 입력에 대해 사전 학습된 모델은 별도의 수정 없이도 처리할 수 있다. 대신, 이런한 입력을 모델이 처리할 수 있는 순서가 있는 시퀀스로 변환한다. 이 접근법은 작업 간에 아키텍처를 크게 변경할 필요를 없애준다. 또한, 모든 변형에는 무작위로 초기화된 시작과 종료 토큰을 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure1.png"
width="1058"
height="450"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="564px"
>&lt;/p>
&lt;h4 id="textual-entailment">Textual entailment&lt;/h4>
&lt;p>텍스트 함의에서는, 전제 $p$와 가설 $h$를 구분자 &lt;code>$&lt;/code>로 연결한다.&lt;/p>
&lt;h4 id="similarity">Similarity&lt;/h4>
&lt;p>유사성 경우, 비교되는 두 문장의 순서는 정해져 있지 않으므로, 텍스트 두 개를 다른 순서로 이어붙여 각각을 독립적으로 처리하여 두 시퀀스 표현 $h^m_l$을 생성한다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning">Question Answering and Commonsense Reasoning&lt;/h4>
&lt;p>컨텍스트 문서 $z$, 질문 $q$, 가능한 답변들 $\lbrace a_k \rbrace$을 받는다. 각 가능한 답변을 문맥 문서와 질문에 연결하고, 구분자 토큰을 추가해 시퀀스 $[z; q;$ &lt;code>$&lt;/code>; $a_k]$ 를 만든다. 이 시퀀스들은 독립적으로 처리되고, softmax 계층을 통해 정규화되어 답변들에 대한 출력 분포를 생성한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;h4 id="unsupervised-pre-training-2">Unsupervised pre-training&lt;/h4>
&lt;p>언어 모델 학습에 BooksCorpus 데이터셋을 사용한다. 이는 다양한 장르의 7천개가 넘는 미발행 책들을 포함하며, 연속적인 긴 텍스트를 통해 모델이 long term depency를 학습할 수 있다. ELMo에서 사용된 1B Word Benchmark 데이터셋은 문장들이 서로 섞여 있어 long term depency를 학습하기 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table1.png"
width="1068"
height="192"
srcset="https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="556"
data-flex-basis="1335px"
>&lt;/p>
&lt;h4 id="model-speciﬁcations">Model speciﬁcations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">layer&lt;/td>
&lt;td style="text-align:center">12-layer decoder-only transformer with masked self-attention heads&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">state dimension&lt;/td>
&lt;td style="text-align:center">decoder: 768, attention heads: 12, position-wise FFN: 3072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">optimizer&lt;/td>
&lt;td style="text-align:center">Adam&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">learning rate&lt;/td>
&lt;td style="text-align:center">max: 2.5e-4, schedule: cosine annealing, warm-up step: 2,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">schedule&lt;/td>
&lt;td style="text-align:center">100 epochs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">64 random sample $\times$ 512 token/sample&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">weight initialization&lt;/td>
&lt;td style="text-align:center">$N(0, 0.02)$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">subword segmentation&lt;/td>
&lt;td style="text-align:center">BPE (40,000 merges)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">regularization&lt;/td>
&lt;td style="text-align:center">L2($w=0.01$)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">activation function&lt;/td>
&lt;td style="text-align:center">Gaussian Error Linear Unit(GELU)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">position embedding&lt;/td>
&lt;td style="text-align:center">learned positoin embeddings&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">pre-processing&lt;/td>
&lt;td style="text-align:center">cleaning: ftfy, tokenizer : spaCy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="fine-tuning-details">Fine-tuning details&lt;/h4>
&lt;p>명시되지 않은 것들은 사전학습에 사용된 hyperparameter를 재사용했다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Learning rate&lt;/td>
&lt;td style="text-align:center">max: 6.25e-5, warm-up: 0.2% of training&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">epochs&lt;/td>
&lt;td style="text-align:center">3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">auxiliary objective weight($\lambda$)&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="supervised-ﬁne-tuning-1">Supervised ﬁne-tuning&lt;/h3>
&lt;p>자연어 추론, 질문 응답, 의미론적 유사성, 텍스트 분류등의 평가를 진행하였고, 그 중 일부는 GLUE benchmark에 포함되어 있다.&lt;/p>
&lt;h4 id="natural-language-inference">Natural Language Inference&lt;/h4>
&lt;p>자연어 추론(NLI) 작업, 즉 텍스트 함의를 인식하는 것은 문장 쌍을 읽고, 그들 사이의 관계를 함의, 모순 또는 중립 중 하나로 판단하는 것으로, 이미지 캡션(SNLI), 텍스트 변환된 연설, 대중 소설, 정부 보고서(MNLI), 위키백과 기사(QNLI), 과학 시험(SciTail) 또는 뉴스 기사(RTE)를 포함한 다양한 출처의 다섯 개의 데이터셋을 사용해서 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table2.png"
width="1084"
height="324"
srcset="https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="334"
data-flex-basis="802px"
>&lt;/p>
&lt;p>다섯 가지 데이터셋 중 네 가지에서 좋은 성능을 보여주었으며, MNLI에서 1.5%, SciTail에서 5%, QNLI에서 5.8%, SNLI에서 0.6%의 성능 향상을 보였다. 이는 GPT 모델이 여러 문장을 더 잘 이해하고, 언어적 모호성의 측면을 처리할 수 있다는 것을 보여준다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning-1">Question answering and commonsense reasoning&lt;/h4>
&lt;p>질문 응답 작업은 한 문장이나 여러 문장을 이해하는 능력을 평가한다. 중고등학교 시험의 영어 지문과 질문이 포함된 RACE 데이터셋을 사용한 평가에서 좋은 성능을 보여주었다. 또한, 여러 문장의 이야기 중에서 올바른 결말을 고르는 Story Cloze 평가에서도 GPT 모델은 이전 최고 성능을 크게 능가하였다. 이 결과는 GPT 모델이 넓은 범위에 걸친 문맥 정보를 잘 처리할 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table3.png"
width="1078"
height="312"
srcset="https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="345"
data-flex-basis="829px"
>&lt;/p>
&lt;h4 id="semantic-similarity">Semantic Similarity&lt;/h4>
&lt;p>의미론적 유사성(또는 패러프레이즈 감지) 작업은 두 문장이 의미적으로 동일한지 여부를 판단한다. 뉴스 출처에서 수집된 Microsoft Paraphrase(MRPC), Quora Question Pairs(QQP), 그리고 Semantic Textual Similarity benchmark(STS-B) 데이터셋을 사용한다. 이 중 STSB와 QQP에서 좋은 성늘을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table4.png"
width="1076"
height="382"
srcset="https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;h4 id="classiﬁcation">Classiﬁcation&lt;/h4>
&lt;p>텍스트 분류로 사용한 데이터셋은 문법적으로 맞는지를 판단하는 Corpus of Linguistic Acceptability(CoLA)와 단순 이진분류 평가인 Stanford Sentiment Treebank(SST-2)을 사용하였다. CoLA에서 35.0 에서 45.4점으로, SST-2에서 68.9 에서 72.8점으로 상승하였으며, GLUE benchmark에서도 72.8점으로 이전 최고 성능을 크게 능가하였다.&lt;/p>
&lt;p>GPT모델은 평가한 12개의 데이터셋 중 9개에서 state-of-the-art를 달성하였다. 그리고 STS-B(약 5.7k)와 같은 작은 데이터셋부터 가장 큰 SNLI(약 550k)와 같은 크기의 다양한 데이터셋에서 잘 작동함을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="analysis">Analysis&lt;/h2>
&lt;h3 id="impact-of-number-of-layers-transferred">Impact of number of layers transferred&lt;/h3>
&lt;p>unsupervised pre-training에서 supervised target task로 transfer하는 layer 개수의 영향을 분석했다. MultiNLI와 RACE에서 성능을 관찰했고 transferring embeddings이 성능을 향상시킨다는 것과 각 transformer layer가 최대 9%까지 성능을 향상시킨다는 결과를 얻었다. 이는 pre-trained model의 각 layer가 target task를 푸는 데 유용한 기능을 포함함을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure2.png"
width="1070"
height="478"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;h3 id="zero-shot-behaviors">Zero-shot Behaviors&lt;/h3>
&lt;p>Trasformer를 사용한 language model이 pre-training에 효과적인 이유에 대한 가설로, Generative model이 학습하는 target tasks가 language modeling의 성능을 향상에 도움을 준다고 생각했고, 이를 검증하기 위해 pre-training 업데이트 횟수에 따른 target tasks의 성능을 fine-tuning없이 측정하였다.&lt;/p>
&lt;p>실험 결과 pre-training 업데이트 횟수에 따라 안정적 &amp;amp; 지속적으로 관련 taget task의 성능이 증가하는 것을 확인할 수 있었으며 이는 generative pre-training이 관련 task의 학습에 도움을 준다는 것을 의미한다. 반면, LSTM의 경우에는 업데이트 횟수에 따라 일관되게 안정적으로 증가하지 않고 분산을 가지면서 증가하는데, 이는 LSTM 보다 더 구조화된 transformer의 attentional memory가 transfer learning에 도움을 준다는 것을 의미한다.&lt;/p>
&lt;h3 id="ablation-studies">Ablation studies&lt;/h3>
&lt;p>세 가지 ablation study를 통해 다음의 결과를 얻었다. 첫째, 미세조정 시 보조 목적함수의 도움이 큰 데이터셋에서는 두드러지지만 작은 데이터셋에서는 그렇지 않다는 것을 확인하였다. 둘째, LSTM과 Transformer를 비교한 결과, LSTM은 오직 MRPC 데이터셋에서만 Transformer를 능가하는 것을 확인하였다. 마지막으로, 사전학습 없이 지도학습을 진행한 Transformer는 모든 작업에서 성능이 저하되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table5.png"
width="1074"
height="208"
srcset="https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="516"
data-flex-basis="1239px"
>&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>생성적 사전 학습과 미세조정을 사용한 모델을 통해 강력한 자연어 이해를 구현하였다. GPT 모델은 연속된 텍스트로 이루어진 다양한 말뭉치로 사전학습된 모델은 일반 지식(world knowledge)과 long term depency 처리하는 능력을 가질 수 있었다. 이를 통해, 우리는 지도학습 없이도 특정 작업의 성능을 향상시키는 것이 가능하다는 것을 보여주었으며, 특히 Trasformer 모델과 long term depency가 있는 텍스트 데이터셋이 이 접근법에서 잘 작동함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Transformers</title><link>https://kurtkim.github.io/p/transformers/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/transformers/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>recurrent와 convolutional을 완전히 제거하고 attention mechanism에만 기반한 새로운 신경망 아키텍처인 transformer를 제안한다. 이 모델은 더 우수한 품질을 제공하면서 병렬화가 가능하고 학습 시간이 훨씬 적게 든다. 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 뛰어넘는 성능을 보였고, 영어 구문 분석에도 성공적으로 적용되었다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>RNN, 특히 LSTM과 GRU는 언어 모델링과 기계 번역 등에서 state-of-the-art로 인정받았다. 이후에도 이러한 모델과 아키텍처의 한계를 끊임없이 넓혀가고 있다.&lt;/p>
&lt;p>recurrent 모델은 입력과 출력 시퀀스의 위치에 따라 계산을 분류하며, 이는 순차적인 특성으로 인해 학습 예제 내의 병렬화를 방해한다. 이는 메모리 제약이 있는 긴 시퀀스에서 중요한 문제가 된다. 최근의 연구는 계산 효율성을 향상시키는 방법을 제시하였지만, sequential computation의 근본적인 제약은 여전히 남아 있다.&lt;/p>
&lt;p>attention mechanism은 시퀀스 모델링에 있어 핵심 역할을 하며, 입력이나 출력 시퀀스의 거리에 관계 없이 종속성을 모델링할 수 있다. 그러나 대부분의 경우, attention mechanism은 recurrent 네트워크와 함께 사용된다.&lt;/p>
&lt;p>recurrent을 배제하고 attention mechanism에만 의존하는 transformer를 제안한다. transformer는 더 많은 병렬화를 가능하게 하고, 8개의 P100 GPU에서 단 12시간 학습만으로 state-of-the-art를 달성할 수 있었다.&lt;/p>
&lt;hr>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>sequential computation을 줄이는 것은 Extended Neural GPU, ByteNet, ConvS2S 등의 핵심 목표인데, 이들은 모두 CNN을 사용해 모든 입력과 출력 위치에 대한 표현을 병렬로 계산한다. 그러나 이 모델들은 두 임의의 위치간의 관계를 학습하는데 필요한 연산 수가 위치 간 거리에 따라 증가하므로, 먼 위치 간의 종속성을 학습하기 어렵다. transformer는 이를 상수 수의 연산으로 줄이지만, attention-weighted를 평균화함으로써 해상도가 감소하는 비용이 따르며, 이는 Multi-Head Attention을 통해 상쇄시킨다.&lt;/p>
&lt;p>self-attention은 단일 시퀀스의 다양한 위치를 연관시켜 시퀀스의 표현을 계산하는 방법으로, 독해, 요약, 텍스트 함의 파악, 작업 독립적 문장 표현 학습 등 다양한 작업에 성공적으로 활용되었다.&lt;/p>
&lt;p>end-to-end memory network는 recurrent attention mechanism을 기반으로 하며, 간단한 언어 질문 응답 및 언어 모델링 작업에서 좋은 성능을 보여주었다.&lt;/p>
&lt;p>transformer는 시퀀스에 정렬된 RNN이나 convolution을 사용하지 않고, 완전히 self-attention에 의존하여 입력과 출력의 표현을 계산하는 최초의 transduction 모델이다.&lt;/p>
&lt;hr>
&lt;h2 id="model-architecture">Model Architecture&lt;/h2>
&lt;p>대부분의 neural sequence transduction 모델은 encoder-decoder 구조를 가지고 있습니다. encoder는 기호 표현의 입력 시퀀스 $(x_1, &amp;hellip;, x_n)$를 연속적인 표현의 시퀀스 $z = (z_1, &amp;hellip;, z_n)$로 변환하고, 이를 기반으로 decoder는 한 번에 하나씩 기호의 출력 시퀀스 $(y_1, &amp;hellip;, y_m)$를 생성한다. 이때 모델은 이전에 생성된 기호를 추가 입력으로 사용하는 auto-regressive 방식을 취한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/figure1.png"
width="660"
height="932"
srcset="https://kurtkim.github.io/p/transformers/images/figure1_hud7e6f7a5842be66fe4891f11600f3af9_164918_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/figure1_hud7e6f7a5842be66fe4891f11600f3af9_164918_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="70"
data-flex-basis="169px"
>&lt;/p>
&lt;p>transformer는 encoder와 decoder 모두에 대해 쌓인 self-attention과 point-wise, fully connected layer을 사용하여 encoder-decoder 구조를 따른다.&lt;/p>
&lt;h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks&lt;/h3>
&lt;p>&lt;strong>Encoder:&lt;/strong> encoder는 $N = 6$개의 동일한 계층으로 이루어져 있으며, 각 계층은 multi-head self-attention mechanism과 positionwise fully connected feed-forward network의 두 sub-layer로 구성된다. 각 하위 계층은 residual connection과 $LayerNorm(x + Sublayer(x))$를 통해 처리되며, 모든 하위 계층과 임베딩 계층은 차원 $d_{model} = 512$인 출력을 생성한다.&lt;/p>
&lt;p>&lt;strong>Decoder:&lt;/strong> decoder는 $N = 6$개의 동일한 계층으로 구성되며, encoder의 출력에 multi-head attention을 수행하는 세 번째 sub-layer이 추가된다. 각 sub-layer 주변의 residual connection과 layer normalization를 사용하며, 후속 위치에 주의를 기울이는 것을 방지하기 위해 decoder의 self-attention sub-layer을 수정한다. 이러한 수정은 위치 $i$의 예측이 $i$보다 작은 위치에서의 알려진 출력에만 의존하도록 보장한다.&lt;/p>
&lt;h3 id="attention">Attention&lt;/h3>
&lt;p>attention 함수는 query와 key-value 쌍을 벡터 형태의 출력으로 매핑하며, 출력은 값들의 가중치 합으로 계산된다. 이때 각 값의 가중치는 query와 해당 key의 호환성에 따라 결정된다.&lt;/p>
&lt;h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention&lt;/h4>
&lt;p>&amp;ldquo;Scaled Dot-Product Attention&amp;quot;은 query와 key의 차원이 $d_k$, 값의 차원이 $d_v$인 입력을 처리한다. query와 모든 key의 내적을 계산하고, 이를 $\sqrt{d_k}$로 나눈 후, softmax 함수를 적용하여 값에 대한 가중치를 얻는다.&lt;/p>
&lt;p>여러 쿼리들을 동시에 처리하기 위해 행렬 $Q$에 패킹하고, key와 value 또한 각각 행렬 $K$와 $V$에 패킹한다. 그리고 이를 이용해 출력 행렬을 계산한다:&lt;/p>
&lt;p>$$ Attention(Q, K, V) = softmax({{QK^\intercal}\over{\sqrt{d_k}}})V $$&lt;/p>
&lt;p>가장 흔히 사용되는 attention 함수는 additive attention과 dot-product (multiplicative) attention입니다. dot-product attention은 알고리즘과 ${{1}\over{\sqrt{d_k}}}$의 스케일링 요소를 제외하면 동일하며, additive attention은 feed-forward network를 이용해 호환성 함수를 계산한다. 두 방법은 이론적으로 유사하지만, dot-product attention은 최적화된 행렬 곱셈 코드를 통해 더 빠르고 공간 효율적으로 구현될 수 있다.&lt;/p>
&lt;p>$d_k$ 값이 작은 경우 두 메커니즘이 유사하게 작동하지만, $d_k$ 값이 크면 스케일링 없는 dot-product attention의 성능이 떨어잔다. 이는 dot-product 값의 크기 증가로 인해 softmax 함수의 기울기가 매우 작아지는 것을 방지하기 위해, dot-product을 ${{1}\over{\sqrt{d_k}}}$로 스케일링한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/figure2.png"
width="902"
height="492"
srcset="https://kurtkim.github.io/p/transformers/images/figure2_hu8092e7ec43193a1f60e58782f6994482_111256_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/figure2_hu8092e7ec43193a1f60e58782f6994482_111256_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="440px"
>&lt;/p>
&lt;h4 id="multi-head-attention">Multi-Head Attention&lt;/h4>
&lt;p>$d_{model}$-dimensional key, value, query에 single attention 함수를 사용하는 대신, 각각을 다른 선형 변환을 통해 $d_k$, $d_k$, $d_v$ 차원으로 $h$번 변환하는 것이 유익하다는 것을 발견하였다. 이 변환된 query, key, value에 대해 병렬로 attention 함수를 수행하면, $d_v$ 차원의 출력 값이 나오며, 이들은 연결되고 다시 변환되어 최종 값이 생성된다.&lt;/p>
&lt;p>multi-head attention은 다른 표현 하위 공간에서 다른 위치의 정보에 동시에 주의를 기울일 수 있게 해주는 반면, single attention head는 이를 평균화하여 방해한다.&lt;/p>
&lt;p>$$ MultiHead(Q, K, V) = Concat(head_1, &amp;hellip;, head_h)W^O $$
$$ where \ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) $$&lt;/p>
&lt;p>프로젝션은 parameter 행렬 $W_i^Q \in \mathbb{R}^{d_{model \times d_k}}$, $W_i^K \in \mathbb{R}^{d_{model \times d_k}}$, $W_i^V \in \mathbb{R}^{d_{model \times d_v}}$ 그리고 $W^O \in \mathbb{R}^{hd_v \times d_{model}}$이다.&lt;/p>
&lt;p>이 작업에서는 8개의 병렬 attention layer를 사용하며, 각 계층에 대해 $d_k = d_v = d_{model} / h = 64$를 사용한다. 각 head의 차원이 줄어들었기 때문에 전체 계산 비용은 전체 차원의 single-head attention과 유사하다.&lt;/p>
&lt;h4 id="applications-of-attention-in-our-model">Applications of Attention in our Model&lt;/h4>
&lt;p>transformer는 세 가지 다른 방식으로 multi-head attention을 사용한다:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;encoder-decoder attention&amp;quot;에서는 이전 decoder 계층에서 query가 생성되고, encoder의 출력에서 메모리 key와 value이 생성된다. 이를 통해 decoder의 모든 위치가 입력 시퀀스 전체에 주의를 기울일 수 있습니다. 이는 일반적인 sequence-to-sequence 모델의 encoder-decoder attention mechanism을 따른다.&lt;/li>
&lt;li>encoder에는 self-attention layer가 있으며, 이 layer에서는 모든 key, value, query가 encoder의 이전 layer의 출력에서 생성된다. 이를 통해 encoder의 각 위치가 이전 layer의 모든 위치에 주의를 기울일 수 있다.&lt;/li>
&lt;li>decoder의 self-attention layer는 decoder의 각 위치가 그 위치를 포함해 그 이전의 모든 위치에 주의를 기울일 수 있게 한다. auto-regressive 속성을 유지하기 위해, 불법적인 연결에 해당하는 값을 마스킹 아웃($-\infty$로 설정)하여 decoer 내부의 정보 흐름을 제한한다.&lt;/li>
&lt;/ul>
&lt;h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks&lt;/h3>
&lt;p>encoder와 decoder의 각 layer에는 각 위치에 독립적으로 적용되는 fully connected feed-forward network가 포함되어 있으며, 이는 두 개의 linear transformation과 그 사이의 ReLU activation 함수로 구성된다.&lt;/p>
&lt;p>$$ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $$&lt;/p>
&lt;p>linear transformation은 다른 위치에도 동일하게 적용되지만, layer마다 다른 parameter를 사용한다. 이는 kernel size 1의 두 개의 convolution으로도 설명할 수 있다. 입력과 출력의 차원은 $d_{model} = 512$이고, inner-layer의 차원은 $d_{ff} = 2048$이다.&lt;/p>
&lt;h3 id="embeddings-and-softmax">Embeddings and Softmax&lt;/h3>
&lt;p>학습된 임베딩을 사용하여 입력 토큰과 출력 토큰을 벡터로 변환하며, 학습된 linear transformation과 softmax 함수를 사용해 decoder 출력을 다음 토큰 확률로 변환한다. 두 임베딩 layer와 pre-softmax linear transformatio에서 동일한 가중치 행렬을 공유하고, 임베딩 layer에서는 이 가중치에 ${{1}\over{\sqrt{d_k}}}$를 곱한다.&lt;/p>
&lt;h3 id="positional-encoding">Positional Encoding&lt;/h3>
&lt;p>transformer 모델은 recurrence와 convolution이 없기 때문에, 시퀀스의 토큰 위치에 대한 정보를 주입함으로써 시퀀스의 순서를 활용한다. 이를 위해, &amp;ldquo;positional encoding&amp;quot;을 입력 임베딩에 더하며, 이는 임베딩과 동일한 차원을 가진다. positional encoding은 학습되거나 고정될 수 있다.&lt;/p>
&lt;p>다른 주파수의 sine 함수와 cosine 함수를 사용한다:&lt;/p>
&lt;p>$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$
$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) $$&lt;/p>
&lt;p>positional encoding의 각 차원이 sinusoid에 해당하도록 sine 함수와 코사인 cosine를 사용한다. 이 함수를 선택한 이유는 모델이 상대적 위치에 따라 주의를 쉽게 배울 수 있도록 하기 위해서이다. 즉, 어떤 고정된 오프셋 $k$에 대해서도, $PE_{pos+k}$는 $PE_{pos}$의 선형 함수로 표현될 수 있다.&lt;/p>
&lt;p>학습된 positional embedding을 사용해 실험해봤고, 두 방식이 거의 동일한 결과를 생성함을 확인하였다. sinusoidal 버전을 선택한 이유는 학습 중에 접한 것보다 더 긴 시퀀스 길이로 extrapolate 할 수 있을 것이라 판단했기 때문이다.&lt;/p>
&lt;hr>
&lt;h2 id="why-self-attention">Why Self-Attention&lt;/h2>
&lt;p>recurrent및 convolutional layer와 self-attention layer을 비교한다. 이들은 모두 가변 길이의 심볼 표현 시퀀스를 동일한 길이의 다른 시퀀스로 매핑하는데 사용된다. self-attention 사용의 동기를 설명하기 위해, 세 가지 조건을 고려한다.&lt;/p>
&lt;p>하나는 각 계층에서의 전체 계산 복잡성이다. 또 다른 하나는 병렬화할 수 있는 계산량으로, 이는 필요한 최소 연속 작업의 수로 측정된다.&lt;/p>
&lt;p>세 번째는 네트워크 내에서 long-range dependency 사이의 경로 길이이다. long-range dependency를 학습하는 것은 시퀀스 변환 작업의 주요 도전 과제이다. 이를 학습하는 능력은 네트워크 내에서 신호가 이동하는 경로의 길이에 크게 영향을 받는다. 입력과 출력 시퀀스의 임의의 위치 사이의 경로가 짧을수록 long-range dependency을 학습하기 쉽다. 따라서, 다른 layer 유형으로 구성된 네트워크에서 두 입력과 출력 위치 사이의 최대 경로 길이도 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table1.png"
width="1018"
height="218"
srcset="https://kurtkim.github.io/p/transformers/images/table1_hu13e34a3f917253fd28a66dff1519f55e_47240_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table1_hu13e34a3f917253fd28a66dff1519f55e_47240_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="466"
data-flex-basis="1120px"
>&lt;/p>
&lt;p>self-attention layer는 연속적인 연산을 통해 모든 위치를 연결하며, recurrent layer에 비해 더 빠른 계산 속도를 제공한다. 특히, 시퀀스 길이가 표현 차원보다 작은 경우에 더욱 그렇다. 매우 긴 시퀀스를 처리하는 작업의 계산 성능을 높이기 위해, self-attention은 출력 위치를 중심으로 한 입력 시퀀스의 이웃만을 고려하도록 제한될 수 있다. 이는 최대 경로 길이를 $O(n/r)$로 증가시키며, 이에 대한 연구를 미래에 더 진행할 계획이다.&lt;/p>
&lt;p>커널 너비가 $k &amp;lt; n$인 단일 convolutional layer는 모든 입력과 출력 위치를 연결하지 않는다. 이를 위해선 복수의 convolutional layer이 필요하고, 이로 인해 네트워크 내 두 위치 사이의 가장 긴 경로가 늘어난다. 보통 convolutional layer는 recurrent layer보다 $k$배의 비용이 더 들지만, 분리 가능한 convolution을 사용하면 복잡성이 크게 줄어든다. 그러나 $k = n$인 경우에도, 분리 가능한 convolution의 복잡성은 self-attention layer와 point-wise feed-forward layer의 결합과 동일하다.&lt;/p>
&lt;p>self-attention은 더 해석 가능한 모델을 만들 수 있는 이점이 있다. 우리 모델에서는 각 attention head가 다른 작업을 수행하도록 학습하며, 이들 중 많은 헤드가 문장의 구문적 및 의미적 구조와 관련된 행동을 보이는 것으로 파악되었다.&lt;/p>
&lt;hr>
&lt;h2 id="training">Training&lt;/h2>
&lt;h3 id="training-data-and-batching">Training Data and Batching&lt;/h3>
&lt;p>약 450만 개의 문장 쌍을 포함하는 표준 WMT 2014 영어-독일어 데이터셋으로 학습했다. 더 큰 WMT 2014 영어-불어 데이터셋도 사용하였다. 각 학습 배치는 약 25000개의 소스 토큰과 타겟 토큰을 포함하는 문장 쌍을 포함하였다.&lt;/p>
&lt;h3 id="hardware-and-schedule">Hardware and Schedule&lt;/h3>
&lt;p>8개의 NVIDIA P100 GPU에서 모델을 학습시켰다. 기본 모델들은 각 학습 단계마다 약 0.4초가 걸렸고, 총 100,000단계 또는 12시간 동안 학습되었다. 큰 모델들은 단계 시간이 1.0초였고, 300,000단계 또는 3.5일 동안 학습되었다.&lt;/p>
&lt;h3 id="optimizer">Optimizer&lt;/h3>
&lt;p>Adam optimizer, $\beta_1 = 0.9$, $\beta_2 = 0.98$ 그리고 $\epsilon = 10^{−9}$를 사용하였다. 다음의 공식에 따라서 learning rate을 변화시켰다:&lt;/p>
&lt;p>$$ lrate = d_{model}^{−0.5} · min(\text{step_num}^{−0.5}, \text{step_num} · \text{warmup_steps}^{−1.5}) $$&lt;/p>
&lt;p>warmup_steps 동안 learning rate을 선형적으로 증가시키고, 그 이후에는 단계 수의 역제곱에 비례하여 감소시킨다. warmup_steps = 4000을 사용하였다.&lt;/p>
&lt;h3 id="regularization">Regularization&lt;/h3>
&lt;p>세 가지 유형의 regularization를 사용한다:&lt;/p>
&lt;p>&lt;strong>Residual Dropout&lt;/strong> 각각의 sub-layer의 출력과 encoder 및 decoder 스택의 임베딩과 positional encoding의 합에 드롭아웃을 적용합니다. 기본 모델에서는 dropout rate로 $P_{drop} = 0.1$을 사용한다.&lt;/p>
&lt;p>&lt;strong>Label Smoothing&lt;/strong> 학습 동안에는 $\epsilon_{ls}= 0.1$의 값을 가진 라벨 스무딩을 사용하였다. 이는 모델이 더 불확실하게 학습하도록 만드므로 혼란스러움(perplexity)을 증가시키지만, 정확도와 BLEU 점수는 향상시킨다.&lt;/p>
&lt;hr>
&lt;h2 id="results">Results&lt;/h2>
&lt;h3 id="machine-translation">Machine Translation&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table2.png"
width="950"
height="416"
srcset="https://kurtkim.github.io/p/transformers/images/table2_hu74a18d247471b463bc514720d41dbf06_99215_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table2_hu74a18d247471b463bc514720d41dbf06_99215_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;p>WMT 2014 영어-독일어 번역 작업에서, large transformer 모델은 이전 모델들을 2.0 BLEU 이상 뛰어넘어 state-of-the-art인 28.4의 BLEU 점수를 달성하였다. 이 모델은 8개의 P100 GPU에서 3.5일 동안 학습되었다. 기본 모델조차도 이전의 모든 모델과 앙상블을 능가하며, 경쟁 모델의 학습 비용의 일부에 불과했다.&lt;/p>
&lt;p>WMT 2014 영어-불어 번역 작업에서, BLEU 점수 41.0을 달성하여 이전에 발표된 모든 단일 모델들을 능가했고, 이전 state-of-the-art 모델의 학습 비용의 1/4 미만이었다. 이 모델은 dropout rate로 $P_{drop} = 0.1$을 사용했다.&lt;/p>
&lt;p>기본 모델에 대해 마지막 5개의 체크포인트를 평균한 단일 모델을 사용했고, 큰 모델에 대해선 마지막 20개의 체크포인트를 평균냈다. beam search를 통해 beam size 4와 length penalty $\alpha = 0.6$을 사용했다. 이 값들은 개발 세트에서 실험 후 결정되었다. 추론 시 최대 출력 길이는 입력 길이 + 50으로 설정되었으나, 가능하다면 일찍 종료한다.&lt;/p>
&lt;p>모델 학습에 사용된 부동 소수점 연산의 수는 학습 시간, 사용된 GPU의 수, 각 GPU의 단정밀도 부동 소수점 용량의 추정치를 곱하여 추정하였다.&lt;/p>
&lt;h3 id="model-variations">Model Variations&lt;/h3>
&lt;p>transformer의 다양한 요소의 중요성을 평가하기 위해, 기본 모델을 다양하게 변형하며 개발 세트인 newstest2013에서의 영어-독일어 번역 성능 변화를 측정하였다. beam search을 사용했지만 체크포인트 평균화는 사용하지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table3.png"
width="1088"
height="704"
srcset="https://kurtkim.github.io/p/transformers/images/table3_hu33e15ffc6a9c7fa71d0bbbf9c9b1cc6c_122384_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table3_hu33e15ffc6a9c7fa71d0bbbf9c9b1cc6c_122384_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="370px"
>&lt;/p>
&lt;p>single-head attention은 최적 설정보다 0.9 BLEU가 떨어지며, head 수가 너무 많아져도 품질이 떨어진다.&lt;/p>
&lt;p>attention key 크기를 줄이면 모델 품질이 떨어진다는 것을 확인하였다. 이는 복잡한 호환성 함수가 필요할 수 있음을 시사한다. 더 큰 모델이 더 좋고, dropout이 over-fitting을 피하는 데 매우 유용하다는 것을 확인했다. sinusoidal positional encoding을 learned positional embedding으로 대체했을 때 기본 모델과 거의 동일한 결과를 얻었다.&lt;/p>
&lt;h3 id="english-constituency-parsing">English Constituency Parsing&lt;/h3>
&lt;p>transformer가 다른 작업에 일반화할 수 있는지 확인하기 위해, 구조적 제약이 강하고 입력보다 긴 출력을 가진 영어 구성성 파싱 작업에 대한 실험을 수행하였다. RNN sequence-to-sequence 모델은 이 작업에서 state-of-the-art를 달성하지 못하였다.&lt;/p>
&lt;p>Penn Treebank의 Wall Street Journal (WSJ) 부분에 대해 약 4K 개의 학습 문장을 사용하여 $d_{model} = 1024$의 4-layer transformer를 학습시켰다. 또한, 약 17M 문장을 포함하는 대형 말뭉치를 사용하여 반지도학습 환경에서도 학습시켰다. WSJ만을 대상으로 하는 경우 16K 토큰의 어휘를, 반지도학습 설정에서는 32K 토큰의 어휘를 사용하였다.&lt;/p>
&lt;p>dropout, learning rate, beam size를 결정하기 위해 Section 22 개발 세트에서 몇 가지 실험을 수행했고, 모든 다른 parameter는 기본 번역 모델에서 변경되지 않았다. 추론 시에는 최대 출력 길이를 입력 길이 + 300으로 늘렸다. beam size 21과 $\alpha = 0.3$을 모든 설정에 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/transformers/images/table4.png"
width="878"
height="404"
srcset="https://kurtkim.github.io/p/transformers/images/table4_hua6a988743e74ec9f8c4e6bb5341f2d83_117514_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/transformers/images/table4_hua6a988743e74ec9f8c4e6bb5341f2d83_117514_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="521px"
>&lt;/p>
&lt;p>작업 특화 튜닝이 없음에도 이전 모델들보다 더 좋은 성능을 보였고, 이는 Recurrent Neural Network Grammar을 제외한 모든 이전에 보고된 모델들보다 더 좋은 결과를 가져왔다.&lt;/p>
&lt;p>RNN sequence-to-sequence 모델과는 달리, transformer는 오직 WSJ 학습 세트의 40K 문장만을 이용하여 학습했음에도 BerkeleyParser를 능가하는 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>transformer는 attention 기반으로 만들어진 첫 시퀀스 transduction 모델로, encoder-decoder 구조의 recurrent layer를 multi-headed self-attention로 대체하였다.&lt;/p>
&lt;p>transformer는 recurrent나 convolution 기반 아키텍처보다 빠르게 학습되며, WMT 2014 영어-독일어와 영어-프랑스어 번역 작업에서 state-of-the-art를 달성하였다. 이 중 영어-독일어 작업에서는 이전의 모든 앙상블보다 더 뛰어난 성능을 보여주었다.&lt;/p>
&lt;p>attention 기반 모델을 다른 작업에 적용하고, 텍스트 이외의 다양한 입력과 출력 문제에 transformer를 확장하려 한다. 또한, 큰 이미지, 오디오, 비디오 등을 효율적으로 처리하기 위해 restricted attention mechanism을 연구하고, 생성 과정을 덜 순차적으로 만드는 것을 목표로 하고 있다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>