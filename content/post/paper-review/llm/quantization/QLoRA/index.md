+++
author = "Kurt"
title = "QLoRA"
date = "2024-04-20"
description = "Efficient Finetuning of Quantized LLMs"
categories = [
    "Paper Review"
]
tags = [
    "LLM",
    "Quantization",
]
draft = true
+++

## Abstract

QLoRA는 단일 48GB GPU에서 65B parameter 모델을 효율적으로 미세 조정 할 수 있는 방법으로, 메모리 사용량을 줄이면서도 전체 16비트 미세 조정 성능을 유지한다. 이 방법은 동결된 4비트 양자화 사전 학습 모델을 통해 gradient를 LoRA로 backpropagate 한다. Guanaco 모델은 Vicuna 벤치마크에서 기존 모델들을 능가하며, 단일 GPU를 사용해 24시간만에 ChatGPT의 99.3% 성능에 도달한다. QLoRA는 성능 저하 없이 메모리를 절약하는 여러 혁신을 도입했으며, 1,000개 이상의 모델을 미세 조정 해 다양한 데이터셋과 모델 규모에서 상세한 분석을 제공한다. 이 연구는 작은 고품질 데이터셋에서 최신 기술 결과를 달성할 수 있음을 보여주며, GPT-4 평가가 인간 평가의 합리적 대안임을 보여준다. 또한, 현재 챗봇 벤치마크의 신뢰성 문제를 지적하고, 모든 모델과 코드를 공개한다.

---

## Introduction

대규모 언어 모델(LLMs)을 미세 조정하는 것은 성능 향상과 바람직한 행동 조정에 효과적이지만, 매우 비용이 많이 든다. 예를 들어, 65B parameter 모델을 16비트로 미세 조정하려면 780GB 이상의 GPU 메모리가 필요하다. 양자화 기술로 메모리 사용량을 줄일 수 있으나, 이는 추론 시에만 유효하고 학습 시에는 적용되지 않는다.

QLORA 방법을 통해, 처음으로 4비트 양자화 모델을 성능 저하 없이 미세 조정할 수 있음을 입증하였다. 이는 새로운 고정밀 기술로 모델을 4비트로 양자화하고, 양자화된 가중치를 통한 backpropagating gradient로 조정 가능한 Low-rank Adapter 가중치를 추가하는 방식이다.

QLORA는 65B parameter 모델의 GPU 메모리 요구량을 780GB에서 48GB 미만으로 대폭 줄였다. 이를 통해 최대 규모의 공개 모델을 단일 GPU에서 세부 조정할 수 있게 되었으며, 실행 시간과 예측 성능은 기존 16비트 기준선과 동일하다. QLORA를 사용해 Guanaco 모델을 훈련시켜 ChatGPT와의 성능 격차를 거의 해소했으며, 가장 작은 Guanaco 모델은 5GB 메모리로 Vicuna 벤치마크에서 Alpaca 모델을 크게 앞섰다.

QLORA는 성능 손실 없이 메모리 사용을 줄이기 위해 여러 혁신을 적용한다: (1) 정규 분포 데이터에 최적화된 4비트 NormalFloat는 기존의 4비트 정수 및 부동 소수점보다 우수한 결과를 제공한다. (2) 이중 양자화는 양자화 상수를 추가로 양자화하여 parameter 당 평균 0.37비트를 절약한다. (3) 페이지 옵티마이저는 긴 시퀀스를 처리할 때 메모리 스파이크를 방지한다. 이러한 기법들을 통합해, QLORA는 네트워크의 모든 계층에 어댑터를 적용하며 이전 방법들에서의 정확도 타협을 대부분 해결한다.

QLORA의 효율성 덕분에 기존 세부 조정 방식으로는 불가능한 크기의 모델들에 대한 지시사항 세부 조정과 챗봇 성능 연구를 수행할 수 있다. 이를 통해 80M에서 65B 파라미터 범위의 다양한 데이터셋과 모델 구조에서 1,000개 이상의 모델을 학습시켰다. 주요 발견으로, 데이터 품질이 크기보다 중요하며, 예를 들어 작은 데이터셋이 큰 데이터셋을 성능 면에서 능가하였다. 또한, 언어 이해 벤치마크에서의 성능이 챗봇 벤치마크에서의 성능을 보장하지 않음을 확인하였다. 이는 작업에 적합한 데이터셋의 중요성을 강조한다.

인간 평가자와 GPT-4를 사용한 챗봇 성능 분석에서, 토너먼트 방식을 통해 모델 간 경쟁이 이루어졌다. 이 경쟁에서는 Elo 점수로 순위가 결정되며, 대체로 GPT-4와 인간 평가의 순위가 일치하지만 강한 불일치 사례도 발견되었다. 이는 모델 기반 평가가 비용 효율적이긴 하지만, 일정한 불확실성을 가지고 있음을 시사한다.

Guanaco 모델에 대한 질적 분석을 통해 챗봇 벤치마크 결과를 보완하며, 이를 통해 정량적 평가에서 놓친 성공 및 실패 사례를 드러낸다.

인간 평가자와 GPT-4 주석이 달린 모든 모델 생성물을 공개하고, 코드베이스와 CUDA 커널을 오픈 소스로 제공한다. 또한, Hugging Face 트랜스포머 스택에 이 연구의 방법을 통합하여 접근성을 높였다. 7/13/33/65B 크기의 모델용 어댑터와 8개 지시사항 데이터셋에서 학습된 32개의 미세 조정 모델을 공개한다.

---

## Background

**Block-wise k-bit Quantization** 양자화는 더 많은 비트를 가진 데이터 타입을 더 적은 비트의 데이터 타입으로 변환하는 과정이다. 이 과정에서 입력 데이터는 입력 요소들의 절대 최대값을 기준으로 정규화하여 저비트 데이터 타입의 전체 범위를 활용하도록 재조정된다. 예를 들어, 32비트 부동 소수점(FP32) 텐서를 [-127, 127] 범위의 8비트 정수(Int8) 텐서로 변환하는 경우가 있다.

$$ X^{Int8} = round \big( {{127}\over{absmax(X^{FP32}}} X^{FP32} \big) = round(c^{FP32} · X^{FP32}) $$

여기서 $c$는 양자화 상수 또는 양자화 스케일이다. 역양자화는 반대 과정이다:

$$ dequant(c^{FP32}, X^{Int8}) = {{X^{Int8}}\over{c^{FP32}}} = X^{FP32} $$

이 접근법의 문제는 입력 텐서 내의 큰 값(이상치)으로 인해 양자화 빈이 제대로 활용되지 않는다는 것이다. 이를 해결하기 위해, 입력 텐서를 각자의 양자화 상수 $c$를 가진 독립적으로 양자화되는 블록으로 나누는 방법이 일반적이다. 이 과정은 입력 텐서 $X$를 $n$개의 크기 $B$ 블록으로 나누고, 각 블록을 독립적으로 양자화하여 양자화된 텐서와 양자화 상수 ci를 생성하는 것으로 공식화할 수 있다.

**Low-rank Adapters** Low-rank Adapter (LoRA) 미세조정은 전체 모델 parameter를 고정하고 작은 학습 가능한 parameter 세트(adapter)를 사용해 메모리 요구사항을 줄이는 방법이다. 이 방법은 추가 요인화된 투영을 통해 선형 투영을 확장하며, stochastic gradient descent를 통해 어댑터를 업데이트하여 손실 함수를 최적화한다.

$$ Y = XW + sXL_1 L_2 $$

여기서 $L1 \in \mathbb{R}^{h×r}$과 $L2 \in \mathbb{R}^{r×o}$이며, $s$는 스칼라이다.

**Memory Requirement of Parameter-Efficient Finetuning** LoRA 학습 중 메모리 요구사항은 어댑터의 수와 크기에 관한 중요한 논의 사항이다. LoRA의 낮은 메모리 사용량 덕분에, 더 많은 어댑터를 사용해도 전체 메모리 사용량을 크게 증가시키지 않고 성능을 향상시킬 수 있다. 대부분의 메모리 사용량은 LoRA parameter가 아닌 활성화 gradient에서 발생하며, 예를 들어 7B LLaMA 모델을 학습할 때 LoRA 입력 gradient는 567MB, LoRA parameter는 26MB를 차지합니다. gradient 체크포인팅을 통해 입력 gradient는 시퀀스 당 평균 18MB로 줄어들며, 이는 전체 학습 메모리 사용량을 크게 증가시키지 않고 더 많은 어댑터를 사용할 수 있음을 의미한다. 이 점은 전체 16비트 정밀도 성능을 회복하는 데 중요하다.

---

## QLORA Finetuning





---

## Reference

* [Paper](https://arxiv.org/pdf/2305.14314.pdf)
* [GitHub](https://github.com/artidoro/qlora)