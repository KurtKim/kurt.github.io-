+++
author = "Kurt"
title = "Mamba 2: Transformers are SSMs"
date = "2024-10-10"
description = "Generalized Models and Efficient Algorithms Through Structured State Space Duality"
categories = [
    "Paper Review"
]
tags = [
    "LLM",
    "Milestone",
]
draft = true
+++

## Abstract

트랜스포머는 언어 모델링의 주요 아키텍처였지만, Mamba와 같은 상태공간 모델(SSM)은 최근 소규모에서 중규모에 이르는 성능에서 트랜스포머와 동등하거나 이를 초과하였다. SSM과 주의(attention) 변형 간의 이론적 연결을 탐구하고, 상태 공간 쌍대(SSD) 프레임워크를 통해 Mamba의 선택적 SSM을 정제한 새로운 아키텍처(Mamba-2)를 설계하였다. Mamba-2는 2-8배 더 빠르면서도 언어 모델링에서 트랜스포머와 경쟁력을 유지한다.

---

## Introduction

Transformer 모델, 특히 인과적 디코더 모델(GPT, Llama 등)은 현대 딥러닝의 핵심 기술이지만, 시퀀스 길이에 따른 비효율성 문제가 있다. 이에 대응하여 구조화된 상태 공간 모델(SSM)이 등장했으며, 이는 시퀀스 길이에 따라 선형적으로 확장되고 생성 시 일정한 상태 크기를 유지하며 장기 의존성 작업에서 우수한 성능을 보여준다. 하지만 SSM은 Transformer와 독립적으로 개발되어 이해와 실험이 어렵고, 효율적 학습에도 여전히 도전이 따른다.

이 연구의 목표는 SSM과 다양한 어텐션 구조 간의 이론적 연결을 구축해, Transformer의 최적화 기법을 SSM에 적용하여 더 효율적이고 성능이 우수한 모델을 개발하는 것이다. Linear Attention 프레임워크 처럼 이중 형태의 이론적 동등성을 통해 효율적 학습과 추론을 가능하게 하며, 이 논문은 SSM과 어텐션의 강점을 결합하는 다양한 관점을 제시한다.

**State Space Duality.** 구조화된 상태 공간 모델(SSM)과 다양한 형태의 어텐션을 연결하는 프레임워크를 structured state space duality (SSD)이라고 하며, 이는 서브 제곱 매개변수와 곱셈 복잡성을 갖는 구조화된 행렬의 추상화를 통해 이루어진다. 시퀀스 모델을 표현하기 위해 행렬 변환과 텐서 수축 두 가지 프레임워크를 개발하여 이중성에 대한 서로 다른 관점을 제공한다. 이 연구의 기술적 기여는 다음과 같다:

- 상태 공간 모델과 반분리 행렬(semi-separable matrices) 간의 동등성을 보여주며, 이는 SSM에 대한 새로운 특성과 알고리즘을 드러내는 우리의 프레임워크의 핵심이다. 상태 공간 모델을 계산하는 방법들은 구조화된 행렬에 대한 다양한 행렬 곱셈 알고리즘으로 재구성될 수 있다.
- 선형 어텐션의 이론을 개선하여, 텐서 수축을 통해 재귀적 형태의 명확한 증명을 제공하고 이를 구조화된 마스크 어텐션(SMA)의 새로운 가족으로 일반화한다.
- SSM과 SMA가 서로의 쌍대 관계에 있는 큰 교집합을 가지고 있으며, 이들은 SSM과 유사한 선형 형태와 어텐션과 유사한 제곱 형태를 모두 가진다고 보여준다. 또한, 빠른 재귀적 형태를 가지는 커널 어텐션 방법은 반드시 SSM이어야 한다고 증명한다.

이 프레임워크는 시퀀스 모델을 이해하고 개선할 수 있는 다양한 방향성을 제공한다.

**Efficient Algorithms.** 이 프레임워크는 SSM을 계산하기 위한 새로운 효율적이고 쉽게 구현 가능한 알고리즘을 제시한다. 반분리 행렬의 블록 분해를 기반으로 한 SSD 알고리즘은 선형 SSM 재귀와 제곱 쌍대 형태를 활용하여 학습 및 추론 계산, 메모리 사용량 등에서 최적의 효율성을 제공한다. SSD는 Mamba의 최적화된 선택적 스캔 구현보다 2-8배 빠르며, 최대 8배 더 큰 재귀 상태 크기를 허용한다. 또한, SSD는 최적화된 소프트맥스 어텐션 구현(FlashAttention-2)과 경쟁력이 있으며, 시퀀스 길이 2K에서 교차하고 16K에서 6배 더 빠르다.

**Architecture Design.** SSM을 채택하는 주요 장애물 중 하나는 Transformer에 맞춰진 하드웨어 효율적인 최적화 및 병렬 처리 기술이다. 프레임워크는 기존 어텐션 기술을 활용하여 SSM의 아키텍처 설계 선택을 개선할 수 있도록 한다. 예를 들어, 다중 헤드 어텐션(MHA)의 헤드 개념을 SSM에 도입하고, Mamba 아키텍처가 다중 입력 SSM(MIS)이며 다중 값 어텐션(MVA)과 유사함을 보여주고, 다양한 헤드 구조를 가진 Mamba의 변형을 비교한다.

Mamba 블록에 약간의 수정을 가하여 텐서 병렬 처리를 구현한다(예: Megatron 스타일). 주요 아이디어는 그룹화된 값 어텐션(GVA) 헤드 구조를 도입하고, 데이터 의존적 프로젝션을 블록 시작 부분에서 병렬로 수행하도록 하는 것이다.

수정된 병렬 Mamba 블록과 SSD를 내부 SSM 계층으로 사용하여 Mamba-2 아키텍처를 개발하였다. Mamba와 동일한 설정에서 Mamba-2의 Chinchilla 스케일링 법칙을 조사한 결과, 퍼플렉시티와 월 시계 시간에서 Mamba와 Transformer++를 모두 초월함을 발견하였다. 또한, Pile 데이터셋에서 다양한 크기의 Mamba-2 모델을 학습시킨 결과, 표준 다운스트림 평가에서 Mamba 및 오픈 소스 Transformer보다 우수한 성능을 보였으며, 특히 300B 토큰으로 훈련된 2.7B 매개변수를 가진 Mamba-2가 Mamba-2.8B, Pythia-2.8B, Pythia-6.9B보다 더 뛰어난 성과를 나타내었다.

**Systems Optimizations.** SSD 프레임워크는 SSM과 Transformers를 연결하여 Transformers의 시스템 최적화 연구를 활용할 수 있게 한다.

- 텐서 병렬 처리(TP)는 대규모 Transformer 모델을 학습하기 위해 각 레이어를 동일한 노드의 GPU에 분할하는 기법이다. Mamba-2는 TP 친화적으로 설계되어 블록당 동기화 지점을 절반으로 줄인다.
- 매우 긴 시퀀스에 대해서는 어텐션 블록을 위한 시퀀스 병렬 처리가 개발되었다. 재귀 상태를 장치 간에 전달하여 SSM, 특히 Mamba-2를 시퀀스 병렬로 훈련하는 방법을 설명한다.
- Transformer는 가변 길이 시퀀스에 대해 패딩 토큰을 제거하고 어텐션을 수행하는 복잡한 기법이 필요하지만, Mamba-2는 패딩 토큰 없이 효율적으로 학습될 수 있다.

---

## Background and Overview

### Structured State Space Models

구조화된 상태 공간 시퀀스 모델(Structured State Space Sequence Models, S4)은 RNN, CNN 및 고전적인 상태 공간 모델과 관련된 최신 시퀀스 모델로, 1차원 시퀀스 $ x \in \mathbb{R}^T $를 잠재 상태 $ h \in \mathbb{R}^{(T,N)} $를 통해 $ y \in \mathbb{R}^T $로 매핑하는 연속 시스템에서 영감을 받았다.

구조화된 상태 공간 모델(SSM)의 일반적인 이산 형태는 다음과 같은 방정식으로 표현된다:

$$
h_t = A h_{t-1} + B x_t \\\
y_t = C^\top h_t 
$$

여기서 $ A $, $ B $, $ C $는 각각 차원이 $ (N,N) $, $ (N,1) $, $ (N,1) $인 행렬이다. 구조화된 SSM은 깊은 신경망에서 효율적인 시퀀스 변환을 위해 $ A $ 행렬이 구조화되어야 한다고 명명되었다. 초기에는 대각선 및 저계수(low-rank) 구조(DPLR)와 대각선 구조가 도입되었으며, 현재까지 가장 인기 있는 구조로 남아 있다.

이 연구에서는 상태 공간 모델(SSM)을 구조화된 SSM으로 지칭하며, 여러 유형이 존재하고 주요 신경 시퀀스 모델(연속 시간, 재귀적, 컨볼루션 모델 등)과 깊은 연관이 있다.

**Continuous-time Models.** 원래의 구조화된 상태 공간 모델(SSM)은 시퀀스가 아닌 함수 $x(t) \in \mathbb{R}$에서 $y(t) \in \mathbb{R}$로의 연속 시간 맵으로 시작되었다. 연속 시간 관점에서 방정식에서 행렬 $(A, B)$는 기본 매개변수 $(\overset{˙}{A}, \overset{˙}{B})$와 매개변수화된 스텝 크기 $\Delta \overset{˙}{}$에서 생성되며, "연속 매개변수" $(\Delta, \overset{˙}{A}, \overset{˙}{B})$는 고정된 공식을 통해 "이산 매개변수" $(A, B)$로 변환된다. 이때 쌍 $(f_A, f_B)$는 이산화 규칙(discretization rule)이라고 불린다.

**Remark 1.** 주요 모델은 이전 연구와 동일한 매개변수화 및 이산화 단계를 따르지만, 설명과 표기를 단순화하기 위해 이 부분은 생략한다. 이전 연구에서는 연속 매개변수 $(\overset{˙}{A}, \overset{˙}{B})$와 이산 매개변수 $(A, B)$를 각각 $(A, B)$와 $(\bar{A}, \bar{B})$로 표기했으나, 이산 매개변수에 집중하기 위해 표기를 변경하였다.

**Recurrent Models.** 방정식은 입력 $ x $에 대해 선형인 재귀 형태를 가지며, 구조화된 상태 공간 모델(SSM)은 이를 통해 RNN의 일종으로 볼 수 있다. 선형성 덕분에 SSM은 전통적인 RNN의 순차적 계산을 피하면서도 여전히 시퀀스 변환에서 완전한 표현력을 유지한다.

**Convolutional Models.** SSM의 동역학이 시간에 따라 일정할 때, 이 모델은 선형 시간 불변 모델(Linear Time-Invariant, LTI)로 간주된다. 이 경우 SSM은 컨볼루션과 동등하며, (i) 컨볼루션 커널이 SSM 매개변수 $ (A, B, C) $를 통해 암묵적으로 매개변수화되고, (ii) 커널이 일반적으로 글로벌하다는 점에서 CNN의 일종으로 볼 수 있다. 고전 신호 처리 이론에 따르면 모든 잘 동작하는 컨볼루션은 SSM으로 표현 가능하다.

일반적으로 이전의 LTI SSM은 효율적인 병렬 훈련을 위해 컨볼루션 모드를 사용하고, 자기 회귀 추론을 위해 재귀 모드로 전환한다.

**Selective State Space Models.** 파라미터 $ (A, B, C) $가 시간에 따라 변하는 선택적 SSM은 Mamba에서 도입되었으며, 각 타임스텝에서 입력을 선택적으로 무시하거나 집중할 수 있는 기능을 제공한다. 이는 정보 밀도가 높은 데이터에서 LTI SSM보다 우수한 성능을 보이며, 상태 크기 $ N $가 증가할수록 더 많은 정보를 처리할 수 있다. 그러나 이 모델은 재귀 모드에서만 계산 가능하고, 효율성을 위해 세심한 하드웨어 인식 구현이 필요하다. 결과적으로, CNN 및 Transformer에 비해 여전히 덜 효율적이다.

시간 불변 SSM은 연속적, 재귀적, 컨볼루션 시퀀스 모델과 밀접하게 관련되어 있지만, 어텐션과는 직접적인 관련이 없다. 이 논문에서는 선택적 SSM과 주의 사이의 더 깊은 관계를 보여주고, 이를 통해 SSM의 학ㅂ 속도를 크게 향상시키면서 동시에 훨씬 더 큰 상태 크기 $ N $을 허용한다.

**Structured SSMs as Sequence Transformations.**

**Definition 2.1.** 시퀀스 변환(sequence transformation)은 매개변수화된 함수 $Y = f_{\theta}(X)$를 사용하여 시퀀스 $Y$를 변환하는 과정이다. 여기서 $X, Y \in \mathbb{R}^{(T, P)}$이고, 𝜃는 매개변수 집합이다. $T$는 시퀀스 또는 시간 축을 나타내며, $X_t, Y_t \in \mathbb{R}^P$입니다.

시퀀스 변환(예: SSMs, 자기-주의)은 심층 시퀀스 모델의 핵심이며, Transformer와 같은 신경망 아키텍처에 통합된다. SSM은 $P = 1$로 적용되며, $P > 1$로 일반화할 때는 입력을 $P$개의 독립적인 시퀀스로 보고 각각에 SSM을 적용한다.

**Definition 2.2.** SSM 연산자 $\text{SSM}(A, B, C)$는 시퀀스 변환 $ X \in \mathbb{R}^{(T, P)} \mapsto Y \in \mathbb{R}^{(T, P)} $이다.

SSM에서 N 차원은 상태 크기 또는 상태 확장 계수로, 입력/출력 크기를 N배로 확장하여 계산 효율성에 영향을 미친다.

많은 시퀀스 변환, 예를 들어 주의(attention)는 시퀀스 차원을 따라 하나의 행렬 곱셈으로 표현될 수 있다.

**Definition 2.3.** 시퀀스 변환 $ Y = f_{\theta}(X) $를 매트릭스 변환으로 정의하며, $ Y = M_{\theta}X $ 형태로 표현된다. 시퀀스 변환을 매트릭스 $ M $과 동일시하고, 문맥상 명확할 때는 $ \theta $ 의존성을 생략한다.

### Attention



---

## Reference

* [Paper](https://arxiv.org/pdf/2405.21060)
* [Github](https://github.com/state-spaces/mamba)