+++
author = "Kurt"
title = "D2M-GAN"
date = "2024-08-20"
description = "Quantized GAN for Complex Music Generation from Dance Videos"
categories = [
    "Paper Review"
]
tags = [
    "Audio",
    "Music Generation",
]
draft = true
+++

## Abstract

Dance2Music-GAN (D2M-GAN)을 소개한다. D2M-GAN은 댄스 비디오에 맞춰 복잡한 음악을 생성하는 새로운 프레임워크로, 댄스 비디오 프레임과 신체 움직임을 입력으로 받아 적합한 음악을 생성한다. 기존의 조건부 음악 생성 방식과 달리, VQ 오디오 표현을 사용하여 다양한 스타일의 댄스 음악을 생성하며, 광범위한 실험과 평가를 통해 효과성을 입증하였다. 실제 TikTok 비디오를 포함한 데이터셋을 제공하며, 향후 연구에 유용한 출발점이 될 것이다. 

---

## Introduction

"음악과 춤이 조화를 이룰 때, 그 마법은 마음과 정신을 사로잡는다." 춤과 음악은 오랜 세월 동안 우리의 삶을 풍요롭게 해왔으며, TikTok과 같은 플랫폼에서 춤 영상이 인기를 끌면서 그 중요성이 현대 사회에서도 부각되고 있다. 이에 따라 춤 동작과 음악 간의 생성 작업을 연구하는 새로운 연구들이 활발히 진행 중이다.

춤 비디오에서 음악을 생성하는 것은 두 가지 이유로 어려운 과제다. 첫째, 음악 신호는 고차원적이고 시간적 일관성이 필요하지만, 춤 생성 연구는 상대적으로 저차원적인 움직임 데이터를 출력한다. 이를 해결하기 위해 중간 상징적 오디오 표현(예: 피아노 롤, MIDI)을 사용하지만, 이는 음악의 유연성이 제한된다. 둘째, 각 악기마다 별도 모델이 필요해 복잡한 음악 스타일과 실제 상황에서 일반화가 어렵다.

![](images/figure1.png)

이 격차를 해소하기 위해, 벡터 양자화된 오디오 표현을 사용해 춤 비디오에서 복잡한 음악 샘플을 생성하는 새로운 적대적 멀티모달 프레임워크를 제안한다. VQ-VAE와 VQ-GAN의 성공을 바탕으로 양자화된 벡터를 중간 오디오 표현으로 사용하여, 기존 상징적 표현보다 실제 음악을 더 잘 표현한다. 이 프레임워크는 시각적 프레임과 춤 동작을 입력받아 오디오 VQ 표현을 생성하고, 이를 JukeBox 디코더로 raw 오디오로 복원한다. 또한, 계층적 구조를 통해 확장성과 음악의 충실도를 높인다.

마지막으로, TikTok에서 수집한 445개의 춤 비디오와 85개의 노래로 이루어진 실제 연계된 춤-음악 데이터셋을 제공한다. 이는 기존 데이터셋보다 더 도전적이고 실제 상황을 잘 반영해, 향후 연구의 새로운 기준을 제시한다.

데이터셋을 활용해 제안된 프레임워크의 효율성과 견고성을 입증하는 실험을 진행하였다. 비트, 장르, 일관성 측면에서 춤과의 대응성 및 음악의 전반적인 품질을 평가한 결과, 이 논문의 모델이 여러 음악적 요소에서 경쟁 방법들을 능가하는 그럴듯한 춤 음악을 생성할 수 있음을 확인하였다.

요약하자면, 주요 기여는 다음과 같다:

* D2M-GAN을 제안하여, 벡터 양자화(VQ) 표현을 통해 춤 비디오에서 복잡하고 자유형식의 음악을 생성한다.
* VQ 생성기와 다중 스케일 판별기를 사용하여 음악의 시간적 상관관계와 리듬을 효과적으로 포착하고 복잡한 음악을 생성한다.
* 조건부 생성 음악에 대한 포괄적인 평가 프로토콜을 도입하고, D2M-GAN이 기존 방법들보다 더 복잡하고 그럴듯한 음악을 생성함을 보여준다.
* 현장에서 촬영된 춤 비디오로 구성된 새로운 데이터셋을 생성하여, 조건부 음악 생성에 대한 도전적인 설정을 확립하고 프레임워크의 우수성을 입증한다.

---

## Related Work

**Audio, Vision and Motion.** 최근 멀티모달 학습에서는 오디오, 비전, 모션 데이터를 결합하는 연구가 증가하고 있으며, 이러한 학습된 오디오-비주얼 표현은 다양한 작업에 활용된다. 반면, 이 연구와 관련된 또 다른 분야는 움직임과 소리 간의 상관관계를 조사해왔다. 대부분의 연구는 오디오 신호를 기반으로 2D 포즈 또는 3D 동작을 생성하거나, 움직임으로부터 소리를 생성하는 모델을 제안한다. Zhao et al. 은 동작 경로로부터 소리를 생성하고, Gan et al. 은 공연 비디오에서 음악을 생성하며, Di et al. 은 동작과 리드믹 특성에 따라 비디오 배경 음악을 생성하였다. 반면, 이 연구는 비전과 모션 데이터를 입력으로 받아 음악을 생성하는 세 가지 모달리티를 결합한다.

**Music Generation.** raw 음악 생성은 고차원 오디오 데이터와 복잡한 시간적 상관관계로 인해 어렵다. 기존 접근법은 중간 오디오 표현을 사용하여 계산 요구를 줄이고 학습을 단순화한다. Musegan은 1D 피아노 롤을 사용하고, Music Transformer는 2D MIDI 유사 표현을, Melgan은 멜-스펙트로그램을 사용한다. 최근 JukeBox는 벡터 양자화(VQ) 표현을 기반으로 한 모델을 소개하며, 이 연구의 프레임워크도 VQ 표현을 채택하였다.

**Vector Quantized Generative Models.** VQ-VAEs는 디스크리트 코드와 학습된 프라이어를 사용해 이미지와 오디오 생성에서 효과를 보여주었다. VQ-VAE는 이미지, 비디오, 음성을 생성하며, 개선된 버전은 다중 스케일 구조를 갖춘다. Esser et al. 은 VQ 표현을 GAN 기반 고해상도 이미지 생성에 활용하고, Dhariwal et al. 은 VQ-VAE 기반 음악 생성 모델 JukeBox를 제안하였다. VQ 표현은 복잡한 음악 장르를 유연하게 표현하며, 데이터 차원을 줄인다. 이 연구의 프레임워크는 GAN과 VAE를 결합하여 춤 비디오로부터 VQ 표현을 생성하고, VAE 기반 디코더로 음악을 합성한다.

## Method






---

## Reference

* [Paper](https://arxiv.org/pdf/2204.00604)
* [GitHub](https://github.com/L-YeZhu/D2M-GAN)