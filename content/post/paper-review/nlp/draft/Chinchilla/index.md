+++
author = "Kurt"
title = "Chinchilla"
date = "2024-01-27"
description = "Training Compute-Optimal Large Language Models"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

주어진 컴퓨팅 예산 하에서 transformer 언어 모델을 학습시키기 위한 최적의 모델 크기와 토큰 수를 조사했다. 현재 대형 언어 모델들은 학습이 부족하며, 이는 학습 데이터의 양을 일정하게 유지하면서 모델을 확장하는 최근의 추세 때문이다. 모델 크기와 학습 토큰 수는 동일하게 확장되어야 하며, 이를 검증하기 위해 Gopher와 동일한 컴퓨팅 예산을 사용하는 Chinchilla 모델을 학습시켰다. Chinchilla는 다양한 평가 작업에서 뛰어난 성능을 보였으며, MMLU 벤치마크에서는 평균 정확도 67.5%를 달성하여 Gopher에 비해 7% 이상 향상되었다.

---

## Introduction




---

## Reference

* [Paper](https://arxiv.org/pdf/2203.15556.pdf)