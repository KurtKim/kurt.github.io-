+++
author = "Kurt"
title = "Gopher"
date = "2024-01-13"
description = "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

이 논문에서는 수천만 개의 parameter를 가진 모델에서 Gopher라는 280B 개의 parameter를 가진 모델에 이르는 다양한 규모의 Transformer 기반 언어 모델 성능을 분석한다. 이러한 모델들은 152개의 다양한 작업에서 state-of-the-art를 보여주며, 규모 확장은 주로 독해, 팩트 체크, 유해한 언어 식별 등에서 가장 큰 이익을 가져다준다. 또한, 학습 데이터셋과 모델의 행동에 대한 종합적인 분석을 제공하고, AI 안전성에 대한 언어 모델의 적용 및 하류에서의 유해성 완화 방법을 논의한다.

---

## Reference

* [Paper](https://arxiv.org/pdf/2112.11446.pdf)
