+++
author = "Kurt"
title = "LLaMA 2"
date = "2024-04-06"
description = "Open Foundation and Fine-Tuned Chat Models"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

이 연구에서는 70억에서 700억 parameter 규모의 사전 학습 및 미세 조정된 대규모 언어 모델 모음인 Llama 2를 개발하고 출시하였다. 대화 사용 사례에 특화된 Llama 2-Chat은 테스트한 대부분의 벤치마크에서 오픈 소스 채팅 모델보다 우수하며, 유용성과 안전성 측면에서 폐쇄 소스 모델의 대안이 될 수 있다. Llama 2-Chat의 미세 조정 및 안전성 개선 방법을 자세히 소개해, 커뮤니티가 이를 기반으로 LLMs의 책임 있는 개발에 기여할 수 있게 한다.

---

## Introduction




---

## Reference

* [Paper](https://arxiv.org/pdf/2307.09288.pdf)
* [Github](https://github.com/meta-llama/llama)