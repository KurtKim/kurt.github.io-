+++
author = "Kurt"
title = "UL2"
date = "2024-02-02"
description = "Unifying Language Learning Paradigms"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

이 논문에서는 데이터셋과 설정에 걸쳐 보편적으로 효과적인 사전 학습 모델에 대한 통합 프레임워크를 제시한다. 아키텍처 원형과 사전 학습 목표를 분리하고, 다양한 사전 학습 목표가 어떻게 변환될 수 있는지를 보여준다. 또한, 다양한 사전 학습 패러다임을 함께 결합하는 Mixture-of-Denoisers (MoD)를 제안하고, downstream 미세 조정이 특정 사전 학습 체계와 연관되는 모드 전환 개념을 도입한다. 이 방법으로, 이 모델은 다양한 NLP 작업에서 최고의 성능을 달성하였으며, 특히 in-context 학습에서 강력한 결과를 보였다.

---

## Reference

* [Paper](https://arxiv.org/pdf/2205.05131v1.pdf)
* [Github](https://github.com/google-research/google-research/tree/master/ul2)