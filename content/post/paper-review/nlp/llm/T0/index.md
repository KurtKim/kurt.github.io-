+++
author = "Kurt"
title = "T0"
date = "2024-01-05"
description = "Multitask Prompted Training Enables Zero-Shot Task Generalization"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

최근의 연구에서 거대 언어 모델들이 다양한 작업에 대해 zero-shot 일반화를 잘 보여주고 있다. 이는 언어 모델의 사전학습 과정에서 암시적으로 multitask 학습이 이루어지기 때문이라는 가설이 있다. 이에 반해, 연구진은 zero-shot 일반화를 명시적인 multitask 학습으로 직접 유도할 수 있는지를 테스트하였다. 이를 위해 자연어 작업을 사람이 읽을 수 있는 프롬프트 형태로 변환하는 시스템을 개발하였고, 이를 사용해 다양한 supervised 데이터셋을 변환하였다. 결과적으로, 이 모델은 여러 표준 데이터셋에서 강력한 zero-shot 성능을 보여주었으며, 특히 그 크기가 16배인 모델을 능가하는 결과를 보였다. 또한 BIG-bench 벤치마크의 일부 작업에서도 우수한 성능을 보였다.

---

## Introduction

![](images/figure1.png)

최근의 연구에서는 거대 언어 모델이 새로운 작업에 대해 합리적인 zero-shot 일반화를 보여주는 것으로 나타났다. 이 모델들은 언어 모델링 목표에 대해서만 학습되었음에도 불구하고, 명시적으로 학습되지 않은 새로운 작업에 대해 상대적으로 잘 수행할 수 있다. 이는 거대 언어 모델이 암시적인 multitask 학습 과정을 통해 새로운 작업에 일반화한다는 가설을 뒷받침한다. 하지만 이 능력은 모델의 크기가 충분히 커야 하며, 프롬프트의 표현에 민감하다는 것을 고려해야 한다.

multitask 학습이 얼마나 암시적인지는 아직 뚜렷이 알려지지 않았다. 최근 언어 모델의 사전학습 말뭉치 규모를 고려하면, 일부 자연어 처리(NLP) 작업들이 해당 말뭉치에서 명시적으로 나타나 모델이 직접 학습하는 것이 합리적으로 보인다. 예를 들어, 퀴즈 질문과 답변을 담은 웹사이트는 바로 closed-book 질문 대답 작업에 대한 지도 학습 데이터로 사용될 수 있다. 이러한 multitask supervision이 zero-shot 일반화에서 큰 역할을 한다는 가설을 세웠다.

이 논문에서는 언어 모델을 supervised이며 massively multitask 방식으로 명시적으로 학습하는 방법에 대해 연구한다. 자연어 프롬프트로 표현된 다양한 작업들을 사용하여 모델이 보류된 작업에 더 잘 일반화하고 프롬프트의 단어 선택에 강건하게 만드는 것을 목표로 한다. 이를 위해, 구조화된 데이터셋에 대한 간단한 템플릿 언어를 사용하여 자연어 작업을 프롬프트 형식으로 변환하고, 공공 기여자들로부터 프롬프트를 수집하는 인터페이스를 개발하였다. 그 후, T5 encoder-decoder 모델의 변형을 일부 작업에 대해 학습하고, 학습되지 않은 작업과 프롬프트를 평가하였다.

이 논문의 실험은 multitask 프롬프트 학습이 보류된 작업에 대한 일반화를 향상시키고, 더 넓은 범위의 프롬프트 학습이 프롬프트 단어 선택에 대한 robustness를 향상시키는지를 연구한다. 실험 결과, multitask 학습은 zero-shot 작업 일반화를 가능하게 하며, 모델은 GPT-3의 성능을 대부분의 보류된 데이터셋에서 매치하거나 초과한다. 또한, 데이터셋당 더 많은 프롬프트에 대한 학습이 보류된 작업에 대한 성능의 중간값을 향상시키고 변동성을 감소시키는 것을 확인하였다. 하지만, 더 넓은 범위의 데이터셋에서의 프롬프트 학습은 변동성을 일관되게 감소시키지는 않는다.

---

## Related Work

---

## Reference

* [Paper](https://arxiv.org/pdf/2110.08207.pdf)
* [Github](https://github.com/bigscience-workshop/t-zero)