+++
author = "Kurt"
title = "Chinchilla"
date = "2024-01-27"
description = "Training Compute-Optimal Large Language Models"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

주어진 컴퓨팅 예산 하에서 transformer 언어 모델을 학습시키기 위한 최적의 모델 크기와 토큰 수를 조사했다. 현재 대형 언어 모델들은 학습이 부족하며, 이는 학습 데이터의 양을 일정하게 유지하면서 모델을 확장하는 최근의 추세 때문이다. 모델 크기와 학습 토큰 수는 동일하게 확장되어야 하며, 이를 검증하기 위해 Gopher와 동일한 컴퓨팅 예산을 사용하는 Chinchilla 모델을 학습시켰다. Chinchilla는 다양한 평가 작업에서 뛰어난 성능을 보였으며, MMLU 벤치마크에서는 평균 정확도 67.5%를 달성하여 Gopher에 비해 7% 이상 향상되었다.

---

## Introduction

최근에는 500B 개 이상의 parameter를 가진 대형 언어 모델들이 소개되었다. 이런 대형 autoregressive transformer들은 zero-shot, few-shot, 미세 조정 등 다양한 평가 방법을 통해 많은 작업에서 뛰어난 성능을 보여주었다.

대형 언어 모델 학습의 컴퓨팅 및 에너지 비용은 상당히 크며, 모델 크기 증가에 따라 더욱 증가한다. 학습에 할당된 컴퓨팅 예산은 대개 사전에 알려져 있고, 이런 대형 모델은 일반적으로 한 번만 학습시킬 수 있으므로, 주어진 예산에 대해 최적의 모델 hyperparameter를 정확하게 추정하는 것이 중요하다.

Kaplan et al. (2020)의 연구에 따르면, 언어 모델의 parameter 수와 성능 사이에는 지수법칙 관계가 있다. 이에 따라 모델이 커질수록 성능 향상을 기대한다. 그러나 대형 모델은 가장 낮은 손실로 학습시키지 않아야 한다는 결론에 도달하였다. 특히, 컴퓨팅 예산을 10배 늘릴 경우, 모델 크기와 학습 토큰 수는 동등한 비율로 증가해야 한다고 발견했다.

최근에 학습된 많은 대형 모델들은 Kaplan et al. (2020)과 GPT-3의 학습 방법을 따라, 컴퓨팅을 증가할 때 주로 모델 크기를 증가시키는 방식으로 약 3000억 개의 토큰에 대해 학습되었다.

이 연구에서는 주어진 FLOPs 예산 하에서 모델 크기와 학습 토큰 수 간의 균형을 어떻게 맞춰야 할지를 다시 살펴본다. 이를 위해 모델 parameter 수와 학습 토큰 수에 따른 최종 사전 학습 손실을 모델링하고, 이를 최소화하는 방향으로 연구를 진행하였다.

$$ N_{opt}(C), D_{opt}(C) =  \underset{𝑁,𝐷 s.t. FLOPs(N, D) = C}{argmin} L(N, D) $$

![](images/figure1.png)

컴퓨팅 예산의 최적 분배를 설명하는 함수를 400개 이상의 다양한 크기의 모델을 기반으로 추정하였다. 이 접근법은 Kaplan et al. (2020)의 결과와 크게 다르다.

추정에 따르면, Gopher를 학습시키는 데 사용된 컴퓨팅 예산으로는 크기가 4배 작고 토큰이 4배 더 많이 학습된 모델이 최적이라고 예측한다. 이를 확인하기 위해 1.4T 개의 토큰에 대해 더 최적화된 70B 모델인 Chinchilla를 학습시켰고, 이 모델은 크기가 더 큰 Gopher보다 더 뛰어난 성능을 보여주었다. 더 작은 모델의 이점은 개선된 성능 외에도 추론 비용 감소와 하드웨어 호환성 향상에 있다.

---

## Related Work


---

## Reference

* [Paper](https://arxiv.org/pdf/2203.15556.pdf)