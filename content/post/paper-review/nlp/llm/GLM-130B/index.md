+++
author = "Kurt"
title = "GLM-130B"
date = "2024-02-17"
description = "An Open Bilingual Pre-trained Model"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "LLM",
]
draft = true
+++

## Abstract

130B parameter를 가진 bilingual(영어, 중국어) 사전 학습 언어 모델인 GLM-130B를 소개한다. 이 모델은 기술적, 엔지니어링적 도전을 극복하면서, GPT-3와 같은 수준의 성능을 달성하였다. GLM-130B는 다양한 영어 벤치마크에서 GPT-3를, 중국어 벤치마크에서는 ERNIE TITAN 3.0 260B를 뛰어넘었다. 또한, 추가 학습 없이 INT4 quantization에 성공하였고, 저렴한 GPU에서도 효과적으로 작동한다.

---

## Introduction

---

## Reference

* [Paper](https://arxiv.org/pdf/2210.02414.pdf)
* [GitHub](https://github.com/THUDM/GLM-130B)