+++
author = "Kurt"
title = "P-Tuning v2"
date = "2024-03-26"
description = "Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
categories = [
    "Paper Review"
]
tags = [
    "NLP",
    "PEFT",
]
draft = true
+++

## Abstract

prompt tuning은 학습 시 저장 공간과 메모리 사용을 줄이지만, 기존 방법들은 NLU에서 일반적인 모델에 잘 작동하지 않고 어려운 시퀀스 라벨링 작업에 대한 범용성이 부족하다. 그러나, 적절히 최적화된 prompt tuning은 다양한 모델 규모와 NLU 작업에 대해 보편적으로 효과적이며, 조정된 parameter가 극히 적음에도 미세조정과 동등한 성능을 제공한다. P-Tuning v2는 NLU에 최적화된 Deep Prompt Tuning의 구현으로, 미세조정의 대안이자 미래 연구의 강력한 기준이 될 수 있다.

---

## Introduction





---

## Reference

* [Paper](https://arxiv.org/pdf/2110.07602.pdf)
