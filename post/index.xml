<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on K2H Blog</title><link>https://kurtkim.github.io/post/</link><description>Recent content in Posts on K2H Blog</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Mon, 15 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://kurtkim.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>FLAN</title><link>https://kurtkim.github.io/p/flan/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/flan/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>이 논문은 언어 모델의 zero-shot 학습 능력을 향상시키는 방법을 연구한다. &amp;ldquo;instruction tuning&amp;rdquo; 이라는 방법을 통해 미처 볼 수 없었던 작업에서의 zero-shot 성능을 크게 향상시킬 수 있음을 보여준다.&lt;/p>
&lt;p>137B 개의 parameter를 가진 사전 학습된 언어 모델을 60개 이상의 NLP 데이터셋에 대한 instruction tuning을 통해, 이 모델인 FLAN은 보이지 않는 작업 유형에서 월등한 성능을 보여준다. FLAN은 여러 데이터셋에서 zero-shot GPT-3를 능가하고, 몇몇 작업에서는 few-shot GPT-3를 크게 앞선다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>대규모 언어 모델은 few-shot 학습을 잘 수행하지만, zero-shot 학습에서는 성공적이지 못하며, 이는 사전 학습 데이터와 비슷하지 않은 프롬프트에서 모델이 작업을 수행하기 어렵기 때문일 수 있다.&lt;/p>
&lt;p>이 논문에서는 대규모 언어 모델의 zero-shot 성능을 향상시키는 방법을 연구한다. 60개 이상의 NLP 데이터셋을 자연어 지시문으로 표현하여 137B parameter의 언어 모델을 미세 조정하는 방식을 사용한다. 이 결과 생성된 모델을 FLAN(Finetuned Language Net)이라고 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure1.png"
width="912"
height="636"
srcset="https://kurtkim.github.io/p/flan/images/figure1_hu53cc1b2de5c2393ea95504fc3ec0baa4_152068_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure1_hu53cc1b2de5c2393ea95504fc3ec0baa4_152068_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>NLP 데이터셋을 작업 유형별로 그룹화하여 FLAN의 zero-shot 성능을 평가한다. 특정 작업(예: 자연어 추론)을 평가하기 위해 해당 작업을 제외한 다른 모든 작업에서 FLAN을 조정하고, 그 후에 zero-shot 자연어 추론 성능을 평가한다.&lt;/p>
&lt;p>FLAN은 기본 137B-parameter 모델의 zero-shot 성능을 크게 향상시키며, 25개의 데이터셋 중 20개에서 GPT-3의 zero-shot을 능가한다. 또한 특정 작업에서는 GPT-3의 few-shot 성능까지 능가한다. instruction tuning에서 작업 클러스터 수를 늘리는 것이 성능을 향상시키며, 충분한 모델 규모에서만 instruction tuning의 이점이 나타난다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure2.png"
width="1020"
height="342"
srcset="https://kurtkim.github.io/p/flan/images/figure2_huc400aba4a6ad9b0723bf44ae631c524a_108997_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure2_huc400aba4a6ad9b0723bf44ae631c524a_108997_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="715px"
>&lt;/p>
&lt;p>instruction tuning은 언어 모델이 추론 시 텍스트 상호작용에 더 잘 응답하도록 미세조정을 통한 지도학습을 사용하는 간단한 방법이다. 이 방법은 언어 모델이 지시문만을 통해 작업을 수행하는 능력을 보여준다.&lt;/p>
&lt;h2 id="flan-instruction-tuning-improves-zero-shot-learning">FLAN: Instruction Tuning Improves Zero-Shot Learning&lt;/h2>
&lt;p>instruction tuning의 목표는 언어 모델이 NLP 지시문에 더 잘 응답하도록 향상시키는 것이다. 지시문을 통해 설명된 작업을 수행하도록 언어 모델을 교육함으로써, 보이지 않는 작업에 대해서도 지시문을 따를 수 있게 한다. 작업 유형별로 데이터셋을 그룹화하고, 남은 클러스터에서 instruction tuning을 하면서 볼 수 없는 작업의 성능을 평가한다.&lt;/p>
&lt;h3 id="tasks--templates">Tasks &amp;amp; Templates&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure3.png"
width="1068"
height="348"
srcset="https://kurtkim.github.io/p/flan/images/figure3_hu5dd5a15e50bf0eaf0feb3af4dc50d61a_193664_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure3_hu5dd5a15e50bf0eaf0feb3af4dc50d61a_193664_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="306"
data-flex-basis="736px"
>&lt;/p>
&lt;p>기존의 연구 데이터셋을 지시문 형식으로 변환하여, 자원 집약적인 새로운 데이터셋 생성을 피한다. Tensorflow Datasets에서 공개적으로 이용 가능한 62개의 텍스트 데이터셋을 하나의 혼합물로 집계하며, 이 데이터셋들은 12개의 작업 클러스터 중 하나로 분류된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure4.png"
width="1002"
height="342"
srcset="https://kurtkim.github.io/p/flan/images/figure4_hu72e09559eb9945d8e01e74a1d5b5f08c_123105_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure4_hu72e09559eb9945d8e01e74a1d5b5f08c_123105_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="703px"
>&lt;/p>
&lt;p>각 데이터셋에 대해, 작업을 설명하는 10개의 고유한 자연어 지시문 템플릿을 작성하며, 다양성을 높이기 위해 일부 템플릿은 원래의 작업을 뒤집는 방식으로 구성된다. 이후 모든 데이터셋의 혼합물에서 사전 학습된 언어 모델을 instruction tuning하며, 각 데이터셋의 예제는 해당 데이터셋에 대한 무작위로 선택된 지시 템플릿으로 형식화된다.&lt;/p>
&lt;h3 id="evaluation-splits">Evaluation Splits&lt;/h3>
&lt;p>FLAN이 instruction tuning에서 본적 없는 작업에 대해 어떻게 수행하는지를 알고자 한다. 본적 없는 작업을 정의하기 위해, instruction tuning 중에 보지 않은 작업 클러스터에 속한 모든 데이터셋을 본적 없는 것으로 간주한다. 따라서, 특정 작업 클러스터에서 zero-shot FLAN을 평가하려면, 각각 다른 작업 클러스터를 보류한 모델을 instruction tuning합니다.&lt;/p>
&lt;h3 id="classification-with-options">Classification With Options&lt;/h3>
&lt;p>작업의 출력 공간은 클래스 중 하나(classiﬁcation) 또는 자유 텍스트(generation)이 된다. FLAN은 decoder만 있는 언어 모델의 지시 조정 버전이므로, 생성 작업에 대한 추가 수정 없이도 자유 텍스트로 자연스럽게 응답한다.&lt;/p>
&lt;p>분류 작업에서는 &amp;ldquo;예&amp;quot;와 &amp;ldquo;아니오&amp;quot;와 같은 두 가지 출력만 고려하는 순위 분류 방법을 사용하였다. 하지만 이 방법은 답변의 확률 분포가 원치 않는 방식으로 나타날 수 있다. 따라서, 분류 작업의 끝에 OPTIONS 토큰과 해당 작업의 출력 클래스 목록을 추가하여 모델이 분류 작업에 응답할 때 원하는 선택지를 인식하게 한다.&lt;/p>
&lt;h3 id="training-details">Training Details&lt;/h3>
&lt;p>&lt;strong>Model architecture and pretraining.&lt;/strong> 137B parameter의 LaMDA-PT라는 decoder-only transformer 언어 모델을 사용한다. 이 모델은 웹 문서, 대화 데이터, 위키백과 등을 통해 사전 학습되었고, SentencePiece 라이브러리를 사용해 32k 어휘로 토큰화되었다. 사전 학습 데이터의 약 10%는 비영어이다. LaMDA-PT는 언어 모델 사전 학습만을 가지고 있다.&lt;/p>
&lt;p>&lt;strong>Instruction tuning procedure.&lt;/strong> FLAN은 LaMDA-PT의 instruction tuning 버전이다. 모든 데이터셋을 혼합하여 무작위로 샘플링하며, 데이터셋 당 최대 30k의 학습 예제를 사용한다. 모델은 30k의 그래디언트 단계 동안 미세조정되며, 입력 시퀀스와 목표 시퀀스의 길이는 각각 1024와 256입니다. 이 튜닝 과정은 TPUv3에서 약 60시간이 소요된다.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>FLAN은 다양한 작업들에서 평가되며, 이는 natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, struct-to-text 등을 포함한다. 각 작업 클러스터는 다른 체크포인트를 사용하며, 각 데이터셋의 성능은 모든 템플릿에 대한 평균 성능으로 평가된다. 또한, 개발 세트의 성능이 가장 좋은 템플릿을 사용하여 테스트 세트의 성능도 측정한다.&lt;/p>
&lt;p>LaMDA-PT의 zero-shot과 few-shot 결과를 GPT-3의 프롬프트와 동일하게 보고한다. 이는 instruction tuning이 얼마나 효과적인지 직접적으로 보여주는 기준선이다. 결과적으로, instruction tuning은 대부분의 데이터셋에서 LaMDA-PT의 성능을 크게 향상시켰다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure5.png"
width="1074"
height="774"
srcset="https://kurtkim.github.io/p/flan/images/figure5_hu2fa5a6362367fa9dc5272a014132c211_159127_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure5_hu2fa5a6362367fa9dc5272a014132c211_159127_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>zero-shot FLAN은 25개 데이터셋 중 20개에서 GPT-3 175B를 능가하며, 19개 데이터셋 중 13개에서는 GLaM 64B/64E를 능가한다.&lt;/p>
&lt;p>instruction tuning은 NLI, QA, translation, struct-to-text 등의 과제에 효과적이며, 언어 모델링으로 직접 구성된 과제에서는 효과적이지 않다.&lt;/p>
&lt;p>&lt;strong>Natural language inference (NLI).&lt;/strong> 5개의 NLI 데이터셋에서, FLAN은 모든 기준 모델을 크게 능가했습니다. FLAN은 NLI를 &amp;ldquo;Does &lt;!-- raw HTML omitted --> mean that &lt;!-- raw HTML omitted -->?&amp;ldquo;라는 더 자연스러운 질문으로 표현하여 훨씬 높은 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Reading comprehension.&lt;/strong> FLAN은 MultiRC와 OBQA에서 기준 모델을 능가하였다. 또한, BoolQ에서는 GPT-3를 크게 능가하였다.&lt;/p>
&lt;p>&lt;strong>Closed-book QA.&lt;/strong> FLAN은 모든 네 개의 데이터셋에서 GPT-3를 능가하였다. ARC-e와 ARC-c에서는 GLaM보다 더 좋은 성능을 보였지만, NQ와 TQA에서는 약간 낮은 성능을 보였다.&lt;/p>
&lt;p>&lt;strong>Translation.&lt;/strong> FLAN은 GPT-3 논문에서 평가된 세 개의 데이터셋인 프랑스어-영어, 독일어-영어, 루마니아어-영어에 대한 기계 번역 성능을 평가하였다. FLAN은 모든 평가에서 zero-shot GPT-3를 능가했지만, 대부분의 경우 few-shot GPT-3보다 성능이 떨어졌다. FLAN은 영어로 번역하는 데 강한 결과를 보였지만, 영어에서 다른 언어로 번역하는 것은 상대적으로 약했다.&lt;/p>
&lt;p>&lt;strong>Additional tasks.&lt;/strong> instruction tuning은 많은 언어 모델링 과제의 성능을 향상시키지 못하는 한계가 있다. 7개의 상식 추론 및 공통 참조 해결 과제 중 FLAN은 3개 과제에서만 LaMDA-PT를 능가하였다. 하지만, zero-shot FLAN은 일반적으로 zero-shot LaMDA-PT를 능가하며, few-shot LaMDA-PT와 비슷하거나 더 나은 성능을 보여주었다.&lt;/p>
&lt;h2 id="ablation-studies--further-analysis">Ablation Studies &amp;amp; Further Analysis&lt;/h2>
&lt;h3 id="number-of-instruction-turning-clusters">Number Of Instruction Turning Clusters&lt;/h3>
&lt;p>이 연구에서는 instruction tuning이 어떻게 모델의 zero-shot 성능을 향상시키는지를 중점으로 살펴보았다. 첫 번째 축소 실험에서는 instruction tuning에 사용된 클러스터와 과제의 수가 성능에 어떻게 영향을 미치는지를 검토하였다. 이때 NLI, closed-book QA, commonsense reasoning을 평가 클러스터로 보류하고, 나머지 클러스터를 instruction tuning에 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure6.png"
width="682"
height="422"
srcset="https://kurtkim.github.io/p/flan/images/figure6_hub2ee1795a43435d66f715189ce4efe1b_103115_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure6_hub2ee1795a43435d66f715189ce4efe1b_103115_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>instruction tuning에 추가 클러스터와 과제를 추가하면, 보류된 세 개의 클러스터에서의 평균 성능이 향상됨을 확인했다. 테스트한 일곱 개의 클러스터에서 성능이 포화되지 않아 보이므로, instruction tuning에 더 많은 클러스터가 추가되면 성능이 더욱 향상될 수 있을 것으로 보인다. 하지만, 감정 분석 클러스터에서는 최소한의 추가 가치만을 볼 수 있었다.&lt;/p>
&lt;h3 id="scaling-laws">Scaling Laws&lt;/h3>
&lt;p>언어 모델의 zero-shot과 few-shot 능력이 더 큰 모델에 대해 크게 향상된다는 연구 결과를 바탕으로, instruction tuning의 이점이 모델 규모에 어떻게 영향을 받는지를 살펴보았다. 모델 규모를 422M, 2B, 8B, 68B, 137B로 설정하고 instruction tuning의 효과를 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure7.png"
width="574"
height="398"
srcset="https://kurtkim.github.io/p/flan/images/figure7_hu06d61b58778b9d23f80bcb7bfddd86e8_57546_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure7_hu06d61b58778b9d23f80bcb7bfddd86e8_57546_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>100B parameter 규모의 두 모델에서는 instruction tuning이 보류된 과제에서의 성능을 크게 향상시켰다. 그러나, 8B 및 더 작은 모델에서는 instruction tuning이 보류된 과제에서의 성능을 저하시켰다. 이는 작은 규모의 모델에서 instruction tuning 중 사용되는 과제를 학습하는 것이 모델의 전체 용량을 차지하게 되어, 새로운 과제에서 성능이 떨어지게 만들 수 있기 때문일 수 있다.&lt;/p>
&lt;h3 id="role-of-instructions">Role Of Instructions&lt;/h3>
&lt;p>마지막 ablation study에서는 미세 조정 중 지시문의 역할을 살펴보았다. 지시문 없이 모델이 어떻게 수행하는지 살펴보기 위해, 지시문이 없는 두 가지 미세 조정 설정을 고려하였다. 하나는 템플릿이 없는 설정으로, 모델에게 입력과 출력만이 주어지는 것이고, 다른 하나는 데이터셋 이름 설정으로, 각 입력이 과제와 데이터셋의 이름으로 시작된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure8.png"
width="478"
height="368"
srcset="https://kurtkim.github.io/p/flan/images/figure8_hue286e52e5e2eb558effda57d405b1d3a_46002_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure8_hue286e52e5e2eb558effda57d405b1d3a_46002_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="129"
data-flex-basis="311px"
>&lt;/p>
&lt;p>자연스러운 지시문을 사용한 FLAN의 미세 조정 절차와 두 가지 ablation study을 비교하였다. 이 두 ablation study는 각각 템플릿이 없는 설정과 데이터셋 이름만을 사용한다. 결과에서 두 축소 설정 모두 FLAN보다 훨씬 나쁜 성능을 보여, 보이지 않는 과제에서의 zero-shot 성능에 지시문을 사용한 학습이 결정적임을 나타냈다.&lt;/p>
&lt;h3 id="instructions-with-few-shot-exemplars">Instructions With Few-Shot Exemplars&lt;/h3>
&lt;p>few-shot 예시가 추론 시간에 사용 가능할 때 instruction tuning이 어떻게 사용될 수 있는지 연구하였다. few-shot 설정의 형식은 zero-shot 형식을 기반으로 한다. 학습 시간과 추론 시간 모두에서 예시는 학습 세트에서 무작위로 추출되며, 예시의 수는 16개로 제한하고 전체 시퀀스 길이가 960 토큰 미만이 되도록 했다. 실험은 보이지 않는 과제에 대한 few-shot 예시를 오직 추론 시간에만 사용하는 동일한 과제 분할과 평가 절차를 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure9.png"
width="1088"
height="292"
srcset="https://kurtkim.github.io/p/flan/images/figure9_hu553b7afb6075f4d3935015206efa02e8_52139_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure9_hu553b7afb6075f4d3935015206efa02e8_52139_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="894px"
>&lt;/p>
&lt;p>few-shot 예시는 zero-shot FLAN에 비해 모든 과제 클러스터의 성능을 향상시킨다. 예시는 특히 크거나 복잡한 출력 공간을 가진 과제에 효과적이며, 이는 예시가 모델이 출력 형식을 더 잘 이해하는 데 도움이 되기 때문일 가능성이 있다. 또한, 모든 과제 클러스터에서 템플릿 간의 표준 편차는 퓨샷 FLAN에서 더 낮아, 프롬프트 엔지니어링에 대한 민감도가 줄어든 것을 나타낸다.&lt;/p>
&lt;h3 id="instruction-turning-facilitates-prompt-turning">Instruction Turning Facilitates Prompt Turning&lt;/h3>
&lt;p>instruction tuning이 모델의 지시문에 대한 반응 능력을 향상시키는 것을 확인했기 때문에, FLAN이 NLP 과제를 수행하는 데 더 적합하다면, 소프트 프롬프트를 사용하여 추론을 수행할 때도 더 나은 성능을 달성해야 한다. 추가 분석으로, SuperGLUE 과제 각각에 대해 연속 프롬프트를 훈련시켰고, 이는 특정 과제에 대한 프롬프트 튜닝을 수행할 때, 동일한 클러스터에 있는 다른 과제가 instruction tuning 동안 보이지 않게 하는 클러스터 분할을 따랐다. 프롬프트 튜닝 설정은 Lester et al.의 절차를 따르되, 몇 가지 변화를 주었고, 이 변화들이 LaMDA-PT의 성능을 향상시키는 것으로 확인되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/flan/images/figure10.png"
width="376"
height="314"
srcset="https://kurtkim.github.io/p/flan/images/figure10_hu068f59284c4acff449424ced936d048c_32927_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/flan/images/figure10_hu068f59284c4acff449424ced936d048c_32927_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;p>모든 시나리오에서 프롬프트 튜닝은 LaMDA-PT보다 FLAN에서 더 잘 작동하였다. 특히 low-resource 설정에서는, FLAN에서의 프롬프트 튜닝이 LaMDA-PT에서의 것보다 10% 이상 성능이 향상되었다. 이 결과는 instruction tuning이 NLP 과제를 수행하는 데 더 바람직한 모델을 만드는 데 어떻게 기여할 수 있는지를 보여준다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>이 논문은 zero-shot 학습, 프롬프팅, 다중 과제 학습, NLP 응용 프로그램을 위한 언어 모델 등 여러 넓은 연구 영역과 관련이 있다. 이러한 넓은 영역에 대한 이전 연구를 확장된 관련 연구 섹션에서 설명하고, 이 논문의 연구와 가장 밀접하게 연관된 범위가 좁은 두 개의 하위 영역을 설명하였다.&lt;/p>
&lt;p>모델에 지시문에 대한 반응을 요청하는 방식은 QA 기반 과제 구성과 유사하며, 이는 NLP 과제를 통일하는 것을 목표로 한다. 이 방법들은 주로 다중 과제 학습에 초점을 맞추며, 사전 학습된 LMs의 기존 지식을 사용하는 것에 크게 기반하지 않는다. 이 연구의 작업은 모델 규모와 과제 범위 모두에서 최근의 일부 연구를 초월한다.&lt;/p>
&lt;p>언어 모델의 성공으로 모델이 지시문을 따르는 능력에 대한 연구가 진행되고 있다. 최근 연구에서는 지시문과 few-shot 예시를 이용해 BART를 미세 조정하고, 이를 통해 보이지 않는 과제에 대한 few-shot 성능을 향상시킬 수 있음을 보여주었다. 또한, T5를 미세 조정하는 등의 방법으로 zero-shot 학습을 개선하고, 미세 조정과 강화 학습을 병행하여 인간 평가자가 선호하는 출력을 생성하는 연구도 있다.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>지시문으로 표현된 여러 과제에 대해 모델을 미세 조정하면 보이지 않는 과제에서의 성능이 향상된다는 것을 보여주었다. FLAN은 미세 조정되지 않은 모델보다 성능이 좋고, zero-shot GPT-3를 능가한다. 또한, 충분한 모델 규모에서만 instruction tuning에 의한 성능 향상이 나타나며, 이는 다른 프롬프팅 방법과도 결합될 수 있다.&lt;/p>
&lt;p>언어 모델의 다양한 능력은 specialist 모델과 generalist 모델 사이의 균형에 대한 관심을 끌어내었다. 레이블이 있는 데이터가 specialist 모델을 개선하는 데 도움이 될 것으로 예상되지만, instruction tuning을 통해 이 데이터가 큰 언어 모델이 보이지 않는 다양한 과제를 수행하는 데도 도움이 될 수 있음을 보여주었다. 이는 과제 특정 학습이 일반 언어 모델링과 보완적이라는 것을 보여주며, generalist 모델에 대한 추가 연구를 촉진한다.&lt;/p>
&lt;p>이 연구의 한계점은 과제를 클러스터에 할당하는 데 있는 주관성과 짧은 지시문의 사용에 대한 연구의 한정성이다. 개별 예시가 모델의 사전 훈련 데이터에 포함되어 있을 수 있지만, 이것이 결과에 크게 영향을 미쳤다는 증거는 찾지 못하였다. 또한, FLAN 137B의 규모는 그것을 서비스하는 데 비용이 많이 든다. 향후 instruction tuning 연구는 더 많은 과제 클러스터를 수집하고, 다언어 실험을 진행하며, downstream classiﬁer 학습 데이터를 생성하고, 편향과 공정성에 대한 모델 행동을 개선하는 방향으로 진행될 수 있다.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>이 논문은 지시문에 기반한 zero-shot 과제를 수행하는 대규모 언어 모델의 능력을 향상시키는 간단한 방법을 연구하였다. FLAN은 GPT-3에 비해 더 우수한 결과를 보여주며, 대규모 언어 모델이 지시문을 따를 수 있는 잠재력을 보여주었다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2109.01652.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/flan" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-3</title><link>https://kurtkim.github.io/p/gpt-3/</link><pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-3/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>최근의 연구는 대량의 텍스트 말뭉치로 사전 학습한 후 특정 작업에 대해 미세 조정하는 것으로 많은 NLP 작업과 벤치마크에서 상당한 성과를 보여주었다. 일반적으로 과제에 중립적인 구조를 가지지만, 이 방법은 여전히 수천 개 또는 수만 개의 예제로 이루어진 과제별 미세 조정 데이터셋을 필요로 한다. 인간은 보통 몇 가지 예제나 간단한 지시사항만으로도 새로운 언어 작업을 수행할 수 있지만, 현재의 NLP 시스템은 이를 여전히 어려워한다. 이 연구에서는 언어 모델의 규모를 확장함으로써 과제 중립적이고 소수의 예제로 이루어진 작업 성능을 크게 개선하는 것을 보여준다. 때로는 이전의 state-of-the-art 미세 조정 접근법과 경쟁력을 갖출 수도 있다. 구체적으로, 1750억 개의 parameter를 가진 GPT-3라는 autoregressive 언어 모델을 학습시키고, 이를 소수의 예제로 평가해보았다. 모든 작업에서 GPT-3는 어떠한 그래디언트 업데이트나 미세 조정 없이 적용되며, 작업 및 소수의 예제는 모델과의 텍스트 상호작용을 통해 명시된다. GPT-3는 번역, 질의응답, 문맥 채우기 작업뿐만 아니라 단어 섞기, 새로운 단어를 문장에 사용하기, 3자리 산술 연산을 수행하는 등의 실시간 추론이나 도메인 적응이 필요한 작업과 같은 많은 NLP 데이터셋에서 강력한 성능을 보여주었다. 한편, GPT-3의 소수 학습은 여전히 어려운 몇몇 데이터셋과 대규모 웹 말뭉치에서의 훈련에 관련된 방법론적 문제가 있다는 점도 확인했다. 마지막으로, GPT-3는 인간 평가자가 사람이 작성한 기사와 구분하기 어려운 뉴스 기사 샘플을 생성할 수 있음을 발견하였다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>최근 NLP 시스템은 사전 학습된 언어 표현을 다양한 작업에 유연하게 적용하는 추세가 있다. 초기에는 단어 벡터를 사용한 단일 계층 표현을 과제 특정 아키텍처에 적용했으나, 후에는 RNN을 사용한 다계층 표현을 도입하였다. 최근에는 과제 특정 아키텍처의 필요성을 완전히 제거하고, 사전 학습된 recurrent 또는 transformer 언어 모델을 직접 미세 조정하는 방식이 사용되고 있다.&lt;/p>
&lt;p>이러한 패러다임은 많은 어려운 작업에서 진전을 이루었지만, 여전히 작업 특정 데이터셋과 미세 조정이 필요한 한계가 있다. 원하는 작업에서 높은 성능을 달성하기 위해 수천에서 수십만 개의 예제로 이루어진 작업 특정 데이터셋에서 미세 조정이 필요하다. 이러한 한계를 제거하는 것이 중요하다.&lt;/p>
&lt;p>실용적인 관점에서, 모든 새로운 작업에 대한 대규모 레이블링된 예제 데이터셋의 필요성은 언어 모델의 적용 범위를 제한한다. 유용한 언어 작업의 범위는 매우 넓지만, 많은 작업들에 대해 큰 규모의 지도 학습 데이터셋을 수집하는 것은 어렵고, 이 과정이 각각의 새로운 작업마다 반복되어야 한다.&lt;/p>
&lt;p>학습 데이터의 거짓 상관관계를 이용하는 가능성은 모델의 표현력과 학습 분포의 좁음에 따라 증가하며, 이는 사전 학습 후 미세 조정 패러다임에 문제를 일으킬 수 있다. 모델은 사전 학습 동안 정보를 흡수하기 위해 크게 설계되지만, 후에는 좁은 작업 분포에서 미세 조정되며, 이로 인해 학습 분포에 과도하게 특화되어 분포 외부에서는 잘 일반화되지 않을 수 있다. 따라서, 미세 조정된 모델의 성능은 실제 기본 작업에 대한 성능을 과장할 수 있다.&lt;/p>
&lt;p>인간은 대부분의 언어 작업을 배우기 위해 큰 규모의 지도 학습 데이터셋을 필요로 하지 않는다. 간단한 지시사항이나 몇 가지 예제만으로도 새로운 작업을 수행할 수 있다. 이런 적응성은 인간이 여러 작업과 기술을 자연스럽게 섞거나 전환할 수 있게 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.1.png"
width="1256"
height="576"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.1_hue76f5a594fb31d1204e64ba6064c924d_124228_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.1_hue76f5a594fb31d1204e64ba6064c924d_124228_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="523px"
>&lt;/p>
&lt;p>이러한 문제를 해결하기 위한 한 방법은 메타 학습이다. 이는 언어 모델이 훈련 시에 다양한 기술과 패턴 인식 능력을 개발하고, 추론 시에 이를 활용해 원하는 작업에 빠르게 적응하거나 인식하도록 하는 것을 의미한다. 최근 연구에서는 &amp;ldquo;in-context learning&amp;quot;을 통해 이를 시도하였는데, 이는 모델이 자연 언어 지시사항이나 작업의 몇 가지 예제에 조건화되어, 단순히 다음에 무엇이 오는지 예측하여 작업을 완성하도록 하는 방식이다.&lt;/p>
&lt;p>메타 학습 방법은 약간의 잠재력을 보였지만, 미세 조정에 비해 성능이 크게 떨어진다. 특히, Natural Questions에서는 4%, CoQa에서는 55 F1이라는 결과를 보였는데, 이는 최신 기술에 비해 크게 뒤처져 있다. 따라서 메타 학습이 언어 작업을 해결하는 실질적인 방법이 되려면 큰 개선이 필요하다.&lt;/p>
&lt;p>최근 언어 모델링은 transformer 언어 모델의 용량이 크게 증가하는 추세를 보이고 있다. parameter 수가 100M에서 시작해 최근에는 17B개에 이르렀고, 이런 증가는 텍스트 합성과 NLP 작업에서 성능 개선을 가져왔다. 로그 손실이 규모와 함께 개선되는 추세를 보이기 때문에, 문맥 내 학습 능력도 규모와 함께 크게 향상될 수 있을 것으로 보인다.&lt;/p>
&lt;p>이 논문에서는 175B 개의 parameter를 가진 언어 모델, GPT-3의 학습과 그 문맥 내 학습 능력을 테스트한다. GPT-3는 다양한 NLP 데이터셋과 새로운 작업들에 대해 평가되며, 각 작업은 &amp;ldquo;few-shot learning&amp;rdquo;, &amp;ldquo;one-shot learning&amp;rdquo;, &amp;ldquo;zero-shot&amp;rdquo; 학습의 세 가지 조건 하에서 평가된다. GPT-3는 원칙적으로 미세 조정 설정에서도 평가될 수 있지만, 이는 미래의 연구로 남겨두었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.2.png"
width="1076"
height="592"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.2_hu58589bb76b1fcb23740d2bba78e6b7e6_155952_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.2_hu58589bb76b1fcb23740d2bba78e6b7e6_155952_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="436px"
>&lt;/p>
&lt;p>자연 언어 작업 설명과 문맥 내 예제 수가 늘어날수록 모델의 성능이 향상되며, 모델 크기가 커질수록 few-shot learning이 크게 향상된다. 이러한 학습 곡선은 미세 조정이나 그래디언트 업데이트 없이, 단순히 제공된 데모 수를 늘려가며 이루어진다.&lt;/p>
&lt;p>GPT-3는 NLP 작업에서 zero-shot과 one-shot 설정에서 좋은 결과를 보이며, few-shot 설정에서는 때때로 state-of-the-art 모델과 경쟁하거나 초과한다. 예컨대, GPT-3는 CoQA에서 zero-shot에서 81.5 F1, one-shot에서 84.0 F1, few-shot에서 85.0 F1을 달성하였다. 비슷하게, TriviaQA에서는 zero-shot에서 64.3%, one-shot에서 68.0%, few-shot에서 71.2%의 정확도를 보여주었다.&lt;/p>
&lt;p>GPT-3는 unscrambling words, performing arithmetic 등의 작업에서 one-shot과 few-shot 능력을 보여준다. 또한, GPT-3는 few-shot 설정에서 사람들이 인간이 만든 기사와 구별하기 어려운 합성 뉴스 기사를 생성할 수 있다.&lt;/p>
&lt;p>GPT-3의 규모에도 불구하고, ANLI와 같은 자연어 추론 작업이나 RACE, QuAC과 같은 일부 읽기 이해 데이터셋에서 few-shot 성능이 어려움을 겪는 일부 작업들을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure1.3.png"
width="948"
height="598"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure1.3_hue386bd87315498dbfeade35aee90dfda_440734_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure1.3_hue386bd87315498dbfeade35aee90dfda_440734_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="158"
data-flex-basis="380px"
>&lt;/p>
&lt;p>&amp;ldquo;data contamination&amp;quot;에 대한 체계적인 연구를 수행하였다. 이는 테스트 데이터셋의 콘텐츠가 웹에 존재하기 때문에, Common Crawl과 같은 데이터셋에서 모델을 학습시킬 때 발생할 수 있는 문제이다. data contamination이 대부분의 데이터셋에서 GPT-3의 성능에 미치는 영향은 적지만, 결과가 과대 평가될 수 있는 몇몇 데이터셋을 식별하였다.&lt;/p>
&lt;p>더 작은 모델들을 학습시켜 성능을 zero, one, few-shot 설정에서 GPT-3와 비교하였다. 대부분의 작업에서 모델 용량과 함께 성능이 상대적으로 부드럽게 스케일링되는 것을 보았다. 특히, 모델 용량이 커짐에 따라 zero, one, few-shot 성능 간의 차이가 더욱 커지는 것으로 보아, 큰 모델이 더 능숙한 메타 학습자일 수 있음을 시사한다.&lt;/p>
&lt;p>마지막으로, GPT-3가 보여주는 넓은 범위의 능력에 대해, 편향, 공정성, 그리고 보다 넓은 사회적 영향에 대한 우려를 논의하고, 이러한 관점에서 GPT-3의 특성에 대한 초기 분석을 시도한다.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>GPT-3의 사전 학습 접근법은 기존의 방법과 유사하나, 모델 크기, 데이터셋 크기 및 다양성, 학습 기간을 확장하였다. 컨텍스트 내에서 학습하는 다양한 설정을 체계적으로 탐색하였고, 이러한 설정은 작업 특정 데이터에 얼마나 의존하는지에 따라 다르게 위치할 수 있다. 스펙트럼에서는 적어도 네 가지 주요 포인트를 식별할 수 있다:&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure2.1.png"
width="1022"
height="914"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure2.1_hu141a938b4157b8a30b49f5a1188b7faf_232340_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure2.1_hu141a938b4157b8a30b49f5a1188b7faf_232340_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Fine-Tuning (FT)&lt;/strong> 최근 방식은 원하는 작업에 맞는 감독 데이터셋으로 사전 학습된 모델의 가중치를 업데이트하는 것이다. 이 방법의 이점은 많은 벤치마크에서 강력한 성능을 보여준다는 것이고, 단점은 각 작업마다 새로운 대규모 데이터셋이 필요하고, 분포 외에서는 일반화가 잘 안 될 수 있으며, 학습 데이터의 임의적인 특징을 이용할 수 있다는 것이다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Few-Shot (FS)&lt;/strong> 모델이 추론 시간에 작업의 몇 가지 예시를 조건으로 받지만 가중치 업데이트는 허용되지 않는 설정이다. 이 방법의 장점은 작업 특정 데이터에 대한 요구가 크게 줄어들고, 과도하게 좁은 분포를 학습하는 가능성이 줄어든다는 것이다. 단점은 이 방법의 결과가 미세 조정된 최신 모델보다 낮았다는 것이며, 작은 양의 작업 특정 데이터가 여전히 필요하다. few-shot 학습은 넓은 작업 분포를 기반으로 학습하고, 새로운 작업에 빠르게 적응하는 것을 포함한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>One-Shot (1S)&lt;/strong> 작업에 대한 자연어 설명 외에 하나의 예시만 허용된다는 점에서 few-shot과 다르다. one-shot은 일부 작업이 사람들에게 전달되는 방식과 가장 일치하기 때문에 few-shot과 zero-shot과 구별된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Zero-Shot (0S)&lt;/strong> 자연어 지시문만 모델에게 제공되며, 예시는 허용되지 않는 방식이다. 이 방법은 최대한의 편리성을 제공하지만, 가장 도전적인 설정이기도 하다. 예시 없이 작업의 형식을 이해하는 것은 어려울 수 있지만, zero-shot은 사람들이 작업을 수행하는 방식과 가장 가깝다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="model-and-architectures">Model and Architectures&lt;/h3>
&lt;p>GPT-2와 동일한 모델과 아키텍처를 사용하면서, transformer 계층에서 alternating dense 패턴과 locally banded sparse attention 패턴을 교대로 사용하는 점이 다르다. 모델 크기에 따른 ML 성능의 의존성을 연구하기 위해, 125M 개의 parameter에서 175B 개의 parameter까지 다양한 크기의 8가지 모델을 훈련시켰다. 가장 큰 모델을 GPT-3라고 부른다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table2.1.png"
width="1158"
height="326"
srcset="https://kurtkim.github.io/p/gpt-3/images/table2.1_hu7a06fdbc940bf5eee33681a10d2f9dc4_86292_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table2.1_hu7a06fdbc940bf5eee33681a10d2f9dc4_86292_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="355"
data-flex-basis="852px"
>&lt;/p>
&lt;p>각 모델은 학습 가능한 parameter 수, layer 수, bottleneck layer의 단위 수 등으로 구성되어 있다. 모든 모델은 2048 토큰의 컨텍스트 window를 사용하며, 데이터 전송을 최소화하기 위해 GPU에 모델을 깊이와 너비 차원을 따라 분할한다. 각 모델의 아키텍처 parameter는 계산 효율성과 GPU 간의 로드 밸런싱에 기반하여 선택되었다. 이전 연구에 따르면, 검증 손실은 이러한 parameter에 대해 상당히 넓은 범위에서 크게 민감하지 않다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>언어 모델 데이터셋은 Common Crawl 데이터셋으로 확장되어 1 trillion 단어를 수집하였다. 이런 크기의 데이터셋은 가장 큰 모델을 학습시키기에 충분하지만, Common Crawl의 필터링되지 않은 버전은 품질이 낮다. 그래서 데이터셋의 품질을 향상시키기 위해 세 가지 절차를 거쳤습니다: (1) 고품질 참조 말뭉치와 유사한 Common Crawl의 버전을 다운로드하고 필터링, (2) 중복 제거를 통해 데이터셋의 중복을 방지, (3) 고품질 참조 말뭉치를 학습 데이터에 추가하여 다양성을 늘렸다.&lt;/p>
&lt;p>추가 데이터셋으로는 WebText 데이터셋의 확장 버전, 두 개의 인터넷 기반 책 말뭉치(Books1과 Books2), 그리고 영어 Wikipedia가 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table2.2.png"
width="924"
height="258"
srcset="https://kurtkim.github.io/p/gpt-3/images/table2.2_hu40bee715897d3cadbc87965d4593db55_52624_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table2.2_hu40bee715897d3cadbc87965d4593db55_52624_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="358"
data-flex-basis="859px"
>&lt;/p>
&lt;p>CommonCrawl 데이터는 필터링 전 45TB, 필터링 후 570GB로, 약 4000억 바이트 쌍 인코딩 토큰에 해당한다. 학습 중에는 품질이 높은 데이터셋을 더 자주 샘플링하며, 이는 고품질 학습 데이터를 위해 약간의 과적합을 받아들인다.&lt;/p>
&lt;p>인터넷 데이터에서 사전 학습된 언어 모델은 데이터 오염이 발생할 우려가 있다. 이를 줄이기 위해 모든 벤치마크의 개발 및 테스트 세트와 겹치는 부분을 찾아 제거하려 했으나, 일부 겹치는 부분을 무시하는 버그가 있었다. 학습의 비용 문제로 인해 다시 모델을 학습하는 것은 비현실적이었다.&lt;/p>
&lt;h3 id="training-process">Training Process&lt;/h3>
&lt;p>대형 모델은 큰 배치 크기를 사용하나 작은 learning rate가 필요하다. 학습 중 gradient noise scale을 측정하여 배치 크기를 결정하였다. 메모리 부족을 방지하기 위해 모델 병렬성을 사용하였고, 모든 모델은 Microsoft의 고대역폭 클러스터에서 V100 GPU로 학습되었다.&lt;/p>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>few-shot 학습에서는 각 작업의 학습 세트에서 무작위로 $K$개의 예제를 추출하여 평가하였다. LAMBADA와 Storycloze는 지도 학습 세트가 없으므로, 개발 세트에서 추출한 예제를 사용한다. Winograd는 하나의 데이터셋만 있으므로, 그 데이터셋에서 직접 예제를 추출한다.&lt;/p>
&lt;p>$K$는 0부터 모델의 컨텍스트 창이 허용하는 2048까지의 값이 될 수 있으며, 대체로 10에서 100개의 예제를 수용한다. $K$의 더 큰 값이 일반적으로 좋지만 항상 그런 것은 아니므로, 개발 세트와 테스트 세트가 있는 경우, 개발 세트에서 $K$의 몇 가지 값을 실험하고 최적의 값을 테스트 세트에서 사용한다. 일부 작업에서는 예시 외에도 자연어 프롬프트를 사용한다.&lt;/p>
&lt;p>객관식 작업에서는 $K$개의 컨텍스트와 정확한 완성 예제를 제공하고, 각 완성의 가능성을 비교한다. 대부분 작업에서는 토큰 당 가능성을 비교하여 길이를 정규화하지만, ARC, OpenBookQA, RACE 같은 일부 데이터셋에서는 완성의 무조건적 확률 $ {P(completion | context)}\over{P(completion | answer_context)} $로 정규화하여 추가적인 이익을 얻는다. &amp;ldquo;Answer: &amp;quot; 또는 &amp;ldquo;A: &amp;ldquo;는 완성이 답이어야 함을 알리는 프롬프트로 사용된다.&lt;/p>
&lt;p>이진 분류 작업에서는 옵션에 &amp;ldquo;True&amp;quot;나 &amp;ldquo;False&amp;quot;와 같은 의미 있는 이름을 부여하고, 객관식 문제처럼 처리한다.&lt;/p>
&lt;p>자유형식 완성 작업에서는 beam width가 4이고 길이 패널티가 $\alpha = 0.6$인 beam search를 사용한다. 모델은 F1 유사도 점수, BLEU, 또는 정확한 일치를 기준으로 평가한다.&lt;/p>
&lt;p>테스트 세트가 공개적으로 사용 가능한 경우, 모델 크기와 학습 설정별로 최종 결과를 보고한다. 테스트 세트가 비공개인 경우, 개발 세트 결과를 보고한다. 제출이 가능한 데이터셋(SuperGLUE, TriviaQA, PiQa)에 대해서는 테스트 서버에 결과를 제출하고, 그 외의 경우에는 개발 세트 결과를 보고한다.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.1.png"
width="816"
height="634"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.1_hu492030114f856045d4aed1d58b28bf5f_188084_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.1_hu492030114f856045d4aed1d58b28bf5f_188084_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="308px"
>&lt;/p>
&lt;p>효율적인 학습 계산을 사용하면 언어 모델링 성능이 power-law를 따르는 것을 확인하였다. 이 경향을 더 확장하면 power-law에서 약간 벗어나는 것을 볼 수 있다. cross-entropy 손실의 개선이 학습 코퍼스의 특정 세부사항을 모델링함으로써만 이루어진다는 우려도 있지만, 실제로는 다양한 자연어 작업에서 일관된 성능 향상을 가져왔다.&lt;/p>
&lt;p>8개의 모델(175B 개의 parameter를 가진 GPT-3와 7개의 작은 모델)을 다양한 데이터셋에서 평가한다. 데이터셋은 유사한 작업을 나타내는 9개의 카테고리로 그룹화한다.&lt;/p>
&lt;p>언어 모델링과 유사한 작업, &amp;lsquo;closed book&amp;rsquo; 질문 응답 작업, 언어 간 번역 능력, Winograd Schema와 유사한 작업, 상식 추론 또는 질문 응답 작업, 읽기 이해 작업, SuperGLUE 벤치마크를, 그리고 NLI를 평가한다. 마지막으로 인텍스트 학습 능력을 조사하기 위한 추가 작업을 발명하고 평가한다. 이 모든 평가는 few-shot, one-shot, zero-shot 설정에서 이루어진다.&lt;/p>
&lt;h3 id="language-modeling-cloze-and-completion-tasks">Language Modeling, Cloze, and Completion Tasks&lt;/h3>
&lt;p>GPT-3의 성능을 전통적인 언어 모델링 작업뿐만 아니라 관심 있는 단일 단어를 예측하거나, 문장이나 단락을 완성하거나, 텍스트의 가능한 완성 사이에서 선택하는 등의 관련 작업을 테스트한다.&lt;/p>
&lt;h4 id="language-modeling">Language Modeling&lt;/h4>
&lt;p>Penn Tree Bank (PTB) 데이터셋에서 GPT-3의 zero-shot perplexity를 계산하였다. PTB는 현대 인터넷 이전에 만들어진 데이터셋이므로 학습 데이터에 포함되지 않았다. 가장 큰 모델은 PTB에서 perplexity 20.50을 달성하여 state-of-the-art를 달성하였다. PTB는 전행적인 언어 모델링 데이터셋이므로 one-shot이나 few-shot 평가는 적용되지 않았다.&lt;/p>
&lt;h4 id="lambada">LAMBADA&lt;/h4>
&lt;p>LAMBADA 데이터셋은 텍스트의 장거리 의존성을 테스트한다. 최근에는 언어 모델의 크기를 늘리는 것이 더 이상 벤치마크의 성능 향상에 별 도움이 안 된다는 의견이 있었다. 그러나 GPT-3는 zero-shot 설정에서 LAMBADA에서 76%의 결과를 보여주며, 이전 최고 기록보다 8% 향상시키는 결과를 보여주었다.&lt;/p>
&lt;p>LAMBADA는 few-shot 학습의 유연성을 보여준다. 표준 언어 모델은 문장의 마지막 단어를 예측하는 것이 어렵지만, few-shot 학습은 이를 클로즈 테스트로 제시하고 언어 모델이 한 단어의 완성을 예측하도록 한다. 이전의 stop-word ﬁlter 방법보다 효과적인 해결책을 제공합니다.&lt;/p>
&lt;p>다음과 같은 빈칸 채우기 형식을 사용한다:&lt;/p>
&lt;p>$$ \text{Alice was friends with Bob. Alice went to visit her friend ____} \rightarrow \text{Bob} $$
$$ \text{George bought some baseball equipment, a ball, a glove, and a ____} \rightarrow $$&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.2.png"
width="864"
height="232"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.2_hub0494855f6e89e2303b1180ef92365b1_46023_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.2_hub0494855f6e89e2303b1180ef92365b1_46023_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="893px"
>&lt;/p>
&lt;p>GPT-3는 few-shot 설정에서 86.4%의 정확도를 보여, 이전 최고 기록보다 18% 이상 증가하였다. 모델 크기가 커질수록 퓨샷 성능이 크게 향상되었다. 그러나 빈칸 채우기 방법은 one-shot에서는 zero-shot보다 성능이 떨어졌다. 이는 모든 모델이 패턴을 인식하기 위해 여러 예제가 필요하기 때문일 것으로 보인다.&lt;/p>
&lt;p>테스트 세트에서 LAMBADA 데이터셋의 일부가 학습 데이터에 포함된 것으로 확인되었지만, 성능에 불필요한 영향을 미치지는 않는 것으로 분석되었다.&lt;/p>
&lt;h4 id="hellaswag">HellaSwag&lt;/h4>
&lt;p>HellaSwag 데이터셋은 이야기나 지시사항의 최선의 결말을 선택하는 것이다. GPT-3는 one-shot에서 78.1%, few-shot에서 79.3%의 정확도를 보여주었다. 이는 1.5B parameter 언어 모델의 75.4%를 능가하지만, 다목적 모델 ALUM의 85.6%에는 미치지 못하였다.&lt;/p>
&lt;h4 id="storycloze">StoryCloze&lt;/h4>
&lt;p>GPT-3는 5문장 이야기의 결말을 선택하는 StoryCloze 2016 데이터셋에서는 zero-shot에서 83.2%, few-shot에서 87.7%의 정확도를 보여주었다. 이는 BERT 기반 모델의 최고 기록보다 4.1% 낮지만, 이전 zero-shot 결과에 비해 10% 향상된 수치이다.&lt;/p>
&lt;h3 id="closed-book-question-answering">Closed Book Question Answering&lt;/h3>
&lt;p>GPT-3가 광범위한 사실에 대한 질문에 얼마나 잘 대답하는지를 측정한다. 일반적으로 이 작업은 정보 검색 시스템과 모델을 사용해 수행되며, 이를 &amp;ldquo;open-book&amp;quot;이라고 부른다. 하지만 최근에는 &amp;ldquo;closed-book&amp;rdquo; 방식으로 큰 언어 모델이 직접 질문에 답하는 것이 효과적이라는 연구 결과가 나왔다. 이 가설을 GPT-3로 테스트하며, Natural Questions, WebQuestions, TriviaQA 세가지 데이터셋에서 평가를 진행하였다. 이 평가는 외부 콘텐츠와 Q&amp;amp;A 데이터셋에 대한 미세조정을 허용하지 않는 엄격한 closed-book 설정에서 수행된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.3.png"
width="990"
height="254"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.3_hubc1a03863b33702af63f12c643950a56_62395_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.3_hubc1a03863b33702af63f12c643950a56_62395_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>GPT-3는 TriviaQA에서 zero-shot 64.3%, one-shot 68.0%, few-shot 71.2%의 결과를 보여주었다. zero-shot 결과만으로도 미세조정된 T5-11B를 14.2%, Q&amp;amp;A 맞춤형 범위 예측을 사용한 버전을 3.8% 초과하였다. one-shot 결과는 3.7% 향상되며, 오픈 도메인 QA 시스템의 최고 기록과 동일하게 되었다. few-shot 결과는 3.2% 향상시켰다.&lt;/p>
&lt;p>WebQuestions에서 GPT-3는 zero-shot 14.4%, one-shot 25.3%, few-shot 41.5%의 결과를 보여주었다. 이는 미세조정된 T5-11B의 37.4%, 특정 사전 학습 절차를 사용하는 T5-11B+SSM의 44.7%와 비교된다. few-shot 설정에서 GPT-3의 성능은 state-of-the-art를 달성한 미세조정 모델과 근접하다. 또한, WebQs의 질문이나 답변 스타일이 GPT-3에게는 이질적인 것으로 보여지지만, few-shot 설정에서 GPT-3는 이에 적응하며 높은 성능을 회복하는 것으로 보인다.&lt;/p>
&lt;p>Natural Questions에서 GPT-3는 zero-shot 14.6%, one-shot 23.0%, few-shot 29.9%의 성과를 보여주었다. 이는 미세조정된 T5 11B+SSM의 36.6%와 비교되는 결과이다. zero-shot에서 few-shot으로 크게 향상된 성능은 분포의 변화를 보여주며, 이는 TriviaQA와 WebQS에 비해 덜 경쟁력 있는 성능을 설명할 수 있다. 특히, NQs 질문들이 Wikipedia에 대한 매우 세부적인 지식을 요구하므로, 이는 GPT-3의 용량과 사전 학습 분포의 한계를 시험할 수 있다.&lt;/p>
&lt;p>세 가지 데이터셋 중 하나에서 GPT-3의 one-shot 성능은 오픈 도메인의 최고 성능과 일치하고, 나머지 두 데이터셋에서는 미세조정을 하지 않아도 최고 성능에 근접한다. 모든 데이터셋에서, 모델 크기에 따라 성능이 부드럽게 확장되는 것을 확인하였고, 이는 모델의 용량이 직접적으로 모델의 parameter 흡수된 &amp;lsquo;knowledge&amp;rsquo;으로 변환된다는 생각을 반영할 수 있다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>GPT-2는 용량 문제로 인해 다국어 문서를 영어로만 필터링했지만, 일부 다국어 능력을 보여주었다. 프랑스어와 영어 간 번역에서도 의미 있는 성과를 보였다. GPT-3에서는 용량을 크게 향상시키고 학습 데이터셋을 확대하여 다른 언어를 더 많이 포함하였다. GPT-3의 학습 데이터는 주로 영어(93%)이지만, 다른 언어의 텍스트도 7% 포함한다. 번역 능력을 더 잘 이해하기 위해, 분석에 독일어와 루마니아어를 추가하였다.&lt;/p>
&lt;p>기존의 비지도 학습 기계 번역은 주로 단일 언어 데이터셋과 back-translation을 사용하지만, GPT-3는 여러 언어를 혼합한 학습 데이터에서 학습한다. 이는 단어, 문장, 문서 수준에서 언어들을 결합한다. GPT-3는 특정 작업을 위해 맞춤화되지 않은 단일 학습 목표를 사용한다. 그러나, one-shot / few-shot 설정은 적은 양의 쌍으로 된 예시를 사용하기 때문에 엄밀히 말해 이전의 비지도 작업과는 비교가 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.4.png"
width="962"
height="314"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.4_hu7bd1cbed43e7b566a815aa687486c811_68971_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.4_hu7bd1cbed43e7b566a815aa687486c811_68971_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="306"
data-flex-basis="735px"
>&lt;/p>
&lt;p>zero-shot GPT-3는 작업 설명만을 받지만 최근의 비지도 NMT 결과보다 성능이 떨어진다. 그러나 각 번역 작업에 대해 한 예시만 제공하면 성능이 크게 향상되며, few-shot 설정에서 더욱 향상된다. GPT-3의 성능은 언어 방향에 따라 크게 달라진다. 영어로 번역할 때는 이전의 비지도 NMT 작업을 능가하지만 반대 방향으로는 성능이 떨어진다. En-Ro의 경우 성능이 이전 비지도 NMT 작업보다 훨씬 낮다. Fr-En과 De-En에서 few-shot GPT-3는 최고의 지도 학습 결과를 능가하고, Ro-En에서는 전체 최고 성능과 비슷한 성능을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.4.png"
width="910"
height="604"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.4_hu82b1ebcae102ece625cbe1c0e223e818_183098_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.4_hu82b1ebcae102ece625cbe1c0e223e818_183098_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>모든 언어 쌍과 zero-shot, one-shot, few-shot 설정에서 모델 용량이 증가함에 따라 성능이 부드럽게 향상되는 추세가 확인되었다.&lt;/p>
&lt;h3 id="winograd-style-tasks">Winograd-Style Tasks&lt;/h3>
&lt;p>Winograd Schemas Challenge는 대명사가 가리키는 단어를 찾는 NLP 작업이다. 언어 모델은 기존 Winograd 데이터셋에서는 좋은 성능을 보였지만, 더 어려운 Winogrande 데이터셋에서는 성능이 떨어졌다. 이는 GPT-3에서도 확인되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.5.png"
width="600"
height="200"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.5_hu7b018836446e63d3c9d1ef5b542571f1_33042_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.5_hu7b018836446e63d3c9d1ef5b542571f1_33042_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="300"
data-flex-basis="720px"
>&lt;/p>
&lt;p>GPT-3는 원래의 273개의 Winograd 스키마에서 테스트되었고, zero-shot, one-shot, few-shot 설정에서 각각 88.3%, 89.7%, 88.6%의 성능을 보여주었다. 이는 모든 경우에서 state-of-the-art와 인간의 성능을 몇 포인트 밑돌게 강력한 결과를 보여준다. 학습 데이터 중 일부 Winograd 스키마에서 오염 분석이 이루어졌지만, 이것이 결과에 미치는 영향은 작았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.5.png"
width="910"
height="606"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.5_hud3e66a69a890d4d782e96b2e04d9b5b1_139301_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.5_hud3e66a69a890d4d782e96b2e04d9b5b1_139301_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>더 어려운 Winogrande 데이터셋에서 GPT-3는 zero-shot에서 70.2%, one-shot에서 73.2%, few-shot에서 77.7%의 성능을 보여주었다. 이는 미세 조정된 RoBERTA 모델의 79%, 최첨단 모델인 T5의 84.6%, 그리고 인간의 성능인 94.0%와 비교된다.&lt;/p>
&lt;h3 id="common-sense-reasoning">Common Sense Reasoning&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.6.png"
width="938"
height="198"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.6_hu6eeb6cfaedf1899bf3ff81e3c156f85b_50199_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.6_hu6eeb6cfaedf1899bf3ff81e3c156f85b_50199_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="473"
data-flex-basis="1136px"
>&lt;/p>
&lt;p>PhysicalQA (PIQA)라는 데이터셋에서 GPT-3는 zero-shot 81.0%, one-shot 80.5%, few-shot 82.8%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 79.4%에 비해 우수하며, 인간의 성능보다는 약 10% 떨어지지만, state-of-the-art의 성능을 one-shot과 few-shot에서 능가하였다. 하지만, PIQA가 데이터 오염 가능성을 가질 수 있어 결과를 보수적으로 표시하였다.&lt;/p>
&lt;p>ARC 데이터셋에서 GPT-3는 &amp;ldquo;Challenge&amp;rdquo; 버전에서 zero-shot 51.4%, one-shot 53.2%, few-shot 51.5%의 정확도를, &amp;ldquo;Easy&amp;rdquo; 버전에서는 68.8%, 71.2%, 70.1%의 정확도를 달성하였다. 이는 미세 조정된 RoBERTa의 성능에 근접하거나 약간 능가하였지만, UniﬁedQA의 성능에 비하면 아직도 많이 뒤떨어져 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.6.png"
width="886"
height="590"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.6_hua09445c20b3052295af69029a7e053b3_120087_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.6_hua09445c20b3052295af69029a7e053b3_120087_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>OpenBookQA에서 GPT-3는 zero-shot에서 few-shot으로 넘어갈 때 성능이 크게 향상되었지만, state-of-the-art에 비해 아직 20점 이상 뒤떨어져 있다. GPT-3의 few-shot 성능은 미세 조정된 BERT Large와 비슷하다.&lt;/p>
&lt;p>GPT-3의 in-context 학습은 상식 추론 작업에서 일관성 없는 결과를 보였지만, OpenBookQA에서는 크게 향상되었다. 또한, GPT-3는 모든 평가에서 새 PIQA 데이터셋의 state-of-the-art를 달성하였다.&lt;/p>
&lt;h3 id="reading-comprehension">Reading Comprehension&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.7.png"
width="942"
height="200"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.7_hu9e83a0e91cedfa41f8005f1bd265357f_45405_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.7_hu9e83a0e91cedfa41f8005f1bd265357f_45405_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1130px"
>&lt;/p>
&lt;p>reading comprehension 작업에서 GPT-3를 평가해 보았다. 다양한 답변 형식을 가진 5개의 데이터셋을 사용하였고, GPT-3의 성능은 데이터셋에 따라 크게 다르며, 다양한 답변 형식에 대한 능력을 보여주었다. 일반적으로 GPT-3는 각 데이터셋에 대한 초기 기준선과 비슷한 성능을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.7.png"
width="900"
height="584"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.7_hu382e3ffe17c80967f5475d70a8823a4a_125791_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.7_hu382e3ffe17c80967f5475d70a8823a4a_125791_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="369px"
>&lt;/p>
&lt;p>GPT-3는 자유 형식의 대화 데이터셋인 CoQA에서 가장 좋은 성능을 보였고, 대화 행동과 답변 선택을 요구하는 QuAC에서는 가장 나쁜 성능을 보였다. DROP 데이터셋에서는 few-shot 설정에서 BERT 기준선을 앞섰지만, 사람의 성능과 최첨단 방법에는 미치지 못하였다. SQuAD 2.0에서는 few-shot 학습 능력을 보여주며 성능을 향상시켰고, RACE에서는 상대적으로 약한 성능을 보였지만, 초기 작업과는 경쟁력을 가졌다.&lt;/p>
&lt;h3 id="superglue">SuperGLUE&lt;/h3>
&lt;p>GPT-3를 더 체계적으로 평가하고 다른 모델들과 비교하기 위해, SuperGLUE 벤치마크라는 표준화된 데이터셋에서도 평가를 진행하였다. few-shot 설정에서는 모든 작업에 대해 학습 세트에서 무작위로 추출한 32개의 예제를 사용하였다. WSC와 MultiRC를 제외한 모든 작업에서는 각 문제의 컨텍스트로 사용할 새로운 예제 집합을 샘플링하였다. WSC와 MultiRC에서는, 모든 문제의 컨텍스트로 동일한 예제 집합을 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.8.png"
width="1092"
height="384"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.8_hud3a21f9e74fc9295279ca3d9a09ce1b9_83462_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.8_hud3a21f9e74fc9295279ca3d9a09ce1b9_83462_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="682px"
>&lt;/p>
&lt;p>GPT-3는 다양한 작업에서 성능이 다르게 나타났다. COPA와 ReCoRD에서는 거의 최고 수준에 근접한 성능을 보였고, WSC에서는 80.1%의 높은 성능을 보였다. BoolQ, MultiRC, RTE에서는 합리적인 성능을 보였고, CB에서는 75.6%의 성능을 보였다.&lt;/p>
&lt;p>WiC에서 GPT-3의 few-shot 성능이 49.4%로 상대적으로 약하다는 것을 발견하였다. 두 문장을 비교하는 일부 작업에서 GPT-3는 약한 경향이 있다. 이는 RTE와 CB의 낮은 점수를 설명할 수 있다. 그러나 이런 약점에도 불구하고, GPT-3는 8개 작업 중 4개에서 미세 조정된 BERT-Large를 능가하며, 2개 작업에서는 state-of-the-art에 가깝다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.8.png"
width="1252"
height="596"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.8_hu631d470470991d10be8ea0f9f0a7f95a_188012_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.8_hu631d470470991d10be8ea0f9f0a7f95a_188012_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="210"
data-flex-basis="504px"
>&lt;/p>
&lt;p>모델 크기와 예시 수가 증가함에 따라 few-shot SuperGLUE 점수가 개선되는 것을 확인하였다. GPT-3는 각 작업당 8개 미만의 예시만으로도 미세 조정된 BERT-Large를 능가하는 전체 SuperGLUE 점수를 얻을 수 있었다.&lt;/p>
&lt;h3 id="nli">NLI&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.9.png"
width="888"
height="598"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.9_hu1aca7b926a856f62daae8569f85a6da9_143695_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.9_hu1aca7b926a856f62daae8569f85a6da9_143695_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>자연어 추론(NLI)은 두 문장 간의 관계를 이해하는 능력을 평가한다. GPT-3는 이 작업에서 랜덤(56%)보다 약간 더 잘 수행되는 반면, few-shot 설정에서는 BERT Large와 유사한 수준으로 수행한다. 적대적 자연어 추론(ANLI) 데이터셋에서는 GPT-3가 라운드 3에서 약간의 진전을 보여주었다. 이러한 결과는 NLI가 여전히 언어 모델에게 어려운 작업이며, 진전의 시작 단계에 불과하다는 것을 시사한다.&lt;/p>
&lt;h3 id="synthetic-and-qualitative-tasks">Synthetic and Qualitative Tasks&lt;/h3>
&lt;p>GPT-3의 능력을 테스트하기 위해, 간단한 계산, 새로운 패턴 인식, 비정상적인 작업에 빠르게 적응하는 등의 작업을 제공한다. 테스트에는 산술, 단어의 글자 재배열, SAT 스타일의 유사성 문제 해결, 그리고 새로운 단어 사용, 문법 수정, 뉴스 기사 생성 등이 포함된다. 이러한 합성 데이터셋은 언어 모델의 테스트 시간 행동에 대한 추가 연구를 촉진하기 위해 공개될 예정이다.&lt;/p>
&lt;h4 id="arithmetic">Arithmetic&lt;/h4>
&lt;p>GPT-3가 특정 작업에 대한 학습 없이 간단한 산술 연산을 수행하는 능력을 테스트하기 위해, 자연어로 간단한 산술 문제를 묻는 10개의 작은 테스트를 개발하였다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>2 digit addition (2D+)&lt;/strong> 모델에게는 두 정수를 더하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 &amp;ldquo;Q: What is 48 plus 76? A: 124&amp;quot;와 같은 형태로 질문된다.&lt;/li>
&lt;li>&lt;strong>2 digit subtraction (2D-)&lt;/strong> 모델에게는 두 정수를 빼라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 답변은 음수일 수 있다. 예를 들어 &amp;ldquo;Q: What is 34 minus 53? A: -19&amp;quot;와 같은 형태로 질문된다.&lt;/li>
&lt;li>&lt;strong>3 digit addition (3D+)&lt;/strong> 2자리 수 덧셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>3 digit subtraction (3D-)&lt;/strong> 2자리 수 뺄셈과 같지만, 숫자는 [0, 1000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>4 digit addition (4D+)&lt;/strong> 3자리 수 덧셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>4 digit subtraction (4D-)&lt;/strong> 3자리 수 뺄셈과 같지만, 숫자는 [0, 10000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>5 digit addition (5D+)&lt;/strong> 4자리 수 덧셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>5 digit subtraction (5D-)&lt;/strong> 4자리 수 뺄셈과 같지만, 숫자는 [0, 100000) 범위에서 균일하게 샘플링된다.&lt;/li>
&lt;li>&lt;strong>2 digit multiplication (2Dx)&lt;/strong> 모델에게는 두 정수를 곱하라는 질문이 제시된다. 이 정수들은 [0, 100) 범위에서 균일하게 샘플링되며, 예를 들어 &amp;ldquo;Q: What is 24 times 42? A: 1008&amp;quot;와 같은 형태로 질문된ㄴ다.&lt;/li>
&lt;li>&lt;strong>One-digit composite (1DC)&lt;/strong> 모델에게는 마지막 두 숫자에 괄호가 있는 세 개의 1자리 숫자에 대해 복합 연산을 수행하라는 질문이 제시된다. 예를 들어, &amp;ldquo;Q: What is 6+(4*8)? A: 38&amp;quot;입니다. 세 개의 1자리 숫자는 [0, 10) 범위에서 균일하게 선택되며, 연산은 { +, -, * } 중에서 균일하게 선택된다.&lt;/li>
&lt;/ul>
&lt;p>10개의 모든 작업에서 모델은 정확한 답변을 생성해야 한다. 각 작업에 대해 작업의 2,000개의 무작위 인스턴스 데이터셋을 생성하고, 모든 모델을 이러한 인스턴스에서 평가한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.9.png"
width="1028"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.9_hub4937564fbe4c24425801dd2b8a216e0_37913_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.9_hub4937564fbe4c24425801dd2b8a216e0_37913_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="611"
data-flex-basis="1468px"
>&lt;/p>
&lt;p>GPT-3는 few-shot 설정에서 평가되었고, 작은 숫자에 대한 덧셈과 뺄셈에서 높은 정확도를 보여주었다. 2자리 숫자에 대한 연산에서는 덧셈에서 100%, 뺄셈에서 98.9%의 정확도를 보였으며, 3자리 숫자에 대한 연산에서는 덧셈에서 80.2%, 뺄셈에서 94.2%의 정확도를 달성하였다. 숫자의 자릿수가 증가함에 따라 성능은 감소하지만, 4자리 연산에서는 25-26%, 5자리 연산에서는 9-10%의 정확도를 보여주었다. GPT-3는 또한 계산이 복잡한 2자리 곱셈에서 29.2%의 정확도를 달성하였다. 마지막으로, GPT-3는 단일 자릿수 복합 연산(예를 들어, 9*(7+5))에서 21.3%의 정확도를 보였습니다. 이는 GPT-3가 단일 연산을 넘어서 일부 견고성을 가지고 있다는 것을 나타낸다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.10.png"
width="912"
height="586"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.10_hu9cb0749638ecda1cccfc10c302e9818b_203746_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.10_hu9cb0749638ecda1cccfc10c302e9818b_203746_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>작은 모델들은 이러한 모든 작업에서 성능이 좋지 않다 - 심지어 13B parameter 모델(175B parameter 전체 GPT-3 다음으로 큰 모델)조차도 2자리 덧셈과 뺄셈을 절반 정도의 시간만 해결할 수 있고, 다른 모든 연산은 10% 미만의 시간에 해결할 수 있다.&lt;/p>
&lt;p>one-shot과 zero-shot 성능은 few-shot 성능에 비해 다소 낮지만, 이는 작업에 대한 적응이 중요함을 보여준다. 그러나 one-shot 성능은 아직도 강하며, 전체 GPT-3의 zero-shot 성능은 더 작은 모델들의 few-shot 학습보다 월등히 뛰어나다.&lt;/p>
&lt;p>모델이 단순히 특정 산술 문제를 기억하는 것인지를 확인하기 위해, 테스트 세트의 3자리 산술 문제를 가져와서 &amp;ldquo;&lt;!-- raw HTML omitted --> + &lt;!-- raw HTML omitted --> =&amp;ldquo;와 &amp;ldquo;&lt;!-- raw HTML omitted --> plus &lt;!-- raw HTML omitted -->&amp;rdquo; 형태로 학습 데이터에서 찾아보았다. 2,000개의 덧셈 문제 중에서는 17개(0.8%)만 일치하였고, 2,000개의 뺄셈 문제 중에서는 2개(0.1%)만 일치하였다. 이는 올바른 답변의 일부분만이 기억되었을 수 있다는 것을 시사한다. 또한, 잘못된 답변의 검사는 모델이 &amp;ldquo;1&amp;quot;을 올리지 않는 등의 오류를 종종 범하는 것으로 나타낸다. 이는 모델이 실제로 표를 기억하는 것이 아니라 관련 계산을 수행하려고 시도하고 있다는 것을 시사한다.&lt;/p>
&lt;p>전반적으로 GPT-3는 few-shot, one-shot, 심지어 zero-shot 설정에서도 복잡한 산술에 대해 합리적인 능숙도를 보여준다.&lt;/p>
&lt;h4 id="word-scrambling-and-manipulation-tasks">Word Scrambling and Manipulation Tasks&lt;/h4>
&lt;p>GPT-3의 새로운 기호 조작 학습 능력을 테스트하기 위해, 문자를 섞거나 추가하거나 삭제하여 왜곡된 단어를 복구하는 5가지 &amp;lsquo;character manipulation&amp;rsquo; 작업을 설계하였다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cycle letters in word (CL)&lt;/strong> 모델에게 문자가 순환된 단어와 &amp;ldquo;=&amp;rdquo; 심볼이 주어지면, 원래의 단어를 생성해야 한다. 예를 들어, &amp;ldquo;lyinevitab&amp;quot;이 주어지면 &amp;ldquo;inevitably&amp;quot;를 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Anagrams of all but ﬁrst and last characters (A1)&lt;/strong> 모델에게 첫 번째와 마지막 문자를 제외한 모든 문자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;criroptuon&amp;quot;이 주어지면 &amp;ldquo;corruption&amp;quot;을 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Anagrams of all but ﬁrst and last 2 characters (A2)&lt;/strong> 모델에게 첫 두 글자와 마지막 두 글자를 제외한 모든 글자가 무작위로 섞인 단어가 주어지면, 원래의 단어를 복구해야 한다. 예를 들어, &amp;ldquo;opoepnnt&amp;quot;가 주어지면 &amp;ldquo;opponent&amp;quot;를 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Random insertion in word (RI)&lt;/strong> 모델에게 단어의 각 글자 사이에 무작위의 구두점이나 공백 문자가 삽입된ㄴ 글자가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;s.u!c/c!e.s s i/o/n&amp;quot;이 주어지면 &amp;ldquo;succession&amp;quot;을 출력해야 한다.&lt;/li>
&lt;li>&lt;strong>Reversed words (RW)&lt;/strong> 모델에게 거꾸로 철자된 단어가 주어지면, 원래의 단어를 출력해야 한다. 예를 들어, &amp;ldquo;stcejbo&amp;quot;가 주어지면 &amp;ldquo;objects&amp;quot;를 출력해야 한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.10.png"
width="626"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.10_hu01f4d73623cef66e0014a00542e2dd02_30607_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.10_hu01f4d73623cef66e0014a00542e2dd02_30607_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="894px"
>&lt;/p>
&lt;p>각 작업에 대해 가장 빈번한 10,000개의 단어를 사용하여 10,000개의 예시를 생성하였다. few-shot 결과는 모델 크기가 커질수록 성능이 부드럽게 증가하는 경향을 보여주었다. 전체 GPT-3 모델은 RI 38.6%, A1 40.2%, A2 15.1%를 달성하였다. 그러나 어느 모델도 RW는 불가능하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.11.png"
width="886"
height="590"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.11_hua09445c20b3052295af69029a7e053b3_146982_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.11_hua09445c20b3052295af69029a7e053b3_146982_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="360px"
>&lt;/p>
&lt;p>one-shot 설정에서는 성능이 크게 약해져서 절반 이상 떨어지고, zero-shot 설정에서는 대부분의 작업을 수행하지 못하였다. 이는 모델이 테스트 단계에서 이러한 작업을 실제로 배우는 것을 나타내며, 이러한 작업들이 사전 학습 데이터에는 거의 나타나지 않기 때문에 zero-shot으로 수행하는 것이 어렵다.&lt;/p>
&lt;p>&amp;ldquo;in-context learning curves&amp;quot;을 통해 성능을 정량적으로 측정할 수 있다. 이는 in-context 예시의 수에 따른 작업 성능을 나타낸다. 큰 모델일수록 in-context 정보를 더 효과적으로 활용할 수 있음을 알 수 있다. 이는 작업 예시와 자연 언어 작업 설명 모두를 포함한다.&lt;/p>
&lt;p>이러한 작업을 해결하려면 문자 수준의 조작이 필요하며, BPE 인코딩은 단어의 큰 부분을 조작한다. 따라서, 이 작업에 성공하려면 BPE 토큰을 조작하는 것뿐만 아니라 그들의 하위 구조를 이해하고 분해해야 한다. 또한, CL, A1, A2는 bijective가 아니므로, 모델이 올바른 암호화 해제를 찾기 위해 검색을 수행해야 한다. 이러한 기술은 복잡한 패턴 매칭과 계산이 필요하다.&lt;/p>
&lt;h4 id="sat-analogies">SAT Analogies&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure3.12.png"
width="910"
height="618"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure3.12_hufc1bf3f66494f3e19257540e127aa8f7_118792_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure3.12_hufc1bf3f66494f3e19257540e127aa8f7_118792_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="353px"
>&lt;/p>
&lt;p>GPT-3는 374개의 &amp;ldquo;SAT analogy&amp;rdquo; 문제를 통해 테스트되었다. 이 작업에서 GPT-3는 few-shot 65.2%, one-shot 59.1%, zero-shot 53.7%의 성능을 보여주었다. 이는 대학 지원자들의 평균 57%보다 높다. 결과는 규모에 따라 개선되며, 1750억 모델은 130억 파라미터 모델에 비해 10% 이상 개선되었다.&lt;/p>
&lt;h4 id="news-article-generation">News Article Generation&lt;/h4>
&lt;p>GPT-3는 &amp;ldquo;news articles&amp;quot;를 생성하는 능력을 테스트하였다. 그러나 GPT-3의 학습 데이터는 뉴스 기사에 비중이 덜 두어져 있어, 뉴스 기사를 생성하는 것이 덜 효과적이었다. 이를 해결하기 위해, GPT-3의 few-shot 학습 능력을 활용해 세 개의 이전 뉴스 기사를 제공하여 모델을 조건화하였다. 그 결과, 제안된 다음 기사의 제목과 부제목을 가지고, 모델은 &amp;ldquo;news&amp;rdquo; 장르의 짧은 기사를 신뢰성 있게 생성할 수 있었다.&lt;/p>
&lt;p>GPT-3가 생성한 뉴스 기사의 품질을 평가하기 위해, 사람들이 GPT-3가 생성한 기사와 실제 기사를 구별하는 능력을 측정하기로 결정하였다. 이는 생성적 언어 모델의 품질을 측정하는 중요한 방법으로, 이전에도 비슷한 연구가 있었다.&lt;/p>
&lt;p>사람들이 모델이 생성한 텍스트를 얼마나 잘 감지하는지 확인하기 위해, newser.com에서 기사 제목과 부제목 25개를 선택하고, 다양한 크기의 언어 모델로 이를 완성하였다. 이후 미국 참가자 80명에게 이 기사들을 제시하고, 각 참가자에게 기사가 사람이 작성했는지 아니면 기계가 작성했는지 판단하도록 요청하였다.&lt;/p>
&lt;p>모델의 훈련 데이터에는 없는 기사들을 선택하였고, 모든 모델은 같은 맥락을 사용하여 훈련되었다. 각 모델에게는 같은 기사 제목과 부제목이 프롬프트로 제공되었다. 추가적으로 참가자의 집중력을 통제하기 위한 실험도 진행했는데, 이는 고의적으로 품질이 낮은 기사를 생성한 &amp;ldquo;control model&amp;quot;을 사용했습니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.11.png"
width="1208"
height="372"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.11_huf0b222ef12e7e22eecb2ac5755f5cd7c_99834_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.11_huf0b222ef12e7e22eecb2ac5755f5cd7c_99834_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="324"
data-flex-basis="779px"
>&lt;/p>
&lt;p>고의적으로 품질이 낮은 기사를 모델이 만들었다고 판별하는 평균 인간 정확도는 약 86%였다. 반면, 175B parameter 모델이 생성한 기사를 감지하는 평균 인간 정확도는 우연 수준인 52%에 불과하였다. 모델 크기가 커질수록 인간이 모델이 생성한 텍스트를 감지하는 능력이 줄어드는 것으로 보인다.&lt;/p>
&lt;p>GPT-3가 생성한 합성 기사의 예시는 대부분 인간이 진짜 내용과 구별하기 어렵다. 하지만 사실적인 오류는 기사가 모델이 생성했음을 나타낼 수 있다. 왜냐하면 모델은 인간 작가와 달리 특정 사실에 대한 접근이 없기 때문이다. 또한 반복, 비연속성, 이상한 표현도 모델이 생성한 텍스트의 지표가 될 수 있다.&lt;/p>
&lt;p>인간은 더 많은 토큰을 관찰할수록 모델이 생성한 텍스트를 더 잘 감지한다. 이를 검증하기 위해, 평균 길이가 569단어인 로이터의 12개 세계 뉴스 기사에 대해 GPT-3가 평균 498단어로 생성한 기사를 사용하여 실험을 진행하였다. 약 80명의 미국 참가자를 대상으로 한 두 가지 실험을 통해 GPT-3와 통제 모델이 생성한 기사를 감지하는 인간의 능력을 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table3.12.png"
width="992"
height="168"
srcset="https://kurtkim.github.io/p/gpt-3/images/table3.12_hu8227a4efaba093d1188ab82ee37e1d3a_36753_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table3.12_hu8227a4efaba093d1188ab82ee37e1d3a_36753_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="590"
data-flex-basis="1417px"
>&lt;/p>
&lt;p>통제 모델로부터 나온 고의적으로 나쁜 긴 기사를 감지하는 인간의 평균 정확도는 약 88%였다. 반면에, GPT-3가 생성한 긴 기사를 인식하는 인간의 평균 정확도는 약 52%로 거의 우연에 가까웠다. 이는 GPT-3가 약 500단어의 뉴스 기사를 생성할 때, 인간이 쓴 것과 구별하기 어렵다는 것을 의미한다.&lt;/p>
&lt;h4 id="learning-and-using-novel-words">Learning and Using Novel Words&lt;/h4>
&lt;p>새로운 단어를 배우고 활용하는 능력을 GPT-3로 테스트해 보았다. 존재하지 않는 단어 &amp;lsquo;Gigamuru&amp;rsquo; 같은 단어의 정의를 제공하고, 그 단어를 문장에서 사용하도록 요청하였다. 결과적으로, GPT-3는 새로운 단어를 문장에서 사용하는 작업에 대해 능숙함을 보였다. 심지어 &amp;ldquo;screeg&amp;quot;이라는 단어에 대해 그럴등한 변형(&amp;ldquo;screeghed&amp;rdquo;)을 생성하며, 이 단어를 약간 어색하게 사용하였지만 장난감 칼 싸움을 묘사하는 가능성을 보여주었다.&lt;/p>
&lt;h4 id="correcting-english-grammar">Correcting English Grammar&lt;/h4>
&lt;p>영어 문법 교정은 few-shot 학습에 아주 적합한 작업이다. GPT-3를 이용해 이를 테스트하였다. 이를 위해 &amp;ldquo;Poor English Input: &lt;!-- raw HTML omitted --> \ n Good English Output: &lt;!-- raw HTML omitted -->&amp;rdquo; 형식의 문장을 주고, 한 가지 인간이 생성한 교정 예를 제공한 후, 다른 5개 문장의 교정을 요청했습니다.&lt;/p>
&lt;h2 id="measuring-and-preventing-memorization-of-benchmarks">Measuring and Preventing Memorization Of Benchmarks&lt;/h2>
&lt;p>학습 데이터셋은 인터넷에서 가져왔기 때문에, 벤치마크 테스트 세트가 학습 데이터에 포함된 것일 수 있다. 이런 테스트 오염을 정확히 파악하는 것은 아직 확립된 방법이 없는 새로운 연구 분야이다. 대규모 모델 학습 시 오염을 조사하지 않는 것이 일반적이지만, 사전 학습 데이터셋의 규모가 커지고 있어 이 문제에 점점 더 주목할 필요가 있다고 생각한다.&lt;/p>
&lt;p>학습 데이터와 평가 데이터셋이 겹치는 문제는 실제로 존재한다. Common Crawl 데이터에 기반한 언어 모델을 처음 학습시킨 연구 중 하나에서는 평가 데이터셋과 겹치는 학습 문서를 감지하고 제거하였다. GPT-2와 같은 다른 연구에서도 이러한 중복을 분석하였고, 결과적으로 학습과 테스트 데이터가 겹치는 경우 모델 성능이 약간 향상되었지만, 겹치는 데이터의 비율이 작아 전체 결과에는 크게 영향을 미치지 않았다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure4.1.png"
width="1020"
height="580"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure4.1_hu04d42d25b1438872c3c992071044d67e_331505_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure4.1_hu04d42d25b1438872c3c992071044d67e_331505_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="422px"
>&lt;/p>
&lt;p>GPT-3는 데이터셋과 모델 크기가 GPT-2보다 훨씬 크며, 대량의 Common Crawl 데이터를 포함하고 있어 오염 가능성이 늘어났다. 그러나, 데이터량이 많아 GPT-3 175B는 중복 제거된 검증 세트에 대해 크게 과적합되지 않았다. 따라서, 오염은 자주 발생할 것으로 보이지만 그 효과는 예상만큼 크지 않을 것으로 보인다.&lt;/p>
&lt;p>학습 데이터와 벤치마크의 개발 및 테스트 세트 간 중복을 찾아 제거하려 하였으나, 버그로 인해 감지된 중복이 일부만 제거되었다. 모델을 재학습하는 것은 비용 문제로 불가능했다. 그래서 남은 중복이 결과에 미치는 영향을 자세히 조사하였다.&lt;/p>
&lt;p>각 벤치마크에 대해, 13-gram 중복이 있는 예시를 제거하여 오염이 없는 &amp;ldquo;clean&amp;rdquo; 버전을 만들었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure4.2.png"
width="1128"
height="518"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure4.2_hub0ecb8bfd9ab51991516fe584c9897b4_137279_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure4.2_hub0ecb8bfd9ab51991516fe584c9897b4_137279_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>클린 벤치마크에서 GPT-3를 평가한 결과, 전체 데이터셋의 점수와 비슷한 경우, 오염이 결과에 큰 영향을 미치지 않는 것으로 판단되었다. 만약 클린 벤치마크의 점수가 낮다면, 오염이 결과를 과대 평가하고 있다는 의미이다. 그러나 대부분의 경우, 성능 변화는 미미하며, 오염 수준과 성능 차이는 연관되지 않는 것으로 나타났다. 이를 바탕으로, 오염이 성능에 큰 영향을 미치지 않았다는 결론을 내렸다.&lt;/p>
&lt;p>(1) 모델이 클린 버전에서 상당히 더 나쁜 성능을 보이거나, 또는 (2) 잠재적 오염이 매우 높아 성능 차이를 측정하기 어려운 몇 가지 특정 사례를 더 자세히 살펴보았다.&lt;/p>
&lt;p>6개의 벤치마크 그룹(Word Scrambling, Reading Comprehension, PIQA, Winograd, language modeling tasks, German to English translation)이 추가 조사를 위해 지정되었다. 이 중복 분석은 매우 보수적으로 설계되었으므로 일부 잘못된 긍정 결과가 있을 것으로 예상한다.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Reading Comprehension:&lt;/strong> QuAC, SQuAD2, DROP의 작업 예시 중 90% 이상이 잠재적 오염으로 판단되었지만, 수동 검사 결과 원본 텍스트는 학습 데이터에 있지만 질문/답변 쌍은 없었다. 즉, 모델은 배경 정보만 얻을 수 있고 특정 질문에 대한 답을 기억할 수 없었다.&lt;/li>
&lt;li>&lt;strong>German translation:&lt;/strong> WMT16 독일어-영어 테스트 세트의 25%가 잠재적 오염으로 표시되었지만, 검사 결과, 학습 데이터와 유사한 쌍의 문장이 포함된 사례는 없었다. 대부분은 뉴스에서 논의된 이벤트의 일부를 포함하는 단일 언어 일치였다.&lt;/li>
&lt;li>&lt;strong>Reversed Words and Anagrams:&lt;/strong> &amp;ldquo;alaok = koala&amp;rdquo; 형태의 작업에서, 중복이 ﬂagged되었지만, 이는 대부분 회문이나 간단한 재정렬 예시였다. 중복된 부분은 적지만, 단순한 작업을 제거하면 난이도가 증가하고 잘못된 신호가 발생한다. 심볼 삽입 작업은 높은 중복을 보이지만 성능에는 영향을 미치지 않았다. 이는 작업이 비문자 문자 제거에 중점을 두고 있고, 중복 분석이 이러한 문자를 무시하기 때문이다.&lt;/li>
&lt;li>&lt;strong>PIQA:&lt;/strong> 예시의 29%가 오염되었다고 표시되었고, 클린 부분 집합에서 성능이 3% 감소했다. 테스트 데이터셋은 학습 세트 이후에 출시되었지만, 일부 웹 페이지는 학습 세트에 포함되어 있었다. 메모리 용량이 훨씬 적은 작은 모델에서도 비슷한 감소를 보아, 이는 통계적 편향일 가능성이 높다. 하지만 이 가설을 엄밀하게 증명할 수는 없으므로, PIQA 결과에는 별표를 표시하였다.&lt;/li>
&lt;li>&lt;strong>Winograd:&lt;/strong> 예시의 45%가 중복으로 표시되었고, 클린 부분집합에서 성능이 2.6% 감소했다. 중복 데이터를 검사한 결과, 학습 세트에 132개의 Winograd 스키마가 다른 형식으로 포함되어 있었다. 성능 감소가 작지만, Winograd 결과에 별표를 표시했다.&lt;/li>
&lt;li>&lt;strong>Language modeling:&lt;/strong> GPT-2에서 측정된 4개의 Wikipedia 언어 모델링 벤치마크와 Children’s Book Test 데이터셋이 대부분 학습 데이터에 포함되어 있었다. 클린한 부분 집합을 신뢰성 있게 추출할 수 없어 이 데이터셋들의 결과는 보고하지 않았다. Penn Tree Bank는 그 연령 때문에 영향을 받지 않아, 주요 언어 모델링 벤치마크로 사용하였다.&lt;/li>
&lt;/ul>
&lt;p>오염이 높지만 성능에 미치는 영향이 거의 없는 데이터셋을 검사해 실제 오염 정도를 확인하였다. 이들은 대부분 실제 오염이 없거나, 작업의 답을 알려주는 오염이 없었다. 하지만 LAMBADA는 심각한 오염이 있음에도 성능에 미치는 영향이 매우 작았다. 빈칸 채우기 형식은 가장 단순한 형태의 기억을 배제하지만, 이 논문에서 LAMBADA에서 큰 향상을 보였으므로, 결과 섹션에서 잠재적 오염을 언급했다.&lt;/p>
&lt;p>오염 분석의 한계는 클린 부분 집합이 원래 데이터셋과 같은 분포에서 추출되었는지 확신할 수 없다는 점이다. 기억이 결과를 과대평가하면서 클린 부분 집합이 더 쉽게 되게 하는 통계적 편향이 정확히 상쇄되는 가능성이 있다. 그러나 0에 가까운 이동이 많아 이는 불가능할 가능성이 크고, 기억이 적은 작은 모델들에서도 눈에 띄는 차이를 찾지 못하였다.&lt;/p>
&lt;p>데이터 오염의 영향을 측정하고 기록하기 위해 최선을 다했고, 심각성에 따라 문제 결과를 주목하거나 완전히 제거하였다. 벤치마크 설계와 모델 학습에서 이 중요하고 미묘한 문제를 해결하기 위한 많은 작업이 아직 남아 있다.&lt;/p>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>GPT-3에 대한 분석에는 여러 가지 한계점이 있다. 이에 대해 설명하고 미래의 연구 방향을 제안한다.&lt;/p>
&lt;p>GPT-3와 그 분석에는 한계가 있다. GPT-3는 텍스트 합성과 여러 NLP 작업에서 향상되었지만, 문서 수준에서 의미적으로 반복되거나, 긴 문장에서 일관성을 잃는 등의 문제가 있다. 또한 &amp;ldquo;common sense physics&amp;quot;과 같은 분야에서 어려움을 겪는 것으로 보였다. GPT-3의 문맥 중심 학습 성능은 &amp;ldquo;comparison&amp;rdquo; 작업이나 읽기 이해 작업 등에서 뚜렷한 차이를 보였다. 이는 GPT-3이 다른 많은 작업에서 강력한 성능을 보이고 있음에도 불구하고 눈에 띈다.&lt;/p>
&lt;p>GPT-3의 한계는 그 구조와 알고리즘에 기인한다. 이는 양방향 아키텍처나 노이즈 제거와 같은 훈련 목표를 포함하지 않는 실험 설계 때문이다. 이로 인해 GPT-3는 빈칸 채우기 작업, 두 내용을 비교하는 작업, 긴 구절을 신중히 고려한 후 짧은 답변을 생성하는 작업 등에서 성능이 떨어졌다. 이런 원인으로 GPT-3는 WIC, ANLI, QuAC 및 RACE와 같은 몇몇 작업에서 뒤떨어지는 성능을 보였다. 큰 규모의 양방향 모델을 만들거나, 양방향 모델을 몇 번이나 한 번도 시도하지 않는 학습과 함께 작동하게 하는 것은 미래의 연구 방향으로 유망히다.&lt;/p>
&lt;p>이 논문에서 설명하는 방법론의 근본적인 제한은 모든 토큰을 동등하게 취급하고 중요한 예측과 그렇지 않은 예측을 구별하지 못한다는 점이다. 또한, 언어 시스템은 단순히 예측을 만드는 것이 아니라 목표 지향적인 행동을 취해야 하며, 대규모 언어 모델은 다른 경험 영역에 기반을 두지 않아 세계에 대한 많은 맥락을 부족하게 한다. 이러한 이유로, self-supervised 예측의 확장은 한계에 도달하고, 다른 접근법으로 보완해야 한다. 이를 위해 인간으로부터 목표 함수를 학습하거나, 강화 학습으로 미세 조정하거나, 이미지 등의 추가적인 모달리티를 추가하는 방향이 유망해 보인다.&lt;/p>
&lt;p>언어 모델의 주요 제한 중 하나는 사전 학습 단계에서의 샘플 효율성이 낮다는 것이다. GPT-3는 테스트 시간에 인간과 가까운 샘플 효율성을 보이지만, 사전 학습 과정에서 인간이 평생 동안 접하는 텍스트보다 훨씬 많은 텍스트를 본다는 문제가 있다. 사전 학습의 샘플 효율성을 개선하는 것은 미래의 연구 방향으로, 물리적 세계에 기반을 두는 것이나 알고리즘의 개선을 통해 이루어질 수 있다.&lt;/p>
&lt;p>GPT-3의 few-shot 학습에서의 한 제한은, 실제로 추론 시에 새로운 작업을 &amp;ldquo;처음부터(from scratch)&amp;rdquo; 학습하는지, 아니면 학습 도중 배운 작업을 단순히 인식하고 식별하는지에 대한 불확실성이다. 이는 테스트 시간에 정확히 동일한 분포에서 작업을 인식하거나, 같은 작업을 다른 형식으로 인식하거나, QA 같은 일반적인 작업 스타일에 적응하거나, 완전히 새로운 기술을 배우는 것 등, 넓은 범위에 걸쳐 있다. 어떤 작업에서는 새롭게 배우는 경향이 있고, 다른 작업에서는 사전 학습 동안에 배워야 하는 상황도 있다. 결국 인간이 무엇을 처음부터 배우는지, 무엇을 이전의 경험으로부터 배우는지조차 확실하지 않다. 이러한 이해의 불확실성은 few-shot 학습의 원리를 정확히 파악하는 데 중요한 미래의 연구 방향을 제시한다.&lt;/p>
&lt;p>GPT-3와 같은 대규모 모델의 한계는, 추론을 수행하는데 비용이 많이 들고 불편하다는 점이다. 이는 이러한 크기의 모델의 실질적인 적용을 어렵게 만든다. 이 문제를 해결할 수 있는 한 가지 방법은, 대규모 모델을 특정 작업에 맞게 관리 가능한 크기로 축소하는 것이다. 이는 아직 수백억 개의 매개변수 규모에서 시도되지 않았지만, 새로운 도전과 기회를 제공할 수 있다.&lt;/p>
&lt;p>GPT-3는 대부분의 딥러닝 시스템과 마찬가지로 결정의 해석이 어렵고, 새로운 입력에 대한 예측이 반드시 잘 조정되지 않으며, 학습 데이터의 편향을 유지하는 등의 한계를 가지고 있다. 특히, 학습 데이터의 편향이 모델이 편견 있는 내용을 생성하도록 이끌 수 있는 문제는 사회적 관점에서 큰 우려사항이다.&lt;/p>
&lt;h2 id="broader-impacts">Broader Impacts&lt;/h2>
&lt;p>언어 모델은 자동 완성, 문법 도움 등의 다양한 이점을 제공하지만, 잠재적으로 해로운 응용 분야도 있다. GPT-3는 텍스트 생성의 품질을 향상시키고, 합성 텍스트와 인간이 쓴 텍스트를 구별하는 어려움을 증가시키므로, 언어 모델의 좋은 사용과 나쁜 사용을 모두 발전시킬 수 있다.&lt;/p>
&lt;p>해로움을 연구하고 완화하기 위한 노력을 자극하기 위해서 향상된 언어 모델의 잠재적인 해로움에 초점을 맞추면, 주요 문제는 GPT-3와 같은 언어 모델의 고의적인 오용 가능성과 모델 내의 편향, 공정성, 표현 문제입니다.&lt;/p>
&lt;h3 id="misuse-of-language-models">Misuse of Language Models&lt;/h3>
&lt;p>언어 모델의 악의적인 사용은 모델을 원래 의도와 다른 환경이나 목적으로 재사용하는 경우가 많아 예상하기 어렵다. 이를 위해 위협과 잠재적 영향을 식별하고, 위험성을 평가하는 보안 위험 평가 프레임워크를 사용한다.&lt;/p>
&lt;h4 id="potential-misuse-applications">Potential Misuse Applications&lt;/h4>
&lt;p>텍스트 생성에 의존하는 모든 사회적인 해로운 활동은 강력한 언어 모델로 인해 강화될 수 있다. 오해, 스팸, 피싱, 법적 남용, 부정한 학술 작성 등이 예시이다. 고품질의 텍스트 생성을 할 수 있는 언어 모델은 이런 활동의 장벽을 낮추고 효과를 높일 수 있다.&lt;/p>
&lt;p>텍스트 합성의 품질이 향상됨에 따라 언어 모델의 오용 가능성이 증가한다. GPT-3가 사람이 쓴 것과 구별하기 어려운 텍스트를 생성하는 능력은 이에 대한 우려를 높인다.&lt;/p>
&lt;h4 id="threat-actor-analysis">Threat Actor Analysis&lt;/h4>
&lt;p>위협 행위자는 기술과 자원 수준에 따라 분류된다. 이는 악의적 제품을 만들 수 있는 낮은 기술력을 가진 행위자부터 장기적인 목표를 가진 국가 후원의 고도로 기술화된 그룹까지 다양하다.&lt;/p>
&lt;p>오해 전략, 악성 소프트웨어 배포, 컴퓨터 사기 등이 논의되는 포럼을 모니터링하여 저수준 및 중간 수준의 행위자들이 언어 모델에 대해 어떻게 생각하는지 파악하고 있다. 2019년 GPT-2의 처음 출시 이후 오용에 대한 논의가 있었지만, 그 이후로는 실험적인 사례가 줄었고, 성공적인 배포는 없었다. 이러한 오용 논의는 언어 모델 기술의 미디어 보도와 관련이 있었다. 이러한 행위자들로부터의 즉각적인 오용 위협은 없지만, 신뢰성이 크게 향상되면 상황이 바뀔 수 있다.&lt;/p>
&lt;p>APT들은 보통 공개적으로 작전을 논의하지 않기 때문에, 전문 위협 분석가들과 상의하였다. GPT-2 출시 이후, 언어 모델을 사용하여 이익을 볼 수 있는 작전에서 눈에 띄는 변화는 없었다. 현재의 언어 모델이 텍스트 생성에 있어 훨씬 뛰어나다는 설득력 있는 증거가 없으며, 모델의 내용을 &amp;ldquo;targeting&amp;quot;하거나 &amp;ldquo;controlling&amp;quot;하는 방법이 초기 단계에 있기 때문에, 언어 모델에 많은 자원을 투자하는 것은 가치가 없다는 평가를 받았다.&lt;/p>
&lt;h4 id="external-incentive-structures">External Incentive Structures&lt;/h4>
&lt;p>각각의 위협 행위자 그룹은 그들의 목표를 달성하기 위해 전략, 기술, 절차(TTPs)를 사용한다. 이는 확장성과 배포의 용이성 등 경제적 요인에 의해 영향을 받는다. 피싱은 낮은 비용, 적은 노력, 높은 수익률로 악성 소프트웨어를 배포하고 로그인 정보를 훔칠 수 있기 때문에 모든 그룹에서 매우 인기가 있다. 언어 모델을 사용하여 기존의 TTPs를 보완하면 배포 비용이 더욱 줄어들 것으로 예상된다.&lt;/p>
&lt;p>사용의 용이성은 TTPs 채택에 큰 영향을 미친다. 언어 모델의 출력은 확률적이고, 인간의 피드백 없이는 일관된 성능을 내기 어렵다. 만약 사회적 미디어의 허위 정보 봇이 대부분의 시간 동안 신뢰할 수 있는 출력을 생성하지만 가끔 비일관적인 출력을 생성한다면, 이 봇을 운영하는 데 필요한 인간의 노동량을 줄일 수 있다. 그러나 출력을 필터링하기 위해 인간이 여전히 필요하므로, 작업의 확장성은 제한된다.&lt;/p>
&lt;h3 id="fairness-bias-and-representation">Fairness, Bias, and Representation&lt;/h3>
&lt;p>학습 데이터의 편향으로 인해 모델은 편견이나 스테레오타입을 생성할 수 있다. 이는 기존 스테레오타입을 강화하고, 불온한 묘사를 생성하는 등의 방식으로 특정 그룹에 해를 끼칠 수 있다. 그래서 우리는 GPT-3의 공정성, 편향, 대표성에 대한 한계를 이해하기 위해 편향 분석을 수행하였다.&lt;/p>
&lt;p>목표는 GPT-3의 완전한 특성화가 아니라, 그 한계와 행동에 대한 초기 분석을 제공하는 것이다. 성별, 인종, 종교 등의 편향에 초점을 맞추고 있지만, 다른 카테고리의 편향도 존재하며 이는 후속 연구에서 다루어질 수 있다. 이는 초기 분석이며 모델의 모든 편향을 반영하지는 않는다.&lt;/p>
&lt;p>이 논문의 분석은 인터넷에서 학습된 모델이 인터넷 규모의 편향을 가지고 있다는 것을 나타낸다. 모델은 학습 데이터의 스테레오타입을 반영하는 경향이 있다. 성별, 인종, 종교 등의 편향을 찾아내기 위해 175B parameter 모델과 작은 모델을 분석하였다.&lt;/p>
&lt;h4 id="gender">Gender&lt;/h4>
&lt;p>GPT-3에서 성별 편향 조사는 성별과 직업 사이의 연관성에 집중했다. 대부분의 직업은 남성 식별자가 뒤따르는 확률이 더 높았으며, 이는 높은 학력을 요구하는 직업이나 신체적 노동을 요구하는 직업에서 특히 두드러졌다. 반면, 여성 식별자가 뒤따르는 확률이 더 높은 직업은 산모 도우미, 간호사, 리셉션니스트, 가정부 등이었다.&lt;/p>
&lt;p>&amp;ldquo;&amp;ldquo;The competent { occupation } was a&amp;quot;이나 &amp;ldquo;The incompetent { occupation } was a&amp;quot;이라는 맥락으로 바뀌었을 때, 대부분의 직업은 여전히 남성 식별자를 더 높은 확률로 따르는 경향이 있었다. 이는 원래의 중립적 프롬프트와 비교했을 때도 마찬가지였다. 평균 직업 편향은 중립 변형에서 -1.11, 유능한 변형에서 -2.14, 무능한 변형에서 -1.15로 측정되었다.&lt;/p>
&lt;p>Winogender 데이터셋에서 대명사 해결을 수행하여 대부분의 직업을 남성과 연관짓는 모델의 경향성을 입증하였다. 예를 들어, &amp;ldquo;The advisor met with the advisee because she wanted to get advice about job applications. ‘She’ refers to the&amp;quot;와 같은 맥락을 제공하고, &amp;lsquo;advisor&amp;rsquo;와 &amp;lsquo;advisee&amp;rsquo; 중 어느 쪽을 &amp;lsquo;she&amp;rsquo;로 가장 적합하게 할당하는지를 측정하였다.&lt;/p>
&lt;p>언어 모델이 사회적 편향, 예를 들어 여성 대명사를 참여자 위치와 더 많이 연관짓는 경향 등을 학습했음을 발견하였다. GPT-3 175B 모델은 이 작업에서 가장 높은 정확도(64.17%)를 보였고, 이는 편향 문제가 언어 모델을 오류에 취약하게 만들 수 있는 곳에서, 큰 모델이 작은 모델보다 더 강건하다는 초기적인 증거를 제공한다.&lt;/p>
&lt;p>단어들이 어떤 단어와 같이 나타나는지 분석하는 공존 테스트를 수행하였다. 이를 위해 데이터셋의 모든 프롬프트에 대해 여러번의 출력을 생성하여 샘플 세트를 만들었다. 성별에 대한 분석에서 여성은 &amp;ldquo;beautiful&amp;quot;과 &amp;ldquo;gorgeous&amp;quot;와 같은 외모 지향적인 단어로 더 자주 묘사되었으며, 반면에 남성은 더 다양한 형용사로 묘사되었다는 것을 발견하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/table6.1.png"
width="1268"
height="474"
srcset="https://kurtkim.github.io/p/gpt-3/images/table6.1_huf876a6fd221b608902ba5ef0f70ab308_102346_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/table6.1_huf876a6fd221b608902ba5ef0f70ab308_102346_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="267"
data-flex-basis="642px"
>&lt;/p>
&lt;p>모델에서 가장 선호하는 상위 10개의 형용사와 이들이 대명사 지시어와 얼마나 자주 함께 나타나는지를 보여준다. &amp;ldquo;Most Favored&amp;quot;은 한 카테고리와 비교해 다른 카테고리와 더 자주 함께 나타나는 단어를 의미한다. 이를 이해하기 쉽게 하기 위해, 각 성별에 대한 모든 단어들의 공존 횟수의 평균도 제시되었다.&lt;/p>
&lt;h4 id="race">Race&lt;/h4>
&lt;p>GPT-3의 인종 편향을 조사하기 위해, 특정 인종을 나타내는 용어로 대체된 특정 프롬프트를 사용하여 샘플을 생성하고, 생성된 샘플에서 단어의 공존을 측정하였다. 이전 연구가 언어 모델이 다른 특징에 따라 다른 감정의 텍스트를 생성한다는 것을 보여준 것을 바탕으로, 인종이 어떻게 감정에 영향을 미치는지를 조사했다. 각 인종과 과도하게 공존하는 단어의 감정은 Senti WordNet을 사용하여 측정하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-3/images/figure6.1.png"
width="688"
height="498"
srcset="https://kurtkim.github.io/p/gpt-3/images/figure6.1_hu84badc25f627ac23d8237755c126dbaa_103946_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-3/images/figure6.1_hu84badc25f627ac23d8237755c126dbaa_103946_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>모델 전반에 걸쳐 &amp;ldquo;아시아인&amp;quot;은 일관되게 높은 감정을 가지고 있었으며, 7개의 모델 중 3개에서 1위를 차지하였다. 반면에 &amp;ldquo;흑인&amp;quot;은 일관되게 낮은 감정을 가지고 있었으며, 7개의 모델 중 5개에서 최하위를 차지하였다. 이런 차이는 큰 모델 크기에서 약간 줄어들었다. 이 분석은 다양한 모델의 편향성을 보여주며, 감정, 엔티티, 입력 데이터 간의 관계에 대한 보다 세밀한 분석의 필요성을 강조한다.&lt;/p>
&lt;h4 id="religion">Religion&lt;/h4>
&lt;p>무신론, 불교, 기독교, 힌두교, 이슬람교, 유대교와 같은 종교 용어와 함께 나타나는 단어를 연구하기 위해, 각각의 종교에 대한 특정 프롬프트에 대해 800개의 모델 출력을 생성했다. 이 프롬프트는 &amp;ldquo;{ Religion practitioners } are&amp;rdquo; (Eg. &amp;ldquo;Christians are&amp;rdquo;)과 같은 형태였다. 그런 다음 모델이 자연스럽게 문장을 완성하도록 하여, 단어의 공존을 연구하는 말뭉치를 생성하였다.&lt;/p>
&lt;p>모델이 종교 용어와의 연관성을 나타내는 방식은 종종 이 용어들이 실제 세상에서 어떻게 표현되는지를 반영한다는 것을 확인하였다. 예를 들어, 이슬람교와 관련하여 &amp;lsquo;라마단&amp;rsquo;, &amp;lsquo;예언자&amp;rsquo;, &amp;lsquo;모스크&amp;rsquo;와 같은 단어들이 다른 종교보다 더 자주 등장하며, &amp;lsquo;폭력적인&amp;rsquo;, &amp;lsquo;테러리즘&amp;rsquo;, &amp;lsquo;테러리스트&amp;rsquo; 등의 단어는 이슬람교와 더 크게 연관되어 GPT-3에서 이슬람교에 대한 상위 40개 선호 단어에 포함되었다.&lt;/p>
&lt;h4 id="future-bias-and-fairness-challenges">Future Bias and Fairness Challenges&lt;/h4>
&lt;p>이 초기 분석을 통해 발견된 편향을 공유하고, 대규모 생성 모델의 편향을 파악하는 본질적인 어려움을 강조하며, 추가 연구를 촉진하고자 한다. 이는 지속적인 연구 영역이 될 것으로 기대하며, 성별, 인종, 종교를 연구의 시작점으로 설정했음을 밝혔다. 이런 선택에는 주관성이 내재해 있는 것을 인지하고 있다.&lt;/p>
&lt;p>언어 시스템의 편향을 파악하는 것뿐만 아니라 개입하는 것이 중요하며, 이를 위해선 편향 완화에 대한 공통 어휘 구축이 필요하다. 더 많은 연구가 필요하며, 이는 NLP 외부의 문헌과의 연계, 해를 끼치는 규범적 진술의 명확한 표현, 그리고 NLP 시스템에 영향을 받는 커뮤니티의 실제 경험에 대한 관여를 포함해야 한다. 편향 완화 작업은 단순히 &amp;ldquo;편향 제거&amp;quot;를 목표로 하는 것이 아니라, 전체적인 방식으로 접근해야 한다.&lt;/p>
&lt;h3 id="energy-usage">Energy Usage&lt;/h3>
&lt;p>대규모 사전 학습은 에너지 집약적이며, GPT-3 175B 훈련은 1.5B 파라미터의 GPT-2 모델에 비해 많은 계산을 소비했다. 따라서 이러한 모델의 비용과 효율성을 인식하는 것이 중요하다.&lt;/p>
&lt;p>대규모 사전 학습의 사용은 모델의 효율성을 다루는 새로운 관점을 제공한다. 이는 학습에 필요한 자원뿐만 아니라, 모델의 수명 동안 이러한 자원이 어떻게 분산되는지를 고려해야 한다. 학습 중에는 많은 자원을 소비하지만, 학습이 완료된 모델은 효율적이다. 또한, 모델 증류와 같은 기법을 사용하면 이러한 모델의 비용을 더욱 줄일 수 있으며, 알고리즘의 발전은 시간이 지남에 따라 이러한 모델의 효율성을 자연스럽게 더욱 증가시킬 수 있다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>언어 모델의 성능을 향상시키기 위한 연구는 크게 세 가지 방향으로 진행되었다. 첫 번째는 parameter와 계산량을 함께 증가시키는 것으로, 이는 모델 크기를 계속해서 증가시키는 방식이다. 두 번째는 parameter 수는 늘리지만 계산량은 늘리지 않는 방식으로, 이는 모델의 정보 저장 용량을 늘리는 데 초점을 맞추었다. 세 번째는 parameter를 증가시키지 않고 계산을 증가시키는 방식이다. GPT-3 연구는 첫 번째 접근 방식에 초점을 맞추고, 이전 모델보다 10배 큰 모델을 개발하였다.&lt;/p>
&lt;p>언어 모델 성능에 대한 규모의 영향을 체계적으로 연구한 여러 연구에서는 모델이 확장됨에 따라 손실률에서 부드러운 멱법칙 추세를 발견하였다. 이 추세는 모델이 계속 확장됨에 따라 대체로 계속될 것으로 보이며, 다양한 downstream task 에서도 규모가 커짐에 따라 부드러운 성능 향상이 관찰되었다.&lt;/p>
&lt;p>반대 방향의 연구는 가능한 한 작은 언어 모델에서도 강한 성능을 유지하려는 시도이다. 이에는 ALBERT와 언어 모델의 축소에 대한 일반적이고 특정한 접근법이 포함되어 있다. 이런 기술은 GPT-3 연구와 보완적일 수 있으며, 큰 모델의 처리 시간과 메모리 사용량을 줄일 수 있다.&lt;/p>
&lt;p>미세조정된 언어 모델이 많은 벤치마크 작업에서 인간 수준의 성능에 근접하면서, 더 어려운 혹은 개방형 작업을 구성하는 데 많은 노력이 기울여져 왔다. 이런 작업들에는 질문 응답, 읽기 이해, 그리고 기존 언어 모델에게 어려운 데이터셋을 고의로 만드는 것이 포함된다.&lt;/p>
&lt;p>많은 연구들이 질문-응답에 집중했으며, 이는 테스트한 작업들 중 상당 부분을 차지한다. 최근의 연구로는 11B 개의 매개변수를 가진 언어 모델을 미세조정한 연구와, 테스트 시점에 대량의 데이터에 집중하는 연구가 있다. GPT-3는 문맥 내 학습에 중점을 두는 것이 특징이며, 이는 미래에 다른 연구와 결합될 수 있다.&lt;/p>
&lt;p>언어 모델의 메타러닝은 이전 연구에서 활용되었지만, 그 결과는 제한적이었다. 언어 모델 메타러닝은 내부 루프와 외부 루프의 구조를 가지고 있으며, 이는 일반적인 머신러닝에 적용된 메타러닝과 유사하다. GPT-3는 모델의 문맥을 이전 예제로 채우는 것으로, 이는 모델의 활성화를 통해 적응하는 내부 루프와 가중치를 업데이트하는 외부 루프를 가지고 있다. 또한, few-shot auto-regressive density estimation과 low-resource NMT를 few-shot 학습 문제로 연구한 사례도 있다.&lt;/p>
&lt;p>이전 연구들도 사전 학습된 언어 모델과 경사 하강법을 결합하여 few-shot 학습을 할 방법을 탐색했다. 또한, 레이블이 거의 없는 상황에서의 미세조정 방법을 연구하는 반지도 학습과 같은 비슷한 목표를 가진 분야도 있다.&lt;/p>
&lt;p>자연 언어로 다중 과제 모델에 지시를 하는 방법은 처음으로 지도 학습 환경에서 공식화되었고, 일부 과제에 언어 모델에 사용되었다. 이와 유사한 개념이 text-to-text transformer에서도 탐색되었지만, 이 경우에는 문맥 학습이 아닌 다중 과제 미세조정에 적용되었다.&lt;/p>
&lt;p>다중 과제 학습은 언어 모델의 일반성과 전이 학습 능력을 향상시키는 방법으로, 여러 과제를 함께 미세조정하며 가중치를 업데이트한다. 이 방법은 단일 모델을 가중치 업데이트 없이 다양한 과제에 사용하거나, 새로운 과제에 대한 가중치 업데이트 시 샘플 효율성을 향상시킬 수 있다. 다중 과제 학습은 초기에는 좋은 결과를 보였지만, 데이터셋 구성과 훈련 커리큘럼 설정에 대한 수작업이 필요한 한계가 있다. 하지만 대규모 사전 학습은 텍스트 예측을 통해 암시적으로 다양한 과제를 포함하는 방법을 제공한다. 미래의 연구 방향은 다중 과제 학습에 대해 더 넓은 범위의 명시적 과제를 생성하는 것일 수 있다.&lt;/p>
&lt;p>지난 두 해 동안 언어 모델의 알고리즘은 매우 크게 발전했다. denoising-based bidirectionality, preﬁxLM and encoder-decoder architectures, random permutations during training, architectures that improve the efﬁciency of sampling, improvements in data and training procedures, and efﬁciency increases in the embedding parameters 등이 포함된다. 이런 기술들은 downstream task에서 큰 이익을 가져다주며, 이러한 알고리즘 발전을 GPT-3에 통합하면 downstream task 성능이 향상될 가능성이 높다. GPT-3의 규모와 이런 알고리즘 기법을 결합하는 것은 미래 연구의 유망한 방향이다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>175B 개의 parameter를 가진 언어 모델을 소개하였고, 이 모델은 다양한 NLP 작업에서 강력한 성능을 발휘하며, 또한 고품질의 샘플을 생성하며, 미세 조정 없이도 성능의 확장성이 대략 예측 가능하다는 것을 보여주었다. 그리고 이 모델의 사회적 영향에 대해서도 논의하였다. 이러한 결과들은 큰 언어 모델이 적응형, 일반 언어 시스템의 개발에 중요한 요소가 될 수 있음을 시사한다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/gpt-3" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ELECTRA</title><link>https://kurtkim.github.io/p/electra/</link><pubDate>Fri, 12 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/electra/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>BERT와 같은 masked language modeling(MLM) 방법은 일부 토큰을 [MASK]로 바꿔 원래 토큰을 재구성하는 모델을 훈련하지만, 이를 효과적으로 하려면 많은 계산이 필요하다. 대신에, replaced token detection 라는 더 효율적인 사전 학습 작업을 제안한다. 이 방법은 일부 토큰을 가능성 있는 다른 토큰으로 바꾸는 방식으로 입력을 변형하고, 변형된 토큰의 원래 값을 예측하는 대신 각 토큰이 생성자 샘플로 대체되었는지 여부를 예측하는 모델을 학습시킨다. 이 방법은 모든 입력 토큰에 대해 작업을 정의하므로 MLM보다 효율적이다. 결과적으로, 이 방식으로 학습한 컨텍스트 표현은 동일한 모델 크기, 데이터, 계산량을 가진 BERT보다 월등히 높은 성능을 보여준다. 특히, 작은 모델에서는 GLUE 자연어 이해 벤치마크에서 GPT를 능가하는 모델을 한 GPU에서 4일 동안 훈련할 수 있다. 대규모에서도 이 방법은 RoBERTa와 XLNet의 성능에 비교할 수 있으며, 더 적은 계산량을 사용하면서도 동일한 계산량을 사용할 때 그 모델들을 능가한다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>현재 state-of-the-art의 언어 표현 학습 방법은 denoising autoencoder이다. 이 방법은 일부 토큰을 마스킹하거나 어텐션을 적용하여 네트워크가 원래 입력을 복원하도록 학습한다. 하지만 이러한 masked language modeling (MLM)은 효과적이지만 상당한 계산 비용을 요구한다.&lt;/p>
&lt;p>&amp;ldquo;replaced token detection&amp;quot;는 BERT와 XLNet의 불일치 문제를 해결하기 위한 사전 학습 작업이다. 이 방법은 일부 토큰을 대체하여 입력을 손상시키는데, 이를 구분하는 네트워크를 판별자로 사전 학습시킨다. 이 방식은 MLM과 달리 모든 입력 토큰에서 학습하므로 계산적으로 효율적이다. 또한 GAN과 유사하지만 텍스트에 GAN을 적용하기 어려워 maximum likelihood로 손상된 토큰을 생성하는 생성자를 학습시킨다.&lt;/p>
&lt;p>ELECTRA는 효율적으로 토큰 대체를 정확하게 분류하는 encoder를 학습하는 접근 방식이다. BERT와 비교하여 ELECTRA는 모든 입력 위치에서 학습하며, 학습 속도와 downstream task에서의 정확도에서 우수한 성능을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/figure1.png"
width="1078"
height="492"
srcset="https://kurtkim.github.io/p/electra/images/figure1_hub5e22e6e60873ae3e2aa86fbd6b7dd0d_98468_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/figure1_hub5e22e6e60873ae3e2aa86fbd6b7dd0d_98468_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="525px"
>&lt;/p>
&lt;p>ELECTRA는 다른 사전 학습 방법에 비해 계산 및 parameter를 더 효율적으로 사용하여 더 나은 성능을 보여준다. GLUE 벤치마크에서, ELECTRA-Small 모델은 BERT보다 5점 더 높은 성능을 보여주었고, ELECTRA-Large 모델은 RoBERTa와 XLNet과 비슷한 성능을 보이지만 훨씬 적은 계산량을 사용한다. ELECTRA-Large는 GLUE에서 ALBERT보다 우수한 결과를 얻었고, SQuAD 2.0에서 새로운 state-of-the-art를 달성하였다. ELECTRA는 언어 표현 학습에 있어서 계산 및 parameter 효율성이 뛰어난 판별적인 방법이다.&lt;/p>
&lt;h2 id="method">Method&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/figure2.png"
width="1040"
height="290"
srcset="https://kurtkim.github.io/p/electra/images/figure2_hu2de35ae22d69441a87566217b479d28e_57047_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/figure2_hu2de35ae22d69441a87566217b479d28e_57047_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="358"
data-flex-basis="860px"
>&lt;/p>
&lt;p>생성자 $G$와 판별자 $D$라는 두 개의 신경망을 사용한다. 이들은 입력 토큰 시퀀스 $x = [x_1, &amp;hellip;, x_n]$$를 문맥화된 벡터 표현의 시퀀스 $h(x) = [h_1, &amp;hellip;, h_n]$로 매핑한다. 주어진 위치 $t$에 대해, 생성자는 특정 토큰 $x_t$를 생성할 확률을 출력한다.&lt;/p>
&lt;p>$$ p_G(x_t|x) = exp(e(x_t)^\intercal h_G(x)_t) \ / \sum_{x&amp;rsquo;} exp(e(x&amp;rsquo;)^\intercal h_G(x)_t) $$&lt;/p>
&lt;p>$e$는 토큰 임베딩을 나타낸다. 주어진 위치 $t$에 대해, 판별자는 토큰 $x_t$가 실제 데이터에서 나온 것인지를 예측한다.&lt;/p>
&lt;p>$$ D(x, t) = sigmoid(w^\intercal h_D (x)_t) $$&lt;/p>
&lt;p>생성자는 masked language modeling(MLM)을 수행하기 위해 학습된다. 입력 $x = [x_1, x_2, &amp;hellip;, x_n]$이 주어지면, MLM은 우선 1부터 n 사이의 임의의 위치를 선택하여 $m = [m_1, &amp;hellip;, m_k]$를 마스크한다. 선택된 위치의 토큰은 [MASK] 토큰으로 대체된다. 이를 $x^{masked} = REPLACE(x, m, [MASK])$로 표기한다. 그런 다음 생성자는 마스크된 토큰의 원래 식별자를 예측하는 방법을 학습한다. 판별자는 데이터의 토큰과 생성자 샘플에 의해 대체된 토큰을 구별하는 방법을 학습한다. 구체적으로는, 마스크된 토큰을 생성자 샘플로 대체하여 손상된 예제 $x^{corrupt}를 생성하고, 판별자는 $x^{corrupt}에서 원래 입력 $x$와 일치하는 토큰을 예측하도록 학습된ㄴ다. 형식적으로, 모델의 입력은 다음과 같이 구성된다:&lt;/p>
&lt;p>$$ m_i \sim unif\{1, n \} \ \text{for} i = 1 \ \text{to} \ k \quad \quad x^{masked} = REPLACE(x, m, [MASK]) $$
$$ \hat{x}_i \sim p_G(x_i | x^{masked}) \ \text{for} i \in m \quad \quad x^{corrupt} = REPLACE(x, m, \hat{x}) $$&lt;/p>
&lt;p>손실 함수는 다음과 같다.&lt;/p>
&lt;p>$$ \mathbf{L}_{MLM}(x, \theta_G) = \mathbb{E} \big( \sum_{i \in m} - log \ p_G(x_i | x^{masked}) \big) $$&lt;/p>
&lt;p>$$ \mathbf{L}_{Disc}(x, \theta_D) = \mathbb{E} \big( \sum_{t=1}^n - \mathbb{1} (x^{corrupt}_t = x_t) log \ D(x^{corrupt}, t) - \mathbb{1} (x^{corrupt}_t \neq x_t) log(1 - D(x^{corrupt}, t)) $$&lt;/p>
&lt;p>GAN과 유사하지만 몇 가지 주요한 차이점이 있다. 생성자가 올바른 토큰을 생성하면 &amp;ldquo;진짜&amp;quot;로 간주되며, 적대적으로 훈련되는 대신 maximum likelihood로 학습된다. 일반적인 GAN과 달리 생성자에게 입력으로 노이즈 벡터를 제공하지 않는다.&lt;/p>
&lt;p>결합된 손실을 최소화한다.&lt;/p>
&lt;p>$$ \underset{\theta_G, \theta_D}{min} \sum_{x \in \mathbf{X}} \mathbf{L}_{MLM}(x, \theta_G) + \lambda \mathbf{L}_{Disc}(x, \theta_D) $$&lt;/p>
&lt;p>대규모의 원시 텍스트 데이터에 대해 결합 손실을 최소화한다. 손실의 기대값을 단일 샘플로 근사화하고, 생성자의 손실은 판별자로 역전파하지 않는다. 사전 학습 후에는 생성자를 버리고 판별자를 downstream task에서 세밀하게 조정한다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="experimental-setup">Experimental Setup&lt;/h3>
&lt;p>GLUE 벤치마크와 SQuAD 데이터셋을 사용하여 다양한 언어 이해 작업을 평가한다. GLUE 작업은 텍스트 간 연역, 질문-답변 연역, 문장 재구성, 질문 재구성, 텍스트 유사도, 감성 분석, 문장 수용성 등을 다루며, SQuAD는 질문에 대한 정확한 답변을 선택하는 작업이다. 사전 학습은 BERT 데이터를 사용하고, 일부 모델은 XLNet 데이터를 사용한다. 사전 학습과 평가는 주로 영어 데이터를 기반으로 하지만, 향후 다국어 데이터에도 적용할 수 있다.&lt;/p>
&lt;p>ELECTRA는 BERT와 비슷한 구조와 hyperparameter를 가지고 있다. GLUE에 대한 미세조정에서는 ELECTRA 위에 단순한 선형 분류기를 추가하고, SQuAD에 대해서는 ELECTRA 위에 XLNet의 질문-답변 모듈을 추가하였다. 결과는 동일한 사전 훈련 체크포인트에서 10번의 파인튜닝 실행의 중앙값을 사용한다.&lt;/p>
&lt;h3 id="model-extensions">Model Extensions&lt;/h3>
&lt;p>모델에 여러 확장 기법을 제안하고 평가한다. 다른 명시가 없는 한, 이 실험들은 BERT-Base와 동일한 모델 크기와 훈련 데이터를 사용한다.&lt;/p>
&lt;p>&lt;strong>Weight Sharing&lt;/strong> 생성자와 판별자 사이의 가중치를 공유하여 사전 학습의 효율성을 높이는 것을 제안한다. 작은 생성자를 사용하고, 생성자와 판별자의 임베딩을 공유한다. 생성자의 입력과 출력 토큰 임베딩은 항상 연결된다.&lt;/p>
&lt;p>500k step 동안 생성자와 판별자의 가중치 공유 전략을 비교한 결과, 토큰 임베딩을 공유하는 것이 가장 효과적이었다. 이는 masked language modeling이 토큰 임베딩을 학습하는 데 도움이 되기 때문이다. encoder 가중치를 공유하는 것은 큰 향상을 가져오지 않았으며, 생성자와 판별자의 크기가 동일해야 한다는 단점이 있다.&lt;/p>
&lt;p>&lt;strong>Smaller Generators&lt;/strong> ELECTRA 학습 시, 생성자와 판별자가 동일한 크기라면 계산량이 두 배로 증가한다. 따라서 생성자 크기를 줄여야 한다. 실험 결과, 생성자는 판별자의 1/4에서 1/2 크기가 가장 좋은 성능을 보여주었다. 이는 생성자가 너무 강력하면 판별자의 학습을 방해할 수 있기 때문이다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/figure3.png"
width="1068"
height="404"
srcset="https://kurtkim.github.io/p/electra/images/figure3_hue64db30dd87fa6042472793cc96064cb_106744_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/figure3_hue64db30dd87fa6042472793cc96064cb_106744_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="634px"
>&lt;/p>
&lt;p>&lt;strong>Training Algorithms&lt;/strong> ELECTRA를 위해 다른 학습 알고리즘을 탐색했지만 성능을 향상시키지 못하였다. 대신 다음의 2 step 학습 절차를 사용하여 실험해 보았다.&lt;/p>
&lt;ul>
&lt;li>$n$ step 동안 $\mathbf{L}_{MLM}$을 사용하여 생성자만 학습한다.&lt;/li>
&lt;li>판별자의 가중치를 생성자의 가중치로 초기화한 다음, 생성자의 가중치를 고정한 상태에서 $n$ step 동안 $\mathbf{L}_{Disc}$로 판별자를 학습한다.&lt;/li>
&lt;/ul>
&lt;p>생성자와 판별자의 크기가 동일해야 초기화가 가능합니다. 초기화 없이 판별자를 학습하면 판별자가 주요 클래스 이외의 것을 학습하지 못할 수 있다. 반면, 생성자와 판별자를 함께 학습하면 판별자에게 생성자를 점차 개선시킬 수 있는 커리큘럼을 제공한다. 또한, 생성자를 GAN과 같이 적대적으로 훈련하고, 생성자에서 샘플링하는 이산 연산을 위해 강화학습을 사용하는 방법도 탐색하였다.&lt;/p>
&lt;p>2 step 학습에서 생성적 목적에서 판별적 목적으로 전환한 후에 downstream task 성능이 향상되었지만, 종합 훈련을 뛰어넘지는 못하였다. 적대적 훈련은 maximum-likelihood 학습보다 성능이 떨어지는데, 이는 적대적 생성자의 masked language modeling 성능이 낮고 다양성이 부족하기 때문이다.&lt;/p>
&lt;h3 id="small-models">Small Models&lt;/h3>
&lt;p>이 연구에서는 사전 학습의 효율성을 개선하기 위해 BERT-Base를 기반으로 작은 모델을 개발하였다. 이를 위해 시퀀스 길이, 배치 크기, 은닉 차원 크기, 토큰 임베딩을 줄였다. BERT-Small 모델과 동일한 hyperparameter로 학습하여 공정한 비교를 수행하였다. 또한, ELMo와 GPT와 같이 리소스를 적게 사용하는 사전 학습 방법과 BERT-Base와 비교 가능한 ELECTRA 모델의 결과도 확인하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/table1.png"
width="1074"
height="362"
srcset="https://kurtkim.github.io/p/electra/images/table1_huf394489a714e64b5cdeb9b728243e34e_107756_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/table1_huf394489a714e64b5cdeb9b728243e34e_107756_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="296"
data-flex-basis="712px"
>&lt;/p>
&lt;p>ELECTRA-Small은 크기에 비해 높은 GLUE 점수를 기록하여 다른 모델들보다 우수한 성능을 보여준다. 작은 모델이지만 BERT-Small보다 5점 높은 점수를 기록하며, 큰 GPT 모델보다도 우수한 결과를 보여준다. ELECTRA-Small은 수렴에 가깝게 학습되며, 6시간만 학습해도 합리적인 성능을 얻을 수 있다. 또한, ELECTRA의 중간 크기인 base-sized 모델은 BERT-Base보다 우수한 성능을 보이며, BERT-Large보다도 뛰어난 결과를 얻었다. ELECTRA의 강력한 성능은 상대적으로 적은 계산 비용으로 사전 학습 모델을 개발하고 적용하는 데 도움이 된다.&lt;/p>
&lt;h3 id="large-models">Large Models&lt;/h3>
&lt;p>큰 ELECTRA 모델은 BERT-Large와 같은 크기이지만 더 오랜 시간 동안 학습된다. ELECTRA-400K는 RoBERTa의 1/4의 계산 비용으로 학습되었으며, ELECTRA-1.75M은 RoBERTa와 유사한 계산 비용으로 학습되었다. 배치 크기는 2048이고, XLNet 사전 학습 데이터를 사용하였다. ELECTRA-400K와 동일한 hyperparameter와 학습 시간으로 BERT-Large 모델도 학습되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/table2.png"
width="1070"
height="282"
srcset="https://kurtkim.github.io/p/electra/images/table2_hu161f4080330e270ecfcc557d17544d41_86339_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/table2_hu161f4080330e270ecfcc557d17544d41_86339_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="379"
data-flex-basis="910px"
>&lt;/p>
&lt;p>ELECTRA-400K는 RoBERTa와 XLNet와 비슷한 성능을 보이지만, 학습에 필요한 계산량은 1/4로 줄어들었다. ELECTRA-1.75M은 더 오랜 시간을 투자하여 학습되었으며 대부분의 GLUE 작업에서 다른 모델들보다 우수한 성능을 보여준다. BERT 모델은 예상보다 성능이 낮게 나왔는데, 이는 hyperparameter 튜닝이나 RoBERTa 학습 데이터 사용에 더 많은 고려가 필요할 것으로 보인다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/table3.png"
width="1074"
height="224"
srcset="https://kurtkim.github.io/p/electra/images/table3_hu39a4c02d38d569be2cac67a555957da5_62895_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/table3_hu39a4c02d38d569be2cac67a555957da5_62895_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="479"
data-flex-basis="1150px"
>&lt;/p>
&lt;p>ELECTRA의 이점은 GLUE 테스트 세트에서도 확인되었으나, 모델들이 사용한 추가적인 기법들로 인해 완전히 동일한 비교는 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/table4.png"
width="1074"
height="440"
srcset="https://kurtkim.github.io/p/electra/images/table4_hu91a3b8a93592595d02ac0fe7dd7e1bf3_117245_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/table4_hu91a3b8a93592595d02ac0fe7dd7e1bf3_117245_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="585px"
>&lt;/p>
&lt;p>ELECTRA는 GLUE 결과와 일관성을 보여준다. 같은 계산 자원을 사용하는 경우, ELECTRA가 masked-language-modeling 방법보다 우수한 성과를 보여준다. ELECTRA-400K는 RoBERTa-100K와 BERT 기준 모델보다 더 좋은 성능을 가지며, ELECTRA-1.75M은 SQuAD 2.0에서 이전 모델들보다 높은 점수를 받았다. ELECTRA-Base는 BERT-Base와 XLNet-Base보다 우수한 결과를 보여주며, 대부분의 지표에서 BERT-Large를 뛰어넘는다. ELECTRA는 SQuAD 2.0에서 SQuAD 1.1보다 성능이 좋다. 이는 모델이 실제 토큰과 가짜 토큰을 구별하는 대체된 토큰 감지 기능이 응답 가능성 분류에 특히 적합하기 때문이다.&lt;/p>
&lt;h3 id="efficiency-analysis">Efficiency Analysis&lt;/h3>
&lt;p>ELECTRA의 이점이 어디에서 나오는지 이해하기 위해, BERT와 ELECTRA 사이에 다른 사전 학습 목표를 비교하였다.&lt;/p>
&lt;p>&lt;strong>ELECTRA 15%&lt;/strong> ELECTRA와 동일하지만, 판별자 손실은 입력에서 마스크 처리된 15%의 토큰에 대해서만 계산된다. 다시 말해서, 판별자 손실의 합인 $\mathbf{L}_{Disc}$은 1부터 $n$까지가 아닌 $i \in m$에 대해 계산된다. 이렇게 함으로써 ELECTRA 15% 모델은 일부 토큰에만 집중하여 손실을 계산하게 된다.&lt;/p>
&lt;p>&lt;strong>Replace MLM&lt;/strong> ELECTRA 모델이 [MASK] 토큰에 노출되는 사전 학습 단계와 노출되지 않는 미세 조정 단계 사이의 불일치를 해결하는 데서 어느 정도 이득을 얻는지를 테스트한다. 마스크 처리된 토큰을 [MASK]로 대체하는 대신 생성 모델의 토큰으로 대체하는 것이 특징이다.&lt;/p>
&lt;p>&lt;strong>All-Tokens MLM&lt;/strong> 이 모델은 Replace MLM과 비슷한데, 마스크 처리된 토큰을 생성 모델의 샘플로 대체하고, 입력의 모든 토큰의 신원을 예측한다. 복사 확률 $D$를 출력하기 위해 sigmoid layer를 사용하며, 모델 출력은 입력 토큰에 $D$ 가중치와 $1-D$를 곱한 MLM softmax의 출력으로 구성된다. 이 모델은 BERT와 ELECTRA를 조합한 것으로, [MASK] 토큰에 대한 예측과 다른 토큰에 대해서는 입력을 복사하는 방식으로 학습된다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/table5.png"
width="1072"
height="108"
srcset="https://kurtkim.github.io/p/electra/images/table5_huacb6fbffa014db7f95518a2b69ff6a39_20164_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/table5_huacb6fbffa014db7f95518a2b69ff6a39_20164_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="992"
data-flex-basis="2382px"
>&lt;/p>
&lt;p>ELECTRA는 모든 입력 토큰에 대한 손실을 가지는 것이 성능 향상에 큰 영향을 미치며, BERT는 [MASK] 토큰으로 인한 사전 학습 및 세부 학습 불일치로 인한 약간의 성능 저하가 있다. 또한, BERT는 이 문제를 완전히 해결하기에는 부족한 것으로 나타났고, All-Tokens MLM은 BERT와 ELECTRA 사이의 차이를 크게 줄이는 역할을 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/electra/images/figure4.png"
width="1004"
height="320"
srcset="https://kurtkim.github.io/p/electra/images/figure4_hub43d0214d334ff5424dee67fec78262a_73758_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/electra/images/figure4_hub43d0214d334ff5424dee67fec78262a_73758_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="313"
data-flex-basis="753px"
>&lt;/p>
&lt;p>ELECTRA는 빠른 학습뿐만 아니라 다른 이점들로 인해 All-Tokens MLM에 비해 개선되었다. 작은 모델일수록 ELECTRA의 이득이 커지며, 완전히 학습된 경우 BERT보다 더 높은 정확도를 보여준다. ELECTRA는 parameter를 더 효율적으로 사용하여 BERT보다 성능을 높일 수 있다. 그러나 ELECTRA의 parameter 효율성을 완전히 이해하기 위해서는 추가적인 분석이 필요하다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>&lt;strong>Self-Supervised Pre-training for NLP&lt;/strong> Self-supervised learning은 단어 및 문맥 표현을 학습하는 데 사용된다. BERT는 masked-language modeling 작업을 통해 Transformer를 사전 학습한다. BERT를 확장한 모델들이 있으며, ELECTRA는 BERT와 비교하여 효율적인 사전 학습을 제공한다. 최근에는 BERT를 작은 모델로 축소하는 연구도 진행되고 있다. ELECTRA-Small은 사전 학습 속도에 초점을 맞춰 처음부터 학습되었다.&lt;/p>
&lt;p>&lt;strong>Generative Adversarial Networks&lt;/strong> GAN은 고품질 가짜 데이터 생성에 효과적이며, 이 논문의 방법과 유사한 방식으로 GAN의 판별자를 후속 작업에 활용할 수 있다. 텍스트 데이터에도 GAN을 적용할 수 있지만, 최신 기법은 standard maximum-likelihood 학습에 비해 아직 성능이 낮다. 생성자는 MaskGAN과 유사하게 삭제된 토큰을 채우는 방식으로 학습된다.&lt;/p>
&lt;p>&lt;strong>Contrastive Learning&lt;/strong> contrastive learning은 관찰된 데이터와 가짜 음성 샘플을 구분하는 방법이다. 이 방법은 텍스트, 이미지, 비디오 데이터 등 다양한 모달리티에 적용될 수 있다. ELECTRA는 Noise-Contrastive Estimation (NCE)와 관련이 있는데, 이 방법은 실제와 가짜 데이터를 구별하기 위해 이진 분류기를 학습시킨다.&lt;/p>
&lt;p>Word2Vec은 NLP의 초기 사전 학습 방법 중 하나로 contrastive learning을 사용한ㄴ다. ELECTRA는 CBOW와 Negative Sampling의 대규모 버전으로 볼 수 있다. CBOW는 주변 문맥을 통해 입력 토큰을 예측하고, Negative Sampling은 이진 분류 작업으로 재구성한다. CBOW는 bag-of-vectors 인코더와 단순한 제안 분포를 사용한다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>대체 토큰 감지라는 새로운 자기 지도 학습 작업을 제안하였다. 이 작업은 텍스트 인코더를 학습시켜 입력 토큰과 고품질 부정 샘플을 구별하게 한다. 계산 효율성이 우수하며 하위 작업에서 더 좋은 성능을 보인다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2003.10555.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/electra" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>T5</title><link>https://kurtkim.github.io/p/t5/</link><pubDate>Wed, 10 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/t5/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>전이 학습은 데이터가 풍부한 작업에서 먼저 모델을 사전 훈련시킨 후, 이를 downstream task에 미세 조정하는 방식으로, 자연어 처리(NLP)에서 중요한 기법이다. 이 논문에서는 모든 텍스트 기반 언어 문제를 텍스트-텍스트 형식으로 변환하는 통합 프레임워크를 통해 NLP를 위한 전이 학습 기법을 탐색한다. 이 연구에서는 사전 훈련 목표, 아키텍처, 라벨이 없는 데이터셋, 전이 접근법 등 다양한 요소를 비교 분석하여, 요약, 질문 응답, 텍스트 분류 등 여러 분야에서 state-of-the-art를 달성하였다. 또한, 이러한 연구를 통한 데이터셋, 사전 훈련된 모델, 코드를 공개하여, NLP를 위한 전이 학습 연구를 더욱 촉진시키고자 한다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리(NLP) 머신러닝 모델 훈련은 모델이 텍스트를 이해하고 적절하게 처리하는 능력을 개발하는 것을 목표로 하며, 이는 단어의 철자와 의미부터 고수준의 지식까지 다양한 요소를 포함한다. 최근에는 데이터가 풍부한 작업에서 모델을 사전 학습하는 것이 일반적이며, 이를 통해 모델은 다양한 작업에 활용할 수 있는 일반적인 능력과 지식을 개발하게 된다. 특히 NLP에서는 레이블이 없는 대량의 텍스트 데이터를 이용한 비지도 학습으로 사전 학습이 진행되며, 이 방법은 주요 NLP 벤치마크에서 state-of-the-art를 달성하는 데 사용되었다.&lt;/p>
&lt;p>자연어 처리(NLP)에서의 전이 학습에 대한 최근 연구는 다양한 사전 학습 목표와 레이블 없는 데이터 세트, 벤치마크 등을 개발했다. 이 분야는 빠르게 발전하고 있지만, 그로 인해 다양한 기법을 비교하거나 이해하는 것이 어려워졌다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure1.png"
width="1170"
height="426"
srcset="https://kurtkim.github.io/p/t5/images/figure1_hu1038ec56a8248b7b1a8e5f6fb8351c28_110308_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure1_hu1038ec56a8248b7b1a8e5f6fb8351c28_110308_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="274"
data-flex-basis="659px"
>&lt;/p>
&lt;p>모든 텍스트 처리 문제를 &amp;ldquo;Text-to-Text&amp;quot;의 일관된 문제로 바라보며, 이를 통해 다양한 NLP 문제에 대한 성능을 평가하고 전이 학습의 한계를 탐색하고자 한다. 이 연구의 목표는 새로운 방법을 제안하는 것이 아니라, 현재 이 분야가 어디에 서 있는지를 종합적으로 이해하는 것이다. 이를 위해 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;라는 웹에서 수집한 영어 텍스트 데이터 세트를 사용한다. 또한, 데이터가 부족한 환경에서의 전이 학습의 중요성을 인식하여, 코드, 데이터 세트, 그리고 사전 학습된 모델을 공개한다.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>모든 문제를 &amp;ldquo;Text-to-Text&amp;rdquo; 변환하는 접근법과 레이블 없는 텍스트 데이터를 구성한 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;를 제안한다. 모델과 프레임워크의 이름은 &amp;ldquo;Text-to-Text Transfer Transformer(T5)&amp;ldquo;이다.&lt;/p>
&lt;h3 id="model">Model&lt;/h3>
&lt;p>NLP의 초기 전이 학습은 RNN을 이용했지만, 최근에는 Transformer 아키텍처 기반의 모델이 일반적이다. Transformer는 처음에는 기계 번역에 효과적이었지만, 이후 다양한 NLP 환경에서 널리 사용되었다. 그래서 논문에서 연구되는 모든 모델은 Transformer 아키텍처를 기반으로 하며, 이 아키텍처에서 크게 벗어나지는 않았다.&lt;/p>
&lt;p>Transformer의 핵심 구성 요소는 self-attention으로, 시퀀스의 각 요소를 시퀀스의 나머지 부분의 가중 평균으로 대체한다. 원래 Transformer는 encoder-decoder 아키텍처로 설계되었지만, 최근에는 언어 모델링이나 분류, 범위 예측 작업에 적합한 아키텍처를 생성하는 다양한 형태의 self-attention을 사용한 single Transformer layer stack 모델이 일반적이다.&lt;/p>
&lt;p>T5의 encoder-decoder Transformer는 입력 토큰을 임베딩으로 매핑하고 이를 인코더에 전달한다. 인코더는 &amp;ldquo;self-attention layer&amp;quot;과 &amp;ldquo;feed-forward network&amp;quot;를 포함하며, 각 입력에 layer normalization와 residual skip connection을 적용한다. dropout은 네트워크 전체에 적용된다. 디코더는 인코더와 비슷하지만, 인코더 출력에 self-attention mechanism이 추가되고, autoregressive 또는 causal self-attention을 사용한다. 디코더 출력은 dense layer로 전달되고, 모든 attention mechanism은 독립적인 &amp;ldquo;head&amp;quot;로 나누어져 있다.&lt;/p>
&lt;p>Transformer 모델은 순서에 상관 없는 self-attention 특성 때문에 position signal을 제공한다. 초기에는 sinusoidal position signal이나 learned position embeddings을 사용했지만, 최근에는 relative position embeddings이 주로 사용되고 있다. 이는 &amp;ldquo;key&amp;quot;와 &amp;ldquo;query&amp;quot;의 오프셋에 따라 다른 임베딩을 생성한다. 우리는 position embedding을 간소화하여 attention weight 계산에 사용되는 스칼라로 만들었다. 모든 layer가 position embedding parameter를 공유하며, 각 layer의 attention head는 다른 position embedding을 사용한다. 이 모델은 원래의 Transformer와 비슷하지만, layer normalization 위치와 position embedding 체계가 다르다.&lt;/p>
&lt;p>모델의 확장성을 실험하기 위해 parameter와 layer를 늘리고, 그 성능 변화를 관찰했다. 복잡한 큰 모델 학습을 위해 모델과 데이터 병렬성을 사용하고, 5개의 TPU 파드를 활용한 Cloud TPU Pods에서 모델을 학습시켰다.&lt;/p>
&lt;h3 id="the-colossal-clean-crawled-corpus">The Colossal Clean Crawled Corpus&lt;/h3>
&lt;p>레이블이 없는 데이터의 품질, 특성, 크기가 어떤 영향을 미치는지 분석한다. 이를 위해 웹에서 스크랩된 텍스트를 제공하는 Common Crawl을 사용한다. Common Crawl은 이전에 언어 모델 훈련, 상식적 추론, 기계 번역 텍스트 채굴, 사전 훈련 데이터 세트, 최적화기 테스트 등 다양한 NLP 연구에 활용된 바 있다.&lt;/p>
&lt;p>Common Crawl은 웹에서 스크랩된 텍스트를 제공하는 공개 아카이브이다. 매월 약 20TB의 텍스트 데이터를 생성하지만, 이 중 대부분은 자연언어가 아닌 메뉴, 오류 메시지, 중복 텍스트 등의 쓸데없는 텍스트이다. 또한, 작업에 도움이 되지 않을 것 같은 내용도 많이 포함되어 있다. 이러한 문제를 해결하기 위해, 다음과 같은 방법들을 사용한다:&lt;/p>
&lt;ul>
&lt;li>마침표, 느낌표, 물음표, 인용 부호를 포함한 문장만을 사용한다.&lt;/li>
&lt;li>3문장 미만의 페이지는 제외하고, 적어도 5단어 이상 포함된 문장만을 사용한다.&lt;/li>
&lt;li>&amp;ldquo;불순한, 야한, 외설적인 또는 그 외 나쁜 단어 목록&amp;quot;에 있는 단어가 포함된 페이지는 모두 삭제한다.&lt;/li>
&lt;li>스크랩된 페이지의 대다수는 자바스크립트(Javascript)가 활성화 되어야 한다는 경고문을 포함한다. 따라서 자바스크립트 단어를 포함한 모든 라인을 삭제한다.&lt;/li>
&lt;li>일부 페이지는 “lorem ipsum” 플레이스홀더를 포함한다. 따라서 “lorem ipsum”구가 있는 모든 페이지를 삭제한다.&lt;/li>
&lt;li>일부 페이지에는 코드가 포함되어 있다. “{” 문구가 대다수의 프로그래밍 언어(웹에서 많이 사용되는 자바스크립트와 같이)에서 출몰하고 자연 텍스트에서는 나타나지 않기 때문에, “{” 를 포함한 모든 페이지를 삭제한다.&lt;/li>
&lt;li>스크랩된 페이지 중 일부는 위키백과에서 가져온 것이었고, 인용 표시자(e.g. [1], [citation needed], etc.)가 있다. 이러한 표시자를 모두 를 모두 삭제한다.&lt;/li>
&lt;li>많은 페이지에는 보일러플레이트 정책 공지가 있다. &amp;ldquo;terms of use&amp;rdquo;, &amp;ldquo;privacy policy&amp;rdquo;, &amp;ldquo;cookie policy&amp;rdquo;, &amp;ldquo;uses cookies&amp;rdquo;, &amp;ldquo;use of cookies&amp;rdquo;, &amp;ldquo;use cookies&amp;quot;라는 문자열을 포함한 줄은 모두 삭제한다.&lt;/li>
&lt;li>데이터셋 중복을 제거하기 위해, 데이터셋에서 두 번 이상 나타난 3문장 스팬은 하나만 남기고 모두 삭제한다.&lt;/li>
&lt;/ul>
&lt;p>대부분의 작업이 영어 텍스트에 초점을 두고 있기 때문에, 0.99의 확률로 영어로 분류되지 않은 페이지를 제거하기 위해 &amp;ldquo;langdetect&amp;quot;를 사용하였다. 하지만, 이전 데이터 세트의 필터링 방법, 공개 여부, 범위 등이 제한적이라고 판단하여 새로운 데이터 세트를 만들기로 결정하였습니다.&lt;/p>
&lt;p>2019년 4월의 웹 텍스트를 다운로드하고 필터링하여 기본 데이터 세트를 구축하였다. 이 결과, 대부분의 사전 학습 데이터 세트보다 훨씬 크고(750GB), 깨끗하며 자연스러운 영어 텍스트 컬렉션을 만들었다. 이를 &amp;ldquo;Colossal Clean Crawled Corpus(C4)&amp;ldquo;라고 부르며, TensorFlow 데이터 세트의 일부로 공개하였다.&lt;/p>
&lt;h3 id="downstream-tasks">Downstream Tasks&lt;/h3>
&lt;p>이 논문의 목표는 일반적인 언어 학습 능력을 측정하는 것이다. 이를 위해, 다양한 벤치마크를 통해 기계 번역, 질문 응답, 추상적 요약, 텍스트 분류 등의 성능을 연구하였다. 이에는 GLUE와 SuperGLUE 텍스트 분류, CNN/Daily Mail 요약, SQuAD 질문 응답, 그리고 WMT 영어에서 독일어, 프랑스어, 루마니아어로의 번역이 포함되었다. 모든 데이터는 TensorFlow 데이터 세트에서 수집하였다.&lt;/p>
&lt;p>GLUE와 SuperGLUE는 각각 일반적인 언어 이해 능력을 테스트하기 위해 설계된 텍스트 분류 작업들의 모음이다:&lt;/p>
&lt;ul>
&lt;li>Sentence acceptability judgment (CoLA)&lt;/li>
&lt;li>Sentiment analysis (SST-2)&lt;/li>
&lt;li>Paraphrasing/sentence similarity (MRPC, STS-B, QQP)&lt;/li>
&lt;li>Natural language inference (MNLI, QNLI, RTE, CB)&lt;/li>
&lt;li>Coreference resolution (WNLI and WSC)&lt;/li>
&lt;li>Sentence completion (COPA)&lt;/li>
&lt;li>Word sense disambiguation (WIC)&lt;/li>
&lt;li>Question answering (MultiRC, ReCoRD, BoolQ)&lt;/li>
&lt;/ul>
&lt;p>GLUE와 SuperGLUE 벤치마크의 데이터 세트를 사용하며, 모든 작업들을 하나의 작업으로 취급하여 데이터 세트를 결합하기 위해 미세 조정을 진행하였다. 또한, SuperGLUE 작업에는 Definite Pronoun Resolution (DPR) 데이터 세트도 포함시켰다.&lt;/p>
&lt;p>CNN/Daily Mail 데이터 세트는 텍스트 요약 작업으로 적용되었고, SQuAD는 일반적인 질문 응답 벤치마크이다. WMT 영어-독일어, 영어-프랑스어, 영어-루마니아어 번역에는 각각 표준 훈련 데이터와 검증 세트를 사용한다. 모든 사전 학습은 영어 데이터로만 진행되며, 모델이 새로운 언어의 텍스트를 생성하도록 배우기 위해 번역 학습이 필요하다.&lt;/p>
&lt;h3 id="input-and-output-format">Input and Output Format&lt;/h3>
&lt;p>모든 작업을 &amp;ldquo;text-to-text&amp;rdquo; 형식으로 표현하여 단일 모델을 훈련시킨다. 이 방식은 사전 학습과 미세 조정에 대해 일관된 훈련 목표를 제공한다. 모델은 작업에 관계없이 maximum likelihood 목표로 훈련되며, 수행해야 할 작업을 지정하기 위해 원래 입력 시퀀스 앞에 작업 특정 텍스트 접두어(prefix)를 추가한다.&lt;/p>
&lt;p>Text-to-text 프레임워크는 다양한 NLP 작업을 통일된 형식으로 변환한다. McCann et al이 제안한 &amp;ldquo;Natural Language Decathlon&amp;quot;과 비슷하지만, 이 논문에서는 각 작업을 개별적으로 미세조정하고 짧은 작업 접두어를 사용한다. 또한 전이 학습에 초점을 맞추며, 기계 번역과 추상적 요약 등의 생성적 작업을 처리할 수 있는 프레임워크를 제안한다.&lt;/p>
&lt;p>대부분의 작업을 text-to-text 형식으로 쉽게 변환했으며, 유사성 점수를 예측하는 STS-B는 점수를 반올림하고 숫자 문자열로 변환하여 처리하였다. 이를 통해 STS-B 회귀 문제를 21 클래스 분류 문제로 재구성하였다.&lt;/p>
&lt;p>또한, Winograd 작업과 WSC 작업에서는 모호한 대명사를 강조하고, 모델이 대명사가 가리키는 명사를 예측하도록 훈련시켰다. DPR 데이터 세트는 대략 1,000개의 대명사 해결 예제를 추가하여 사용하였다.&lt;/p>
&lt;p>WNLI의 훈련 및 검증 세트는 WSC와 많이 중복되므로, 훈련 데이터로의 유출을 방지하기 위해 WNLI에서 훈련하지 않았으며, 평균 GLUE 점수에도 포함시키지 않았다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>한 번에 하나씩 설정을 변경하면서 체계적으로 기여도를 연구하였다. 예를 들어, 나머지 실험 파이프라인을 고정하고 다양한 비지도 목표의 성능을 측정했다. 이 방법은 이차 효과를 놓칠 수 있지만, 모든 요인의 조합을 탐색하는 것은 비용이 많이 든다. 미래의 연구에서는 다양한 접근법의 조합을 더 철저하게 고려하는 것이 유익할 것으로 예상된다.&lt;/p>
&lt;p>이 논문의 목표는 다양한 작업에 대해 다양한 접근법을 비교하는 것으로, 가능한 한 많은 요소를 고정하려고 한다. 이를 위해, 기존의 접근법을 정확하게 따르지는 않았다. 예를 들어, BERT와 같은 encoder-only 모델은 생성 작업에는 적합하지 않다. 따라서 우리가 고려하는 모델 중 어느 것도 BERT와 정확히 같지 않다. 대신, BERT의 목표와 유사한 목표를 고려하고, BERT와 유사하게 작동하는 모델 아키텍처를 고려하였다.&lt;/p>
&lt;h3 id="baseline">Baseline&lt;/h3>
&lt;p>간단한 denoising 목표를 사용하여 표준 Transformer를 사전 학습하고, 각 downstream task에서 별도로 미세 조정을 진행한다.&lt;/p>
&lt;h4 id="model-1">Model&lt;/h4>
&lt;p>T5는 standard encoder-decoder Transformer를 사용한다. 많은 NLP 전이 학습 방법이 single “stack” 구조를 사용하지만, 이 연구에서는 standard encoder-decoder 구조가 생성과 분류 작업에서 좋은 결과를 얻는다는 것을 확인하였다.&lt;/p>
&lt;p>$BERT_BASE$와 유사한 크기와 구성의 인코더와 디코더로 설계되었다. 인코더와 디코더는 각각 12개의 블록으로 이루어져 있으며, 이 블록들은 self-attention, encoder-decoder attention, feed-forward network를 포함하고 있다. 모델은 총 약 2억 2천만 개의 parameter를 가지고 있으며, 모든 부분에서 0.1의 드롭아웃 확률을 사용하여 정규화된다.&lt;/p>
&lt;h4 id="training">Training&lt;/h4>
&lt;p>모든 작업은 text-to-text로 구성되며, 이를 통해 standard maximum likelihood를 사용하여 학습한다. 최적화는 AdaFactor를 사용하고, 테스트 시에는 가장 높은 확률의 logit을 선택하는 greedy decoding을 사용한다.&lt;/p>
&lt;p>각 모델은 C4에서 524,288 단계동안 사전 학습 후 미세 조정을 진행한다. 최대 시퀀스 길이는 512이며, 배치 크기는 128 시퀀스이다. 배치는 대략 65,536 토큰을 포함하도록 한다. 이는 총 34B 토큰에 대한 사전 학습에 해당하며, BERT나 RoBERTa에 비해 상당히 적다. 하지만 이런 방식을 사용하면 합리적인 계산 비용으로 충분한 사전 학습을 할 수 있다. 사전 학습 동안 데이터는 반복하지 않는다.&lt;/p>
&lt;p>사전 학습 동안, &amp;ldquo;inverse square root&amp;rdquo; learning rate schedule을 사용한다. 초기 $10^4$ 단계 동안 learning rate를 0.01로 유지한 후 지수적으로 감소시킨다. triangular learning rate를 실험해 봤지만, 학습 단계의 총 수를 미리 알아야 하므로, 더 일반적인 inverse square root schedule을 선택하였다.&lt;/p>
&lt;p>모든 작업에 대해 262,144 단계 동안 모델을 미세 조정한다. 이는 대규모와 소규모 데이터 세트를 가진 작업 사이의 균형을 위해 선택되었다. 미세 조정 시 128개의 길이 512 시퀀스를 가진 배치를 사용하고, 학습률은 0.001로 유지한다. 5,000step마다 체크포인트를 저장하며, 가장 높은 검증 성능을 보인 체크포인트의 결과를 보고한다. 여러 작업에 미세 조정된 모델의 경우, 각 작업마다 최적의 체크포인트를 독립적으로 선택한다.&lt;/p>
&lt;h4 id="vocabulary">Vocabulary&lt;/h4>
&lt;p>SentencePiece를 사용하여 텍스트를 WordPiece 토큰으로 인코딩하며, 32,000개의 어휘를 사용한다. T5 모델이 다른 언어를 처리할 수 있도록, 영어, 독일어, 프랑스어, 루마니아어 데이터를 혼합하여 SentencePiece 모델을 훈련시켰다. 이 어휘는 모델의 입력과 출력에 모두 사용되며, 미리 결정된 고정된 언어 세트만 처리할 수 있다.&lt;/p>
&lt;h4 id="unsupervised-objective">Unsupervised Objective&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure2.png"
width="908"
height="322"
srcset="https://kurtkim.github.io/p/t5/images/figure2_hud0a0f0860146969ba93fdd537c6f86ff_60008_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure2_hud0a0f0860146969ba93fdd537c6f86ff_60008_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;p>레이블이 없는 데이터를 활용하여 모델을 사전 학습하는데는, 레이블이 필요하지 않지만 일반화 가능한 지식을 모델에게 가르치는 목표가 필요하다. 최근 &amp;ldquo;denoising&amp;rdquo; 또는 &amp;ldquo;masked language modeling&amp;quot;이라는 목표가 효과적이라는 것이 밝혀졌다. 이는 모델이 입력에서 누락되거나 손상된 토큰을 예측하도록 하는 방식이다. 이에 영감을 받아, 입력 시퀀스에서 무작위로 선택한 15%의 토큰을 드롭아웃하는 목표를 설정하였다. 이 목표는 사전 훈련의 계산 비용을 줄이는 데 도움이 된다.&lt;/p>
&lt;h4 id="baseline-performance">Baseline Performance&lt;/h4>
&lt;p>이상적으로는 모든 실험을 여러 번 반복해야 하지만, 실험의 수가 많으면 비용이 높다. 대신, 기본 모델을 10번 새로 학습하고, 이 실행들의 분산이 각 실험 변형에 적용될 것으로 가정한다. 또한, 사전 학습 없이 모델을 218 step 동안 학습한 후 성능을 측정하여, 사전 학습이 얼마나 도움이 되는지 파악한다.&lt;/p>
&lt;p>GLUE와 SuperGLUE는 모든 하위 작업의 평균 점수를, 번역 작업은 SacreBLEU에서 제공하는 BLEU 점수를 확인한다. WMT 영어에서 독일어, 프랑스어, 루마니아어로의 점수를 각각 EnDe, EnFr, EnRo라 표기한다. CNN/Daily Mail과 SQuAD는 상관성이 높은 지표만 확인한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table1.png"
width="1106"
height="174"
srcset="https://kurtkim.github.io/p/t5/images/table1_hub03ae50202d8dc9ff0432f7515727b68_41938_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table1_hub03ae50202d8dc9ff0432f7515727b68_41938_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="635"
data-flex-basis="1525px"
>&lt;/p>
&lt;p>T5 모델은 $BERT_{BASE}$와 비슷한 성능을 보여준다(SQuAD에서 80.88, MNLI-matched에서 84.24). 하지만 T5 모델은 encoder-decoder 모델로, 사전 학습 단계가 더 적어 직접 비교는 어렵다. 대부분의 벤치마크에서 사전 학습이 성능을 크게 향상시키는 것을 확인했고, WMT 영어에서 프랑스어로의 작업을 통해 high-resource regime 상태에서의 전이 학습을 테스트하였다. 데이터가 제한된 작업에서 사전 학습이 얼마나 성능을 향상시키는지 강조하는 동시에, 전이 학습의 주요 이점 중 하나로 데이터 효율성의 개선을 강조한다.&lt;/p>
&lt;p>대부분의 작업에서 표준 편차는 작업의 기준 점수의 1% 미만이다. 하지만 GLUE와 SuperGLUE 벤치마크의 CoLA, CB, COPA와 같은 low-resource 작업에서는 이 규칙이 적용되지 않는다. 예를 들어, CB 작업에서 기준 모델의 평균 F1 점수는 91.22이고 표준 편차는 3.237이었다. 이런 변동성은 검증 세트의 예제 수가 적은 것이 원인일 수 있다. GLUE와 SuperGLUE 점수는 각 벤치마크의 작업 점수의 평균으로 계산되기 때문에 이러한 높은 변동성 때문에 이 점수만으로 모델을 비교하는 것은 어려울 수 있다.&lt;/p>
&lt;h3 id="architectures">Architectures&lt;/h3>
&lt;p>Transformer는 처음에는 encoder-decoder 구조로 소개되었지만, 최근 NLP 전이 학습 연구에서는 다른 구조를 더 많이 사용하고 있다.&lt;/p>
&lt;h4 id="model-structures">Model Structures&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure3.png"
width="954"
height="382"
srcset="https://kurtkim.github.io/p/t5/images/figure3_hu58b7a3434dc501afb6d7e3563e21094b_77654_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure3_hu58b7a3434dc501afb6d7e3563e21094b_77654_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="249"
data-flex-basis="599px"
>&lt;/p>
&lt;p>아키텍처를 구분하는 주요 요소는 모델에서 사용하는 &amp;ldquo;mask&amp;quot;이다. Transformer의 self-attention 연산은 시퀀스를 입력받아 동일한 길이의 새로운 시퀀스를 출력한다. 각 출력 항목은 입력 항목의 weighted average를 계산해 생성된다. attention mask는 특정 가중치를 0으로 만들어 특정 출력 시간에서 입력 항목에 attention을 기울일 수 있는 범위를 제한한다. 예를 들어, causal mask는 $j &amp;gt; i$인 경우 가중치를 0으로 만든다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure4.png"
width="1016"
height="420"
srcset="https://kurtkim.github.io/p/t5/images/figure4_hue70c5b5abebaed2b9ecd54b934dd8154_113297_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure4_hue70c5b5abebaed2b9ecd54b934dd8154_113297_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>첫 번째로 고려하는 모델은 encoder-decoder Transformer로, 입력 시퀀스를 받는 encoder와 새로운 출력 시퀀스를 만드는 decoder 두 계층으로 구성되어 있다.&lt;/p>
&lt;p>encoder는 &amp;ldquo;fully-visible&amp;rdquo; attention mask를 사용한다. 이 마스킹은 출력의 각 항목을 만들 때 입력의 모든 항목에 attention을 기울일 수 있게 해준다. 이 마스킹은 &amp;ldquo;prefix&amp;rdquo; 즉, 예측을 만들 때 사용되는 일부 컨텍스트에 주의를 기울일 때 적합하다. BERT도 이와 같은 마스킹 패턴을 사용하며, 특별한 &amp;ldquo;classification&amp;rdquo; 토큰을 입력에 추가한다. 이 토큰에 해당하는 BERT의 출력은 입력 시퀀스를 분류하는 예측을 하는데 사용된다.&lt;/p>
&lt;p>Transformer의 decoder에서 self-attention 연산은 &amp;ldquo;causal&amp;rdquo; 마스킹 패턴을 사용한다. 출력 시퀀스의 $i$번째 항목을 생성할 때, 인과적 마스킹은 모델이 입력 시퀀스의 $j$번째 항목$(j &amp;gt; i)$에 attention을 기울이는 것을 방지한다. 이는 모델이 출력을 생성하는 동안 &amp;ldquo;미래를 보는&amp;rdquo; 것을 방지하기 위해 훈련 중에 사용된다.&lt;/p>
&lt;p>언어 모델은 text-to-text 작업에서 입력과 목표를 연결함으로써 사용될 수 있지만, causal 마스킹 때문에 입력 시퀀스의 특정 항목이 그 이전 항목에만 의존하는 문제가 있다. 이 문제는 Transformer 기반 언어 모델에서 마스킹 패턴을 변경함으로써 해결할 수 있으며, 시퀀스의 접두사 부분에서 완전히 보이는 마스킹을 사용하면 이 문제를 피하면서도 다양한 text-to-text 작업에 효과적일 수 있다. 이 방식은 encoder와 decoder 간에 파라미터를 공유하는 encoder-decoder 모델과 유사하며, 입력과 목표 시퀀스에 걸쳐 전체 attention을 적용한다.&lt;/p>
&lt;p>prefix LM은 BERT와 비슷하게 작동하지만, 분류 작업을 수행하기 위해 Transformer decoder의 출력 레이어에 분류기를 통합한다. 이 모델은 전체 입력을 보고 예측을 출력함으로써 분류 작업을 수행한다.&lt;/p>
&lt;h4 id="comparing-different-model-structures">Comparing Different Model Structures&lt;/h4>
&lt;p>아키텍처를 비교하려면, 각 모델이 같은 수의 parameter를 가지거나, 같은 양의 계산을 필요로 하는 등 의미 있는 방식으로 동일해야 한다. encoder와 decoder가 각각 L개의 레이어를 가진 encoder-decoder 모델은, 2L개의 레이어를 가진 언어 모델과 대략 같은 수의 parameter를 가진다. 그러나 계산 비용 면에서는, L개의 레이어를 가진 언어 모델과 동일하다. 이는 언어 모델이 입력과 출력 시퀀스 모두를 처리해야 하지만, encoder-decoder 모델은 각각 입력과 출력 시퀀스만을 처리하기 때문이다.&lt;/p>
&lt;p>비교를 위해, encoder-decoder 모델의 여러 구성을 고려하였다. $BERT_{BASE}$ 크기의 레이어 스택에서 레이어와 parameter의 수를 각각 L과 P로, 주어진 입력-타겟 쌍 처리에 필요한 FLOPs의 수를 M으로 표현하겠습니다. 이를 바탕으로 모델들을 비교한다:&lt;/p>
&lt;ul>
&lt;li>encoder와 decoder에 각각 L 레이어가 있는 encoder-decoder 모델. 이 모델은 2P의 parameter와 M FLOPs의 계산 비용을 가진다.&lt;/li>
&lt;li>동일한 모델이지만, 인코더와 디코더 간에 parameter가 공유되어, P의 parameter와 M-FLOP의 계산 비용을 가진다.&lt;/li>
&lt;li>encoder와 decoder에 각각 L/2 레이어가 있는 encoder-decoder 모델로, P의 parameter와 M/2-FLOP의 비용을 가진다.&lt;/li>
&lt;li>L 레이어와 P parameter를 가지며 M FLOPs의 계산 비용이 발생하는 decoder-only 언어 모델.&lt;/li>
&lt;li>같은 아키텍처를 가지지만, 입력에 대한 fully-visible self-attention을 가진 decoder-only prefix LM.&lt;/li>
&lt;/ul>
&lt;h4 id="objectives">Objectives&lt;/h4>
&lt;p>비지도 학습 목표로 기본 언어 모델링과 denoising 목표를 고려하였다. 언어 모델링은 사전 학습 목표로서의 역사적 사용과 모델 아키텍처에 대한 적합성 때문에 포함되었다. 예측 전에 접두사를 입력하는 모델들에 대해, 레이블이 없는 데이터에서 텍스트를 샘플링하고 랜덤한 지점에서 접두사와 타겟으로 분할한다. 표준 언어 모델은 전체 텍스트를 예측하도록 훈련되며, text-to-text 모델을 위한 비지도 denoising 목표는 입력과 타겟을 연결하여 사용한다.&lt;/p>
&lt;h4 id="results">Results&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table2.png"
width="1330"
height="404"
srcset="https://kurtkim.github.io/p/t5/images/table2_huf6638c8c2bc117ff25ac7c6022eeb34d_123609_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table2_huf6638c8c2bc117ff25ac7c6022eeb34d_123609_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>모든 작업에서 denoising 목표를 가진 encoder-decoder 구조가 가장 좋은 성능을 보여주었다. 이 구조는 parameter 수는 가장 많지만 계산 비용은 decoder-only 모델과 같다. encoder와 decoder 간에 parameter를 공유하는 것도 거의 동등한 성능을 보여주었다. 반면, encoder와 decoder의 레이어 수를 줄이면 성능이 크게 저하되었다. denoising 목표를 가진 공유 encoder-decoder 구조는 decoder-only prefix LM 모델보다 성능이 좋았다. 마지막으로, denoising 목표를 사용하는 것이 언어 모델링 목표보다 항상 더 나은 성능을 가져다 준다는 사실을 확인하였다.&lt;/p>
&lt;h3 id="unsupervised-objectives">Unsupervised Objectives&lt;/h3>
&lt;p>비지도 학습 목표의 선택은 모델이 downstream task에 적용할 일반 지식을 획득하는 방법을 제공하므로 중요하며, 이로 인해 다양한 사전 학습 목표가 개발되었다. 많은 경우에 기존의 목표를 그대로 복제하지 않고, text-to-text encoder-decoder 프레임워크에 맞게 수정하거나, 여러 공통 접근법의 개념을 결합한 목표를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table3.png"
width="1524"
height="288"
srcset="https://kurtkim.github.io/p/t5/images/table3_hu4cef11ab70bdbad4421eb2667d9e4b35_103192_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table3_hu4cef11ab70bdbad4421eb2667d9e4b35_103192_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="529"
data-flex-basis="1270px"
>&lt;/p>
&lt;p>라벨이 없는 텍스트 데이터 세트에서 토큰화된 텍스트 범위에 해당하는 토큰 ID의 시퀀스를 처리한다. 토큰 시퀀스는 입력 시퀀스와 목표를 생성하고, 모델은 이를 사용해 maximum likelihood로 목표 시퀀스를 예측하도록 학습한다.&lt;/p>
&lt;h4 id="disparate-high-level-approaches">Disparate High-Level Approaches&lt;/h4>
&lt;p>세 가지 다른 접근법을 사용한 기법들을 비교한다. 첫 번째로, &amp;ldquo;prefix language modeling&amp;rdquo; 목표를 사용하며, 이는 텍스트를 두 부분으로 나눠 encoder 입력과 decoder 예측 대상으로 사용한다. 두 번째로, BERT의 &amp;ldquo;masked language modeling&amp;quot;에서 영감을 받은 목표를 사용하며, 이는 텍스트의 토큰 15%를 손상시키고, 이 중 90%는 마스크 토큰, 10%는 랜덤 토큰으로 대체한다. 세 번째로, &amp;ldquo;deshuffling&amp;rdquo; 목표를 사용하며, 이는 토큰의 순서를 섞은 후 원래 순서를 복원하는 것을 목표로 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table4.png"
width="1148"
height="170"
srcset="https://kurtkim.github.io/p/t5/images/table4_hucd8c36f3593092fa88fcbef0571dc050_48099_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table4_hucd8c36f3593092fa88fcbef0571dc050_48099_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="675"
data-flex-basis="1620px"
>&lt;/p>
&lt;p>전체적으로, BERT 스타일 목표가 가장 뛰어난 성능을 보이지만, prefix language modeling 목표도 번역 작업에서 유사한 성능을 보여준다. 반면에 deshuffling 목표는 다른 두 목표보다 성능이 상당히 떨어진다.&lt;/p>
&lt;h4 id="simplifying-the-bert-objective">Simplifying the BERT Objective&lt;/h4>
&lt;p>BERT 스타일의 denoising 목표는 원래 분류와 범위 예측을 위해 학습된 encoder-only 모델의 사전 학습 기법으로 제안되었다. 따라서 encoder-decoder text-to-text 설정에서 더 나은 성능을 내거나 더 효율적이게 만들 수 있도록 조정하는 것이 가능할 수 있다.&lt;/p>
&lt;p>BERT 스타일 목표의 간단한 변형을 고려하며, 이는 무작위 토큰 교환 단계를 생략한다. 그 결과, 입력의 15% 토큰을 마스크 토큰으로 바꾸고, 모델은 원래 손상되지 않은 시퀀스를 재구성하도록 학습한다. 이를 &amp;ldquo;MASS&amp;rdquo; 목표라고 부릅니다. 또한, decoder에서 긴 시퀀스 자체에 대한 attention를 피할 수 있는 방법을 탐색한다. 이를 위해, 손상된 토큰들을 모두 마스크 토큰으로 대체하거나, 손상된 토큰을 입력 시퀀스에서 완전히 삭제하는 두 가지 전략을 고려한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table5.png"
width="1164"
height="206"
srcset="https://kurtkim.github.io/p/t5/images/table5_hu25f2653fef6f13a93df98915514a50bb_63030_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table5_hu25f2653fef6f13a93df98915514a50bb_63030_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="565"
data-flex-basis="1356px"
>&lt;/p>
&lt;p>원래의 BERT 스타일 목표와 세 가지 대안의 비교는 모든 변형이 비슷하게 수행된다는 것을 보여준다. 예외적으로, 손상된 토큰을 완전히 삭제하는 것이 CoLA에서 훨씬 높은 점수 덕분에 GLUE 점수를 약간 향상시켰는데, CoLA가 주어진 문장이 문법적으로 및 구문론적으로 수용 가능한지 분류하는 것을 포함하고 있으며, 토큰이 누락되었는지 판단할 수 있는 능력이 수용 가능성을 감지하는 데 밀접하게 관련되어 있기 때문일 수 있다. 그러나, 토큰을 완전히 삭제하는 것은 SuperGLUE에서 성능이 떨어졌다. 전체 원래 시퀀스를 예측할 필요가 없는 두 가지 변형은 훈련 시간을 단축시키는 장점이 있다.&lt;/p>
&lt;h4 id="varying-the-corruption-rate">Varying the Corruption Rate&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table6.png"
width="990"
height="212"
srcset="https://kurtkim.github.io/p/t5/images/table6_hu60faf8658ec9e582286a0ecbf3ba4c0c_48105_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table6_hu60faf8658ec9e582286a0ecbf3ba4c0c_48105_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="466"
data-flex-basis="1120px"
>&lt;/p>
&lt;p>지금까지 BERT에서 사용된 15%의 토큰 손상률을 사용하였다. 그러나 text-to-text 프레임워크가 BERT와 다르므로 다른 손상률이 더 효과적인지 테스트해 보았다. 10%, 15%, 25%, 50%의 손상률을 비교했지만, 손상률이 모델 성능에 큰 영향을 미치지는 않았다. 단, 50%의 가장 높은 손상률은 GLUE와 SQuAD에서 성능 저하를 가져왔다. 또한, 높은 손상률은 학습 속도를 느리게 만드는 긴 대상을 만드는 경향이 있다. 따라서 BERT의 기준에 따라 앞으로 15%의 손상률을 사용할 것이다.&lt;/p>
&lt;h4 id="corrupting-spans">Corrupting Spans&lt;/h4>
&lt;p>예측 대상을 짧게 하여 학습 속도를 높이려고 한다. 지금까지의 방법은 각 입력 토큰을 독립적으로 손상시킬지 결정하였고, 연속된 토큰이 손상될 경우 이를 &amp;ldquo;span&amp;quot;으로 취급하여 단일 마스크 토큰으로 대체하였다. 이 방식은 레이블이 없는 텍스트 데이터를 짧은 시퀀스로 변환하지만, 항상 많은 수의 손상된 토큰이 연속적으로 나타나지는 않는다. 따라서 토큰의 span을 특정하여 손상시키는 방식을 사용하면 더 큰 속도 향상을 얻을 수 있습니다. 이러한 방법은 BERT의 사전 학습 목표로도 사용되어 성능 향상을 가져왔다.&lt;/p>
&lt;p>토큰의 연속적인 span을 손상시키는 목표를 테스트하기 위해, 손상시킬 토큰의 비율과 손상된 span의 총 수를 parameter로 사용한다. 예를 들어, 500개의 토큰 시퀀스에서 15%의 토큰을 손상시키고 총 span이 25개가 되도록 지정하면, 손상된 토큰의 총 수는 75개이고 평균 span 길이는 3이 된다. 이 방식은 원래의 시퀀스 길이와 손상률에 따라 span의 길이나 총 span 수를 조절할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table7.png"
width="978"
height="238"
srcset="https://kurtkim.github.io/p/t5/images/table7_huae5c3427b58561a089d0c71ef313c023_55278_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table7_huae5c3427b58561a089d0c71ef313c023_55278_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="410"
data-flex-basis="986px"
>&lt;/p>
&lt;p>span 손상 목표와 독립 동일 분포(i.i.d) 손상 목표를 비교한 결과, 이들 사이에는 큰 차이가 없었다. 하지만 평균 span 길이가 10인 경우에는 일부에서 다른 값들보다 성능이 약간 떨어졌다. 반면에 평균 span 길이가 3인 경우에는 대부분의 비번역 벤치마크에서 i.i.d. 목표를 약간 능가하였다. 또한 span 손상 목표는 평균적으로 더 짧은 시퀀스를 생성함으로써 학습 속도를 빠르게 할 수 있었다.&lt;/p>
&lt;h4 id="discussion">Discussion&lt;/h4>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure5.png"
width="722"
height="378"
srcset="https://kurtkim.github.io/p/t5/images/figure5_hu64e6cb9f6d88cd1de9cb8c348334d5db_76782_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure5_hu64e6cb9f6d88cd1de9cb8c348334d5db_76782_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>denoising 목표가 언어 모델링 및 deshuffling보다 사전 학습에 더 효과적이었다. 또한 denoising 목표의 다양한 변형 사이에는 큰 차이가 없었다. 그러나 목표의 선택이나 parameter화는 시퀀스 길이와 학습 속도에 영향을 미친다. 따라서 denoising 목표의 선택은 주로 계산 비용에 기반해 이루어져야 한다. 또한, 유사한 목표에 대한 추가적인 탐색은 큰 이익을 가져오지 않을 수 있으며, 레이블이 없는 데이터를 활용하는 새로운 방법을 탐색하는 것이 더 유익할 수 있다.&lt;/p>
&lt;h3 id="pre-training-data-set">Pre-training Data set&lt;/h3>
&lt;p>사전 학습 데이터 세트는 전이 학습 파이프라인의 핵심 요소이지만, 새로운 데이터 세트는 종종 중요한 기여로 인식되지 않고, 사전 학습된 모델과 코드와 함께 공개되지 않는다. 그 결과, 다양한 사전 학습 데이터 세트의 비교는 부족하고 &amp;ldquo;표준&amp;rdquo; 데이터 세트도 없다. 최근에는 큰 데이터 세트와 작은 기존 데이터 세트에서의 사전 학습을 비교한 연구가 있다.&lt;/p>
&lt;h4 id="unlabeled-data-sets">Unlabeled Data Sets&lt;/h4>
&lt;p>C4 제작 과정에서, Common Crawl로부터 추출한 웹 텍스트를 필터링하기 위한 다양한 방법을 개발하였다. 이 필터링이 다른 방법과 비교하여 downstream task에서 성능 향상을 가져오는지 측정하려 한ㄴ다. 이를 위해 다양한 데이터 세트에서 사전 학습한 후의 기준 모델 성능을 비교하였다.&lt;/p>
&lt;p>&lt;strong>C4&lt;/strong> 기준이 되는 데이터셋으로, 레이블 없는 데이터 세트에서 사전 훈련하는 것을 고려한다.&lt;/p>
&lt;p>&lt;strong>Unfiltered C4&lt;/strong> C4를 생성할 때 사용한 휴리스틱 필터링의 효과를 측정하기 위해, 필터링을 생략한 C4의 대체 버전을 만들었다. 하지만, 영어 텍스트 추출을 위해 langdetect는 여전히 사용되며, 이로 인해 &amp;ldquo;unfiltered&amp;rdquo; 버전도 어느 정도 필터링이 포함된다.&lt;/p>
&lt;p>&lt;strong>RealNews-like&lt;/strong> 최근 연구에서는 뉴스 웹사이트에서 추출한 텍스트 데이터를 사용하였다. 이를 비교하기 위해, C4를 필터링하여 &amp;ldquo;RealNews&amp;rdquo; 데이터 세트에서 사용된 도메인의 콘텐츠만 포함하도록 한 새로운 레이블 없는 데이터 세트를 생성하였다. C4에서 사용된 필터링 방법을 유지하되, 비뉴스 콘텐츠는 모두 제외하였다.&lt;/p>
&lt;p>&lt;strong>WebText-like&lt;/strong> WebText 데이터 세트는 Reddit에 제출된 웹페이지 중 점수가 3점 이상인 콘텐츠만 사용했다. 비교 가능한 데이터 세트를 만들기 위해, C4에서 OpenWebText 목록에 없는 URL의 콘텐츠를 모두 제거하였다. 하지만, 대부분의 페이지가 Reddit에 나타나지 않아, 결과적으로 콘텐츠가 많지 않았다. 그래서, 2018년 8월부터 2019년 7월까지의 Common Crawl 데이터를 다운로드하여 필터링을 적용하였고, 이를 통해 원래의 WebText 데이터 세트와 비교 가능한 17GB의 데이터 세트를 생성하였다.&lt;/p>
&lt;p>&lt;strong>Wikipedia&lt;/strong> Wikipedia는 엄격한 품질 가이드라인을 준수하는 수백만 개의 협업 글로 이루어져 있다. 이러한 특성 때문에 Wikipedia는 청결하고 자연스러운 텍스트의 신뢰성 있는 출처로 활용되었다. 기사의 마크업이나 참조 섹션을 생략한 TensorFlow Datasets의 영어 Wikipedia 텍스트 데이터를 사용하고 있다.&lt;/p>
&lt;p>&lt;strong>Wikipedia + Toronto Books Corpus&lt;/strong> Wikipedia의 사전학습 데이터를 사용하는 단점은 자연어 텍스트의 한 도메인만을 대표한다는 것이다. 이를 보완하기 위해 BERT는 Wikipedia 데이터와 전자책에서 추출한 텍스트를 담은 Toronto Books Corpus를 결합하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table8.png"
width="1116"
height="266"
srcset="https://kurtkim.github.io/p/t5/images/table8_hub5142a928751920f9041e3dd3f39acdd_75926_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table8_hub5142a928751920f9041e3dd3f39acdd_75926_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="419"
data-flex-basis="1006px"
>&lt;/p>
&lt;p>사전 학습 데이터 세트의 도메인이 제한된 경우, 다양한 데이터 세트를 사용한 것보다 성능이 뛰어날 수 있다. 특히, 해당 도메인과 관련된 작업에서 성능 향상이 두드러진다. 하지만, 이 방법은 모든 도메인의 언어 작업에 빠르게 적응하는 모델을 만드는 목표와는 약간 다르다.&lt;/p>
&lt;p>단일 도메인에서만 사전 학습을 하는 것의 단점은 결과적으로 데이터 세트가 작아진다. WebText와 유사한 변형은 C4 데이터 세트와 같거나 더 좋은 성능을 보였지만, Reddit 기반 필터링은 더 많은 데이터에도 불구하고 C4보다 훨씬 작은 데이터 세트를 만들었다.&lt;/p>
&lt;h4 id="pre-training-data-set-size">Pre-training Data set Size&lt;/h4>
&lt;p>제한된 레이블 없는 데이터 세트 크기의 영향을 테스트하기 위해, C4를 인공적으로 줄인 버전에서 베이스라인 모델을 사전 학습시켰다. 이 때, 다양한 크기의 축소된 C4 변형에서 학습을 진행하였으며, 이는 사전 학습 과정에서 데이터 세트를 각각 64, 256, 1,024, 4,096번 반복하는 것을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table9.png"
width="1122"
height="240"
srcset="https://kurtkim.github.io/p/t5/images/table9_hub7a867d791705562125ad072f0ae3b43_56330_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table9_hub7a867d791705562125ad072f0ae3b43_56330_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="467"
data-flex-basis="1122px"
>&lt;/p>
&lt;p>데이터 세트 크기가 줄어들면서 성능이 저하되는 것을 확인할 수 있다. 이는 모델이 사전 학습 데이터를 기억하기 시작하면서 발생하는 것으로 보인다. 이를 검증하기 위해 각 데이터 세트 크기에 대한 학습 손실을 그렸고, 데이터 세트 크기가 줄어들면서 학습 손실이 크게 감소하는 것을 확인하였다. 이는 모델이 데이터를 기억하고 있음을 나타내는 증거일 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/figure6.png"
width="738"
height="426"
srcset="https://kurtkim.github.io/p/t5/images/figure6_hub75381f4df7e5bfa5754f4d164a6e5ed_71302_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/figure6_hub75381f4df7e5bfa5754f4d164a6e5ed_71302_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>사전 학습 데이터 세트가 64번만 반복될 때, 이러한 효과는 제한적이라는 것을 확인하였다. 이는 일정량의 사전 학습 데이터 반복이 해롭지 않을 수 있음을 나타낸다. 추가적인 사전 학습이 유익하고 레이블이 없는 추가 데이터를 쉽게 얻을 수 있으므로, 가능하면 큰 사전 훈련 데이터 세트를 사용하는 것이 좋다. 더 큰 모델은 작은 사전 학습 데이터 세트에 과적합되는 경향이 더욱 강하게 나타난다.&lt;/p>
&lt;h3 id="training-strategy">Training Strategy&lt;/h3>
&lt;h4 id="fine-tuning-methods">Fine-tuning Methods&lt;/h4>
&lt;p>초기 연구에서는 사전 학습된 모델의 문장 임베딩을 사용하는 작은 분류기의 parameter만 미세 조정하는 것을 제안하였다. 하지만 이 방법은 encoder-decoder 모델에는 적용하기 어렵다. 대신, 모델의 일부 parameter만 업데이트하는 두 가지 대안적인 미세 조정 방법을 고려한다.&lt;/p>
&lt;p>&amp;ldquo;adapter layers&amp;quot;는 원래 모델의 대부분을 고정하고 미세 조정하는 방법이다. Transformer의 각 블록에 dense-ReLU-dense 블록 형태의 adapter layer를 추가하며, 이 layer는 출력 차원이 입력과 같도록 설계된다. 미세 조정 시, adapter layer와 layer normalization parameter만 업데이트되며, 전방향 네트워크의 내부 차원 $d$는 신규 parameter의 수를 결정하는 주요 hyperparameter이다.&lt;/p>
&lt;p>&amp;ldquo;gradual unfreezing&amp;quot;은 시간이 지남에 따라 모델의 parameter를 점차 미세 조정하는 방식이다. 미세 조정 시작 시 최종 층의 parameter만 업데이트하고, 일정 업데이트 후에는 이전 층의 parameter도 포함시키는 방식으로 진행된다. 이 방법은 encoder와 decoder의 층을 동시에 상단부터 점진적으로 언프리징하며, 미세 조정 과정은 12개의 단계로 나누어 진행된다. 이 방식은 데이터 세트 크기의 다양성과 복합 작업의 존재 때문에 채택되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table10.png"
width="1078"
height="266"
srcset="https://kurtkim.github.io/p/t5/images/table10_hu0ed6c9e3c0f7f8b5ed49c577b04da6e5_72206_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table10_hu0ed6c9e3c0f7f8b5ed49c577b04da6e5_72206_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="405"
data-flex-basis="972px"
>&lt;/p>
&lt;p>adapter layers는ㄴ 작업 크기에 따라 차원을 적절히 조정하면 parameter가 적은 상태에서도 미세 조정이 가능하다는 것을 보여준다. 반면에, gradual unfreezing은 미세 조정 시간을 단축시키지만, 모든 작업에서 성능이 약간 떨어진다는 결과를 보여주었다. 이를 통해 언프리징 일정을 더 신중하게 조정하면 더 나은 결과를 얻을 수 있을 것으로 예상된다.&lt;/p>
&lt;h4 id="multi-task-learning">Multi-task Learning&lt;/h4>
&lt;p>Multi-task Learning은 여러 작업을 동시에 학습하는 방법으로, 하나의 모델이 다양한 작업을 수행하도록 학습한다. 이 방법은 데이터 세트를 혼합하여 레이블이 없는 데이터에서도 학습이 가능하다. 중요한 점은 모델이 각 작업에서 적절한 양의 데이터를 학습하도록 하는 것이며, 이는 데이터 세트 크기, 작업 학습의 난이도, 정규화 등에 따라 달라진다. 또한, 한 작업에서의 성과가 다른 작업의 성능을 저해하는 문제를 고려하여, 데이터 비율 설정 전략을 연구하고 있다.&lt;/p>
&lt;p>&lt;strong>Examples-proportional mixing&lt;/strong> 모델이 작업에 overfit되는 속도는 작업의 데이터 세트 크기에 따라 결정된다. 그래서 데이터 세트 크기에 비례하여 샘플링하는 것이 일반적이다. 하지만, 어떤 작업은 데이터 크기가 월등히 크기 때문에, 이 방법을 사용하면 레이블이 없는 데이터가 대부분이 되고 모든 지도 작업이 undertrain 되는 문제가 발생한다. 이를 해결하기 위해, 비율을 계산하기 전에 데이터 세트 크기에 인위적인 &amp;ldquo;limit&amp;quot;을 설정한다. 각 작업에서 샘플하는 확률은 작업의 데이터 세트 크기와 인위적인 제한 작은 값에 비례하도록 설정된다.&lt;/p>
&lt;p>&lt;strong>Temperature-scaled mixing&lt;/strong> 데이터 세트 크기의 큰 차이를 완화하는 다른 방법은 혼합 비율의 &amp;ldquo;temperature&amp;quot;를 조정하는 것이다. 이 방식은 다국어 BERT에서 적용되어, 자원이 적은 언어에 대한 충분한 학습을 보장하였다. 이는 각 작업의 혼합 비율을 1/temperature로 거듭제곱하고, 이 비율이 합쳐져 1이 되도록 재정규화하는 방식으로 이루어진다. 온도가 증가하면 비율은 동등 혼합에 가까워진다. 가장 큰 데이터 세트의 혼합 비율이 감소하는 것을 방지하기 위해 데이터 세트 크기 제한을 큰 값으로 설정한다.&lt;/p>
&lt;p>&lt;strong>Equal mixing&lt;/strong> 각 작업에서 예제를 동일한 확률로 뽑는다. 각 배치의 예제는 학습 데이터 세트 중 무작위로 선택된다. 이 방법은 low-resource 작업에 빠르게 overfit되고 high-resource 작업에 underfit되기 때문에 최적의 전략이 아닐 가능성이 크다. 이 점은 비율 설정이 최적이 아닐 때 발생할 수 있는 문제를 보여주는 참고점이다.&lt;/p>
&lt;p>이러한 mixing 전략을 기본선인 사전 학습 후 미세 조정 결과와 동일한 기준으로 비교하기 위해, 총 스텝 수가 같은 multi-task Learning 모델을 학습시킨다: $2^{19} + 2^{18} =$ 786,432.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table11.png"
width="1156"
height="410"
srcset="https://kurtkim.github.io/p/t5/images/table11_hu7c72fa05249237ddf7e85e8bbb090e64_128090_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table11_hu7c72fa05249237ddf7e85e8bbb090e64_128090_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;p>일반적으로, multi-task Learning은 대부분의 작업에서 사전 학습 후 미세 조정보다 성능이 떨어진다. &amp;ldquo;equal&amp;rdquo; mixing 전략은 특히 성능이 크게 저하되며, 이는 작업에 따른 데이터의 불균형 때문일 수 있다. examples-proportional mixing에서는 대부분의 작업에 대해 모델이 최적의 성능을 얻는 &amp;ldquo;sweet spot&amp;rdquo; $K$ 값이 있다. 또한, temperature-scaled mixing은 대부분의 작업에서 합리적인 성능을 얻는 수단을 제공한다. 별도의 모델이 각각의 작업에 대해 훈련된 것보다 multi-task 모델이 더 나은 성능을 보이지 못한 것은 이전 연구에서도 관찰된 바 있다.&lt;/p>
&lt;h4 id="combining-multi-task-learning-with-fine-tuning">Combining Multi-Task Learning with Fine-Tuning&lt;/h4>
&lt;p>multi-task Learning의 개선된 버전을 연구하고 있다. 이 방법은 모든 작업에 대해 모델을 사전 학습하고, 각각의 작업에 대해 미세 조정하는 방식으로, 이 방식은 &amp;ldquo;MT-DNN&amp;quot;에서 사용되었으며, GLUE 및 기타 벤치마크에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>이 접근법의 세 가지 변형을 고려한다: 첫 번째는 모든 작업을 사전 학습하고 각 작업에 대해 미세 조정하는 것, 두 번째는 하나의 작업을 제외하고 사전 학습한 후 제외된 작업에 대해 미세 조정하는 것, 세 번째는 감독 작업만을 사전 학습하는 것이다. 이 모든 변형에서는 일정 단계 동안 사전 학습 후 미세 조정을 진행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table12.png"
width="1236"
height="234"
srcset="https://kurtkim.github.io/p/t5/images/table12_hua134220b52c750cb2e15f0b84a5494fc_73598_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table12_hua134220b52c750cb2e15f0b84a5494fc_73598_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="528"
data-flex-basis="1267px"
>&lt;/p>
&lt;p>multi-task 사전 학습 후 미세 조정을 한 결과가 기준선과 비교해도 비슷한 성능을 보여주었다. 이는 multi-task 학습 후 미세 조정이 다른 mixing 비율 간의 트레이드오프를 완화하는 데 도움이 될 수 있음을 보여준다. 또한, &amp;ldquo;leave-one-out&amp;rdquo; 학습 방식의 성능은 약간만 떨어졌고, 이는 다양한 작업에 대해 학습한 모델이 새로운 작업에도 적응할 수 있음을 시사한다. 그러나, supervised multi-task 사전 학습은 번역 작업을 제외하고는 성능이 떨어졌다. 이는 번역 작업이 사전 학습에서 덜 이익을 보며, 비지도 사전 학습이 다른 작업에서 중요함을 시사한다.&lt;/p>
&lt;h3 id="scaling">Scaling&lt;/h3>
&lt;p>&amp;ldquo;bitter lesson&amp;quot;은 계산을 늘리는 일반적인 방법이 인간의 전문성에 의존하는 방법보다 우월하다는 주장이다. 이는 자연어 처리의 전이 학습에도 적용될 수 있으며, 규모를 확대하는 것이 더 신중한 설계보다 성능을 향상시킴을 보여주었다. 이 논문에서는 &amp;ldquo;4배 더 많은 컴퓨팅 파워를 얻었다면 어떻게 사용해야 할까?&amp;ldquo;라는 주제로, 규모를 확대하는 다양한 방법을 비교한다.&lt;/p>
&lt;p>220M의 parameter를 가진 기준 모델로 시작한다. 이 모델은 &amp;ldquo;$BERT_{BASE}$&amp;ldquo;와 유사한 크기의 encoder와 decoder를 가지고 있다. 모델 크기를 증가시키기 위해, &amp;ldquo;$BERT_{LARGE}$&amp;ldquo;의 가이드라인을 따라 두 가지 변형을 만들어내었다: encoder와 decoder 각각에 16층과 32층을 가진 모델이다. 이들은 원래 모델보다 매개변수가 2배와 4배 많으며, 계산 비용도 2배와 4배이다. 이 모델들을 사용하여 4배의 계산을 사용하는 세 가지 방법을 고려한다: 4배 많은 step 학습, 2배 큰 모델로 2배 많은 step 학습, 그리고 &amp;ldquo;baseline&amp;rdquo; 학습 step에 대해 4배 큰 모델 학습.&lt;/p>
&lt;p>데이터를 4배 더 처리하는 방법 중 하나는 배치 크기를 4배로 늘리는 것이다. 이는 학습 속도를 빠르게 하지만, 4배 많은 학습 step을 사용하는 것과 다른 결과를 가져올 수 있어 이를 비교하기 위한 추가 실험을 진행하였다. 또한, 추가 계산을 활용하는 다른 방법으로는 모델의 앙상블을 사용하는 것이 일반적이다. 이를 비교하기 위해, 4개의 별도로 학습된 모델의 앙상블 성능을 평가했으며, 하나의 모델을 사전 학습하고 이를 4배로 미세 조정하는 비용 절감 방법도 함께 고려하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table13.png"
width="1130"
height="296"
srcset="https://kurtkim.github.io/p/t5/images/table13_hu2213d3324791a2e622eacc00b289ce4d_82725_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table13_hu2213d3324791a2e622eacc00b289ce4d_82725_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="381"
data-flex-basis="916px"
>&lt;/p>
&lt;p>학습 시간과 모델 크기를 증가시키면 성능이 개선되며, 특히 모델 크기 증가와 앙상블 방법이 더 큰 향상을 가져왔다. 그러나 앙상블 방법은 SuperGLUE에서는 큰 효과를 보이지 못했다. 또한, 스케일링 방법 선택 시, 큰 모델의 미세 조정과 추론 비용, 작은 모델의 학습 시간, 그리고 앙상블의 계산 비용 등을 고려해야 한다. 따라서 모델의 최종 사용을 고려하는 것이 중요하다.&lt;/p>
&lt;h3 id="putting-it-all-together">Putting It All Together&lt;/h3>
&lt;p>자연어 처리 벤치마크에서 얼마나 성능을 끌어올릴 수 있는지 확인하기 위해, baseline 학습 접근법으로 시작하여 다음과 같은 변경을 만든다:&lt;/p>
&lt;p>&lt;strong>Objective&lt;/strong> 기본 모델의 노이즈 제거 목표를 SpanBERT에서 영감을 받은 span-corruption 목표로 교체하였다. 평균 span 길이 3을 사용하고 원래 시퀀스의 15%를 손상시켰는데, 이 방법은 목표 시퀀스 길이가 짧아 계산 효율성이 높으며, 약간 더 나은 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Longer training&lt;/strong> 기본 모델은 작은 양의 사전 학습을 사용하지만, C4 데이터셋의 크기 때문에 데이터를 반복하지 않고도 더 오래 학습할 수 있다. 추가적인 사전 학습이 도움이 되며, 배치 크기와 학습 단계 수를 늘리는 것이 이에 도움이 되는 것을 확인하였다. 약 1M 개의 사전 학습 토큰에 대해 모델을 사전 학습하였고, 몇 가지 작은 데이터셋에서는 C4보다 더 좋은 성능을 보였지만, 이 작은 데이터셋들은 1M 토큰의 사전 학습 과정에서 수백 번 반복될 만큼 충분히 작기 때문에, C4 데이터셋을 계속 사용하기로 결정했다.&lt;/p>
&lt;p>&lt;strong>Model sizes&lt;/strong> 기본 모델 크기를 확장하면 성능이 향상된다는 것을 확인 했지만, 컴퓨팅 자원이 제한된 상황에서는 작은 모델이 유용할 수 있다. 이를 고려하여, 다양한 크기의 모델을 학습시킨다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Base&lt;/strong> 이 모델은 대략 220M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>Small&lt;/strong> 기본 모델을 축소시키기 위해 512의 $d_{model}$, 2,048의 $d_{ff}$, 8개의 attention head, 그리고 encoder와 decoder 각각에 6개의 layer를 사용한다. 이 모델은 대략 60M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>Large&lt;/strong> 기본 모델이 $BERT_{BASE}$ 크기의 encoder와 decoder를 사용하기 때문에, $BERT_{LARGE}$와 비슷한 크기와 구조를 가진 encoder와 decoder를 가진 변형을 고려하였다. 이 변형은 약 770M 개의 parameter를 가지고 있다.&lt;/li>
&lt;li>&lt;strong>3B and 11B&lt;/strong> 더 큰 모델을 사용할 때 가능한 성능을 탐색하기 위해 두 가지 추가 변형을 고려하였다. 두 경우 모두에서 $d_{model} = 1024$, 24개 layer의 encoder와 decoder, 그리고 $d_{ff} = 128$을 사용하였다. &amp;ldquo;3B&amp;rdquo; 변형은 $d_{ff} =$ 16,384와 32개의 attention head를 사용하여 약 2.8B의 parameter를 생성했고, &amp;ldquo;11B&amp;rdquo; 변형은 $d_{ff} =$ 65,536과 128개의 attention head를 사용하여 약 11B개의 parameter를 가진 모델을 생성하였다.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Multi-task pre-training&lt;/strong> 비지도 작업과 지도 작업의 다양한 사전 학습된 모델이 비지도 작업만으로 학습된 모델만큼 잘 작동한다는 것을 확인하였다. 이 방법은 학습 기간 동안 성능을 지속적으로 모니터링 할 수 있어 유용하다. 따라서, 최종 실험에서 이 multi-task 사전 학습 방식을 사용했다. 또한, 더 크고 오래 훈련된 모델은 레이블이 없는 데이터의 더 큰 비율에서 이익을 얻을 것으로 예상했다. 이를 바탕으로, 레이블이 없는 데이터를 위해 특정한 인공 데이터 세트 크기를 사용했고, 모든 모델 변형에 대해 WMT English to French 및 WMT English to German 데이터 세트의 크기를 사전 학습 동안 1M 예제로 제한하였다.&lt;/p>
&lt;p>&lt;strong>Fine-tuning on individual GLUE and SuperGLUE tasks&lt;/strong> GLUE와 SuperGLUE에서 미세 조정할 때, 모든 데이터 세트를 합쳐서 한 번에 모델을 미세 조정하였다. 이 방식은 연구를 단순화하지만, 일부 작업에서는 성능이 약간 떨어진다는 것을 발견하였다. 개별 작업에 미세 조정하는 것은 low-resource 작업에 빠르게 overfit될 위험이 있다. 그래서 각 GLUE와 SuperGLUE 작업에 대한 미세 조정 시 작은 배치 크기를 사용하고, overfit되기 전에 모델의 parameter에 접근할 수 있도록 1,000 단계마다 체크포인트를 저장하였다.&lt;/p>
&lt;p>&lt;strong>Beam search&lt;/strong> 이전 결과는 모두 greedy decoding을 사용하여 보고되었다. 그러나 긴 출력 시퀀스 작업에서는 beam search로 성능이 향상되었다. WMT 번역과 CNN/DM 요약 작업에서는 beam width 4와 길이 패널티 $\alpha = 0.6$을 사용하였다.&lt;/p>
&lt;p>&lt;strong>Test set&lt;/strong> 최종 실험에서는 validation set가 아닌 test set 결과를 보고한다. CNN/Daily Mail은 standard test set를, WMT 작업은 각 언어 쌍에 대한 특정 newstest를 사용했다. GLUE와 SuperGLUE는 벤치마크 평가 서버를 통해 test set 점수를 계산했다. SQuAD의 경우, 벤치마크 서버의 컴퓨팅 자원이 부족해 가장 큰 모델에서의 예측을 얻지 못했으므로, 검증 세트 성능을 계속 보고하게 되었다. 하지만 SQuAD test set에서 가장 높은 성능을 보인 모델이 검증 세트 결과도 보고했으므로, state-of-the-art의 기술과 비교 가능하다.&lt;/p>
&lt;p>위에서 언급한 변경사항들을 제외하고, baseline과 같은 학습 절차와 hyperparameter를 사용한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table14.png"
width="1118"
height="1358"
srcset="https://kurtkim.github.io/p/t5/images/table14_hu9a784fc3b3eaf96fb53a769d5658efb8_304457_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table14_hu9a784fc3b3eaf96fb53a769d5658efb8_304457_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="82"
data-flex-basis="197px"
>&lt;/p>
&lt;p>전반적으로, 24개의 작업 중 18개에서 state-of-the-art를 달성하였다. 가장 큰 모델(11B parameter)이 모든 작업에서 가장 우수한 성능을 보였다. T5-3B 모델은 몇몇 작업에서 이전 state-of-the-art를 능가했지만, 11B parameter로 모델 크기를 확장하는 것이 state-of-the-art를 달성하는 데 가장 중요했다.&lt;/p>
&lt;p>T5는 평균 GLUE 점수에서 90.3의 state-of-the-art를 달성하였고, 특히 자연어 추론 작업에서 이전 state-of-the-art 보다 월등히 높은 성능을 보여주었다. 그러나 이 성능은 여러 모델의 앙상블과 대량의 계산을 활용한 결과였다. SQuAD에서는 Exact Match 점수에서 이전 최고 성능을 1점 이상 능가하였으며, SuperGLUE에서는 평균 점수를 크게 향상시켰지만, 일부 작업에서는 여전히 인간의 성능에 미치지 못했다. WMT 번역 작업에서는 state-of-the-art를 달성하지 못했고, CNN/Daily Mail에서는 state-of-the-art를 달성했지만, 요약의 일관성과는 반드시 연결되지 않았다. 이 모든 결과는 앙상블, 외부 데이터 세트 활용 등 다양한 방법을 통해 성능을 향상시키고 있음을 보여준다.&lt;/p>
&lt;p>T5는 실험 연구의 통찰력과 큰 규모를 결합하여 강력한 성능을 보여준다. baseline 모델의 사전 훈련량 또는 크기를 증가시키면 상당한 향상이 있었고, 이를 바탕으로 T5의 성능 향상에 얼마나 기여했는지 측정하고자 하였다. 이를 위해 표준 baseline 모델, 1 trillion 토큰으로 훈련된 모델 및 T5-Base를 비교하는 실험을 수행하였다. 이 두 모델의 성능 비교를 통해, 체계적인 연구에서 얻은 통찰력이 어떤 영향을 미쳤는지 구체적으로 측정할 수 있다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/t5/images/table15.png"
width="942"
height="176"
srcset="https://kurtkim.github.io/p/t5/images/table15_hu3181bf2f435ab289c1044f5a8f3acab4_38762_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/t5/images/table15_hu3181bf2f435ab289c1044f5a8f3acab4_38762_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="535"
data-flex-basis="1284px"
>&lt;/p>
&lt;p>세 가지 모델 구성의 성능은 T5-Base가 모든 downstream task에서 baseline-1T을 월등히 능가함을 보여준다. 이는 T5의 성공에 크기 뿐만 아니라 다른 비확장 요소들도 기여하고 있음을 나타낸다. 추가적인 사전 학습은 모델의 성능을 향상시키는 데 도움이 된다.&lt;/p>
&lt;h2 id="reflection">Reflection&lt;/h2>
&lt;p>이 분야의 더 나은 발전을 위해 효과적인 접근법을 제안하고자 한다.&lt;/p>
&lt;h3 id="takeaways">Takeaways&lt;/h3>
&lt;p>&lt;strong>Text-to-text&lt;/strong> Text-to-text 프레임워크는 다양한 텍스트 작업에 대해 단일 모델을 훈련시키는 간단한 방법을 제공한다. 이 방법은 생성, 분류, 회귀 작업 등에 성공적으로 적용될 수 있다. 간결함에도 불구하고, 이 프레임워크는 과제별 구조와 비교할 만한 성능을 보여주고, 규모와 결합하면 state-of-the-art를 달성한다.&lt;/p>
&lt;p>&lt;strong>Architectures&lt;/strong> NLP의 전이 학습에서 Transformer의 구조적 변형을 고려했지만, 원래의 encoder-decoder 형태가 text-to-text 프레임워크에서 가장 효과적이었다. encoder-decoder 모델은 더 많은 parameter를 사용하지만, 계산 비용은 비슷하다. encoder와 decoder에서 parameter를 공유하면 전체 parameter 수가 절반으로 줄어들지만 성능 저하는 별로 없다.&lt;/p>
&lt;p>&lt;strong>Unsupervised objectives&lt;/strong> 대부분의 &amp;ldquo;denoising&amp;rdquo; 목표, 즉 임의로 손상된 텍스트를 재구성하는 학습은 text-to-text 설정에서 비슷한 성능을 보였다. 따라서, 계산 효율성을 위해 짧은 대상 시퀀스를 생성하는 목표를 사용하는 것이 좋다.&lt;/p>
&lt;p>&lt;strong>Data sets&lt;/strong> Common Crawl 웹 덤프에서 정리한 &amp;ldquo;Colossal Clean Crawled Corpus (C4)&amp;ldquo;를 소개하였다. 레이블이 없는 데이터로 학습하면 몇몇 downstream task에서 성능이 향상될 수 있지만, 데이터셋 크기가 줄어들 수 있다. 레이블이 없는 데이터셋이 작아서 여러 번 반복되면 성능이 저하될 수 있으므로, 크고 다양한 데이터셋인 C4의 사용이 중요하다는 것을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Training strategies&lt;/strong> 사전 학습된 모델의 모든 parameter를 미세 조정하는 방식이 비록 비용이 많이 들지만 더 우수한 성능을 보여주었다. 여러 작업을 동시에 학습하는 방법을 시도했지만, 특정 작업에 대한 학습 비율을 설정하는 전략을 찾지 못하였다. 하지만, 여러 작업의 혼합에서 사전 학습 후 미세 조정하는 것이 비지도 사전 학습과 비슷한 성능을 보여주었다.&lt;/p>
&lt;p>&lt;strong>Scaling&lt;/strong> 추가 계산을 활용하는 다양한 전략을 비교한 결과, 각 방법이 성능을 크게 향상시켰으나, 더 많은 데이터로 작은 모델을 학습하는 것이 종종 더 적은 단계로 큰 모델을 학습하는 것보다 성능이 떨어졌다. 그러나 모델의 앙상블은 단일 모델보다 더 좋은 결과를 제공하였고, 같은 baseline 모델에서 미세 조정한 앙상블은 모든 모델을 별도로 학습하는 것보다는 성능이 떨어졌지만, 단일 모델보다는 훨씬 더 우수하였다.&lt;/p>
&lt;p>&lt;strong>Pushing the limits&lt;/strong> state-of-the-art를 얻기 위해 큰 모델(최대 11B 개의 parameter)을 학습시켰다. 이를 위해 C4 데이터 세트에서 텍스트를 추출하고, 노이즈를 제거하는 목표를 적용하였다. 또한, 1 trillion 이상의 토큰에 대해 학습을 진행하였고, 결과를 쉽게 복제하고 확장하고 적용하기 위해 우리의 코드, C4 데이터 세트, 그리고 각 T5 변형에 대한 사전 학습된 모델 가중치를 공개하였다.&lt;/p>
&lt;h3 id="outlook">Outlook&lt;/h3>
&lt;p>&lt;strong>The inconvenience of large models&lt;/strong> 큰 모델이 더 좋은 성능을 내는 경향이 있다는 것이 중요한 결과로 나타났다. 하지만 client-side inference이나 federated learning과 같이 작은 모델이 도움이 되는 경우도 있다. low-resource 작업에서 좋은 성능을 얻는 것이 전이 학습의 한 가지 유익한 사용처이다. 따라서 저렴한 모델로 더 강한 성능을 달성하는 방법에 대한 연구를 지지한다. 이런 연구로는 distillation, parameter sharing, 그리고 conditional computation이 있다.&lt;/p>
&lt;p>&lt;strong>More efficient knowledge extraction&lt;/strong> 사전 학습의 목표는 모델에 &amp;ldquo;knowledge&amp;quot;를 제공하여 downstream task의 성능을 향상시키는 것이다. 현재 일반적으로 사용되는 방법은 텍스트의 오염된 부분을 복원하도록 학습시키는 것인데, 이 방법이 모델에 일반 지식을 가르치는 가장 효율적인 방법이 아닐 수도 있다. 더 효율적인 방법으로는, 실제 텍스트와 기계 생성 텍스트를 구분하도록 모델을 사전 학습시키는 방법이 있다.&lt;/p>
&lt;p>&lt;strong>Formalizing the similarity between tasks&lt;/strong> 도메인 데이터에 대한 사전 학습이 downstream task의 성능을 향상시키는 것을 확인하였다. 레이블이 없는 데이터 소스를 선택하는 데에 더 원칙적인 접근을 가능하게 하기 위해, 사전 학습과 downstream task 사이의 &amp;ldquo;similarity&amp;quot;에 대한 엄밀한 개념을 정립하는 것이 필요하다. 이는 컴퓨터 비전 분야에서 이미 일부 연구가 이루어지고 있다. 또한, 작업 간의 관련성에 대한 더 나은 이해는 지도 사전 학습 작업을 선택하는 데에도 도움이 될 수 있다.&lt;/p>
&lt;p>&lt;strong>Language-agnostic models&lt;/strong> 영어로만 사전 학습한 결과가 번역 작업에서 최고 수준의 성과를 내지 못하였다. 이를 해결하기 위해, 어떤 언어의 텍스트든 좋은 성능으로 NLP 작업을 수행할 수 있는 언어에 구애받지 않는 모델을 더 연구하려고 한다. 이는 세계 인구 대다수의 모국어가 영어가 아닌 점을 고려하면 매우 중요한 이슈이다.&lt;/p>
&lt;p>이 논문은 최근 NLP에 대한 전이 학습에 대해 연구하였다. 이 연구가 시작되기 전, 학습 기반 방법이 효율성을 증명받지 못 한 상황에서 이러한 진보가 돌파구를 만들어 주었으며, 특히 전이 학습에 어려운 SuperGLUE 벤치마크에서 거의 인간 수준의 성능을 달성하였다. 이 결과는 우리의 text-to-text 프레임워크, 새로운 C4 데이터셋, 그리고 체계적인 연구에서의 통찰력의 결합에서 비롯된다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/text-to-text-transfer-transformer" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>XLNet</title><link>https://kurtkim.github.io/p/xlnet/</link><pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/xlnet/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>BERT와 같은 denoising autoencoding 기반 사전 학습은 양방향 컨텍스트를 학습할 수 있지만, 마스크를 사용하여 입력을 변조함으로써 의존성을 무시하고 사전 학습과 미세 조정 사이의 괴리를 겪는다. 이를 해결하기 위해, XLNet이라는 새로운 사전 학습 방법을 제안한다. 이 방법은 모든 순열에 대한 기대 가능도를 최대화하여 양방향 컨텍스트를 학습하고, autoregressive 형식을 통해 BERT의 제한을 극복한다. 실험결과 XLNet은 여러 작업에서 BERT를 능가하는 결과를 보여주었다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리 분야에서 Unsupervised representation learning은 큰 성공을 거두었다. 이 방법은 대규모의 레이블이 없는 텍스트 코퍼스에서 신경망을 사전 학습하고, 이를 특정 작업에 맞게 미세 조정하는 방식이다. 이에 대한 연구 중, autoregressive(AR)와 autoencoding(AE)이 가장 좋은 결과를 보여주었다.&lt;/p>
&lt;p>AR 언어 모델링은 텍스트 시퀀스의 확률 분포를 추정하기 위해 autoregressive 모델을 사용한다. 텍스트 시퀀스 $x = (x_1, &amp;hellip;, x_T)$가 주어지면, AR 언어 모델링은 가능성을 순방향 곱인 $p(x) = \prod_{t=1}^T p(x_t | X_{&amp;lt;t})$ 또는 역방향 곱인 $p(x) = \prod_{t=T}^1 p(x_t | X_{&amp;gt;t})$로 분해하며, 각 조건부 분포를 모델링하기 위해 신경망 같은 매개변수 모델을 훈련한다. 그러나 AR 언어 모델은 단방향 컨텍스트만 인코딩하므로 깊은 양방향 컨텍스트를 모델링하는데는 비효율적이다. 이는 언어 이해 작업에서 필요한 양방향 컨텍스트 정보와의 간극을 만든다.&lt;/p>
&lt;p>AE 기반 사전 학습, 예를 들면 BERT는 손상된 입력에서 원본 데이터를 재구성하는 것을 목표로 한다. 입력 토큰의 일부가 [MASK] 같은 특수 기호로 대체되고, 이를 원래 토큰으로 복구하도록 모델이 학습된다. BERT는 양방향 컨텍스트를 사용해 재구성할 수 있으므로, AR 언어 모델링의 양방향 정보 간극을 해결하고 성능을 향상시킬 수 있다. 하지만, BERT가 사전 훈련 중에 사용하는 인위적인 기호들은 미세 조정 시 실제 데이터에는 존재하지 않아, 사전 학습과 미세 조정 사이의 불일치를 초래하며, 예측된 토큰 간의 상호 의존성을 과도하게 단순화한다.&lt;/p>
&lt;p>XLNet은 AR 언어 모델링과 AE의 장점을 모두 활용하면서 단점은 피하는 generalized autoregressive 방법이다.&lt;/p>
&lt;ul>
&lt;li>XLNet은 고정된 순방향이나 역방향 분해 순서를 사용하는 대신, 모든 가능한 분해 순서의 순열에 대한 시퀀스의 기대 log-likelihood를 최대화한다. 이로 인해 각 위치의 컨텍스트는 왼쪽과 오른쪽의 토큰들로 구성될 수 있으며, 모든 위치의 컨텍스트 정보를 활용하며 양방향 컨텍스트를 포착한다.&lt;/li>
&lt;li>XLNet은 generalized AR 언어 모델로서, 데이터 손상에 의존하지 않아 BERT의 사전 학습과 미세 조정 간의 불일치 문제를 겪지 않는다. 또한, autoregressive 목표는 예측된 토큰의 joint probability를 분해하는데 곱셈 규칙을 사용하므로, BERT의 독립 가정을 제거한다.&lt;/li>
&lt;/ul>
&lt;p>추가적으로, XLNet은 사전 학습을 위한 architecture design을 개선하였다.&lt;/p>
&lt;ul>
&lt;li>XLNet은 Transformer-XL의 segment recurrence mechanism과 relative encoding scheme를 사전 학습에 통합하여, 긴 텍스트 시퀀스를 다루는 작업의 성능을 향상시켰다.&lt;/li>
&lt;li>Transformer-XL architecture를 naive하게 적용하는 것은 어렵기 때문에, Transformer-XL network를 reparameterize해서 사용하였다.&lt;/li>
&lt;/ul>
&lt;p>같은 실험 환경에서, XLNet은 언어 이해, 읽기 이해, 텍스트 분류, 문서 랭킹 등 다양한 작업에서 BERT를 능가하는 성능을 보여주었다.&lt;/p>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;p>순열 기반 AR 모델링은 이전에도 연구되었지만, XLNet은 언어 모델이 양방향 컨텍스트를 학습하는 것을 목표로 하고, 이를 위해 two-stream attention을 통해 목표 위치를 hidden state에 통합한다. 이전 모델들과는 다르게, &amp;ldquo;순서 없음&amp;quot;은 입력 시퀀스가 무작위로 순열될 수 있다는 의미가 아니라, 모델이 분포의 다양한 분해 순서를 허용한다는 것을 의미한다.&lt;/p>
&lt;p>다른 관련 아이디어로는 텍스트 생성에서 autoregressive denoising을 수행하는 것이 있다. 이 방법은 고정된 순서만을 고려하고 있다.&lt;/p>
&lt;h2 id="proposed-method">Proposed Method&lt;/h2>
&lt;h3 id="background">Background&lt;/h3>
&lt;p>전통적인 AR 언어 모델링과 BERT를 비교하며, 텍스트 시퀀스가 주어지면 AR 언어 모델링은 forward autoregressive factorization을 통해 likelihood를 최대화하여 사전 학습을 수행한다:&lt;/p>
&lt;p>$$ \underset{\theta}{max} \quad log \ p_{\theta}(x) = \sum_{t=1}^{T} log \ p{\theta}(x_t | x &amp;lt; t) = \sum_{t=1}^{T} log \ {{exp(h_{\theta}(x_{1:t-1})^\intercal e(x_t))}\over{\sum_{x&amp;rsquo;} exp(h_{\theta}(x_{1:t-1})^\intercal e(x&amp;rsquo;))}} $$&lt;/p>
&lt;p>$h_θ(x_{1:t−1})$는 RNN이나 Transformer와 같은 신경 모델로 생성된 컨텍스트 표현이며, $e(x)$는 $x$의 임베딩이다. 반면에, BERT는 노이즈 제거 자동 인코딩에 기반하며, 텍스트 시퀀스 $x$의 일부 토큰을 특수 심볼 [MASK]로 바꿔 손상된 버전 $x$를 만든다. 이후의 훈련 목표는 $x$로부터 마스크된 토큰을 재구성하는 것이다.&lt;/p>
&lt;p>$$ \underset{\theta}{max} \quad log \ p_{\theta}(\bar{x}|\hat{x}) \approx \sum_{t=1}^{T} m_t \ log \ p{\theta}(x_t | \hat{x}) = \sum_{t=1}^{T} m_t \ log \ {{exp(H_{\theta}(\hat{x})^\intercal_t e(x_t))}\over{\sum_{x&amp;rsquo;} exp(H_{\theta}(\hat{x})^\intercal_t e(x&amp;rsquo;))}} $$&lt;/p>
&lt;p>$m_t = 1$은 $x_t$가 마스크되었음을 나타내며, $H_{\theta}$는 텍스트 시퀀스 $x$를 숨겨진 벡터의 시퀀스로 변환하는 Transformer이다. 두 사전 학습 목표의 장단점은 다음에서 비교된다:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Independence Assumption&lt;/strong>: BERT는 모든 마스크된 토큰이 독립적으로 재구성된다는 가정 하에 결합 조건 확률 $p(\bar{x}|\hat{x})$을 분해한다. 반면, AR 언어 모델링은 독립 가정 없이 곱셈 법칙을 사용하여 확률 $p_{\theta}(x)$을 분해한다.&lt;/li>
&lt;li>&lt;strong>Input noise&lt;/strong>: BERT의 입력에는 [MASK]와 같은 인공적인 심볼이 포함되어 있어 사전 학습과 미세 조정간의 불일치가 발생한다. [MASK]를 원래 토큰으로 대체해도 문제가 해결되지 않는다. 반면, AR 언어 모델링은 입력 손상에 의존하지 않아 이 문제가 없다.&lt;/li>
&lt;li>&lt;strong>Context dependency&lt;/strong>: AR 표현 $h_θ(x_{1:t−1})$은 위치 $t$까지의 토큰에만 의존하는 반면, BERT 표현은 양쪽의 문맥 정보에 접근할 수 있다. 따라서, BERT는 양방향 문맥을 더 잘 포착하도록 사전 학습된다.&lt;/li>
&lt;/ul>
&lt;h3 id="objective-permutation-language-modeling">Objective: Permutation Language Modeling&lt;/h3>
&lt;p>orderless NADE에서 아이디어를 차용하여, AR 모델의 장점을 유지하면서 양방향 문맥을 포착하는 순열 언어 모델링을 제안한다. 모델 파라미터가 모든 분해 순서에 공유되면, 모델은 양쪽 모든 위치에서 정보를 수집하도록 학습할 수 있다.&lt;/p>
&lt;p>길이가 $T$인 인덱스 시퀀스의 모든 가능한 순열 집합을 $Z_T$라고 할때, $z_t$와 $z &amp;lt; t$는 순열 $z$의 $t$번째 요소와 첫 $t−1$개 요소를 나타낸다. 그러면, 순열 언어 모델링 목표는 다음과 같이 표현될 수 있다:&lt;/p>
&lt;p>$$ \underset{\theta}{max} \quad \mathbb{E_{\mathbf{z} \sim \mathbf{Z_T}}} \big[ \sum_{t=1}^{T} \ log \ p{\theta}(x_{z_t} | x_{z_{&amp;lt;t}}) \big] $$&lt;/p>
&lt;p>텍스트 시퀀스 $x$에 대해, 인수분해 순서 $z$를 샘플링하고, 이 순서에 따라 가능성 $p_{\theta}(x)$을 분해한다. 모델 파라미터가 모든 인수분해 순서에 공유되므로, $x_t$는 시퀀스 내의 모든 가능한 요소를 볼 수 있어 양방향 문맥을 포착할 수 있다. 또한, AR 프레임워크에 적합하여 독립 가정과 사전 학습-미세조정 차이를 피할 수 있다.&lt;/p>
&lt;h3 id="remark-on-permutation">Remark on Permutation&lt;/h3>
&lt;p>인수분해 순서만을 바꾸며, 시퀀스 순서는 그대로 유지한다. 원래 시퀀스에 대응하는 positional encoding을 사용하고, Transformer의 적절한 attention mask를 활용한다. 이 방법은 모델이 미세 조정 동안 자연스러운 순서의 텍스트 시퀀스를 만나기 때문에 필요하다.&lt;/p>
&lt;h2 id="architecture-two-stream-self-attnetion-for-target-aware-representations">Architecture: Two-Stream Self-Attnetion for Target-Aware Representations&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/figure1.png"
width="1032"
height="486"
srcset="https://kurtkim.github.io/p/xlnet/images/figure1_hu64f0e34f291245cd02d7461e9fe0c5ea_114301_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/figure1_hu64f0e34f291245cd02d7461e9fe0c5ea_114301_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="212"
data-flex-basis="509px"
>&lt;/p>
&lt;p>순열 언어 모델링 목표는 원하는 속성을 가지지만, 표준 Transformer 파라미터화를 사용한 단순한 구현은 문제가 있다. Softmax를 사용하여 다음 토큰 분포를 파라미터화하면:&lt;/p>
&lt;p>$$ p_{\theta}(X_{z_t} = x | x_{z_{&amp;lt; t}}) = {{exp(e(x)^\intercal h_{\theta}(x_{z_t}))}\over{\sum_{x&amp;rsquo;} exp(e(x&amp;rsquo;)^\intercal h_{\theta}(x_{z_t})}} $$&lt;/p>
&lt;p>예측할 위치에 따라 표현이 달라지지 않는다. 이로 인해 대상 위치에 상관없이 동일한 분포가 예측되어 유용한 표현을 학습할 수 없다. 이 문제를 해결하기 위해, 다음 토큰 분포를 대상 위치를 인식하도록 re-parameterize 하는 것을 제안한다:&lt;/p>
&lt;p>$$ p_{\theta}(X_{z_t} = x | x_{z_{&amp;lt; t}}) = {{exp(e(x)^\intercal g_{\theta}(x_{z_t}, z_t))}\over{\sum_{x&amp;rsquo;} exp(e(x&amp;rsquo;)^\intercal g_{\theta}(x_{z_t}, z_t)}} $$&lt;/p>
&lt;p>$g_{\theta}(x_{z_t}, z_t)$는 대상 위치 $z_t$를 입력으로 추가로 받는 새로운 유형의 표현을 나타낸다.&lt;/p>
&lt;h3 id="two-stream-self-attention">Two-Stream Self-Attention&lt;/h3>
&lt;p>target-aware representation의 개념은 대상 예측의 불명확성을 제거하지만, $g_{\theta}(x_{z_t}, z_t)$를 어떻게 형성할 것인지는 복잡한 문제이다. 대상 위치에서 정보를 수집하는 것을 제안하며, 이를 위해 두 가지 요구사항이 필요하다:&lt;/p>
&lt;ul>
&lt;li>토큰 $x_{z_t}$를 예측하기 위해, $g_{\theta}(x_{z_{&amp;lt;t}}, z_t)$는 위치 $z_t$만 사용하고 내용 $x_{z_t}$는 사용하지 않아야 한다.&lt;/li>
&lt;li>다른 토큰 $x_{z_j}$를 예측하기 위해 $(j &amp;gt; t)$, $g_{\theta}(x_{z_{&amp;lt;t}}, z_t)$는 전체 컨텍스트 정보를 제공하기 위해 내용 $x_{z_t}$도 인코딩해야 한다.&lt;/li>
&lt;/ul>
&lt;p>이 모순을 해결하기 위해, 하나 대신 두 세트의 숨겨진 표현을 사용한다.&lt;/p>
&lt;ul>
&lt;li>content representation $h_{\theta}(x_{z_{&amp;lt;t}})$는 컨텍스트와 $x_{z_t}$ 자체를 인코딩하는 Transformer의 standard hidden states와 유사한 역할을 한다.&lt;/li>
&lt;li>query representation $g_{\theta}(x_{z_t}, z_t)$는 컨텍스트 정보와 위치에만 접근할 수 있으며, 내용 $x_{z_t}$에는 접근할 수 없다.&lt;/li>
&lt;/ul>
&lt;p>첫번째 layer query stream은 학습 가능한 벡터로 초기화된다. (i.e $\ g^{(0)}_i = w$)
반면에 content stream은 해당 단어 임베딩으로 설정된다. (i.e $\ h^{(0)}_i = e(x_i)$)
self-attention layer $m = 1, &amp;hellip;, M$에 대해, two-streams의 표현은 공유된 파라미터 세트로 다음과 같이 업데이트된다.&lt;/p>
&lt;p>$$ g_{z_t}^{(m)} \leftarrow Attention(Q = g_{z_t}^{(m-1)}, KV = h_{z_{&amp;lt;t}}^{(m-1)}; \theta) $$
$$ h_{z_t}^{(m)} \leftarrow Attention(Q = h_{z_t}^{(m-1)}, KV = h_{z_{\leq t}}^{(m-1)}; \theta) $$&lt;/p>
&lt;p>여기서 Q, K, V는 attention 연산에서의 query, key, value를 나타낸다. content representation의 업데이트 규칙은 standard self-attention과 같으므로, 미세 조정 중에 query stream을 중단하고 content stream을 일반 Transformer-XL로 사용할 수 있다. last-layer query representation $g_{z_t}^{(m)}$를 사용하여 $p_{\theta}(X_{z_t} = x | x_{z_{&amp;lt; t}})$를 계산할 수 있다.&lt;/p>
&lt;h3 id="partial-prediction">Partial Prediction&lt;/h3>
&lt;p>순열 언어 모델링은 여러 이점이 있지만, 순열로 인해 최적화가 어려워 수렴이 느리다. 이를 해결하기 위해, 분해 순서에서 마지막 토큰만 예측하도록 선택하였다. 즉, 전체를 목표가 아닌 부분과 목표 부분으로 나누고, 목표가 아닌 부분에 따른 목표 부분의 log-likelihood를 최대화한다.&lt;/p>
&lt;p>$$ \underset{\theta}{max} \quad \mathbb{E_{\mathbf{z} \sim \mathbf{Z_T}}} \big[ log \ p{\theta}(x_{z_{&amp;gt;c}} | x_{z_{\geq t}}) \big] = \mathbb{E_{\mathbf{z} \sim \mathbf{Z_T}}} \big[ \sum_{t=c+1}^{|z|} \ log \ p{\theta}(x_{z_t} | x_{z_{&amp;lt; t}}) $$&lt;/p>
&lt;p>분해 순서에 따라 가장 긴 컨텍스트를 가진 부분이 목표로 선택되며, hyperparameter $K$는 약 $1/K$ 토큰이 예측에 선택되도록 사용된다. (i.e $|z| / (|z| - c) \approx K$) 선택되지 않은 토큰들의 query representation을 계산할 필요가 없어, 메모리를 아끼고 속도를 향상시킬 수 있다.&lt;/p>
&lt;h3 id="incorporating-ideas-from-transformer-xl">Incorporating Ideas from Transformer-XL&lt;/h3>
&lt;p>Transformer-XL의 relative positional encoding scheme과 segment recurrence mechasnism을 통합하였으며, 이를 통해 모델이 이전 세그먼트의 hidden state를 재사용하게 할 수 있다. 긴 시퀀스에서 두 세그먼트를 가지고, 첫 번째 세그먼트를 처리한 후 얻은 내용을 캐시하고, 다음 세그먼트에 대해 attention을 업데이트한다.&lt;/p>
&lt;p>$$ h_{z_t}^{(m)} \leftarrow Attention(Q = h_{z_t}^{(m-1)}, KV = \big[ \tilde{h}^{(m-1)}, h_{z_{\leq t}}^{(m-1)} \big]; \theta) $$&lt;/p>
&lt;p>positional encoding은 원래 시퀀스의 실제 위치에만 의존하므로, 얻어진 표현이 있으면 attention 업데이트는 순열에 독립적이다. 이를 통해 이전 세그먼트의 분해 순서를 알지 못해도 메모리를 캐시하고 재사용할 수 있다. 모델은 마지막 세그먼트의 모든 분해 순서에 대해 메모리를 활용하는 방법을 학습하며, query stream도 같은 방식으로 계산된다.&lt;/p>
&lt;h3 id="modeling-multiple-segments">Modeling Multiple Segments&lt;/h3>
&lt;p>다양한 작업에서 여러 입력 세그먼트가 필요하다. XLNet의 사전 학습 과정에서는 이를 고려하여 두 세그먼트를 무작위로 샘플링하고, 이를 하나의 시퀀스로 연결하여 순열 언어 모델링을 수행한다. 동일한 문맥에서만 메모리를 재사용하며, 이 과정은 BERT의 입력 형식 [CLS, A, SEP, B, SEP]을 따른다. 그러나 XLNet-Large는 일관된 성능 향상을 보이지 않아 다음 next sentence prediction를 사용하지 않습니다.&lt;/p>
&lt;h3 id="relative-segment-encodings">Relative Segment Encodings&lt;/h3>
&lt;p>구조적으로, BERT와 달리 XLNet은 Transformer-XL의 relative encoding 개념을 확장하여 세그먼트를 인코딩한다. 두 위치가 같은 세그먼트에 있는지 여부만 고려하여, 세그먼트 인코딩을 이용해 attention 가중치를 계산한다. relative segment encoding을 사용하면 일반화를 개선하고 두 개 이상의 입력 세그먼트를 가진 작업에서 미세 조정할 수 있는 가능성이 있다.&lt;/p>
&lt;h3 id="discussion">Discussion&lt;/h3>
&lt;p>BERT와 XLNet 모두 일부 토큰만 예측하는 부분적 예측을 수행한다. 이는 의미 있는 예측을 위해 필요하며, 충분한 문맥을 가진 토큰만 예측함으로써 최적화의 어려움을 줄인다. 그러나 BERT는 대상 간의 종속성을 모델링할 수 없다.&lt;/p>
&lt;p>BERT와 XLNet의 차이점을 이해하기 위해 예시 [New, York, is, a, city]에 대해 비교 해 보면, 예시에서 [New, York] 두 개의 token을 예측하고 $log \ p(\text{New York} | \text{is a city})$를 maximize한다고 가정한다. 또한, XLNet의 인수분해 순서는 [is, a, city, New, York]이라고 가정한다. BERT와 XLNet의 object는 다음과 같다.&lt;/p>
&lt;p>$$ \mathbf{J}_{BERT} = log \ p(\text{New} | \text{is a city}) + log \ p(\text{York} | \text{is a city}) $$&lt;/p>
&lt;p>$$ \mathbf{J}_{XLNet} = log \ p(\text{New} | \text{is a city}) + log \ p(\text{York} | \text{New, is a city}) $$&lt;/p>
&lt;p>XLNet은 BERT가 생략하는 &amp;lsquo;New&amp;rsquo;와 &amp;lsquo;York&amp;rsquo; 사이의 종속성을 포착할 수 있다. 동일한 대상이 주어진 상황에서 XLNet은 더 많은 종속성 쌍을 학습하며, 더 밀도 높은 훈련 신호를 포함하고 있다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="pretraining-and-implementation">Pretraining and Implementation&lt;/h3>
&lt;p>BooksCorpus와 영어 Wikipedia를 사전 학습 데이터로 사용하고, 추가로 Giga5, ClueWeb 2012-B, 그리고 Common Crawl을 포함시켰다. 이 데이터들은 공격적인 필터링을 통해 짧거나 저품질의 기사를 제거한 후, 총 32.89B의 subword pieces로 토큰화하였다.&lt;/p>
&lt;p>XLNet-Large 모델은 BERT-Large와 같은 아키텍처를 가지고 있다. 사전 학습 동안 시퀀스 길이 512를 사용하였고, BERT와 비교하기 위해 BooksCorpus와 위키백과만을 이용해 훈련을 진행하였다. 그 후, 앞서 설명한 모든 데이터셋을 사용해 훈련을 확장하였고, 마지막으로, XLNet-Base-wikibooks를 기반으로 ablation study를 수행하였다.&lt;/p>
&lt;p>recurrence mechanism이 도입된 XLNet-Large 학습에서는 양방향 데이터 입력 파이프라인을 사용하고, 전방향과 후방향이 각각 배치 크기의 절반을 차지한다. prediction constant $K$는 6으로 설정되었으며, 미세 조정 절차는 BERT를 따른다. 추가로, 특정 길이의 토큰을 예측 대상으로 선택하는 span-based 예측 방법을 사용하였다.&lt;/p>
&lt;h3 id="fair-comparison-with-bert">Fair Comparison with BERT&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table1.png"
width="1067"
height="178"
srcset="https://kurtkim.github.io/p/xlnet/images/table1_hue3d17fe4c9e720db712837542685989d_41995_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table1_hue3d17fe4c9e720db712837542685989d_41995_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="599"
data-flex-basis="1438px"
>&lt;/p>
&lt;p>같은 데이터와 하이퍼파라미터로 훈련된 XLNet은 모든 고려된 데이터셋에서 BERT를 크게 앞서는 것으로 나타났다.&lt;/p>
&lt;h3 id="comparison-with-roberta-scaling-up">Comparison with RoBERTa: Scaling Up&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table2.png"
width="992"
height="210"
srcset="https://kurtkim.github.io/p/xlnet/images/table2_hub13544945018193398b025049834fd0f_54793_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table2_hub13544945018193398b025049834fd0f_54793_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="472"
data-flex-basis="1133px"
>&lt;/p>
&lt;p>ALBERT는 계산량을 크게 증가시키므로 과학적 결론을 도출하기 어려워, 다음 결과에서 제외되었다. 반면, RoBERTa는 전체 데이터를 기반으로 하고, RoBERTa의 하이퍼파라미터를 재사용하는 실험을 진행하였다.&lt;/p>
&lt;p>XLNet은 일반적으로 BERT와 RoBERTa를 능가하는 성능을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table3.png"
width="768"
height="292"
srcset="https://kurtkim.github.io/p/xlnet/images/table3_huf0364417c329187e97368cd7119f3806_68026_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table3_huf0364417c329187e97368cd7119f3806_68026_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table4.png"
width="996"
height="236"
srcset="https://kurtkim.github.io/p/xlnet/images/table4_hu6dfeb6a8f3889ccef2d4baa438dff1a6_53953_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table4_hu6dfeb6a8f3889ccef2d4baa438dff1a6_53953_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="422"
data-flex-basis="1012px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table5.png"
width="1068"
height="287"
srcset="https://kurtkim.github.io/p/xlnet/images/table5_hu4a2fea34ca197f53779b5bdc901342e3_83274_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table5_hu4a2fea34ca197f53779b5bdc901342e3_83274_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="372"
data-flex-basis="893px"
>&lt;/p>
&lt;ul>
&lt;li>SQuAD와 RACE와 같이 더 긴 컨텍스트를 다루는 명확한 추론 작업에 대해, XLNet의 성능 향상이 더 크다. 이러한 더 긴 컨텍스트를 다루는 능력은 XLNet의 Transformer-XL 기반 구조에서 나올 수 있다.&lt;/li>
&lt;li>MNLI(&amp;gt;390K), Yelp(&amp;gt;560K), Amazon(&amp;gt;3M)과 같이 이미 충분한 예제가 있는 분류 작업에 대해서도, XLNet은 큰 성능 향상을 보여주었다.&lt;/li>
&lt;/ul>
&lt;h2 id="ablation-study">Ablation Study&lt;/h2>
&lt;p>다양한 특성을 가진 네 가지 데이터셋을 기반으로 각 설계 선택의 중요성을 이해하기 위한 ablation study를 수행한다. 구체적으로, 연구하고자 하는 세 가지 주요 측면으로는 다음과 같다:&lt;/p>
&lt;ul>
&lt;li>순열 언어 모델링 목표의 효과, 특히 BERT에서 사용하는 denoising auto-encoding 목표와 비교&lt;/li>
&lt;li>Transformer-XL을 백본 아키텍처로 사용하는 것의 중요성&lt;/li>
&lt;li>범위 기반 예측, 양방향 입력 파이프라인, 그리고 다음 문장 예측을 포함한 일부 구현 세부사항의 필요성&lt;/li>
&lt;/ul>
&lt;p>다양한 구현 세부 사항을 가진 6개의 XLNet-Base 변형, 원래의 BERT-Base 모델, 그리고 BERT에서 사용된 denoising auto-encoding 목표로 훈련된 Transformer-XL 기준선을 비교한다. 모든 모델은 공평한 비교를 위해 BERT-Base와 동일한 모델 hyperparameter를 가진 12-layer 아키텍처를 기반으로 하며, Wikipedia와 BooksCorpus에서만 학습하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/xlnet/images/table6.png"
width="828"
height="302"
srcset="https://kurtkim.github.io/p/xlnet/images/table6_hu1098c6cce6ee530c011695b38df4e119_77715_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/xlnet/images/table6_hu1098c6cce6ee530c011695b38df4e119_77715_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="274"
data-flex-basis="658px"
>&lt;/p>
&lt;p>Transformer-XL과 순열 언어 모델링이 XLNet의 성능 향상에 크게 기여한다는 것을 알 수 있다. 메모리 캐싱 메커니즘을 제거하면 성능이 특히 떨어지며, 범위 기반 예측과 양방향 입력 파이프라인이 중요한 역할을 한다. 또한, BERT에서 제안된 다음 문장 예측 목표가 반드시 성능 향상을 가져오지 않으므로, 이를 XLNet에서 제외하였다.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>XLNet은 AR과 AE 방법의 이점을 결합하는 순열 언어 모델링을 사용하는 사전 학습 방법이다. Transformer-XL을 통합하고 two-stream attention mechanism을 설계하여 AR 목표와 원활하게 작동하도록 하였다. XLNet은 다양한 작업에서 이전 사전 학습 방법들 보다 큰 성능 개선을 보여주었다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ALBERT</title><link>https://kurtkim.github.io/p/albert/</link><pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/albert/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>자연어 표현을 사전 학습할 때 모델 크기를 늘리면 종종 성능이 향상되지만, GPU/TPU 메모리 한계와 학습 시간이 길어지는 문제가 있다. 이를 해결하기 위해, BERT의 메모리 소비를 줄이고 훈련 속도를 높이는 두 가지 기법을 제시하였다. 이 기법은 원래의 BERT보다 훨씬 더 잘 확장되는 모델을 만들며, 문장 간 일관성을 모델링하는 self-supervised loss를 사용하여 다문장 입력 작업에 도움이 된다. 결과적으로, BERT-large보다 parameter가 적은 ALBERT 모델은 GLUE, RACE, SQuAD benchmark에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Full network pre-training는 언어 학습에서 중요한 돌파구를 이끌어내었다. 이를 통해 학습 데이터가 제한된 NLP 작업들이 큰 도움을 받았다. 중국의 영어 시험을 위한 RACE 테스트에서 기계 성능은 초기 44.1%에서 최근에는 83.2%로 상승하였다. 이 연구를 통해 성능은 더욱 향상되어 89.4%에 이르렀다. 이는 주로 고성능 사전 학습 언어 표현 능력의 발전 덕분이다.&lt;/p>
&lt;p>이러한 개선사항들은 큰 네트워크가 state-of-the-art를 달성하는 데 중요함을 보여준다. 이제는 큰 모델을 사전 학습하고 작은 모델로 축소하는 것이 일반적인 방법이다. 그래서 이 연구에서는 &amp;ldquo;더 나은 NLP 모델을 가지는 것이 큰 모델을 가지는 것만큼 쉬운 것인지&amp;quot;라는 질문을 던지게 된다.&lt;/p>
&lt;p>이 질문에 대한 장애물은 사용 가능한 하드웨어의 메모리 제한이다. 현재의 state-of-the-art 모델들은 종종 수백만 개 또는 심지어 수십억 개의 parameter를 가지고 있어, 모델을 확장하려고 할 때 이러한 제한에 쉽게 부딪히게 된다. 또한, parameter 수에 비례하는 통신 오버헤드로 인해 분산 훈련시 훈련 속도가 크게 저하될 수 있다.&lt;/p>
&lt;p>이 논문에서는 매개변수가 더 적은 ALBERT 구조를 설계하여 이 문제들을 모두 해결하였다.&lt;/p>
&lt;p>ALBERT는 사전 학습된 모델 확장의 주요 장애물을 제거하는 두 가지 parameter 축소 기법을 사용한다. 첫 번째는 큰 어휘 임베딩 행렬을 작은 행렬로 분해하는 것, 두 번째는 네트워크 깊이에 따른 parameter 증가를 방지하는 것이다. 이 두 기법은 BERT의 매개변수 수를 크게 줄이고, BERT-large와 유사한 ALBERT 구성은 매개변수가 18배 더 적고, 약 1.7배 더 빠르게 훈련될 수 있게 한다. 이 기법들은 훈련을 안정화시키고 일반화에 도움을 주는 정규화 역할도 한다.&lt;/p>
&lt;p>ALBERT의 성능을 더욱 향상시키기 위해, sentence-order prediction(SOP)을 위한 self-supervised loss도 도입하였다. SOP는 주로 문장 간의 일관성에 초점을 맞추고 있으며, 원래 BERT에서 제안된 다음 next sentence prediction(NSP) loss의 비효율성을 해결하도록 설계되었다.&lt;/p>
&lt;p>그 결과, BERT-large보다 parameter는 더 적지만 성능은 훨씬 뛰어난 더 큰 ALBERT 구성으로 확장할 수 있었다. 그리고 자연 언어 이해를 위한 잘 알려진 GLUE, SQuAD, RACE 벤치마크에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="scaling-up-representation-learning-for-natural-language">Scaling up Representation Learning for Natural Language&lt;/h3>
&lt;p>자연 언어의 표현 학습은 다양한 NLP 작업에 유용하며 널리 채택되어왔다. 최근 가장 큰 변화는 단어 임베딩의 사전 학습에서, 전체 네트워크의 사전 학습 후 작업 특정 미세 조정으로 전환되었다는 것이다. 이러한 연구에서는 더 큰 모델 크기가 성능을 향상시키는 것이 종종 보여졌다. 하지만, 모델 크기와 계산 비용 문제로 인해 hidden size 는 1024에서 멈추게 되었다.&lt;/p>
&lt;p>큰 모델로 실험하는 것은 계산적 제약, 특히 GPU/TPU 메모리 제한 때문에 어렵다. 이 문제를 해결하기 위한 여러 방법이 제안되었는데, 그 중에는 gradient checkpointing과 각 layer의 activation 재구성 방법이 있다. 이런 방법들은 속도비용으로 메모리 사용량을 줄인다. 반면에, parameter reduction techniques는 메모리 사용을 줄이면서 훈련 속도를 높인다.&lt;/p>
&lt;h3 id="cross-layer-parameter-sharing">Cross-Layer Parameter Sharing&lt;/h3>
&lt;p>cross-layer parameter sharing의 아이디어는 이전에 Transformer 아키텍처에서 제안되었다. 하지만 이전 연구는 표준 인코더-디코더 작업에 초점을 맞추어져있었다. Dehghani et al. (2018)은 cross-layer parameter sharing를 가진 네트워크가 언어 모델링에서 더 나은 성능을 낸다고 보여주었다. 최근에는, Bai et al. (2019)이 Transformer 네트워크를 위한 Deep Equilibrium Model을 제안하였다. Hao et al. (2019)은 parameter-sharing transformer와 standard transformer를 결합하여 parameter 수를 늘렸다.&lt;/p>
&lt;h3 id="sentence-ordering-objectives">Sentence Ordering Objectives&lt;/h3>
&lt;p>ALBERT는 두 연속된 텍스트 세그먼트의 순서를 예측하는 사전 학습 loss를 사용한다. 이는 담화의 일관성에 관련된 다른 사전 학습 목표와 유사하다. 대부분의 효과적인 목표는 매우 단순하며, 이웃하는 문장의 단어를 예측하는 것에 기반한다. loss는 두 연속된 문장의 순서를 결정하기 위해 학습된 문장 임베딩과 가장 관련이 있다. 하지만, loss는 문장이 아닌 텍스트 세그먼트에 적용된다. BERT는 한 쌍에서 두 번째 세그먼트가 다른 문서의 세그먼트와 바뀌었는지 예측하는 loss을 사용하고, 이는 NSP가 더 도전적인 사전 학습 작업이며 특정 downstream task에 더 유용하다는 것을 보여준다.&lt;/p>
&lt;h2 id="the-elements-of-albert">The Elements of ALBERT&lt;/h2>
&lt;p>ALBERT의 설계 결정사항을 제시하고, BERT 아키텍처와 비교한다.&lt;/p>
&lt;h3 id="model-architecture-choices">Model Architecture Choices&lt;/h3>
&lt;p>ALBERT 아키텍처는 Transformer 인코더를 GELU 비선형성과 함께 사용하는 BERT와 유사하다. vocabulary embedding size는 $E$, encoder layer의 수는 $L$, hidden size는 $H$로 표기하며, feed-forward/ﬁlter size는 $4H$, attention head의 수는 $H/64$로 설정한다.&lt;/p>
&lt;p>ALBERT는 이러한 BERT의 설계에 대해 세 가지 주요한 개선점을 제시한다.&lt;/p>
&lt;h4 id="factorized-embedding-parameterization">Factorized embedding parameterization.&lt;/h4>
&lt;p>BERT, XLNet, RoBERTa 등의 모델에서는 WordPiece 임베딩 크기($E$)와 히든 레이어 크기($H$)가 같게 설정이 되었다. 이러한 결정은 모델링과 실용적인 측면에서 최적이 아닌 것으로 판단된다.&lt;/p>
&lt;p>모델링 측면에서 보면, WordPiece 임베딩은 문맥에 독립적인 표현을, 히든 레이어 임베딩은 문맥에 의존적인 표현을 학습한다. BERT와 같은 표현의 힘은 문맥을 이용한 학습에서 나온다는 것이 확인되었다. 따라서 WordPiece 임베딩 크기 $E$와 히든 레이어 크기 $H$를 분리하면, 모델링 요구사항에 따라 총 모델 parameter를 더 효율적으로 사용할 수 있다. 이는 실질적으로 $H$가 $E$보다 훨씬 커야 한다는 것을 의미한다.&lt;/p>
&lt;p>실용적으로 보면, 자연어 처리에서는 대체로 어휘 크기($V$)가 큰 편이다. 만약 $E$와 $H$가 같다면, $H$를 증가시키는 것은 임베딩 행렬의 크기를 증가시키게 되고, 이로 인해 parameter가 수십억 개가 되어버릴 수 있다. 더욱이, 이런 parameter들은 훈련 도중에는 대부분 희소하게만 업데이트된다.&lt;/p>
&lt;p>ALBERT에서는 임베딩 파라미터를 두 개의 작은 행렬로 분해한다. one-hot vector를 $H$ 크기의 hidden space로 투영하는 대신, 먼저 $E$ 크기의 lower dimensional embedding space로 투영한 후 hidden space으로 투영한다. 이 방식을 통해 임베딩 파라미터를 $O(V \times H)$에서 $O(V \times E \times E \times H)$로 줄일 수 있다. 이 parameter 축소는 $H$가 $E$보다 훨씬 클 때 중요하며, 모든 워드피스에 대해 같은 $E$를 사용하게 된다.&lt;/p>
&lt;h4 id="cross-layer-parameter-sharing-1">Cross-layer parameter sharing&lt;/h4>
&lt;p>ALBERT에서는 parameter 효율성을 향상시키는 방법으로 cross-layer parameter sharing를 제안한다. 여기에는 레이어 간에 feed-forward network (FFN) parameter만 공유하거나, attention parameter만 공유하는 등 여러 가지 방법이 있다. 그러나 ALBERT의 기본 설정은 모든 레이어 간에 모든 parameter를 공유하는 것이다. 이 설정은 특별히 명시되지 않는 한 모든 실험에서 사용된다.&lt;/p>
&lt;p>Universal Transformer와 Deep Equilibrium Models에서도 Transformer 네트워크에 대해 유사한 전략들이 연구되었다. Universal Transformer가 일반 Transformer를 능가한다고 보여주었고, Deep Equilibrium Models가 특정 레이어의 입력과 출력 임베딩이 동일하게 유지되는 균형점에 도달함을 보여주었다. 반면 이 논문의 측정 결과에서는 임베딩들이 수렴하기보다는 진동하고 있다는 것을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/figure1.png"
width="996"
height="326"
srcset="https://kurtkim.github.io/p/albert/images/figure1_hu0f8a585023b11c0ac06fe233d6d0fe1e_63214_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/figure1_hu0f8a585023b11c0ac06fe233d6d0fe1e_63214_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="305"
data-flex-basis="733px"
>&lt;/p>
&lt;p>ALBERT에서 레이어 간의 전환이 BERT보다 훨씬 부드럽다는 것을 관찰할 수 있다. 이는 parameter sharing이 network의 parameter를 안정화하는데 영향을 미친다는 것을 보여주며, 24개의 레이어 후에도 두 메트릭 모두 0으로 수렴하지 않는다.&lt;/p>
&lt;h4 id="inter-sentence-coherence-loss">Inter-sentence coherence loss&lt;/h4>
&lt;p>BERT는 masked language modeling(MLM) loss 외에도 next-sentence prediction(NSP) 이라는 추가적인 loss을 사용한다. NSP는 두 세그먼트가 원문에서 연속으로 나타나는지 예측하는 binary classiﬁcation loss이다. 이 목표는 문장 쌍 간의 관계에 대한 추론을 요구하는 자연어 추론과 같은 downstream task의 성능을 향상시키기 위해 설계되었다. 그러나 이후 연구에서는 NSP의 영향이 불안정하다고 판단하여 제거하기로 하였고, 이 결정은 여러 작업에서 downstream task 성능의 향상으로 지지받았다.&lt;/p>
&lt;p>NSP의 비효율성 뒤에 있는 주요 이유는 MLM과 비교했을 때 작업의 난이도가 부족하다는 것으로, NSP는 주제 예측과 일관성 예측을 하나의 작업에서 혼동시킨다. 그러나 주제 예측은 일관성 예측보다 학습하기 쉽고, MLM 손실을 사용하여 학습하는 것과 더 많이 겹친다.&lt;/p>
&lt;p>이 연구에서는 문장 간 모델링이 중요하다고 주장고 있으며, ALBERT에서는 주로 일관성에 기반한 sentence-order prediction(SOP) loss을 사용한다. SOP 손실은 모델이 담화 수준의 일관성에 대한 더 세밀한 구분을 학습하도록 만든다. NSP는 SOP 작업을 전혀 해결할 수 없지만, SOP는 NSP 작업을 어느 정도 해결할 수 있다. 결과적으로, ALBERT 모델은 다문장 인코딩 작업에 대한 성능을 일관되게 향상시킬 수 있었다.&lt;/p>
&lt;h3 id="model-setup">Model setup&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table1.png"
width="930"
height="224"
srcset="https://kurtkim.github.io/p/albert/images/table1_hu954e7bc0017b0f79a93e8e01016d57c1_47227_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table1_hu954e7bc0017b0f79a93e8e01016d57c1_47227_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="415"
data-flex-basis="996px"
>&lt;/p>
&lt;p>ALBERT 모델은 설계 선택 때문에 BERT 모델보다 parameter 크기가 훨씬 작다. 예를 들어, ALBERT-large는 BERT-large보다 parameter가 약 18배 작다. 이런 parameter 효율성 향상은 ALBERT의 설계 선택에서 가장 중요한 장점이다.&lt;/p>
&lt;h2 id="experimental-results">Experimental Results&lt;/h2>
&lt;h3 id="experimental-setup">Experimental Setup&lt;/h3>
&lt;p>의미 있는 비교를 위해, BERT 설정을 따라서 사전 훈련 기본 모델에 대해 Book Corpus와 영문 Wikipedia를 사용한다. 이 두 말뭉치는 압축되지 않은 텍스트로 약 16GB를 구성한다. 입력은 $[CLS] x_1 [SEP] x_2 [SEP]$ 와 같은 포멧이며, 최대 512 길이로 제한한다. 그리고 10%의 확률로 512보다 짧은 입력 시퀀스를 무작위로 생성한다. BERT와 같이, 30,000의 어휘 크기를 가지며, XLNet처럼 SentencePiece를 사용하여 토큰화한다.&lt;/p>
&lt;p>각 n-gram 마스크의 길이를 무작위로 선택하여 n-gram 마스킹을 사용해 MLM 목표를 위한 마스크된 입력을 생성한다. 길이 n에 대한 확률은 다음과 같이 주어진다:&lt;/p>
&lt;p>$$p(n) = {{1/n}\over{\sum_{k=1}^{N}1/k}} $$&lt;/p>
&lt;p>n-gram의 최대 길이는 3으로 설정하며, 이는 MLM 목표가 최대 3-gram 단어로 구성될 수 있음을 의미한다. 모든 모델 업데이트는 배치 크기 4096과 학습률 0.00176의 $L_{AMB}$ optimizer를 사용하며, 모든 모델은 125,000 step 동안 학습된다. 훈련은 Cloud TPU V3에서 이루어졌으며, 사용된 TPU의 수는 모델 크기에 따라 64에서 512까지 다양했다. 이 실험 설정은 우리가 만든 모든 BERT 버전과 ALBERT 모델에 사용되었다.&lt;/p>
&lt;h2 id="evaluation-benchmarks">Evaluation Benchmarks&lt;/h2>
&lt;h3 id="intrinsic-evalutation">Intrinsic Evalutation&lt;/h3>
&lt;p>학습 상황을 확인하기 위해, SQuAD와 RACE의 개발 세트를 기반으로 개발 세트를 만들었다. MLM과 문장 분류 작업의 정확도를 보고한다. 이 세트는 모델의 수렴 상태를 확인하는데만 사용되며, 모델 선택과 같은 방식으로 downstream evaluation의 성능에 영향을 주지 않는다.&lt;/p>
&lt;h3 id="downstream-evaluation">Downstream Evaluation&lt;/h3>
&lt;p>GLUE benchmark, SQuAD의 두 버전, 그리고 RACE dataset이라는 세 가지 benchmark에서 평가한다. 작업 리더보드를 기반으로 한 최종 비교를 위해 테스트 세트 결과도 측정하며, 개발 세트에서 큰 분산을 가진 GLUE 데이터셋에 대해서는 5회 실행의 중앙값을 측정한다.&lt;/p>
&lt;h3 id="overall-comparison-between-bert-and-albert">Overall Comparison between BERT and ALBERT&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table2.png"
width="1076"
height="218"
srcset="https://kurtkim.github.io/p/albert/images/table2_hu0550922691de2726c6d77f6571f8fd40_66118_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table2_hu0550922691de2726c6d77f6571f8fd40_66118_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="493"
data-flex-basis="1184px"
>&lt;/p>
&lt;p>ALBERT의 디자인 선택사항 중 parameter 효율성 향상이 가장 중요한 장점으로, BERT-large의 parameter의 70%만으로도 ALBERT-xxlarge는 여러 downstream task에서 상당한 개선을 보이고 있다.&lt;/p>
&lt;p>동일한 훈련 구성 하에서, ALBERT 모델들은 BERT 모델들에 비해 더 높은 데이터 처리량을 보인다. BERT-large를 기준으로 할 때, ALBERT-large는 데이터를 처리하는 데 약 1.7배 더 빠르고, 큰 구조 때문에 ALBERT-xxlarge는 약 3배 더 느리다.&lt;/p>
&lt;p>마지막으로, ALBERT의 각 디자인 선택사항이 얼마나 기여하는지 파악하기 위한 ablation experiments을 수행한다.&lt;/p>
&lt;h3 id="factorized-embedding-parameterization-1">Factorized Embedding Parameterization&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table3.png"
width="988"
height="276"
srcset="https://kurtkim.github.io/p/albert/images/table3_hucd20cde220454690378f9647dd10e4ac_76243_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table3_hucd20cde220454690378f9647dd10e4ac_76243_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="357"
data-flex-basis="859px"
>&lt;/p>
&lt;p>BERT 스타일의 비공유 상태에서는 더 큰 임베딩 크기가 약간 더 나은 성능을 보이지만, ALBERT 스타일의 모든 공유 상태에서는 크기 128의 임베딩이 최적으로 보인다. 이 결과를 바탕으로, 모든 추후 설정에서 임베딩 크기 128을 사용하게 된다.&lt;/p>
&lt;h3 id="cross-layer-parameter-sharing-2">Cross-layer parameter sharing&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table4.png"
width="1098"
height="268"
srcset="https://kurtkim.github.io/p/albert/images/table4_hue899a8f796d1baa9493d7bb4c90587fa_87546_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table4_hue899a8f796d1baa9493d7bb4c90587fa_87546_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="409"
data-flex-basis="983px"
>&lt;/p>
&lt;p>all-shared strategy인 ALBERT 스타일은 성능을 떨어뜨리지만, 임베딩 크기가 128일 때는 768일 때보다 저하가 덜 하다다. 성능 저하는 주로 FFN layer parameter sharing으로 인해 발생하며, attention parameter sharing은 성능 저하를 일으키지 않는다.&lt;/p>
&lt;p>다른 레이어 간의 parameter sharing 전략도 가능하지만, 그룹 크기를 줄이면 전체 parameter 수가 크게 증가한다. 따라서 all-shared strategy을 기본으로 선택하였다.&lt;/p>
&lt;h3 id="sentence-order-prediction-sop">Sentence order prediction (SOP)&lt;/h3>
&lt;p>세 가지 실험 조건인 &amp;rsquo;none&amp;rsquo; (XLNet 및 RoBERTa 스타일), NSP (BERT 스타일), SOP (ALBERT 스타일)에 대한 추가 문장 간 손실을 ALBERTbase 설정을 사용하여 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table5.png"
width="996"
height="172"
srcset="https://kurtkim.github.io/p/albert/images/table5_hua005ba83d01af37e02a3f1140a802c9b_45348_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table5_hua005ba83d01af37e02a3f1140a802c9b_45348_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="579"
data-flex-basis="1389px"
>&lt;/p>
&lt;p>NSP loss가 SOP 작업에 판별력을 제공하지 못하며, 주제 전환만 모델링하는 것으로 나타났다. 반면에, SOP loss는 NSP 작업을 상대적으로 잘 처리하며, SOP 작업에 대해서는 더욱 더 잘 처리했다. 더욱이, SOP loss는 여러 문장 인코딩 작업에 대한 downstream task 성능을 일관되게 향상시키는 것으로 나타났으며, 이는 평균 점수 향상이 약 1%라는 것을 의미한다.&lt;/p>
&lt;h3 id="what-if-we-train-for-the-same-amount-of-time">What if we train for the same amount of time?&lt;/h3>
&lt;p>BERT-large의 데이터 처리량은 ALBERT-xxlarge에 비해 약 3.17배 높다. 따라서 같은 학습 시간 동안 모델을 학습시키는 비교를 수행하였다. 400k 학습 단계 후의 BERT-large 모델 (학습 34시간 후)과 ALBERT-xxlarge 모델을 125k 학습 단계 동안 학습하는 데 필요한 시간 (학습 32시간)을 비교한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table6.png"
width="1020"
height="110"
srcset="https://kurtkim.github.io/p/albert/images/table6_hu127742eb5feda8aabb3dd35e8f8960ea_33902_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table6_hu127742eb5feda8aabb3dd35e8f8960ea_33902_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="927"
data-flex-basis="2225px"
>&lt;/p>
&lt;p>ALBERT-xxlarge는 BERT-large보다 훨씬 더 우수하며, 평균적으로 +1.5% 더 좋고, RACE에서는 최대 +5.2%까지 차이가 난다.&lt;/p>
&lt;h3 id="additional-training-data-and-dropout-effects">Additional training data and dropout effects&lt;/h3>
&lt;p>XLNet과 RoBERTa가 모두 사용한 추가 데이터의 영향을 비교한다. 추가 데이터 없을 때와 있을 때의 개발 세트 MLM 정확도를 비교하며, 추가 데이터를 사용할 때 중요한 향상을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table7.png"
width="892"
height="112"
srcset="https://kurtkim.github.io/p/albert/images/table7_hu202f86a529c8ffba927ae35007b9bd74_29608_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table7_hu202f86a529c8ffba927ae35007b9bd74_29608_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="796"
data-flex-basis="1911px"
>&lt;/p>
&lt;p>또한, SQuAD benchmark를 제외한 downstream task에서 성능 개선을 관찰했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/figure2.png"
width="976"
height="366"
srcset="https://kurtkim.github.io/p/albert/images/figure2_hu17e79b127f9b0f6927cb1047fa6dded1_79374_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/figure2_hu17e79b127f9b0f6927cb1047fa6dded1_79374_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="266"
data-flex-basis="640px"
>&lt;/p>
&lt;p>1M step까지 학습한 후에도, 가장 큰 모델들은 훈련 데이터에 과적합되지 않았다. 그래서 드롭아웃을 제거하여 모델의 용량을 늘려서 실험을 계속 하였으며, 드롭아웃을 제거하면 MLM 정확도가 크게 향상된다는 것이 확인되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table8.png"
width="844"
height="104"
srcset="https://kurtkim.github.io/p/albert/images/table8_hu25729e745a9229874603262353045868_27844_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table8_hu25729e745a9229874603262353045868_27844_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="811"
data-flex-basis="1947px"
>&lt;/p>
&lt;p>ALBERT-xxlarge의 중간 평가에서도 드롭아웃 제거가 downstream task에 도움이 된다는 것이 확인되었다. 이는, 드롭아웃이 큰 Transformer 기반 모델의 성능을 해칠 수 있다는 것을 보여준다. 그러나, ALBERT의 기본 네트워크 구조는 Transformer의 특수한 경우로, 이 현상이 다른 Transformer 기반 아키텍처에서도 나타나는지 여부를 확인하기 위해 추가 실험이 필요하다.&lt;/p>
&lt;h3 id="current-state-of-the-art-on-nlu-tasks">Current State-of-the-art on NLU Tasks&lt;/h3>
&lt;p>단일 모델과 앙상블에 대해 미세조정을 수행하고 state-of-the-art 결과를 비교한다. 모든 설정에서 단일 작업 미세조정만 수행하며, 개발 세트에서는 5번의 실행 결과 중 중앙값을 비교한다.&lt;/p>
&lt;p>최종 앙상블 모델에 기여하는 체크포인트는 개발 세트 성능에 따라 선택되며, 체크포인트의 수는 작업에 따라 6에서 17까지 다르다. GLUE와 RACE benchmark의 경우, 다른 학습 단계에서 미세조정된 후보 모델들의 예측을 평균화한다. SQuAD의 경우, 여러 확률을 가진 범위의 예측 점수와 &amp;ldquo;응답할 수 없는&amp;rdquo; 결정의 점수를 평균화한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table9.png"
width="1058"
height="404"
srcset="https://kurtkim.github.io/p/albert/images/table9_hu05cc00972c3874e11ae4c4727ea46cf0_113821_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table9_hu05cc00972c3874e11ae4c4727ea46cf0_113821_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="628px"
>&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/albert/images/table10.png"
width="1080"
height="462"
srcset="https://kurtkim.github.io/p/albert/images/table10_hu60fbbc83e296937a66cc096bdc5ee8b2_118871_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/albert/images/table10_hu60fbbc83e296937a66cc096bdc5ee8b2_118871_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="233"
data-flex-basis="561px"
>&lt;/p>
&lt;p>단일 모델과 앙상블 모델 모두 ALBERT가 모든 벤치마크에서 상당히 향상된 성능을 보여준다. GLUE 점수는 89.4, SQuAD 2.0 테스트 F1 점수는 92.2, RACE 테스트 정확도는 89.4에 달하며, 이는 BERT, XLNet, RoBERTa, 그리고 DCMI+ 등에 비해 큰 향상을 보여준다. 또한, 단일 모델은 86.5%의 정확도를 달성하여, state-of-the-art 앙상블 모델보다 2.4% 더 높다.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>ALBERT-xxlarge는 BERT-large보다 parameter가 적지만 더 좋은 성능을 보인다. 하지만, 큰 구조 때문에 계산 비용이 더 많이 들어간다. 따라서, 다음 단계는 sparse attention와 block attention 등의 방법을 통해 ALBERT의 학습과 추론 속도를 높이는 것이다. 어려운 예제 채굴과 더 효율적인 언어 모델링 훈련 등 별개의 연구 방향이 추가적인 표현력을 제공할 수 있다. 또한, 문장 순서 예측이 더 나은 언어 표현을 이끌어내는 좋은 학습 작업이지만, 추가적인 표현력을 만들어낼 수 있는 현재 자기 self-supervised loss에 포착되지 않은 다른 차원이 있을 수 있다는 가설이 있다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>RoBERTa</title><link>https://kurtkim.github.io/p/roberta/</link><pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/roberta/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델의 사전 학습은 큰 성능 향상을 가져왔지만, 다른 접근법 간의 비교는 어렵다. 학습 과정은 비용이 많이 들며, hyperparameter 선택이 결과에 큰 영향을 미친다. 이 연구는 BERT 학습을 재현하고, 핵심 hyperparameter와 학습 데이터 크기의 영향을 측정하는데, 이를 통해 BERT가 undertrained 상태였으며, 그 이후의 모든 모델의 성능을 맞추거나 능가할 수 있다는 것을 발견했다. RoBERTa는 여러 테스트에서 state-of-the-art를 달성하였고, 이는 이전에 간과된 설계의 중요성을 보여준다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>이 연구는 BERT의 사전 학습 방법을 재현하고, 그 효과를 평가했다. 연구 결과 BERT는 상당히 학습이 부족했으며, 이를 개선하기 위한 새로운 학습 방법 RoBERTa를 제안했다. RoBERTa는 더 큰 배치와 더 많은 데이터에서 모델을 더 오래 학습시키고, 다음 문장 예측 목표를 제거하며, 더 긴 시퀀스에서 학습시키고, 학습 데이터에 적용되는 마스킹 패턴을 동적으로 변경한다. 이 개선된 학습 방법은 GLUE와 SQuAD에서 BERT의 성능을 뛰어넘었다. 또한, 새로운 데이터셋인 CCN EWS를 사용하고, 사전 학습에 더 많은 데이터를 사용하면 성능이 더욱 향상된다는 것을 확인하였다.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>BERT의 사전 학습 방식과 실험적으로 검토할 학습 선택 사항에 대해 간단히 소개한다.&lt;/p>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;p>BERT는 두 개의 토큰 시퀀스인 $x_1, &amp;hellip;, x_N$과 $y_1, &amp;hellip;, y_M$을 연결하여 입력으로 사용한다. 이 두 세그먼트는 특별한 토큰들로 구분되어 BERT에 하나의 입력 시퀀스로 제공된다:&lt;/p>
&lt;p>$$ [CLS], x_1, &amp;hellip;, x_N, [SEP], y_1, &amp;hellip;, y_M, [EOS]. $$&lt;/p>
&lt;p>$M$과 $N$은 $M + N &amp;lt; T$라는 제약하에 있으며, 여기서 $T$는 최대 시퀀스 길이를 제어하는 parameter이다. 모델은 대량의 레이블이 없는 텍스트 말뭉치로 사전 학습된 후, 최종 목표 레이블 데이터를 사용하여 미세 조정된다.&lt;/p>
&lt;h3 id="architecture">Architecture&lt;/h3>
&lt;p>사전 학습 동안, BERT는 masked language modeling과 next sentence prediction 두 가지 작업을 사용한다.&lt;/p>
&lt;h4 id="masked-language-model-mlm">Masked Language Model (MLM)&lt;/h4>
&lt;p>입력 시퀀스의 토큰 중 무작위로 선택된 일부가 특별한 토큰 [MASK]로 대체된다. masked language modeling 목표는 마스크된 토큰을 예측하는 것이다. 입력 토큰의 15%가 선택되어 대체되며, 이 중 80%는 [MASK]로, 10%는 그대로 두고, 나머지 10%는 임의의 어휘 토큰으로 대체된다. 이 과정은 학습 기간 동안 한 번만 수행되지만, 실제로는 데이터가 복제되어 모든 학습 문장에 동일한 마스크가 적용되지 않는다.&lt;/p>
&lt;h4 id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)&lt;/h4>
&lt;p>NSP(Next Sentence Prediction)는 원문에서 두 세그먼트가 이어지는지 예측하는 방법이다. 긍정 예시는 연속된 문장을, 부정 예시는 다른 문서의 세그먼트를 사용해 만든다. NSP 목표는 문장 쌍 간의 관계를 파악하는 자연어 추론 등의 downstream task에서 성능을 향상시키기 위해 설계되었다.&lt;/p>
&lt;h3 id="optimization">Optimization&lt;/h3>
&lt;p>BERT는 Adam 최적화를 사용하며, learning rate는 처음 10,000 step 동안 서서히 높아진 후 선형적으로 감소한다. 모든 레이어와 attention weights에 대해 0.1의 dropout을 사용하고, GELU 활성화 함수를 활용한다. 모델은 1,000,000 업데이트 동안 사전 학습되며, 최대 512 토큰의 256 시퀀스를 포함하는 미니배치를 사용한다.&lt;/p>
&lt;h3 id="data">Data&lt;/h3>
&lt;p>BERT는 총 16GB의 압축되지 않은 텍스트인 BOOK CORPUS와 영어 WIKIPEDIA 조합으로 학습된다.&lt;/p>
&lt;h2 id="experimental-setup">Experimental Setup&lt;/h2>
&lt;p>BERT의 복제 연구에 대한 실험적 설정을 설명한다.&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>FAIRSEQ에서 BERT를 재구현하고, 대부분의 원래 BERT 최적화 hyperparameter를 따랐다. 하지만 peak learning rate과 warmup step 수는 각 설정에 따라 별도로 조정했다. 또한, Adam epsilon 항에 따라 학습이 매우 민감하며, 이를 조정하여 성능을 개선했다.&lt;/p>
&lt;h3 id="data-1">Data&lt;/h3>
&lt;p>BERT 스타일의 사전 학습은 대량의 텍스트에 크게 의존하며, 데이터 크기를 늘릴수록 최종 작업 성능이 향상될 수 있다. 이 연구에서는, 가능한 많은 데이터를 수집하여 실험을 진행하고, 각 비교에 적절한 데이터의 전체적인 품질과 양을 맞출 수 있도록 했다. 압축되지 않은 텍스트로 총 160GB를 넘는 다양한 크기와 도메인의 5개의 영어 말뭉치를 사용하였다:&lt;/p>
&lt;ul>
&lt;li>BOOK CORPUS와 영어 WIKIPEDIA. BERT를 학습시키는데 사용된 원래 데이터이다. (16GB).&lt;/li>
&lt;li>CC-N EWS, CommonCrawl News dataset의 영어 부분에서 수집한 것. 이 데이터는 2016년 9월부터 2019년 2월까지 크롤링된 6300만 개의 영어 뉴스 기사를 포함하고 있다. (필터링 후 76GB).&lt;/li>
&lt;li>OPEN WEB TEXT, WebText 코퍼스의 오픈 소스 재현, 텍스트는 Reddit에서 최소 세 번 이상의 추천을 받은 URL에서 추출된 웹 콘텐츠이다. (38GB)&lt;/li>
&lt;li>STORIES, Winograd 스키마의 스토리 같은 스타일을 맞추기 위해 필터링된 CommonCrawl 데이터의 부분 집합을 포함하고 있다. (31GB).&lt;/li>
&lt;/ul>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>다음 세 가지 benchmark를 사용하여 사전 학습된 모델을 평가한다.&lt;/p>
&lt;h4 id="glue">GLUE&lt;/h4>
&lt;p>GLUE 벤치마크는 자연어 이해 시스템을 평가하기 위한 9개의 dataset 모음이다. 이는 단문 분류 또는 문장 쌍 분류 작업으로 구성되며, 참가자들은 제출 서버와 리더보드를 통해 자신의 시스템을 평가하고 비교할 수 있다.&lt;/p>
&lt;p>학습 데이터에서 사전 학습된 모델을 미세 조정하여 개발 세트에서 결과를 보고하였으며, 이 절차는 원래의 BERT 논문을 따른다.&lt;/p>
&lt;h4 id="squad">SQuAD&lt;/h4>
&lt;p>Stanford Question Answering Dataset(SQuAD)는 문맥과 질문을 제공하며, 작업은 문맥에서 관련 부분을 추출하여 질문에 답한다. SQuAD는 V1.1과 V2.0 두 버전이 있으며, V2.0 버전은 제공된 문맥에서 답하지 않는 질문도 포함하여 작업이 더 도전적이다.&lt;/p>
&lt;p>SQuAD V1.1에서는 BERT와 같은 방법으로 범위를 예측하며, SQuAD V2.0에서는 추가적인 이진 분류기를 사용하여 질문이 답변 가능한지를 예측한다. 이는 분류와 span loss terms를 합산하여 훈련되며, 평가 시 답변 가능으로 분류된 쌍에만 범위 인덱스를 예측한다.&lt;/p>
&lt;h4 id="race">RACE&lt;/h4>
&lt;p>ReAding Comprehension from Examinations(RACE)은 28,000개 이상의 지문과 100,000개 가까운 질문을 포함한 대규모 독해 dataset이다. 이 dataset은 중국의 중고등학생들을 대상으로 한 영어 시험에서 수집되었다. RACE에서는 각 지문에 대해 여러 질문을 풀고, 네 가지 선택지 중 정답을 고르는 작업을 수행한ㄴ다. RACE는 다른 dataset에 비해 긴 문맥을 가지고 있으며, 대부분의 질문에서 추론 능력을 요구한다.&lt;/p>
&lt;h2 id="training-procedure-analysis">Training Procedure Analysis&lt;/h2>
&lt;p>BERT 모델을 성공적으로 사전 학습하기 위해 중요한 선택 사항들을 탐색하고 정량화한다. $BERT_{BASE}$와 동일한 구성 (L = 12, H = 768, A = 12, 110M params)으로 BERT 모델을 학습한다.&lt;/p>
&lt;h3 id="static-vs-dynamic-masking">Static vs. Dynamic Masking&lt;/h3>
&lt;p>BERT는 무작위로 토큰을 마스킹하고 예측한다. 각 학습 인스턴스에서 동일한 마스크를 계속 사용하는 것을 피하기 위해, 학습 데이터를 10번 복제하여 각 시퀀스가 40회의 학습 동안 다양하게 마스킹될 수 있게 했다. 이를 모델에 시퀀스를 제공할 때마다 새로운 마스킹 패턴을 생성하는 동적 마스킹과 비교하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table1.png"
width="590"
height="260"
srcset="https://kurtkim.github.io/p/roberta/images/table1_huefe28a59b1c5340118d249d8656f3095_34810_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table1_huefe28a59b1c5340118d249d8656f3095_34810_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="226"
data-flex-basis="544px"
>&lt;/p>
&lt;p>재구현에서 정적 마스킹은 원래 BERT와 유사한 성능을 보였고, 동적 마스킹은 정적 마스킹과 비슷하거나 약간 더 좋은 성능을 보여주었다. 이 결과와 동적 마스킹의 효율성을 고려하여, 이 후 실험에서는 동적 마스킹을 사용하였다.&lt;/p>
&lt;h3 id="model-input-format-and-next-sentence-prediction">Model Input Format and Next Sentence Prediction&lt;/h3>
&lt;p>원래 BERT 사전 학습에서는 모델이 두 개의 연결된 문서 세그먼트를 관찰하며, 이들은 같은 문서에서 연속적으로 추출되거나 다른 문서에서 추출된다. 모델은 Next Sentence Prediction(NSP) loss를 통해 관찰한 문서 세그먼트가 같은 문서에서 왔는지, 다른 문서에서 왔는지 예측하도록 훈련된다.&lt;/p>
&lt;p>NSP loss의 중요성은 원래 BERT 모델 훈련에서 중요한 요소로 가정되었으며, NSP를 제거하면 성능이 저하되는 것이 관찰되었다. 그러나 최근의 일부 연구에서는 NSP loss의 필요성에 의문을 제기하였다. 이러한 차이점을 더 잘 이해하기 위해, 다양한 훈련 형식을 비교하고 있다.&lt;/p>
&lt;ul>
&lt;li>SEGMENT - PAIR + NSP: BERT의 원래 입력 형식을 따르며 NSP loss를 포함한다. 각 입력은 여러 자연스러운 문장을 포함한 세그먼트 쌍이 있으며, 총 길이는 512 토큰 이하여야 한다.&lt;/li>
&lt;li>SENTENCE - PAIR + NSP: 각 입력은 하나의 문서의 연속적인 부분 또는 별개의 문서에서 추출된 자연스러운 문장 쌍을 포함한다. 이러한 입력은 512 토큰보다 훨씬 짧으므로, 총 토큰 수가 SEGMENT - PAIR + NSP와 비슷하게 유지되도록 배치 크기를 늘린다. NSP loss는 유지된다.&lt;/li>
&lt;li>FULL - SENTENCES: 각 입력은 하나 이상의 문서에서 연속적으로 샘플링된 문장으로 채워져 있으며, 총 길이는 최대 512 토큰이다. 문서가 끝나면 다음 문서에서 샘플링을 시작하고, 문서 사이에는 추가 구분자 토큰을 넣는다. NSP loss는 제거된다.&lt;/li>
&lt;li>DOC - SENTENCES: 입력은 FULL - SENTENCES와 유사하게 구성된다. 문서 끝 부분에서 샘플링된 입력은 512 토큰보다 짧을 수 있어, 이런 경우에는 배치 크기를 동적으로 늘려 FULL SENTENCES와 같은 총 토큰 수를 유지한다. NSP loss는 제거한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table2.png"
width="854"
height="448"
srcset="https://kurtkim.github.io/p/roberta/images/table2_hu68613bddb48bed701794d1b66c589053_103697_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table2_hu68613bddb48bed701794d1b66c589053_103697_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="457px"
>&lt;/p>
&lt;p>원래의 SEGMENT - PAIR 입력 형식과 SENTENCE - PAIR 형식을 비교했으며, 둘 다 NSP loss를 유지하되 후자는 단일 문장을 사용한다. 개별 문장 사용은 모델이 장거리 의존성을 학습하지 못함으로써 성능을 저하시킨다는 것을 발견하였다.&lt;/p>
&lt;p>NSP loss 없이 훈련과 단일 문서의 텍스트 블록을 사용한 학습(DOC - SENTENCES)을 비교했을 때, 이 설정이 $BERT_{BASE}$ 결과를 능가하며, NSP loss를 제거하면 downstream task 성능이 향상된다는 것을 발견하였다.&lt;/p>
&lt;p>마지막으로, 단일 문서의 시퀀스(DOC - SENTENCES)가 여러 문서의 시퀀스(FULL - SENTENCES)보다 약간 더 나은 성능을 보이지만, DOC - SENTENCES 형식은 배치 크기가 변동적이므로, 나머지 실험에서는 FULL SENTENCES를 사용하였다.&lt;/p>
&lt;h3 id="training-with-large-batches">Training with large batches&lt;/h3>
&lt;p>Neural Machine Translation의 이전 연구에 따르면, earning rate가 적절하게 증가할 때 아주 큰 mini-batches를 사용한 학습은 최적화 속도와 성능 모두를 개선시킬 수 있다는 것이 증명되었다. 최근의 연구는 BERT 또한 큰 배치 학습에 적합하다는 것이 밝혀졌다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table3.png"
width="604"
height="194"
srcset="https://kurtkim.github.io/p/roberta/images/table3_hubaf80ecf6afab50cb0332dd70f744a28_32896_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table3_hubaf80ecf6afab50cb0332dd70f744a28_32896_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="747px"
>&lt;/p>
&lt;p>배치 크기를 증가시키면서 $BERT_{BASE}$의 Perplexity와 성능을 비교하였고, 큰 배치로 훈련하면 마스크 언어 모델링 목표의 Perplexity와 정확도가 향상된다는 것을 관찰했다. 또한, 큰 배치는 분산 데이터 병렬 훈련을 통해 더 쉽게 병렬화할 수 있다.&lt;/p>
&lt;h3 id="text-encoding">Text Encoding&lt;/h3>
&lt;p>Byte-Pair Encoding(BPE)는 자연어 말뭉치의 큰 어휘를 처리하기 위해 문자와 단어 수준 표현의 중간 형태를 사용한다. BPE는 훈련 말뭉치의 통계 분석을 통해 추출된 subword unit를 사용한다. byte를 사용한 BPE는, Unknown 토큰 없이도, 50K units의 서브워드 사전으로 학습을 진행할 수 있다.&lt;/p>
&lt;p>기존의 BERT는 character level의 BPE를 사용했다. RoBERTa는 추가 전처리나 토크나이징 없이 larger byte-level BPE로 학습을 진행한다.&lt;/p>
&lt;p>Byte level BPE는 몇 개의 태스크에서 성능이 떨어진다는 단점이 있지만, 성능 하락폭이 크지 않고 universal 인코딩의 장점이 있다고 판단하여 본 연구에서 Byte level BPE를 적용하였다.&lt;/p>
&lt;h2 id="roberta">RoBERTa&lt;/h2>
&lt;p>BERT 사전 학습 절차를 수정하여 end-task 성능을 향상시키는 방안을 제안한다. 이를 모두 합쳐서 그 효과를 평가한 결과, RoBERTa라는 새로운 설정이 만들어졌다. RoBERTa는 동적 마스킹, NSP loss 없는 전체 문장, 큰 미니 배치, 더 큰 바이트 수준 BPE를 사용하여 학습된다.&lt;/p>
&lt;p>또한, 사전 학습에 사용된 데이터와 데이터를 통한 학습 통과 횟수라는 두 가지 중요한 요소를 조사했다. 이를 검증하기 위해 $BERT_{LARGE}$ 아키텍처를 따라 RoBERTa를 훈련시켰고, BOOK CORPUS와 WIKIPEDIA dataset을 사용하여 100K 단계 동안 사전 학습 하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table4.png"
width="1034"
height="498"
srcset="https://kurtkim.github.io/p/roberta/images/table4_hu5396c4466a3949f8b298043e563fba53_107966_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table4_hu5396c4466a3949f8b298043e563fba53_107966_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="498px"
>&lt;/p>
&lt;p>RoBERTa는 원래의 $BERT_{LARGE}$ 결과에 비해 크게 향상된 성능을 보여준다. 추가로, 다양한 데이터셋을 결합하여 훈련시킨 결과, 모든 downstream task에서 성능이 더욱 개선되었다.&lt;/p>
&lt;p>또한, 사전 학습 단계를 100K에서 300K, 그리고 500K로 늘렸을 때, downstream task 성능에서 상당한 향상을 보여주었다. 이렇게 학습된 모델들은 대부분의 작업에서 $XLNet_{LARGE}$를 능가하였다.&lt;/p>
&lt;p>마지막으로, GLUE, SQuaD, 그리고 RACE라는 세 가지 다른 benchmark에서 최적의 RoBERTa 모델을 평가하였다. 이는 모든 다섯 가지 dataset에 대해 500K 단계로 훈련된 RoBERTa를 기준으로 한 것이다.&lt;/p>
&lt;h3 id="glue-results">GLUE Results&lt;/h3>
&lt;p>GLUE에 대해 두 가지 미세 조정 설정을 고려했다. 첫 번째는 각 GLUE 작업에 대해 RoBERTa를 별도로 미세조정 했으며, 각 작업에 대한 중간 개발 세트 결과를 앙상블 없이 측정했다. 두 번째는 GLUE 리더보드를 통해 RoBERTa를 다른 모델과 비교했다. 단일 작업 미세 조정에만 의존하였다. 특히, RTE, STS, 그리고 MRPC에서는 MNLI 단일 작업 모델에서 시작하여 미세 조정 하는 것이 도움이 되었다. 각 작업에 대해 5에서 7개의 모델을 앙상블하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table5.png"
width="1216"
height="434"
srcset="https://kurtkim.github.io/p/roberta/images/table5_huadd96c22cf129e9e57bfc748b9ec272f_127632_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table5_huadd96c22cf129e9e57bfc748b9ec272f_127632_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="672px"
>&lt;/p>
&lt;p>RoBERTa가 GLUE 작업 개발 세트 9개 모두에서 state-of-the-art를 달성하였다. $BERT_{LARGE}$와 동일한 구조를 사용하면서도, $BERT_{LARGE}$와 $XLNet_{LARGE}$를 능가하였다. 이는 모델 구조와 사전학습 목표 이외의 요인들이 중요할 수 있다는 점을 시사한다.&lt;/p>
&lt;p>앙상블 설정에서는, RoBERTa를 GLUE 리더보드에 제출하여 9개 작업 중 4개에서 state-of-the-art를 달성하였고, 가장 높은 평균 점수를 얻었다. 이는 RoBERTa가 다중 작업 미세 조정에 의존하지 않는다는 점에서 특히 주목할만하다.&lt;/p>
&lt;h3 id="squad-results">SQuAD Results&lt;/h3>
&lt;p>SQuAD에 대해 과거의 복잡한 접근법 대신 단순한 방법을 채택하였다. BERT와 XLNet이 추가 QA 데이터셋을 훈련 데이터에 포함시킨 반면, 이 연구에서는 제공된 SQuAD 훈련 데이터만으로 RoBERTa를 미세조정 하였다. 모든 레이어에 대해 동일한 learning rate를 사용하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table6.png"
width="578"
height="432"
srcset="https://kurtkim.github.io/p/roberta/images/table6_hu481d86a4cd9c9a51bdc3b68dcd698e3f_79878_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table6_hu481d86a4cd9c9a51bdc3b68dcd698e3f_79878_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="321px"
>&lt;/p>
&lt;p>RoBERTa는 SQuAD v1.1 개발 세트에서 XLNet과 동일한 성능을 보여주며, SQuAD v2.0에서는 XLNet을 약간 상회하는 새로운 state-of-the-art를 달성하였다.&lt;/p>
&lt;p>RoBERTa를 공개 SQuAD 2.0 리더보드에 제출했고, 추가적인 학습 데이터를 사용하지 않았음에도 불구하고, 단일 모델 제출 중 거의 모든 시스템을 능가하는 성과를 보여주었다. 데이터 증강에 의존하지 않는 시스템 중에서는 가장 높은 점수를 받았다.&lt;/p>
&lt;h3 id="race-results">RACE Results&lt;/h3>
&lt;p>RACE에서는 텍스트, 관련 질문, 네 개의 후보 답변이 제공되며, 시스템은 정답을 분류해야 한다. RoBERTa를 이 작업에 맞게 수정했으며, 각 후보 답변을 해당 질문과 텍스트에 연결하여 처리하였다. 전체 길이가 512 토큰을 초과하지 않도록 조정했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/roberta/images/table7.png"
width="560"
height="248"
srcset="https://kurtkim.github.io/p/roberta/images/table7_hu659b09db86c53974d5e6a7a79677b569_44262_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/roberta/images/table7_hu659b09db86c53974d5e6a7a79677b569_44262_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="225"
data-flex-basis="541px"
>&lt;/p>
&lt;p>RACE 테스트 세트에서의 결과는 RoBERTa가 중학교와 고등학교 환경 모두에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>다양한 사전 학습 방법들은 언어 모델링, 기계 번역, 마스킹된 언어 모델링 등의 목표를 가지고 설계되었다. 최근 연구에서는 마스킹된 언어 모델 목표를 사용해 사전 학습하고, 각 작업에 대해 모델을 미세 조정하는 방법을 사용하였다. 그러나 새로운 방법들은 다중 작업 미세 조정, 엔티티 임베딩 통합, 범위 예측, 자동회귀 사전 학습의 변형 등을 통해 성능을 향상시켰다. 일반적으로, 더 큰 모델을 더 많은 데이터로 훈련함으로써 성능이 향상되었다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>BERT 모델을 사전 학습시키는데 있어 다양한 디자인 결정을 신중하게 평가했다. 모델을 더 오래 훈련하고, 더 큰 배치로 더 많은 데이터를 사용하며, 다음 문장 예측 목표를 제거하고, 더 긴 시퀀스를 훈련하며, 훈련 데이터에 적용된 마스킹 패턴을 동적으로 변경하면 성능이 크게 향상됨을 발견했다. 이러한 개선된 사전 학습 절차를 RoBERTa라 부르며, GLUE, RACE, SQuAD에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/facebookresearch/fairseq" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-2</title><link>https://kurtkim.github.io/p/gpt-2/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-2/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>언어 모델은 웹페이지로 구성된 새로운 데이터셋인 &amp;lsquo;WebText&amp;rsquo;으로 학습 함으로써, 질문 응답, 기계 번역 등의 작업을 명시적인 지도 없이 배우기 시작한다는 것을 발견하였다. 특히, GPT-2는 웹텍스트에 대해 underfitting이지만, zero-shot 환경에서 8개 테스트 언어 모델링 데이터셋 중 7개에서 state-of-the-art를 달성하였다. 즉, 언어 처리 시스템이 자연적 설명으로부터 과제수행능력을 배우는 언어처리모델을 개발할 수 있는 방법을 제안하였다.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>기계 학습 시스템은 대규모 데이터셋, 고용량 모델, 지도 학습을 이용해 학습된 작업에서 우수한 성과를 보이지만, 데이터 분포의 작은 변화나 작업 정의에 민감하게 반응하고,매우 좁은 범위의 문제에서만 뛰어난 성능을 보여주고 있다. 그래서 보다 일반적인 문제 해결 능력을 갖춘 범용적인 모델 개발이 필요하며, 이는 결국 각각의 작업에 대해 훈련 데이터셋을 수동으로 생성하고 라벨링할 필요 없이 다양한 작업을 수행할 수 있는 모델을 의미한다.&lt;/p>
&lt;p>기계 학습 시스템을 만드는 주요한 방법은 훈련 예제를 수집하여 시스템을 학습시키고, independent and identically distributed (IID)에서 성능을 테스트하는 것이다. 이 방법은 좁은 범위의 과제에서는 잘 작동하지만, 범용적인 이해를 필요로 하는 캡션 모델, 독해, 이미지 분류 등에서 높은 성능을 내지 못했으며, 이 방법의 한계를 보여주었다.&lt;/p>
&lt;p>일반화하는 능력이 부족한 주요 원인으로 많은 연구가 단일 영역의 dataset과 단일 과제에만 맞춘 학습에만 치중되어 있기 때문이라고 보고 있다. 이를 개선하기 위해 다양한 도메인과 작업에서 훈련하고 성능을 측정하는 것이 필요하며, GLUE와 decaNLP 같은 benchmark dataset이 제안되었다.&lt;/p>
&lt;p>다중 작업 학습(Multitask learning)는 일반 성능을 향상에 높이는 유망한 방법이지만, NLP에서는 아직 초기 연구 단계이다. 최근의 기계학습 시스템의 일반화를 위해서는 수백에서 수천 개의 학습 샘플을 필요로 하며, 다중 작업 학습을 위해서도 그만큼 많은 수의 효과적인 트레이닝 쌍이 필요하다. 현재의 기술로는 dataset을 필요한 수준까지 계속 확장하는 것이 어려우며, 따라서 다중 작업 학습을 위한 새로운 접근법이 필요하다.&lt;/p>
&lt;p>현재 언어 작업에서 최고 성능을 보이는 모델은 사전 학습과 지도 학습을 결합한 방식을 사용한다. 이 접근법은 오랜 역사를 가지고 있으며, transfer 방식이 점차 유연해지고 있다. 초기에는 단어 벡터가 학습되어 특정 작업에 적용되었고, 그 다음으로는 순환 네트워크의 문맥 표현이 transfer 되었다. 최근 연구에서는 특정 작업에 특화된 아키텍처가 필요 없으며, 대신 self-attention block만으로 충분하다고 제안하고 있다.&lt;/p>
&lt;p>현재의 방법들은 작업 수행을 위해 여전히 지도 학습이 필요하다. 하지만 지도 데이터가 거의 없거나 전혀 없을 때, 언어 모델이 상식적인 추론이나 감성 분석 등의 특정 작업을 수행하는 데 잠재력이 있다는 것이 다른 연구에서 보여져 왔다.&lt;/p>
&lt;p>이 논문에서는 언어 모델이 parameter나 아키텍처 변경 없이 zero-shot setting에서 다양한 작업을 수행할 수 있는 능력을 보여준다. 이 접근법은 전이 학습의 일반화 추세를 이어가며, 작업에 따라 유망한 결과와 경쟁력 있는 성과, 그리고 state-of-the-art를 달성하는 잠재력을 보여준다.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>핵심은 언어 모델링(language modeling)이며, 이는 각 원소가 일련의 symbol $(s_1, s_2, &amp;hellip;, s_n)$ 으로 구성된 예제 $(x_1, x_2, &amp;hellip;, x_n)$ 에서 비지도분포 추정을 하는 것으로 정의된다. 언어의 순차적 특성 때문에 기호들에 대한 결합 확률은 조건부 확률의 곱으로 분해하는 것이 일반적이다.&lt;/p>
&lt;p>$$p(x) = \prod_{i=1}^n p(s_n | s_1, s_2, &amp;hellip;, s_{n-1}) $$&lt;/p>
&lt;p>이 방법은 $p(x)$ 및 $p(s_{n-k}, &amp;hellip;, s_n | s_1, &amp;hellip;, s_{s-k-1})$ 형태의 조건부의 샘플링과 추정을 가능하게 하며, 최근에는 Transformer와 같은 self-attention 아키텍처의 발전으로 이러한 조건부 확률을 계산하는 모델의 표현력이 크게 향상되었다.&lt;/p>
&lt;p>단일 작업을 수행하는 학습은 확률론적 프레임워크에서 조건부 분포 $p(output | input)$를 추정하는 것으로 볼 수 있다. 하지만 일반적인 시스템은 동일한 입력에 대해 수행해야 하는 다양한 작업을 고려해야 한다. 이를 위해, 시스템은 $p(output | input, task)$를 모델링해야 한다. 이는 다중학습과 메타학습 환경에서 다양하게 형식을 갖는다. McCann et al. (2018)은 언어를 활용하여 작업, 입력, 출력을 기호 시퀀스로 지정하는 방법을 제시하였고, 이 방법을 사용하여 MQAN이라는 단일 모델을 훈련시켜 다양한 작업을 수행할 수 있음을 보여주었다.&lt;/p>
&lt;p>언어 모델링은 출력 symbol에 대한 명시적인 지도 없이도 다양한 작업을 학습할 수 있다. 이는 감독된 학습 목표와 비감독된 학습 목표가 실질적으로 같기 때문인데, 이 두 목표의 global minimum은 동일하다. 예비 실험에서는 충분히 큰 언어 모델은 이러한 설정에서 다중 작업 학습을 수행할 수 있지만, 명시적으로 감독된 방법보다 학습 속도가 느리다는 것이 확인되었다.&lt;/p>
&lt;p>대화(dialog)의 맥락에서 자연어를 직접 학습하는 방법은 매력적인 접근법이지만, 상호작용이 필요없는 인터넷 사이트에 존재하는 방대한 양의 데이터를 활용하는 방법을 선택하였다. 충분한 용량을 가진 언어 모델은 자연어 시퀀스에서 작업을 추론하고 수행하며, 이를 통해 더 잘 예측하도록 학습할 것으로 예상된다. 모델은 비지도 다중작업 학습을 수행하게 될 것이며, 다양한 작업에서 언어 모델의 제로샷 성능을 분석하였다.&lt;/p>
&lt;h3 id="training-dataset">Training Dataset&lt;/h3>
&lt;p>이전 연구들은 주로 한정된 도메인의 텍스트를 가지고 언어 모델을 학습시켰다. 하지만 이 논문에서는 가능한 한 다양한 도메인과 맥락에서 작업을 수집하기 위해, 크고 다양한 dataset을 구축하였다.&lt;/p>
&lt;p>다양하고 방대한 텍스트의 출처로 웹 스크랩인 Common Crawl이 유망하지만, 데이터 품질 문제가 있습니다.&lt;/p>
&lt;p>이 dataset을 사용하는 대신, 문서의 품질을 중요시하는 새로운 웹 스크랩을 만들었다. 전체 웹 스크랩을 수동으로 필터링하는 비용을 줄이기 위해, 사람들이 선별한 웹 페이지를 대상으로 했다. 특히, 적어도 3 카르마를 받은 Reddit의 모든 외부 링크를 스크랩했는데, 이는 사용자들이 해당 링크를 유익하거나 재미있게 여겼는지의 지표로 볼 수 있다.&lt;/p>
&lt;p>결과적으로 나온 dataset인 WebText는 45백만 링크의 텍스트 부분집합을 포함하고 있으며, 텍스트 추출을 위해 Dragnet과 Newspaper 내용추출기를 사용하였다. 이 논문의 모든 결과는 2017년 12월 이후 링크를 제외한 초기 버전의 WebText를 사용하며, 이는 de-duplication과 cleaning 과정을 거친 후 40GB, 약 800만 개의 문서를 포함하고 있다. Wikipedia 문서는 분석을 복잡하게 할 수 있어 WebText에서 제외하였다.&lt;/p>
&lt;h3 id="input-representation">Input Representation&lt;/h3>
&lt;p>일반적인 언어 모델은 어떤 문자열의 확률도 계산하고 생성할 수 있어야 한다. 하지만 현재의 대규모 언어 모델은 전처리 과정으로 인해 모델링 가능한 문자열 범위가 제한된다. 유니코드 문자열을 UTF-8 바이트로 처리하는 것은 이를 해결하나, 현재 바이트 수준의 언어 모델은 대규모 데이터셋에서 단어 수준의 언어 모델만큼 효과적이지 않다. WebText에서 바이트 수준 언어 모델을 훈련시키려 했으나, 이와 비슷한 성능 격차를 경험하였다.&lt;/p>
&lt;p>Byte Pair Encoding(BPE)는 문자와 단어 수준 언어 모델링 사이의 중간 지점이다. 자주 나오는 symbol sequence의 단어수준 입력과 자주 나오지 않는 symbol sequence의 글자수준 입력을 적절히 보간(interpolate)한다. BPE 구현은 byte sequence가 아닌 unicode code points에서 동작한다. 이러한 구현은 모든 unicode 문자열을 모델링하기 위해 전체 unicode symbol의 전체 공간을 포함해야 한다. multi-symbol token을 추가하기 전 130,000개가 넘는 token을 포함하는 기본사전을 필요로 하게 된다. 이는 보통의 32,000개에서 64,000개의 token의 사전에 비해 지나치게 크다. 반면, byte수준의 BPE의 사전은 256개의 token만을 필요로 한다. 그러나 BPE를 byte sequence에 직접 적용하면, 토큰 어휘를 구축하기 위한 BPE의 greedy frequency 기반 heuristic 때문에 최적이 아닌 병합이 발생한다. 예를 들어, &amp;lsquo;dog&amp;rsquo;와 같은 일반적인 단어가 다양한 형태로 나타나면서 제한된 어휘 슬롯과 모델 용량이 최적화되지 않을 수 있다. 이를 해결하기 위해 byte sequence에 대해 문자 범주를 넘어서 병합하는 것을 방지하고, 공백에 대한 예외를 추가하여 압축 효율을 향상시키고 단어의 분열을 최소화하였다.&lt;/p>
&lt;p>이 입력 표현법은 단어 수준 언어 모델의 이점과 byte 수준 접근법의 범용성을 결합시킨다. 이러한 접근법은 어떤 unicode 문자열에도 확률을 부여할 수 있으므로, 사전 처리, 토큰화, 어휘 크기와 관계없이 모든 데이터셋에서 언어 모델을 평가할 수 있다.&lt;/p>
&lt;h3 id="model">Model&lt;/h3>
&lt;p>Transformer 기반 아키텍처를 사용하며, 이는 주로 OpenAI GPT 모델을 따른다. 몇 가지 수정사항은 레이어 정규화의 위치 변경, 추가적인 레이어 정규화의 삽입, 모델 깊이를 고려한 초기화 방식의 수정, 그리고 잔여 레이어 가중치의 스케일링이다. 또한, 어휘는 50,257개로 확장되었고, 맥락 크기와 배치 크기도 각각 1024 토큰과 512로 증가시켰다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table2.png"
width="356"
height="188"
srcset="https://kurtkim.github.io/p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table2_hu4052d405d8c00af53725ab1cc3558100_19621_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>크기별로 네 개의 언어 모델을 훈련시키고 벤치마킹하였다. 가장 작은 모델의 크기는 원래의 GPT와 같으며, 두 번째로 작은 모델은 BERT의 가장 큰 모델과 같다. 가장 큰 모델인 GPT-2는 GPT보다 10배 이상 많은 parameter를 가지고 있다. 각 모델의 learning rate는 WebText의 5%인 held-out 샘플을 사용하여 수동 조정하으며, 모든 모델은 여전히 WebText에 underfitted 되었으며 더 오래 학습시키면 더 높은 성능을 얻을 수 있을 것이다.&lt;/p>
&lt;h3 id="language-modeling">Language Modeling&lt;/h3>
&lt;p>GPT-2 모델은 문자 단위(byte level)에서 작동하고, 손실이 큰 전처리나 토큰화가 필요 없으므로 모든 언어 모델 benchmark에서 평가할 수 있다. WebText 언어 모델에 따른 dataset의 로그-확률을 계산하는 방식으로 평가 하였다. WebText 언어 모델은 표준화된 텍스트, 토큰화 유물, 섞인 문장, &lt;!-- raw HTML omitted --> 문자열(40 billion 바이트 중 26번만 발생) 등을 예측해야 하기 때문에 많은 데이터셋에서 일반 분포 밖에서 테스트되어야 한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table3.png"
width="1302"
height="248"
srcset="https://kurtkim.github.io/p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table3_hu0672219cf8c312417ec05fb807289e43_62928_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="525"
data-flex-basis="1260px"
>&lt;/p>
&lt;p>WebText 언어 모델은 도메인과 데이터셋 간에 잘 transfer되며, zero-shot setting에서 8개의 dataset 중 7개에서 state-of-the-art를 달성하였다.&lt;/p>
&lt;h3 id="childrens-book-test">Children’s Book Test&lt;/h3>
&lt;p>Children’s Book Test(CBT)는 다양한 카테고리의 단어에 대한 언어 모델의 성능을 평가하기 위한 테스트로, 생략된 단어에 대한 10개의 가능한 선택 중 올바른 것을 예측한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure2.png"
width="632"
height="342"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure2_hucc3cfd28bfd28dab2bf32e81236256ba_62137_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;p>모델 크기가 증가함에 따라 성능이 지속적으로 개선되며, GPT-2는 일반 명사에서 93.3%, 개체명에서 89.1%의 성능을 달성하였다.&lt;/p>
&lt;h3 id="lambada">LAMBADA&lt;/h3>
&lt;p>LAMBADA dataset은 텍스트의 장거리 의존성(long-range dependencies)을 평가한다. GPT-2는 이 테스트에서의 perplexity를 99.8에서 8.6으로, 정확도를 19%에서 52.66%로 향상시켰다. 추가적으로, stop-word ﬁlter를 추가함으로써 정확도를 63.24%로 더욱 향상시켰다.&lt;/p>
&lt;h3 id="winograd-schema-challenge">Winograd Schema Challenge&lt;/h3>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure3.png"
width="620"
height="466"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure3_hue70bf388100cd8d57dc0d6c9817d7a6c_53535_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;p>Winograd Schema Challenge는 텍스트의 모호성을 해결하는 능력을 통해 시스템의 상식적 추론 능력을 측정하고자 한다. GPT-2는 정확도를 7% 향상시켜 70.70%를 달성하였다.&lt;/p>
&lt;h3 id="reading-comprehension">Reading Comprehension&lt;/h3>
&lt;p>CoQA(The Conversation Question Answering dataset)는 7가지 다른 분야의 문서와 문서에 대한 질문자-답변자 사이의 자연어 대화가 쌍을 이루고 있다. CoQA 테스트는 독해능력과 대화에 기반한 모델의 답변능력을 평가한다. GPT-2는 미세조정 없이 55 F1 score를 달성해 4개 중 3개의 다른 모델을 능가하였다.&lt;/p>
&lt;h3 id="summarization">Summarization&lt;/h3>
&lt;p>GPT-2의 요약 능력은 CNN과 Daily Mail dataset을 사용해서 테스트했다. 문서 이후에 &amp;ldquo;TL;DR:&amp;rdquo; 토큰을 추가하고 Top-k 랜덤 샘플링을 통해 요약을 유도했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table4.png"
width="572"
height="236"
srcset="https://kurtkim.github.io/p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table4_hu04688c90c05b2df30841db6bc90a452e_40450_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>처음 생성된 3개의 문장을 요약 결과로 하여 실험한 결과, 기사의 최근 내용에 초점을 맞추거나 특정 세부사항을 혼동하는 경향이 있다. &amp;ldquo;TL;DR:&amp;rdquo; 토큰 없이 실험한 경우, 성능이 더 하락한 것을 보면 힌트를 통한 Task 유도가 유의한 결과를 냄을 확인할 수 있었다.&lt;/p>
&lt;h3 id="translation">Translation&lt;/h3>
&lt;p>번역 능력은 WMT-14 English-French dataset을 사용해서, 영어-프랑스어, 프랑스어-영어 두가지 경우에서 비교가 진행되었다. 번역 성능은 다른 Task에 비해 좋지 상대적으로 좋지 않다. 영어-프랑스어 테스트에서 5 BLEU를, 프랑스어-영어 테스트에서는 11.5 BLEU를 달성했다.&lt;/p>
&lt;h3 id="question-answering">Question Answering&lt;/h3>
&lt;p>언어 모델에 얼마나 많은 정보가 들어있는지 테스트하기 위해 factoid-style의 질문에 얼마나 정확한 답을 생성하는지 평가한다. Natural Questions dataset을 이용해 GPT-2의 성능을 평가하였고 &amp;lsquo;정확히 일치 하는지&amp;rsquo; 여부(exact match metric)를 지표로 비교한다. 질문의 4.1%에 대해 올바르게 답을 하였고, 이는 기존의 모델들보다 5.3배 높은 정확도이다. 매우 작은 모델들은 대체로 1%를 넘지 못하는 성능을 보였는데, 아직까지는 모델의 크기가 QA에 있어서 매우 중요한 요인이라는 것을 확인할 수 있었다. 또한, 가장 확신하는 1%의 질문에 대해 63.1%의 정확도를 보였지만, 이는 여전히 정보 검색과 문서 질문 답변 추출을 결합한 시스템의 30%에서 50% 범위의 성능보다 훨씬 떨어진다.&lt;/p>
&lt;h2 id="generalization-vs-memorization">Generalization vs Memorization&lt;/h2>
&lt;p>최근 연구에 따르면, 일반적인 이미지 데이터셋에는 상당한 양의 중복된 이미지가 포함되어 있어, 기계 학습 시스템의 일반화 성능을 과대평가하게 만든다. 데이터셋의 크기가 커질수록 이 문제는 더욱 심화될 가능성이 있으며, 이는 테스트 데이터가 얼마나 훈련 데이터에도 포함되어 있는지 분석하는 것이 중요함을 의미한다.&lt;/p>
&lt;p>이를 연구하기 위해, WebText 훈련 데이터의 8-gram을 포함하는 Bloom 필터를 생성하였고, 주어진 데이터셋에 대해 그 데이터셋의 8-gram 중 얼마나 많은 비율이 WebText 훈련 세트에도 포함되어 있는지를 계산하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/table6.png"
width="892"
height="144"
srcset="https://kurtkim.github.io/p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/table6_hu172470c92ab1f519e4f88f1f5948b68f_30103_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="619"
data-flex-basis="1486px"
>&lt;/p>
&lt;p>일반적인 언어 모델 데이터셋의 테스트 세트는 WebText 훈련 세트와 1-6%의 중복을 가지며, 평균 중복률은 3.2%이다. 많은 데이터셋은 자신의 훈련 분할과 더 큰 중복을 가지며, 평균 중복률은 5.9%이다.&lt;/p>
&lt;p>데이터 중복을 최소화하는 방향으로 접근하였으며, 이러한 중복이 성능에 작은, 하지만 일관적인 향상을 가져다 준다는 분석 결과를 얻었다. 중복 제거 기법을 개선함으로써 이러한 문제에 대해 더욱 효과적으로 대응할 수 있다. 그리고 이러한 중복 제거 과정에서는 n-gram 중첩 기반의 방법을 활용하는 것이 중요하다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-2/images/figure4.png"
width="626"
height="572"
srcset="https://kurtkim.github.io/p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-2/images/figure4_hu6224d7555cf4d52f787f03ba1e304417_70891_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="262px"
>&lt;/p>
&lt;p>또한, WebText 언어 모델의 성능이 기억력에 의존하는지를 확인하기 위해 해당 모델이 자체 테스트 세트에서 어떤 성능을 보이는지 검사하였다. 이 결과, 모델 크기가 커짐에 따라 훈련 세트와 테스트 세트에서의 성능이 함께 향상되는 경향을 보였으며, 이로부터 GPT-2가 WebText에 대해 완벽하게 적합하지 않음을 추측할 수 있다.&lt;/p>
&lt;p>마지막으로, GPT-2가 말하는 유니콘의 발견에 대한 뉴스 기사를 작성하는 능력을 보여주었다. 이는 GPT-2의 창의성을 보여주는 한 예로 볼 수 있다.&lt;/p>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>이 연구는 더 큰 dataset에서 학습된 큰 언어 모델의 성능을 측정하는 데 중점을 두었다. 이 연구는 이전 연구와 비슷한 방향성을 가지고 있으며, 우리의 실험 결과는 주어진 목표의 세부 작업에 대한 추세가 큰 파라미터 범위로도 지속되는 것을 확인하였다.&lt;/p>
&lt;p>생성 모델에서는 RNN 언어 모델이 줄 너비를 추적하고 인용문이나 댓글을 감지하는 등의 흥미로운 기능을 배우는 것이 확인되었다. 또한, 위키백과 기사를 생성하도록 훈련된 모델이 언어 간 이름 번역을 배울 수 있음이 관찰되었다.&lt;/p>
&lt;p>iWeb Corpus같이 웹 페이지의 대형 텍스트 말뭉치를 필터링하고 구성하는 다양한 방법, 모든 단어 벡터 표현 학습을 확대하거나, 기계 번역 모델에서 파생된 표현의 사용을 탐색하는 사전학습 방법, seq2seq 모델 등이 연구 되었고, 언어모델의 사전학습이 잡담이나 대화 같은 어려운 생성문제에 맞춰 미세조정할 때 도움이 된다는 것을 밝혀내었다.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>비지도 사전 학습 방법의 표현에 대한 많은 연구가 있었으며, 이는 비지도 학습이 유망한 연구 영역임을 시사한다. GPT-2는 독해에 대해 경쟁력 있는 성능을 보였지만, 요약 등의 작업에 대해서는 아직 기본적인 수준에 불과하다. 많은 NLP 작업에서 GPT-2의 제로샷 성능을 연구했지만, 아직 많은 실용적인 작업에서는 성능이 무작위 수준에 불과한 경우가 많다. 제로샷 성능은 GPT-2의 잠재적 성능의 기준을 설정하지만, 미세 조정을 통한 성능의 상한선은 아직 불분명하다. 더욱이, GPT-2의 추가 훈련 데이터와 용량이 단방향 표현의 비효율성을 극복하기에 충분한지는 아직 불확실하다.&lt;/p>
&lt;p>decaNLP나 GLUE와 갈은 benchmark에서 미세조정 할 것을 계획하고 있으며, GPT-2의 학습데이터와 그 크기가 BERT에서 말한 단방향 표현의 비효율성을 극복할 수 있을 만큼 충분한지도 확실치 않다고 한다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>충분히 크고 다양한 dataset으로 학습된 큰 언어 모델인 GPT-2는 여러 도메인과 데이터셋에서 잘 수행하며, 테스트된 8개 언어 모델링 dataset 중 7개에서 state-of-the-art를 달성하였다. 이는 고용량 모델이 다양한 텍스트에 대한 가능성을 극대화하는 훈련을 통해, 명확한 지도 없이도 많은 작업을 수행하는 법을 배우기 시작한다는 것을 시사한다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/gpt-2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>BERT</title><link>https://kurtkim.github.io/p/bert/</link><pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/bert/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>BERT(Bidirectional Encoder Representations from Transformers)는 Transformer의 양방향 인코더 표현을 사용하는 언어모델로, BERT는 레이블이 없는 텍스트에서 깊은 양방향 표현을 사전 학습함으로써, 하나의 출력 레이어만을 추가해서 다양한 작업에 맞게 미세조정 할 수 있다. 이 모델은 개념적으로 단순하면서도 실증적으로 강력하며, 다양한 자연어 처리 작업에서 새로운 최고 수준의 결과를 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>언어 모델의 사전 학습은 다양한 자연어 처리 작업의 향상에 크게 기여하였다. 이는 문장 전체를 분석하여 문장 간 관계를 예측하는 자연어 추론이나 재구성과 같은 문장 수준의 작업뿐만 아니라, 명명된 개체 인식이나 질문 응답과 같이 토큰 수준의 작업을 포함한다.&lt;/p>
&lt;p>사전 학습된 모델을 downstream tasks에 적용하는 데는 두 가지 전략이 있다: feature-based와 미세조정(fine-tuning)이다. feature-based 접근법은 ELMo와 같이 사전 훈련된 표현을 추가 특성으로 사용하고, 미세 조정 접근법은 GPT와 같이 작업 특정 파라미터를 최소화하고 사전 훈련된 모든 파라미터를 미세 조정한다. 이 두가지 접근 방식은 사전 학습을 하는동안 같은 목적 함수를 공유하며, 일반적인 언어 표현을 학습하기 위해 단방향 언어 학습 모델을 사용한다.&lt;/p>
&lt;p>현재의 기술들은 특히 미세 조정 접근법에 대해 사전 학습된 표현의 가능성을 제한한다고 주장한다. 표준 언어 모델이 단방향적이므로, 사전 훈련 동안 사용할 수 있는 아키텍처가 제한되기 때문이다. OpenAI GPT의 경우, 모든 토큰이 이전 토큰만을 주목하는 &amp;lsquo;왼쪽에서 오른쪽으로&amp;rsquo;의 아키텍처를 사용한다. 이러한 제한은 문장 수준 작업이나 양방향 맥락 통합이 중요한 토큰 수준 작업에 대해 불리하다.&lt;/p>
&lt;p>이 논문에서는 BERT(Bidirectional Encoder Representations from Transformers)를 제안하여 미세 조정 기반 접근법을 개선한다. BERT는 일부 토큰을 무작위로 마스크한 &amp;ldquo;masked language model&amp;quot;을 사용하여 단방향성 제약을 완화하고, 이를 통해 깊은 양방향 Transformer를 사전 학습한다. 그리고 &amp;ldquo;next sentence prediction&amp;rdquo; 작업을 통해 텍스트 쌍 표현을 함께 사전 학습한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>언어 표현에 대한 양방향 사전 훈련의 중요성을 강조한다. BERT는 마스크된 언어 모델을 사용해 깊은 양방향 표현을 사전 학습하는데, 이는 단방향 언어 모델을 사용하는 기존 방법과 대조적이다. 또한, 독립적으로 훈련된 언어 모델을 얕게 연결하는 방식과도 차별화된다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>사전 학습된 표현은 작업별 아키텍처의 필요성을 줄이며, BERT는 다양한 작업에서 최고 수준의 성능을 달성하는 첫 미세 조정 기반 표현 모델이다. 이는 다수의 작업별 아키텍처들의 성능을 능가한다.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BERT는 11가지 자연어 처리(NLP) 작업에 대해 state-of-the-art 성능을 달성했다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>일반적인 언어 표현을 사전 학습하는 것은 오랜 역사를 가지고 있으며, 이 섹션에서는 가장 널리 사용되는 접근법들을 간략하게 검토한다.&lt;/p>
&lt;h3 id="unsupervised-feature-based-approaches">Unsupervised Feature-based Approaches&lt;/h3>
&lt;p>단어의 범용적인 표현 학습은 비신경망 및 신경망 방법을 포함한 여러 연구의 주제였다. 사전 학습된 단어 임베딩은 현대 NLP 시스템의 핵심 부분이며, 처음부터 학습된 임베딩보다 크게 개선된다. 단어 임베딩 벡터를 사전 훈련하기 위해, 언어 모델링 목표와 맥락에서 단어를 구별하는 목표가 사용되었다.&lt;/p>
&lt;p>이러한 접근법은 문장이나 문단 임베딩 등의 더 큰 단위로 확장되었다. 문장 표현을 훈련시키기 위해, 후보 다음 문장을 순위 매기는 연구, 이전 문장의 표현을 기반으로 다음 문장의 단어를 생성하는 연구, 또는 노이즈 제거 오토인코더에서 파생된 연구 등이 있었다.&lt;/p>
&lt;p>ELMo와 그 전임자는 단어 임베딩 연구를 다른 차원으로 확장해, 맥락에 따라 변하는 특징을 추출하였다. 이는 언어 모델의 왼쪽과 오른쪽 표현을 연결해 이루어진다. ELMo는 이를 기존의 작업 특정 아키텍처와 결합해 주요 NLP 벤치마크를 향상시켰다. 또한, 다른 연구들은 LSTM을 이용해 맥락적 표현을 학습하거나, 클로즈 작업을 통해 텍스트 생성 모델의 견고성을 향상시키는 방법을 제안하였다.&lt;/p>
&lt;h3 id="unsupervised-fine-tuning-approaches">Unsupervised Fine-tuning Approaches&lt;/h3>
&lt;p>첫 번째 연구들은 레이블이 없는 텍스트에서 단어 임베딩 파라미터만 사전 학습하였다.&lt;/p>
&lt;p>최근에는 문장이나 문서 인코더가 레이블이 없는 텍스트에서 사전 훈련되고, 지도학습의 다음 작업을 위해 미세조정되었다. 이 방법의 장점은 적은 양의 파라미터만 처음부터 학습하면 된다는 것이며, 이 때문에 OpenAI GPT는 여러 문장 수준 작업에서 최고 성능을 달성하였다. 이러한 모델을 사전 학습하기 위해 왼쪽에서 오른쪽으로의 언어 모델링과 오토인코더가 사용되었다.&lt;/p>
&lt;h3 id="transfer-learning-from-supervised-data">Transfer Learning from Supervised Data&lt;/h3>
&lt;p>대규모 데이터셋을 가진 자연어 추론과 기계 번역 등의 감독 학습 작업에서 효과적인 전이 학습이 보여주었다. 또한, 컴퓨터 비전 연구에서는 ImageNet으로 사전 훈련된 대규모 모델을 미세조정하여 전이 학습의 중요성을 입증하였다.&lt;/p>
&lt;hr>
&lt;h2 id="bert">BERT&lt;/h2>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure1.png"
width="1218"
height="514"
srcset="https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure1_hu00f70a8ca5c71ceb6a922a072bdffe29_165193_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="568px"
>&lt;/p>
&lt;p>BERT는 사전 학습과 미세 조정 두 단계로 이루어진다. 사전 학습에서는 레이블이 없는 데이터로 모델을 학습하며, 미세 조정에서는 사전 훈련된 파라미터로 초기화된 BERT 모델을 downstream tasks의 레이블이 붙은 데이터로 미세 조정한다. 각 downstream tasks는 동일한 사전 훈련된 파라미터로 초기화되지만 별도의 미세 조정된 모델을 가진다.&lt;/p>
&lt;p>BERT는 다양한 작업에 걸친 통일된 아키텍처이며, 사전 학습된 아키텍처와 최종 downstream tasks 아키텍처 사이에는 거의 차이가 없다.&lt;/p>
&lt;h3 id="model-architecture">Model Architecture&lt;/h3>
&lt;p>BERT의 구조는 다중 레이어 양방향 Transformer 인코더로, Transformer의 널리 쓰이는 사용과 거의 동일하기 때문에, 모델 아키텍처의 상세한 설명은 &amp;ldquo;The Annotated Transformer&amp;quot;를 참조하도록 권장한다.&lt;/p>
&lt;p>이 연구에서는 레이어의 개수(Transformer 블록)를 $L$, 은닉의 크기를 $H$, self-attention 헤드 수를 $A$로 표시한다. $BERT_{BASE}$ (L=12, H=768, A=12, Total Parameters=110M)와 $BERT_{LARGE}$ (L=24, H=1024, A=16, Total Parameters=340M)의 두 모델 사이즈에 대한 결과를 비교한다.&lt;/p>
&lt;p>비교를 위해 $BERT_{BASE}$는 같은 모델 사이즈인 OpenAI GPT와 비교하였다. 그러나 중요한 점은, BERT Transformer는 양방향 self-attention을 사용하는 반면, GPT Transformer는 각 토큰이 왼쪽의 컨텍스트에만 주의를 기울일 수 있는 제한된 self-attention을 사용한다.&lt;/p>
&lt;h3 id="inputoutput-representations">Input/Output Representations&lt;/h3>
&lt;p>BERT의 입력 표현은 단일 문장과 문장 쌍(예: 〈질문, 답변〉)을 하나의 토큰 시퀀스에서 명확하게 표현할 수 있다. 여기서 &amp;ldquo;문장(senetence)&amp;ldquo;은 실제 문장이 아닌 텍스트의 일부를, &amp;ldquo;시퀀스(sequence)&amp;ldquo;는 BERT 입력 토큰을 의미하며, 이는 하나 또는 두 개의 문장일 수 있다.&lt;/p>
&lt;p>30,000개의 토큰 단어를 가진 WordPiece임베딩을 사용하며, 모든 문장의 첫번째 토큰은 특별한 분류 토큰([CLS])으로 시작한다. 이 토큰에 해당하는 마지막 은닉 상태(hidden state)는 분류 작업을 위해 총 시퀀스 표현으로 사용된다. 문장 쌍은 하나의 시퀀스로 묶이며, 특별한 토큰([SEP])과 학습된 임베딩을 사용해 문장을 구분한다. 입력 임베딩은 $E$로, [CLS] 토큰과 i번째 입력 토큰의 최종 은닉 벡터는 각각 $C \in R^H$, $T_i \in R^H$ 로 표기한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/figure2.png"
width="984"
height="308"
srcset="https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/figure2_hu31d588c51731c5786afe5c3c0605896d_67252_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="319"
data-flex-basis="766px"
>&lt;/p>
&lt;p>주어진 토큰에 대한 입력 표현은 해당 토큰, 세그먼트, 위치 임베딩을 합하여 구성된다.&lt;/p>
&lt;h3 id="pre-training-bert">Pre-training BERT&lt;/h3>
&lt;p>BERT를 사전 학습하기 위해 전통적인 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로의 언어 모델을 사용하지 않았으며, 두 가지 비지도 학습 작업을 사용하여 BERT를 사전 학습하였다.&lt;/p>
&lt;h4 id="task-1-masked-lm">Task #1: Masked LM&lt;/h4>
&lt;p>깊은 양방향 모델은 왼쪽에서 오른쪽 혹은 오른쪽에서 왼쪽 모델보다 강력하다는 것이 직관적이다. 하지만, 표준 언어 모델은 양방향 조건을 적용하면 각 단어가 간접적으로 자신을 &amp;lsquo;볼&amp;rsquo; 수 있게 되어, 모델이 문맥 속의 목표 단어를 쉽게 예측할 수 있게 되므로, 왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽으로만 훈련된다.&lt;/p>
&lt;p>깊은 양방향 표현을 훈련하기 위해, 입력 토큰의 일부를 무작위로 마스킹하고 그 마스킹된 토큰들을 예측하는 &amp;lsquo;Masked LM&amp;rsquo; 방법을 사용한다. 이 방법은 Cloze 작업으로 불리기도 한다. 마스킹된 토큰에 해당하는 최종 은닉 벡터는 어휘에 대한 출력 softmax에 공급되며, 모든 실험에서 각 시퀀스의 15% 토큰을 무작위로 마스킹한다. 이 방법은 denoising auto-encoders와 달리 마스킹된 단어만을 예측한다.&lt;/p>
&lt;p>양방향 사전 학습 모델을 얻는 방법은 [MASK] 토큰이 미세 조정 과정에서 나타나지 않는 문제로 인해 사전 훈련과 미세 조정 사이에 불일치를 유발한다. 이를 완화하기 위해, &amp;ldquo;마스킹된&amp;rdquo; 단어를 항상 [MASK] 토큰으로 교체하지 않는다. 토큰 위치의 15%를 임의로 선택하고, 선택된 토큰은 80% 확률로 [MASK] 토큰, 10% 확률로 임의의 토큰, 10% 확률로 원래 토큰으로 교체한다. 그 후, 크로스 엔트로피 손실을 사용하여 원래 토큰을 예측한다.&lt;/p>
&lt;h4 id="task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)&lt;/h4>
&lt;p>질문 응답(QA)과 자연어 추론(NLI) 같은 작업들은 두 문장 간의 관계를 이해하는 것에 기초하며, 이는 언어 모델링만으로는 직접적으로 캡처할 수 없다. 이를 해결하기 위해, 문장간 관계를 이해하는 모델을 위한 사전 훈련 과정에서, 어떤 하나의 언어를 사용하는 말뭉치로부터 생성될 수 있는 다음 문장 예측 과제를 2진화(binarized)된 다음 문장 예측 작업을 한다. 구체적으로, 각 사전 훈련 예제에서, 선택된 두 문장 A와 B는 50%의 확률로 실제 연속하는 문장이며, 나머지 50%는 말뭉치에서 임의로 선택된 문장이다. 이 간단한 방법이 QA와 NLI에 큰 도움이 된다는 것을 입증하였다.&lt;/p>
&lt;p>다음 문장 예측(NSP) 작업은 이전 연구와 밀접한 관련이 있지만, 이전 연구에서는 문장 임베딩만을 하위 작업에 전달했으나, BERT는 모든 매개변수를 최종 작업 모델 초기화에 사용한다.&lt;/p>
&lt;h4 id="pre-training-data">Pre-training data&lt;/h4>
&lt;p>사전 학습 절차는 대부분 언어 모델 사전 학습에 대한 기존 연구를 따른다. 이때 사용되는 말뭉치는 BooksCorpus(800M개의 단어)와 영어 위키백과(2,500M개의 단어)이다. 위키백과에서는 텍스트 부분만 추출하며, 긴 연속적인 시퀀스를 추출하기 위해 문장 수준이 아닌 문서 수준의 말뭉치 사용이 중요하다는 점을 강조한다.&lt;/p>
&lt;h3 id="fine-tuning-bert">Fine-tuning BERT&lt;/h3>
&lt;p>Transformer의 자기 self-attention mechanism을 활용한 BERT의 미세 조정은 단일 텍스트나 텍스트 쌍을 포함한 다양한 downstream tasks를 모델링하는데 효과적이다. 텍스트 쌍을 독립적으로 인코딩한 후 양방향 cross attention을 적용하는 것이 일반적이지만, BERT는 이 두 단계를 통합하여 self-attention으로 텍스트 쌍을 인코딩함으로써 두 문장 간의 양방향 cross attention을 효과적으로 포함시킨다.&lt;/p>
&lt;p>각 작업마다 BERT에 작업 특정 입력과 출력을 연결하고 모든 매개변수를 미세 조정한다. 입력에서, 사전 훈련된 문장 A와 B는 다양한 작업(표현 변경, 함축, 질문 답변, 텍스트 분류 등)의 입력 쌍에 상응한다. 출력에서, 토큰 표현은 토큰 수준 작업에, [CLS] 표현은 분류 작업에 사용된다.&lt;/p>
&lt;p>미세 조정은 사전 훈련에 비해 상대적으로 저렴하며, 이 논문의 모든 결과는 동일한 사전 훈련 모델을 기반으로 클라우드 TPU에서는 1시간, GPU에서는 몇 시간 안에 재현 가능하다.&lt;/p>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;p>11가지 자연어 처리(NLP) 작업에 대한 BERT 미세 조정 결과를 보여준다.&lt;/p>
&lt;h3 id="glue">GLUE&lt;/h3>
&lt;p>General Language Understanding Evaluation(GLUE) benchmark는 다양한 자연어 이해 작업의 모음이다.&lt;/p>
&lt;p>GLUE에서 미세 조정을 하기 위해, 입력 시퀀스를 표현하고 첫 번째 입력 토큰([CLS])에 대응하는 최종 벡터를 종합 표현으로 사용한다. 미세 조정시 도입되는 유일한 새로운 매개변수는 분류 레이어의 가중치이다. 그리고 이들을 사용해 standard classiﬁcation 손실을 계산한다.&lt;/p>
&lt;p>batch size 32로 데이터를 3 epoch 동안 미세 조정한다. 각 작업마다 최적의 미세 조정 학습률을 선택하며, $BERT_{LARGE}$는 작은 데이터셋에서 불안정할 때 랜덤 재시작을 사용하여 최적의 모델을 선택한다. 랜덤 재시작에서는 동일한 사전 학습 체크포인트를 사용하지만 데이터 셔플링과 분류기 레이어 초기화는 다르게 합니다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table1.png"
width="1230"
height="272"
srcset="https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table1_huf3a07f0fefdef490b04f70f7c1b26705_79420_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="452"
data-flex-basis="1085px"
>&lt;/p>
&lt;p>$BERT_{BASE}$와 $BERT_{LARGE}$는 모든 작업에서 월등한 성능을 보여주며, 평균 정확도를 각각 4.5%, 7.0% 향상시켰다. 가장 큰 GLUE 작업인 MNLI에서 BERT는 정확도를 4.6% 향상시켰고, 공식 GLUE 리더보드에서는 $BERT_{LARGE}$가 80.5의 점수로 OpenAI GPT의 72.8을 능가했다.&lt;/p>
&lt;p>$BERT_{LARGE}$가 모든 작업에서 $BERT_{BASE}$를 크게 능가하며, 특히 훈련 데이터가 매우 적은 작업에서 그렇다는 것을 발견하였다.&lt;/p>
&lt;h3 id="squad-v11">SQuAD v1.1&lt;/h3>
&lt;p>The Stanford Question Answering Dataset (SQuAD v1.1)는 10만 개의 크라우드 소싱 질문/답변 쌍의 컬렉션이다. 주어진 질문과 답변을 포함하는 위키백과의 문단이 주어지면, 그 문단 내에서 실제 답변의 위치나 범위를 정확하게 예측하는 것이 목표이다.&lt;/p>
&lt;p>질문 응답 작업에서는 입력 질문과 문단을 하나의 연결된 시퀀스로 표현하며, 각각 다른 임베딩을 사용한다. 미세 조정 과정에서는 시작과 끝 벡터만 추가적으로 도입되며, 답변의 시작 단어 확률은 해당 단어와 시작 벡터 간의 내적 후 softmax를 적용하여 계산된다.&lt;/p>
&lt;p>답변 범위의 시작과 끝 위치를 예측하는 공식이 사용되며, 그 중 최대 점수를 가진 범위가 최종 예측값이 된다. 훈련 목표는 정확한 시작과 끝 위치의 로그 가능도 합이며, learning rate 5e-5와 batch size 32로 3 epoch 동안 미세 조정이 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table2.png"
width="548"
height="488"
srcset="https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table2_huc28c19d0056a77485b24c6c1b636d127_93131_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>BERT 모델은 다른 공개 데이터를 사용하여 학습된 모델들을 뛰어넘으며, 특히 TriviaQA에 먼저 미세 조정을 함으로써 성능을 향상시켰다. 단일 BERT 모델만으로도 최고의 앙상블 시스템을 능가하며, TriviaQA 데이터 없이도 모든 기존 시스템을 크게 앞서고있다.&lt;/p>
&lt;h3 id="squad-v20">SQuAD v2.0&lt;/h3>
&lt;p>SQuAD 2.0 작업은 제공된 문단에 짧은 답변이 존재하지 않을 수 있다는 가능성을 허용함으로써 SQuAD 1.1 문제 정의를 확장하였고, 이로 인해 문제가 더 현실적으로 변하였다.&lt;/p>
&lt;p>이 작업을 위해 SQuAD v1.1 BERT 모델을 간단하게 확장하였다. 답변이 없는 질문은 시작과 끝이 [CLS] 토큰에 있는 답변 범위로 취급하였다. 예측 시, 답변이 없는 범위의 점수와 최고의 비-null 범위의 점수를 비교하여, 특정 임계값을 넘을 경우 non-null 답변을 예측하였다. 이 모델에서는 TriviaQA 데이터를 사용하지 않았으며, learning rate 5e-5와 batch size 48로 2 epoch 동안 미세 조정을 수행하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table3.png"
width="554"
height="384"
srcset="https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table3_hucafb5411e878c5b05d50058000c43f06_60324_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>BERT를 사용하지 않는 시스템들과의 결과를 비교하였다. 다른 모델들에 비해 F1 점수가 5.1점 향상되었다.&lt;/p>
&lt;h3 id="swag">SWAG&lt;/h3>
&lt;p>The Situations With Adversarial Generations (SWAG) dataset는 실제 상식 추론을 평가하는 113k개의 문장 쌍 완성 예제를 포함하고 있다. 주어진 문장에 대해, 작업은 네 가지 선택지 중 가장 그럴듯한 답을 선택하는 것이다.&lt;/p>
&lt;p>SWAG dataset에서 미세 조정을 할 때, 각각 주어진 문장과 가능한 연속성을 포함하는 네 개의 입력 시퀀스를 만든다. [CLS] 토큰 표현과 내적을 이루는 벡터는 각 선택지에 대한 점수를 나타내며, 이 점수는 softmax 레이어를 통해 정규화한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table4.png"
width="390"
height="304"
srcset="https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table4_huaa7b68886e46455dbc2a98e05a48a7e3_41001_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="307px"
>&lt;/p>
&lt;p>learning rate가 2e-5이고 batch size가 16인 상태로 모델을 3 epoch 동안 미세 조정하였다. $BERT_{LARGE}$는 ESIM+ELMo 모델을 +27.1%로, OpenAI GPT를 8.3%로 능가하였다.&lt;/p>
&lt;h2 id="ablation-studies">Ablation Studies&lt;/h2>
&lt;p>상대적인 중요성을 더 잘 이해하기 위해 BERT의 여러 면에 걸쳐서 ablation 실험을 수행한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table5.png"
width="606"
height="244"
srcset="https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table5_huef70981bcc644e2ad9fa8119807b5d76_44344_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="596px"
>&lt;/p>
&lt;h3 id="effect-of-pre-training-tasks">Effect of Pre-training Tasks&lt;/h3>
&lt;p>$BERT_{BASE}$의 동일한 사전 학습 데이터, 미세 조정 scheme, 그리고 hyperparameter를 사용하여 두 가지 사전 학습 목표를 평가함으로써 BERT의 깊은 양방향성의 중요성을 입증한다.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>No NSP: &amp;ldquo;다음 문장 예측(NSP)&amp;rdquo; 과제를 하지 않은, &amp;ldquo;Masked LM(MLM)&amp;ldquo;을 사용해 훈련된 양방향 모델&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LTR &amp;amp; No NSP: 왼쪽 컨텍스트만 있는 모델은 표준 LTR LM을 사용해 훈련되며, 이는 미세 조정 시에도 유지된다. 이 모델은 NSP 작업 없이 사전 훈련되었다.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>NSP 작업을 제거하면 QNLI, MNLI, SQuAD 1.1에서 성능이 크게 저하된다. 또한, 양방향 표현을 훈련하는 것은 성능에 중요한 영향을 미치며, 특히 LTR 모델은 모든 작업에서 MLM 모델보다 성능이 떨어진다.&lt;/p>
&lt;p>SQuAD의 경우, 토큰 레벨 은닉 상태가 오른쪽 컨텍스트를 가지고 있지 않기 때문에 LTR 모델이 토큰 예측에서 성능이 떨어질 것이라는 것은 직관적으로 명확하다. 이를 개선하기 위해 무작위로 초기화된 BiLSTM을 추가했지만, 결과는 사전 훈련된 양방향 모델보다 훨씬 떨어진다. 또한, BiLSTM은 GLUE 작업에서의 성능을 저하시킨다.&lt;/p>
&lt;p>LTR과 RTL 모델을 별도로 훈련하는 것은 가능하지만, 이는 단일 양방향 모델보다 비용이 두 배 많이 들고, QA와 같은 작업에 대해 직관적이지 않다. 또한, 이 방식은 모든 계층에서 양방향 컨텍스트를 사용하는 모델보다 성능이 엄격하게 떨어진다.&lt;/p>
&lt;h3 id="effect-of-model-size">Effect of Model Size&lt;/h3>
&lt;p>모델 크기가 미세 조정 작업 정확도에 미치는 영향을 알아본다. BERT 모델을 같은 파라미터와 훈련 절차를 사용한 반면, 레이어의 수, 은닉 상태 개수, 어텐션 헤드 개수를 다르게 학습했다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table6.png"
width="528"
height="286"
srcset="https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table6_huc9ab3a4646faacd803209a81ed695698_48955_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;p>GLUE 작업 결과에 따르면, 더 큰 모델은 레이블이 붙은 훈련 예시가 적은 데이터셋에서도 정확도를 개선하였다. 이는 이미 상대적으로 큰 모델을 기반으로 중요한 개선을 이루어내며, 예를 들어 $BERT_{BASE}$는 110M, $BERT_{LARGE}$는 340M의 파라미터를 가진다. 이는 기존 문헌에서 제시한 Transformer 모델 보다 훨씬 크다.&lt;/p>
&lt;p>모델 크기를 늘리는 것이 대규모 작업에서 성능을 향상시키는 것은 잘 알려져 있지만, 이 연구는 모델이 충분히 사전 훈련되었다면 작은 규모의 작업에서도 큰 개선을 가져올 수 있다는 것을 보여준다. 이전의 연구들은 사전 훈련된 모델의 크기를 늘리는 것이 혼합된 결과를 가져왔지만, 이 연구는 모델이 작업에 직접 미세 조정을 받고, 매우 적은 수의 무작위로 초기화된 추가 파라미터만 사용할 때, 작은 규모의 작업도 크고 표현력 있는 사전 훈련된 표현의 이점을 볼 수 있다.&lt;/p>
&lt;h3 id="feature-based-approach-with-bert">Feature-based Approach with BERT&lt;/h3>
&lt;p>지금까지의 BERT 결과는 모두 미세 조정 방식을 사용했다. 이 방식은 사전 학습된 모델에 분류 계층을 추가하고 모든 파라미터를 하류 작업에 맞게 조정하는 방법이다. 그러나, 사전 학습된 모델에서 고정 특징을 추출하는 특징 기반 접근법도 장점이 있다. 일부 작업은 Transformer 인코더 아키텍처로 표현하기 어려워 특정 작업용 모델이 필요하며, 훈련 데이터의 복잡한 표현을 미리 계산하고 이를 기반으로 저렴한 모델로 실험을 진행하면 계산적으로 이점이 있다.&lt;/p>
&lt;p>이 섹션에서는 BERT를 이름 인식(NER) 작업에 적용하여 두 가지 접근법을 비교한다. BERT 입력에는 대소문자를 구분하는 WordPiece 모델을 사용하고, 데이터에서 제공하는 최대 문서 컨텍스트를 포함한다. 이 작업은 일반적인 방식에 따라 태깅 작업으로 설정되지만, 출력에서는 CRF 계층은 사용하지 않는다. NER 레이블 세트에 대한 토큰 수준 분류기의 입력으로 첫 번째 서브토큰의 표현을 사용한다.&lt;/p>
&lt;p>미세 조정 방식을 제거하기 위해, BERT의 매개변수를 조정하지 않고 특징 기반 방식을 적용하여 활성화 함수를 추출한다. 이 문맥적인 임베딩은 랜덤하게 초기화된 두 계층의 768차원 BiLSTM에 입력으로 사용되며, 이는 분류 레이어 이전에 이루어진다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/bert/images/table7.png"
width="550"
height="458"
srcset="https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/bert/images/table7_hu0b7abc968dac65c5fc4894b739c4ee8e_88243_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="288px"
>&lt;/p>
&lt;p>$BERT_{LARGE}$는 state-of-the-art 방법들과 비슷한 수준의 성능을 보여준다. 가장 효과적인 방법은 사전 훈련된 Transformer의 상위 4개 계층에서 토큰 표현을 결합하는 것이며, 이는 전체 모델을 미세 조정한 것보다 F1에서 0.3만큼 뒤떨어진다. 이는 BERT가 미세 조정과 특징 기반 접근법 모두에 효과적임을 보여준다.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>언어 모델과 전이 학습을 통한 최근 개선은 비지도 사전 학습이 언어 이해 시스템의 중요한 부분임을 보여준다. 이 결과는 low-resource tasks조차 깊은 단방향 아키텍처에서 이익을 얻을 수 있다는 것을 보여주었다. 이 논문에서는 이를 깊은 양방향 아키텍처로 일반화함으로써, 사전 훈련된 동일 모델이 다양한 NLP 작업을 성공적으로 처리할 수 있게 한다는 것을 보여준다.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/bert" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPT-1</title><link>https://kurtkim.github.io/p/gpt-1/</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><guid>https://kurtkim.github.io/p/gpt-1/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>자연어 이해는 텍스트의 함축, 질문에 대한 답변, 의미의 유사성 평가, 문서 분류 등 다양한 작업으로 구성되어 있다. 레이블이 지정된 데이터가 부족한 상황에서, 이 논문은 레이블이 없는 텍스트 데이터에 대해 언어 모델을 (생성적) 사전학습(generative pre-training)하고, 이를 특정 작업에 미세조정(fine-tuning)하는 방식을 제안한다. 이 방법은 모델 아키텍처에 최소한의 변경만을 요구하면서도 효과적인 전이를 달성하였고, 다양한 자연어 이해 벤치마크에서 우수한 성능을 보여주었다. 이 모델은 각 작업에 특별히 설계된 모델을 능가하며, 12개의 작업 중 9개에서 최고 성능을 달성하였다.&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>자연어 처리(NLP)에서 지도 학습의 의존성을 줄이는 것은 중요한데, 이는 대부분의 딥러닝 방법이 수동 레이블링된 대량의 데이터가 필요하기 때문이다. 이런 상황에서 레이블이 없는 데이터에서 언어 정보를 추출할 수 있는 모델은 유용한 대안이 될 수 있으며, 비지도 학습을 통해 학습하는 것이 더 나은 결과를 얻는 경우도 있다. 이를 입증하는 가장 강력한 예는 사전 학습된 단어 임베딩이며, 이는 다양한 NLP 작업에서 성능 향상을 위해 널리 사용되고 있다.&lt;/p>
&lt;p>레이블이 없는 텍스트에서 단어 수준을 넘어서는 정보를 활용하는 것은 어려운 도전 과제이며, 이유는 다음과 같다. 첫째, 텍스트 표현을 학습하고 다른 곳에 유용하게 전이하는 최적화 목표가 무엇인지 확실하지 않다. 둘째, 학습된 표현을 어떤 작업에 가장 효과적으로 적용할 방법이 아직 확립되지 않았다. 이런 불확실성이 효과적인 준지도 학습 방법을 개발하는 것을 어렵게 한다.&lt;/p>
&lt;p>이 연구는 언어 이해 작업에 비지도 사전 학습(unsupervised pre-training)과 지도 미세 조정(supervised fine-tuning)을 결합하는 준지도 학습을 제안한다. 목표는 적은 조정으로 다양한 작업에 적용 가능한 표현을 학습하는 것이다. 레이블이 없는 대량의 텍스트와 수동으로 레이블링된 훈련 예제를 사용하며, 학습은 두 단계로 진행된다. 먼저, 레이블이 없는 데이터로 모델의 초기 파라미터를 학습하고, 그 다음으로 지도 학습을 통해 이 파라미터를 목표 작업에 맞게 조정한다.&lt;/p>
&lt;p>이 연구에서는 다양한 작업에서 뛰어난 성능을 보인 Transformer모델을 사용한다. 이 모델은 텍스트의 장기적인 의존성을 처리하는 더 구조화된 메모리를 제공하므로 강한 전이 성능을 보여준다. 전이 단계에서는 작업 특정 입력 조정을 사용하여 텍스트 입력을 연속 토큰 시퀀스로 처리하며, 이 방식은 사전 학습된 모델의 구조를 최소한으로 변경하면서 효과적으로 미세 조정할 수 있음을 실험적으로 입증한다.&lt;/p>
&lt;p>이 연구는 자연어 추론, 질문 응답, 의미 유사성, 텍스트 분류 등 네 가지 언어 이해 작업에서 모델을 평가하였다. 제시된 모델은 각 작업에 특화된 모델들보다 더 우수한 성능을 보여주었고, 12개 작업 중 9개에서 최고 성능을 보여주었다.&lt;/p>
&lt;hr>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;h3 id="semi-supervised-learning-for-nlp">Semi-supervised learning for NLP&lt;/h3>
&lt;p>이 연구는 자연어에 대한 준지도 학습 범주에 속하며, 이는 시퀀스 라벨링이나 텍스트 분류와 같은 작업에 적용된다. 초기에는 레이블 없는 데이터를 사용해 단어나 구문 수준의 통계를 계산하였지만, 최근에는 레이블이 없는 말뭉치에서 훈련된 단어 임베딩을 활용하여 작업 성능을 향상시키는 방향으로 연구가 진행되고있다. 그러나 이 논문의 목표는 단어 수준 이상의 의미를 포착하는 것이며, 이를 위해 구문이나 문장 수준의 임베딩을 활용하여 텍스트를 벡터 표현으로 인코딩하는 방식을 채택하였다.&lt;/p>
&lt;h3 id="unsupervised-pre-training">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 사전 학습은 좋은 초기화 지점을 찾는 것을 목표로 하며, 이미지 분류, 음성 인식, 엔티티 구분, 기계 번역 등 다양한 작업에서 DNN의 훈련을 돕는데 사용되고있다.&lt;/p>
&lt;p>이 연구는 언어 모델링 목표를 사용하여 신경망을 사전 학습하고, 지도 학습으로 목표 작업에서 미세 조정하는 방식을 따른다. 이 방법은 LSTM을 사용하는 이전의 방법들이 제한적인 예측 능력을 가지는 반면, Transformer는 더 넓은 범위의 언어 구조를 포착할 수 있게 한다. GPT 모델은 자연어 추론, 패러프레이즈 감지, 스토리 완성 등 다양한 작업에서 효과를 보여주었으며, 다른 모델이 새로운 파라미터를 많이 필요로 하는 반면, GPT 모델은 아키텍처에 최소한의 변경만 필요로 한다.&lt;/p>
&lt;h3 id="auxiliary-training-objectives">Auxiliary training objectives&lt;/h3>
&lt;p>보조적인 비지도 학습 목표 추가는 준지도 학습의 변형 형태로, 다양한 NLP 작업을 통해 의미 역할 라벨링을 개선하는데 사용되었다. 최근에는 이러한 보조 목표를 목표 작업에 추가하여 시퀀스 라벨링 작업에서 성능을 향상시켰다. 이 연구에서도 비지도 사전 훈련이 이미 목표 작업과 관련된 다양한 언어적 요소를 학습한다는 것을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="framework">Framework&lt;/h2>
&lt;p>학습은 큰 말뭉치에서 대용량 언어 모델을 학습하는 단계와 레이블이 달린 데이터를 활용해 모델을 목표 작업에 맞게 미세 조정하는 단계로 이루어진다.&lt;/p>
&lt;h3 id="unsupervised-pre-training-1">Unsupervised pre-training&lt;/h3>
&lt;p>비지도 토큰 말뭉치 $U = \lbrace u_1, &amp;hellip; , u_n \rbrace $ 가 주어질때, 다음 Likelihood를 최대화하도록 표준언어모델링 목적함수를 사용한다:&lt;/p>
&lt;p>$$ L_1(U) = \sum_{i} \log{P} (u_i | u_{i-k}, &amp;hellip; , u_{i-1}, \theta) $$&lt;/p>
&lt;p>$k$는 context window의 크기이며, 조건부 확률 $P$는 parameter $\theta$를 가진 신경망을 사용하여 모델링된다. 이 parameter들은 stochastic gradient descent를 사용하여 학습된다.&lt;/p>
&lt;p>GPT 모델은 언어모델로 multi-layer Transformer decoder를 사용하며, 이 모델은 입력 컨텍스트 토큰에 대해 multi-headed self-attention을 적용한 후, position-wise feedforward layer를 적용하여 목표 토큰에 대한 출력 분포를 생성한다:&lt;/p>
&lt;p>$$ h_0 = UW_e + W_p $$
$$ h_l = \text{transformer_block}(h_{l-1}) \forall i \in [1, n] $$
$$ P(u) = \text{softmax}(h_n W^T_e) $$&lt;/p>
&lt;p>$U = (u_{i-k}, &amp;hellip; , u_{i-1}) $ 는 토큰의 컨텍스트 벡터이고, $n$은 layer의 수, $W_e$ 는 토큰 임베딩 행렬, $W_p$ 는 위치 임베딩 행렬이다.&lt;/p>
&lt;h3 id="supervised-ﬁne-tuning">Supervised ﬁne-tuning&lt;/h3>
&lt;p>모델을 학습한 후, parameter를 목표 작업에 맞게 조정한다. 레이블이 지정된 데이터셋 $C$ 는 입력 토큰 $x^1, &amp;hellip; , x^m $ 과 레이블 $y$로 구성된다. 입력은 사전 훈련된 모델을 통과하여 최종 transformer block의 활성값인 $h^m_l$ 을 얻으며, 이는 parameter $W_y$ 와 함께 선형 출력층으로 전달되어 $y$ 를 예측한다:&lt;/p>
&lt;p>$$ P(y|x^1, &amp;hellip; , x^m) = \text{softmax}(h^m_l W_y) $$&lt;/p>
&lt;p>이는 다음을 최대화 한다.&lt;/p>
&lt;p>$$ L_2(C) = \sum_{(x,y)} \log{P(y|x^1, &amp;hellip; , x^m)} $$&lt;/p>
&lt;p>추가로 미세 조정을 위한 보조 목표로 언어 모델링을 포함시키는 것은 지도 모델의 일반화를 향상시키고, 수렴을 가속화하는데 도움이 된다. 구체적으로, weight $\lambda$에 대해 다음을 최적화한다:&lt;/p>
&lt;p>$$ L_3(C) = L_2(C) + \lambda L_1(C) $$&lt;/p>
&lt;p>미세 조정 과정에서 추가 매개변수는 $W_y$ 와 구분자 토큰의 임베딩뿐이다.&lt;/p>
&lt;h3 id="task-speciﬁc-input-transformations">Task-speciﬁc input transformations&lt;/h3>
&lt;p>텍스트 분류같은 일부 작업들은 모델을 직접 미세 조정할 수 있지만, 질문 답변이나 텍스트 함의 같은 작업들은 구조화된 입력을 필요로 하는데, 이러한 입력에 대해 사전 학습된 모델은 별도의 수정 없이도 처리할 수 있다. 대신, 이런한 입력을 모델이 처리할 수 있는 순서가 있는 시퀀스로 변환한다. 이 접근법은 작업 간에 아키텍처를 크게 변경할 필요를 없애준다. 또한, 모든 변형에는 무작위로 초기화된 시작과 종료 토큰을 포함한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure1.png"
width="1058"
height="450"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure1_huf693ec86c2dee30b1295845fdb401f1a_181900_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="564px"
>&lt;/p>
&lt;h4 id="textual-entailment">Textual entailment&lt;/h4>
&lt;p>텍스트 함의에서는, 전제 $p$와 가설 $h$를 구분자 &lt;code>$&lt;/code>로 연결한다.&lt;/p>
&lt;h4 id="similarity">Similarity&lt;/h4>
&lt;p>유사성 경우, 비교되는 두 문장의 순서는 정해져 있지 않으므로, 텍스트 두 개를 다른 순서로 이어붙여 각각을 독립적으로 처리하여 두 시퀀스 표현 $h^m_l$을 생성한다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning">Question Answering and Commonsense Reasoning&lt;/h4>
&lt;p>컨텍스트 문서 $z$, 질문 $q$, 가능한 답변들 $\lbrace a_k \rbrace$을 받는다. 각 가능한 답변을 문맥 문서와 질문에 연결하고, 구분자 토큰을 추가해 시퀀스 $[z; q;$ &lt;code>$&lt;/code>; $a_k]$ 를 만든다. 이 시퀀스들은 독립적으로 처리되고, softmax 계층을 통해 정규화되어 답변들에 대한 출력 분포를 생성한다.&lt;/p>
&lt;hr>
&lt;h2 id="experiments">Experiments&lt;/h2>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;h4 id="unsupervised-pre-training-2">Unsupervised pre-training&lt;/h4>
&lt;p>언어 모델 학습에 BooksCorpus 데이터셋을 사용한다. 이는 다양한 장르의 7천개가 넘는 미발행 책들을 포함하며, 연속적인 긴 텍스트를 통해 모델이 long term depency를 학습할 수 있다. ELMo에서 사용된 1B Word Benchmark 데이터셋은 문장들이 서로 섞여 있어 long term depency를 학습하기 어렵다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table1.png"
width="1068"
height="192"
srcset="https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table1_hu67f0911c3e9c0a1b268879a509ffb6ec_53434_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="556"
data-flex-basis="1335px"
>&lt;/p>
&lt;h4 id="model-speciﬁcations">Model speciﬁcations&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">layer&lt;/td>
&lt;td style="text-align:center">12-layer decoder-only transformer with masked self-attention heads&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">state dimension&lt;/td>
&lt;td style="text-align:center">decoder: 768, attention heads: 12, position-wise FFN: 3072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">optimizer&lt;/td>
&lt;td style="text-align:center">Adam&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">learning rate&lt;/td>
&lt;td style="text-align:center">max: 2.5e-4, schedule: cosine annealing, warm-up step: 2,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">schedule&lt;/td>
&lt;td style="text-align:center">100 epochs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">64 random sample $\times$ 512 token/sample&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">weight initialization&lt;/td>
&lt;td style="text-align:center">$N(0, 0.02)$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">subword segmentation&lt;/td>
&lt;td style="text-align:center">BPE (40,000 merges)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">regularization&lt;/td>
&lt;td style="text-align:center">L2($w=0.01$)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">activation function&lt;/td>
&lt;td style="text-align:center">Gaussian Error Linear Unit(GELU)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">position embedding&lt;/td>
&lt;td style="text-align:center">learned positoin embeddings&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">pre-processing&lt;/td>
&lt;td style="text-align:center">cleaning: ftfy, tokenizer : spaCy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="fine-tuning-details">Fine-tuning details&lt;/h4>
&lt;p>명시되지 않은 것들은 사전학습에 사용된 hyperparameter를 재사용했다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Hyperparameter&lt;/th>
&lt;th style="text-align:center">Descrption&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">dropout&lt;/td>
&lt;td style="text-align:center">0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Learning rate&lt;/td>
&lt;td style="text-align:center">max: 6.25e-5, warm-up: 0.2% of training&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">batch size&lt;/td>
&lt;td style="text-align:center">32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">epochs&lt;/td>
&lt;td style="text-align:center">3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">auxiliary objective weight($\lambda$)&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="supervised-ﬁne-tuning-1">Supervised ﬁne-tuning&lt;/h3>
&lt;p>자연어 추론, 질문 응답, 의미론적 유사성, 텍스트 분류등의 평가를 진행하였고, 그 중 일부는 GLUE benchmark에 포함되어 있다.&lt;/p>
&lt;h4 id="natural-language-inference">Natural Language Inference&lt;/h4>
&lt;p>자연어 추론(NLI) 작업, 즉 텍스트 함의를 인식하는 것은 문장 쌍을 읽고, 그들 사이의 관계를 함의, 모순 또는 중립 중 하나로 판단하는 것으로, 이미지 캡션(SNLI), 텍스트 변환된 연설, 대중 소설, 정부 보고서(MNLI), 위키백과 기사(QNLI), 과학 시험(SciTail) 또는 뉴스 기사(RTE)를 포함한 다양한 출처의 다섯 개의 데이터셋을 사용해서 평가하였다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table2.png"
width="1084"
height="324"
srcset="https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table2_hu495d483b1c75750171478d8e124f7ea5_71854_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="334"
data-flex-basis="802px"
>&lt;/p>
&lt;p>다섯 가지 데이터셋 중 네 가지에서 좋은 성능을 보여주었으며, MNLI에서 1.5%, SciTail에서 5%, QNLI에서 5.8%, SNLI에서 0.6%의 성능 향상을 보였다. 이는 GPT 모델이 여러 문장을 더 잘 이해하고, 언어적 모호성의 측면을 처리할 수 있다는 것을 보여준다.&lt;/p>
&lt;h4 id="question-answering-and-commonsense-reasoning-1">Question answering and commonsense reasoning&lt;/h4>
&lt;p>질문 응답 작업은 한 문장이나 여러 문장을 이해하는 능력을 평가한다. 중고등학교 시험의 영어 지문과 질문이 포함된 RACE 데이터셋을 사용한 평가에서 좋은 성능을 보여주었다. 또한, 여러 문장의 이야기 중에서 올바른 결말을 고르는 Story Cloze 평가에서도 GPT 모델은 이전 최고 성능을 크게 능가하였다. 이 결과는 GPT 모델이 넓은 범위에 걸친 문맥 정보를 잘 처리할 수 있음을 보여준다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table3.png"
width="1078"
height="312"
srcset="https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table3_huadd22c7f91cb998e4bf1e5863627ba4f_62097_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="345"
data-flex-basis="829px"
>&lt;/p>
&lt;h4 id="semantic-similarity">Semantic Similarity&lt;/h4>
&lt;p>의미론적 유사성(또는 패러프레이즈 감지) 작업은 두 문장이 의미적으로 동일한지 여부를 판단한다. 뉴스 출처에서 수집된 Microsoft Paraphrase(MRPC), Quora Question Pairs(QQP), 그리고 Semantic Textual Similarity benchmark(STS-B) 데이터셋을 사용한다. 이 중 STSB와 QQP에서 좋은 성늘을 보여주었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table4.png"
width="1076"
height="382"
srcset="https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table4_hu960b0c42aa968787120d1f3e94295e46_86789_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="676px"
>&lt;/p>
&lt;h4 id="classiﬁcation">Classiﬁcation&lt;/h4>
&lt;p>텍스트 분류로 사용한 데이터셋은 문법적으로 맞는지를 판단하는 Corpus of Linguistic Acceptability(CoLA)와 단순 이진분류 평가인 Stanford Sentiment Treebank(SST-2)을 사용하였다. CoLA에서 35.0 에서 45.4점으로, SST-2에서 68.9 에서 72.8점으로 상승하였으며, GLUE benchmark에서도 72.8점으로 이전 최고 성능을 크게 능가하였다.&lt;/p>
&lt;p>GPT모델은 평가한 12개의 데이터셋 중 9개에서 state-of-the-art를 달성하였다. 그리고 STS-B(약 5.7k)와 같은 작은 데이터셋부터 가장 큰 SNLI(약 550k)와 같은 크기의 다양한 데이터셋에서 잘 작동함을 보여준다.&lt;/p>
&lt;hr>
&lt;h2 id="analysis">Analysis&lt;/h2>
&lt;h3 id="impact-of-number-of-layers-transferred">Impact of number of layers transferred&lt;/h3>
&lt;p>unsupervised pre-training에서 supervised target task로 transfer하는 layer 개수의 영향을 분석했다. MultiNLI와 RACE에서 성능을 관찰했고 transferring embeddings이 성능을 향상시킨다는 것과 각 transformer layer가 최대 9%까지 성능을 향상시킨다는 결과를 얻었다. 이는 pre-trained model의 각 layer가 target task를 푸는 데 유용한 기능을 포함함을 의미한다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/figure2.png"
width="1070"
height="478"
srcset="https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/figure2_hu21f96b797eb02f25e10b12c7ce8aff79_177494_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="537px"
>&lt;/p>
&lt;h3 id="zero-shot-behaviors">Zero-shot Behaviors&lt;/h3>
&lt;p>Trasformer를 사용한 language model이 pre-training에 효과적인 이유에 대한 가설로, Generative model이 학습하는 target tasks가 language modeling의 성능을 향상에 도움을 준다고 생각했고, 이를 검증하기 위해 pre-training 업데이트 횟수에 따른 target tasks의 성능을 fine-tuning없이 측정하였다.&lt;/p>
&lt;p>실험 결과 pre-training 업데이트 횟수에 따라 안정적 &amp;amp; 지속적으로 관련 taget task의 성능이 증가하는 것을 확인할 수 있었으며 이는 generative pre-training이 관련 task의 학습에 도움을 준다는 것을 의미한다. 반면, LSTM의 경우에는 업데이트 횟수에 따라 일관되게 안정적으로 증가하지 않고 분산을 가지면서 증가하는데, 이는 LSTM 보다 더 구조화된 transformer의 attentional memory가 transfer learning에 도움을 준다는 것을 의미한다.&lt;/p>
&lt;h3 id="ablation-studies">Ablation studies&lt;/h3>
&lt;p>세 가지 ablation study를 통해 다음의 결과를 얻었다. 첫째, 미세조정 시 보조 목적함수의 도움이 큰 데이터셋에서는 두드러지지만 작은 데이터셋에서는 그렇지 않다는 것을 확인하였다. 둘째, LSTM과 Transformer를 비교한 결과, LSTM은 오직 MRPC 데이터셋에서만 Transformer를 능가하는 것을 확인하였다. 마지막으로, 사전학습 없이 지도학습을 진행한 Transformer는 모든 작업에서 성능이 저하되었다.&lt;/p>
&lt;p>&lt;img src="https://kurtkim.github.io/p/gpt-1/images/table5.png"
width="1074"
height="208"
srcset="https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_480x0_resize_box_3.png 480w, https://kurtkim.github.io/p/gpt-1/images/table5_hu95c5714341cf48b980d835242421642d_54388_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="516"
data-flex-basis="1239px"
>&lt;/p>
&lt;hr>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>생성적 사전 학습과 미세조정을 사용한 모델을 통해 강력한 자연어 이해를 구현하였다. GPT 모델은 연속된 텍스트로 이루어진 다양한 말뭉치로 사전학습된 모델은 일반 지식(world knowledge)과 long term depency 처리하는 능력을 가질 수 있었다. 이를 통해, 우리는 지도학습 없이도 특정 작업의 성능을 향상시키는 것이 가능하다는 것을 보여주었으며, 특히 Trasformer 모델과 long term depency가 있는 텍스트 데이터셋이 이 접근법에서 잘 작동함을 확인하였다.&lt;/p>
&lt;hr>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a class="link" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>